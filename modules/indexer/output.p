(lp0
(dp1
S'circuitri'
p2
(dp3
S'1020'
p4
(lp5
sS'1023'
p6
(lp7
sS'1022'
p8
(lp9
sS'1067'
p10
(lp11
sS'1014'
p12
(lp13
sS'1006'
p14
(lp15
sS'1005'
p16
(lp17
I2055
asS'1031'
p18
(lp19
sS'1030'
p20
(lp21
sS'1046'
p22
(lp23
ssS'nordisk'
p24
(dp25
S'1082'
p26
(lp27
I3222
assS'orthogon'
p28
(dp29
S'1050'
p30
(lp31
sS'1041'
p32
(lp33
sS'1061'
p34
(lp35
sS'1060'
p36
(lp37
sS'1072'
p38
(lp39
sS'1066'
p40
(lp41
sS'1'
p42
(lp43
I525
asS'1012'
p44
(lp45
sS'1033'
p46
(lp47
sS'1017'
p48
(lp49
sS'1003'
p50
(lp51
sS'104'
p52
(lp53
ssS'parallelogram'
p54
(dp55
g42
(lp56
I1702
assS'yellow'
p57
(dp58
S'1074'
p59
(lp60
I867
assS'interchang'
p61
(dp62
S'107'
p63
(lp64
I1104
asg26
(lp65
ssS'four'
p66
(dp67
S'1062'
p68
(lp69
sS'1083'
p70
(lp71
sS'1065'
p72
(lp73
sS'1052'
p74
(lp75
sS'1056'
p76
(lp77
sS'1077'
p78
(lp79
sS'1078'
p80
(lp81
sg38
(lp82
sS'1073'
p83
(lp84
sS'1070'
p85
(lp86
sS'1039'
p87
(lp88
sS'1018'
p89
(lp90
sS'1019'
p91
(lp92
sg12
(lp93
sS'1015'
p94
(lp95
sS'1016'
p96
(lp97
sg48
(lp98
sS'1010'
p99
(lp100
sg44
(lp101
sS'102'
p102
(lp103
sS'103'
p104
(lp105
sS'100'
p106
(lp107
I1084
asS'101'
p108
(lp109
sS'106'
p110
(lp111
sg20
(lp112
sg52
(lp113
sS'105'
p114
(lp115
sS'1069'
p116
(lp117
sS'1068'
p118
(lp119
sg18
(lp120
sS'1021'
p121
(lp122
sg4
(lp123
sS'1048'
p124
(lp125
sS'1029'
p126
(lp127
sS'1009'
p128
(lp129
sS'1008'
p130
(lp131
sS'1007'
p132
(lp133
sg14
(lp134
sS'1004'
p135
(lp136
sg50
(lp137
sS'1002'
p138
(lp139
sS'1001'
p140
(lp141
ssS'prefix'
p142
(dp143
g94
(lp144
I686
asS'1054'
p145
(lp146
ssS'burkhalt'
p147
(dp148
S'1013'
p149
(lp150
I262
assS'trotman'
p151
(dp152
g14
(lp153
sg16
(lp154
I47
assS'xtr'
p155
(dp156
g130
(lp157
I513
assS'iperhap'
p158
(dp159
g85
(lp160
I479
assS'multicoordin'
p161
(dp162
S'1080'
p163
(lp164
I262
assS'kullbackleibl'
p165
(dp166
g36
(lp167
I69
assS'dela'
p168
(dp169
g4
(lp170
I1553
assS'digit'
p171
(dp172
g30
(lp173
sS'1024'
p174
(lp175
sS'1053'
p176
(lp177
sS'1047'
p178
(lp179
sg22
(lp180
sS'1045'
p181
(lp182
sS'1059'
p183
(lp184
sg10
(lp185
sg63
(lp186
sg20
(lp187
sg52
(lp188
sg78
(lp189
sg14
(lp190
sg16
(lp191
sg135
(lp192
sg50
(lp193
sg138
(lp194
I196
asg44
(lp195
sg114
(lp196
ssS'foldiak'
p197
(dp198
g12
(lp199
sg50
(lp200
I296
assS'hough'
p201
(dp202
g138
(lp203
I1472
assS'figw'
p204
(dp205
g110
(lp206
I1670
assS'lumin'
p207
(dp208
g42
(lp209
I2371
asg118
(lp210
ssS'delv'
p211
(dp212
g124
(lp213
I2978
assS'fur'
p214
(dp215
S'1025'
p216
(lp217
sg30
(lp218
sg116
(lp219
sg130
(lp220
I16
asS'1036'
p221
(lp222
sS'1034'
p223
(lp224
ssS'disturb'
p225
(dp226
g132
(lp227
I2026
asg94
(lp228
sg4
(lp229
sS'1043'
p230
(lp231
sg8
(lp232
ssS'thorbergsson'
p233
(dp234
S'1044'
p235
(lp236
I3142
assS'haber'
p237
(dp238
g176
(lp239
I255
assS'oooo'
p240
(dp241
g78
(lp242
I1974
assS'steinbach'
p243
(dp244
S'1032'
p245
(lp246
I375
assS'prefilt'
p247
(dp248
g183
(lp249
sg22
(lp250
I314
assS'undersampl'
p251
(dp252
g178
(lp253
I1795
assS'photomet'
p254
(dp255
S'1055'
p256
(lp257
I1962
assS'miller'
p258
(dp259
g74
(lp260
sg48
(lp261
sS'1057'
p262
(lp263
sg91
(lp264
sg128
(lp265
sg106
(lp266
I149
asg18
(lp267
sg149
(lp268
ssS'fecx'
p269
(dp270
g140
(lp271
I1025
assS'second'
p272
(dp273
g124
(lp274
sg70
(lp275
sg78
(lp276
sS'1081'
p277
(lp278
sg163
(lp279
sg116
(lp280
sS'1064'
p281
(lp282
sS'1084'
p283
(lp284
sg26
(lp285
sg30
(lp286
sS'1051'
p287
(lp288
sg74
(lp289
sg176
(lp290
sg145
(lp291
sg76
(lp292
sS'1079'
p293
(lp294
sS'1058'
p295
(lp296
sg183
(lp297
sg59
(lp298
sg80
(lp299
sg38
(lp300
sg83
(lp301
sg85
(lp302
sS'1071'
p303
(lp304
sg42
(lp305
I804
asS'1038'
p306
(lp307
sg91
(lp308
sg12
(lp309
sg94
(lp310
sg96
(lp311
sg99
(lp312
sS'1011'
p313
(lp314
sg68
(lp315
sg149
(lp316
sg230
(lp317
sS'1040'
p318
(lp319
sg46
(lp320
sg102
(lp321
sg106
(lp322
sg108
(lp323
sg110
(lp324
sg20
(lp325
sg52
(lp326
sg114
(lp327
sg216
(lp328
sS'1042'
p329
(lp330
sg32
(lp331
sS'1026'
p332
(lp333
sg181
(lp334
sg6
(lp335
sg8
(lp336
sg34
(lp337
sg221
(lp338
sg235
(lp339
sg72
(lp340
sS'1028'
p341
(lp342
sg10
(lp343
sS'1076'
p344
(lp345
sg128
(lp346
sg130
(lp347
sg132
(lp348
sg14
(lp349
sS'1035'
p350
(lp351
sg138
(lp352
sg140
(lp353
sS'1000'
p354
(lp355
ssS'gorman'
p356
(dp357
g114
(lp358
I291
assS'higherord'
p359
(dp360
g344
(lp361
sg283
(lp362
sg149
(lp363
I2684
assS'eson'
p364
(dp365
g295
(lp366
I1624
asg183
(lp367
ssS'semicircular'
p368
(dp369
g350
(lp370
I139
assS'connectionilt'
p371
(dp372
g83
(lp373
I2949
assS'schnurrenberg'
p374
(dp375
g14
(lp376
sg16
(lp377
I1070
assS'cull'
p378
(dp379
g181
(lp380
I1276
assS'ongo'
p381
(dp382
g132
(lp383
I1845
asS'1049'
p384
(lp385
ssS'rltier'
p386
(dp387
g59
(lp388
I1213
assS'here'
p389
(dp390
g329
(lp391
sg70
(lp392
sg26
(lp393
sg277
(lp394
sg163
(lp395
sg72
(lp396
sg281
(lp397
sg283
(lp398
sg303
(lp399
sg30
(lp400
sg287
(lp401
sg74
(lp402
sg176
(lp403
sg145
(lp404
sg256
(lp405
sg76
(lp406
sg262
(lp407
sg295
(lp408
sg183
(lp409
sg38
(lp410
sg83
(lp411
sg85
(lp412
sg124
(lp413
sg42
(lp414
I104
asg306
(lp415
sg89
(lp416
sg91
(lp417
sg12
(lp418
sg20
(lp419
sg18
(lp420
sg99
(lp421
sg313
(lp422
sg44
(lp423
sg350
(lp424
sg118
(lp425
sg174
(lp426
sg32
(lp427
sg245
(lp428
sS'108'
p429
(lp430
sg68
(lp431
sg102
(lp432
sg104
(lp433
sg108
(lp434
sg110
(lp435
sg178
(lp436
sg230
(lp437
sS'10'
p438
(lp439
sS'1027'
p440
(lp441
sg332
(lp442
sg121
(lp443
sg80
(lp444
sg6
(lp445
sg8
(lp446
sg34
(lp447
sg221
(lp448
sg384
(lp449
sg235
(lp450
sg126
(lp451
sg341
(lp452
sg40
(lp453
sg344
(lp454
sg223
(lp455
sg128
(lp456
sg36
(lp457
sg14
(lp458
sg16
(lp459
sS'1063'
p460
(lp461
sg354
(lp462
ssS'herv'
p463
(dp464
g440
(lp465
I19
assS'mccallum'
p466
(dp467
g293
(lp468
I2027
assS'finkel'
p469
(dp470
g176
(lp471
I2630
assS'deterior'
p472
(dp473
g78
(lp474
sg145
(lp475
sg114
(lp476
I1730
assS'k'
p477
(dp478
g80
(lp479
sg293
(lp480
sg344
(lp481
sg78
(lp482
sg59
(lp483
sS'1075'
p484
(lp485
sg38
(lp486
sg83
(lp487
sg303
(lp488
sg438
(lp489
I323
asg116
(lp490
sg36
(lp491
sg460
(lp492
sg68
(lp493
sg72
(lp494
sg281
(lp495
sg10
(lp496
sg40
(lp497
sg283
(lp498
sg70
(lp499
sg26
(lp500
sg277
(lp501
sg163
(lp502
sg91
(lp503
sg94
(lp504
sg96
(lp505
sg48
(lp506
sg99
(lp507
sg313
(lp508
sg44
(lp509
sg149
(lp510
sg429
(lp511
sg102
(lp512
sg104
(lp513
sg106
(lp514
sg108
(lp515
sg63
(lp516
sg52
(lp517
sg114
(lp518
sg128
(lp519
sg130
(lp520
sg14
(lp521
sg16
(lp522
sg135
(lp523
sg50
(lp524
sg138
(lp525
sg140
(lp526
sg354
(lp527
sg306
(lp528
sg87
(lp529
sg245
(lp530
sg46
(lp531
sg20
(lp532
sg18
(lp533
sg221
(lp534
sS'1037'
p535
(lp536
sg223
(lp537
sg350
(lp538
sg174
(lp539
sg440
(lp540
sg332
(lp541
sg121
(lp542
sg4
(lp543
sg8
(lp544
sg126
(lp545
sg341
(lp546
sg30
(lp547
sg287
(lp548
sg74
(lp549
sg176
(lp550
sg145
(lp551
sg256
(lp552
sg76
(lp553
sg262
(lp554
sg295
(lp555
sg183
(lp556
sg230
(lp557
sg329
(lp558
sg32
(lp559
sg318
(lp560
sg178
(lp561
sg22
(lp562
sg181
(lp563
sg235
(lp564
sg384
(lp565
sg124
(lp566
ssS'controversi'
p567
(dp568
g99
(lp569
I2883
assS'attractor'
p570
(dp571
g46
(lp572
I2927
asg384
(lp573
sg535
(lp574
sg262
(lp575
ssS'marino'
p576
(dp577
g484
(lp578
I2572
assS'brought'
p579
(dp580
g34
(lp581
sg94
(lp582
sg74
(lp583
sg76
(lp584
sg149
(lp585
I223
assS'marina'
p586
(dp587
g460
(lp588
I9
assS'topographi'
p589
(dp590
g149
(lp591
I2813
assS'ction'
p592
(dp593
g174
(lp594
sg4
(lp595
I1564
assS'orlitski'
p596
(dp597
g287
(lp598
I3646
asg145
(lp599
ssS'lma'
p600
(dp601
g283
(lp602
I1610
assS'expci'
p603
(dp604
g32
(lp605
I1219
assS'univ'
p606
(dp607
g118
(lp608
sg283
(lp609
sg178
(lp610
sg4
(lp611
sg36
(lp612
sg59
(lp613
sg341
(lp614
sg89
(lp615
sg102
(lp616
sg106
(lp617
I2927
asg99
(lp618
sg20
(lp619
sg354
(lp620
ssS'unit'
p621
(dp622
g329
(lp623
sg70
(lp624
sg78
(lp625
sg277
(lp626
sg163
(lp627
sg116
(lp628
sg283
(lp629
sg85
(lp630
sg303
(lp631
sg26
(lp632
sg30
(lp633
sg176
(lp634
sg80
(lp635
sg76
(lp636
sg262
(lp637
sg295
(lp638
sg183
(lp639
sg484
(lp640
sg38
(lp641
sg83
(lp642
sg114
(lp643
sg63
(lp644
sg42
(lp645
I596
asg89
(lp646
sg91
(lp647
sg245
(lp648
sg94
(lp649
sg96
(lp650
sg18
(lp651
sg221
(lp652
sg535
(lp653
sg223
(lp654
sg350
(lp655
sg230
(lp656
sg174
(lp657
sg293
(lp658
sg32
(lp659
sg429
(lp660
sg68
(lp661
sg102
(lp662
sg104
(lp663
sg108
(lp664
sg110
(lp665
sg178
(lp666
sg52
(lp667
sg22
(lp668
sg216
(lp669
sg438
(lp670
sg440
(lp671
sg318
(lp672
sg121
(lp673
sg4
(lp674
sg36
(lp675
sg124
(lp676
sg126
(lp677
sg281
(lp678
sg10
(lp679
sg40
(lp680
sg344
(lp681
sg130
(lp682
sg132
(lp683
sg14
(lp684
sg16
(lp685
sg138
(lp686
sg140
(lp687
sg354
(lp688
ssS'ortut'
p689
(dp690
g332
(lp691
I874
assS'dnf'
p692
(dp693
g110
(lp694
sg145
(lp695
I240
assS'dna'
p696
(dp697
g344
(lp698
I2773
assS'spoke'
p699
(dp700
g87
(lp701
I1902
assS'dnm'
p702
(dp703
g295
(lp704
I2457
asg183
(lp705
ssS'dnj'
p706
(dp707
g12
(lp708
I942
assS'music'
p709
(dp710
g174
(lp711
I1694
assS'unexploit'
p712
(dp713
g429
(lp714
I2029
assS'strike'
p715
(dp716
g118
(lp717
sg4
(lp718
sg181
(lp719
sg329
(lp720
sg183
(lp721
sg94
(lp722
I3123
assS'until'
p723
(dp724
g26
(lp725
sg277
(lp726
sg30
(lp727
sg256
(lp728
sg76
(lp729
sg262
(lp730
sg295
(lp731
sg183
(lp732
sg83
(lp733
sg42
(lp734
I1473
asg87
(lp735
sg89
(lp736
sg91
(lp737
sg20
(lp738
sg18
(lp739
sg313
(lp740
sg44
(lp741
sg32
(lp742
sg110
(lp743
sg114
(lp744
sg216
(lp745
sg440
(lp746
sg178
(lp747
sg34
(lp748
sg36
(lp749
sg460
(lp750
sg126
(lp751
sg40
(lp752
sg344
(lp753
sg14
(lp754
sg354
(lp755
ssS'relax'
p756
(dp757
g438
(lp758
I577
asg163
(lp759
sg178
(lp760
sg8
(lp761
sg384
(lp762
sg68
(lp763
sg429
(lp764
sg50
(lp765
sg535
(lp766
ssS'relat'
p767
(dp768
g124
(lp769
sg70
(lp770
sg26
(lp771
sg277
(lp772
sg163
(lp773
sg68
(lp774
sg181
(lp775
sg303
(lp776
sg287
(lp777
sg74
(lp778
sg176
(lp779
sg80
(lp780
sg76
(lp781
sg262
(lp782
sg344
(lp783
sg78
(lp784
sg59
(lp785
sg484
(lp786
sg38
(lp787
sg85
(lp788
sg63
(lp789
sg42
(lp790
I258
asg306
(lp791
sg89
(lp792
sg91
(lp793
sg245
(lp794
sg94
(lp795
sg96
(lp796
sg48
(lp797
sg221
(lp798
sg313
(lp799
sg44
(lp800
sg149
(lp801
sg118
(lp802
sg116
(lp803
sg174
(lp804
sg293
(lp805
sg32
(lp806
sg350
(lp807
sg429
(lp808
sg318
(lp809
sg46
(lp810
sg102
(lp811
sg104
(lp812
sg106
(lp813
sg108
(lp814
sg110
(lp815
sg20
(lp816
sg52
(lp817
sg22
(lp818
sg230
(lp819
sg438
(lp820
sg440
(lp821
sg332
(lp822
sg121
(lp823
sg4
(lp824
sg6
(lp825
sg8
(lp826
sg36
(lp827
sg460
(lp828
sg235
(lp829
sg72
(lp830
sg341
(lp831
sg10
(lp832
sg40
(lp833
sg223
(lp834
sg128
(lp835
sg132
(lp836
sg14
(lp837
sg16
(lp838
sg135
(lp839
sg138
(lp840
sg140
(lp841
sg354
(lp842
ssS'notic'
p843
(dp844
g484
(lp845
sg32
(lp846
sg68
(lp847
sg178
(lp848
sg26
(lp849
sg80
(lp850
sg295
(lp851
sg183
(lp852
sg145
(lp853
sg85
(lp854
sg40
(lp855
sg42
(lp856
I2641
asg12
(lp857
sg223
(lp858
sg44
(lp859
sg78
(lp860
sg132
(lp861
sg46
(lp862
sg303
(lp863
sg138
(lp864
sg140
(lp865
ssS'nowlan'
p866
(dp867
g329
(lp868
sg59
(lp869
sg295
(lp870
sg183
(lp871
sg460
(lp872
sg313
(lp873
sg91
(lp874
sg221
(lp875
sg138
(lp876
I3432
assS'glass'
p877
(dp878
g384
(lp879
sg106
(lp880
I1057
asg440
(lp881
sg26
(lp882
ssS'holm'
p883
(dp884
g14
(lp885
I2680
assS'exco'
p886
(dp887
g116
(lp888
I1720
assS'hole'
p889
(dp890
g14
(lp891
I3056
asg78
(lp892
ssS'hold'
p893
(dp894
g163
(lp895
sg40
(lp896
sg287
(lp897
sg74
(lp898
sg145
(lp899
sg183
(lp900
sg85
(lp901
sg306
(lp902
sg245
(lp903
sg46
(lp904
sg313
(lp905
sg223
(lp906
sg102
(lp907
sg110
(lp908
sg63
(lp909
sg22
(lp910
sg235
(lp911
sg34
(lp912
sg460
(lp913
sg341
(lp914
sg10
(lp915
sg535
(lp916
sg128
(lp917
sg130
(lp918
sg14
(lp919
sg135
(lp920
sg138
(lp921
sg140
(lp922
I1237
assS'jmf'
p923
(dp924
g429
(lp925
I1513
assS'generalis'
p926
(dp927
g36
(lp928
sg384
(lp929
sg135
(lp930
I1928
asg52
(lp931
ssS'fanin'
p932
(dp933
g135
(lp934
I1327
assS'conceptu'
p935
(dp936
g74
(lp937
sg68
(lp938
sg8
(lp939
sg78
(lp940
sg262
(lp941
sg46
(lp942
sg293
(lp943
sg354
(lp944
I1022
assS'ofth'
p945
(dp946
g118
(lp947
sg178
(lp948
sg22
(lp949
sg277
(lp950
sg163
(lp951
sg99
(lp952
sg59
(lp953
sg68
(lp954
sg126
(lp955
sg10
(lp956
sg40
(lp957
sg85
(lp958
sg36
(lp959
sg303
(lp960
sg354
(lp961
I2806
asg221
(lp962
sg535
(lp963
sg350
(lp964
ssS'photomicrograph'
p965
(dp966
g20
(lp967
sg135
(lp968
I622
assS'isidori'
p969
(dp970
g230
(lp971
I3298
assS'obliqu'
p972
(dp973
g318
(lp974
I2107
asg181
(lp975
ssS'ljii'
p976
(dp977
g283
(lp978
I475
assS'wang'
p979
(dp980
g42
(lp981
I3417
asg36
(lp982
sg332
(lp983
sg63
(lp984
ssS'wand'
p985
(dp986
g329
(lp987
sg287
(lp988
sg36
(lp989
sg126
(lp990
I1037
asg40
(lp991
sg52
(lp992
sg350
(lp993
ssS'giretin'
p994
(dp995
g245
(lp996
I2071
assS'hfsp'
p997
(dp998
g216
(lp999
I2184
assS'computerbas'
p1000
(dp1001
g91
(lp1002
I3044
assS'bizzi'
p1003
(dp1004
g99
(lp1005
I3231
assS'malign'
p1006
(dp1007
g484
(lp1008
sg91
(lp1009
I1785
assS'universitiit'
p1010
(dp1011
g221
(lp1012
sg313
(lp1013
I2317
assS'caution'
p1014
(dp1015
g12
(lp1016
I2350
assS'multiresolut'
p1017
(dp1018
g174
(lp1019
I2659
asg22
(lp1020
ssS'want'
p1021
(dp1022
g440
(lp1023
sg178
(lp1024
sg183
(lp1025
sg59
(lp1026
sg126
(lp1027
sg83
(lp1028
sg138
(lp1029
sg102
(lp1030
sg306
(lp1031
sg130
(lp1032
sg12
(lp1033
sg145
(lp1034
sg48
(lp1035
sg313
(lp1036
sg140
(lp1037
I1060
assS'nelwa'
p1038
(dp1039
g135
(lp1040
I1218
assS'dyke'
p1041
(dp1042
g283
(lp1043
I1883
assS'resfect'
p1044
(dp1045
g384
(lp1046
I1050
assS'hof'
p1047
(dp1048
g32
(lp1049
I1648
assS'hoc'
p1050
(dp1051
g306
(lp1052
I375
asg70
(lp1053
sg74
(lp1054
ssS'softki'
p1055
(dp1056
g70
(lp1057
sg262
(lp1058
I542
assS'ilig'
p1059
(dp1060
g6
(lp1061
I1737
assS'cylind'
p1062
(dp1063
g42
(lp1064
I2296
asg181
(lp1065
ssS'travel'
p1066
(dp1067
g94
(lp1068
I429
asg18
(lp1069
sg176
(lp1070
sg83
(lp1071
ssS'seemor'
p1072
(dp1073
g181
(lp1074
I1
assS'bimod'
p1075
(dp1076
g216
(lp1077
I1672
asg329
(lp1078
sg318
(lp1079
ssS'copious'
p1080
(dp1081
g70
(lp1082
I81
assS'classifi'
p1083
(dp1084
g283
(lp1085
sg30
(lp1086
sg287
(lp1087
sg76
(lp1088
sg295
(lp1089
sg183
(lp1090
sg59
(lp1091
sg484
(lp1092
sg42
(lp1093
I3426
asg87
(lp1094
sg91
(lp1095
sg94
(lp1096
sg221
(lp1097
sg223
(lp1098
sg178
(lp1099
sg110
(lp1100
sg63
(lp1101
sg114
(lp1102
sg230
(lp1103
sg440
(lp1104
sg318
(lp1105
sg121
(lp1106
sg181
(lp1107
sg36
(lp1108
sg281
(lp1109
sg344
(lp1110
sg44
(lp1111
sg78
(lp1112
sg135
(lp1113
sg50
(lp1114
sg138
(lp1115
ssS'how'
p1116
(dp1117
g68
(lp1118
sg70
(lp1119
sg78
(lp1120
sg277
(lp1121
sg281
(lp1122
sg26
(lp1123
sg30
(lp1124
sg74
(lp1125
sg176
(lp1126
sg145
(lp1127
sg80
(lp1128
sg76
(lp1129
sg262
(lp1130
sg295
(lp1131
sg183
(lp1132
sg59
(lp1133
sg484
(lp1134
sg83
(lp1135
sg85
(lp1136
sg42
(lp1137
I210
asg91
(lp1138
sg12
(lp1139
sg94
(lp1140
sg18
(lp1141
sg99
(lp1142
sg313
(lp1143
sg44
(lp1144
sg149
(lp1145
sg116
(lp1146
sg118
(lp1147
sg32
(lp1148
sg245
(lp1149
sg318
(lp1150
sg46
(lp1151
sg102
(lp1152
sg108
(lp1153
sg110
(lp1154
sg52
(lp1155
sg216
(lp1156
sg329
(lp1157
sg440
(lp1158
sg332
(lp1159
sg4
(lp1160
sg6
(lp1161
sg235
(lp1162
sg34
(lp1163
sg36
(lp1164
sg460
(lp1165
sg124
(lp1166
sg72
(lp1167
sg341
(lp1168
sg10
(lp1169
sg40
(lp1170
sg344
(lp1171
sg128
(lp1172
sg130
(lp1173
sg132
(lp1174
sg350
(lp1175
sg50
(lp1176
sg138
(lp1177
sg140
(lp1178
sg354
(lp1179
ssS'hot'
p1180
(dp1181
g128
(lp1182
I1652
assS'playback'
p1183
(dp1184
g116
(lp1185
I531
assS'ihixi'
p1186
(dp1187
g313
(lp1188
I1393
assS'gaussianfilt'
p1189
(dp1190
g22
(lp1191
I2016
assS'tatsuo'
p1192
(dp1193
g20
(lp1194
I16
assS'turk'
p1195
(dp1196
g42
(lp1197
I3521
assS'perspect'
p1198
(dp1199
g216
(lp1200
sg287
(lp1201
sg440
(lp1202
sg121
(lp1203
sg181
(lp1204
sg293
(lp1205
sg68
(lp1206
sg72
(lp1207
sg132
(lp1208
I3405
assS'mueller'
p1209
(dp1210
g245
(lp1211
I35
assS'diagram'
p1212
(dp1213
g18
(lp1214
sg22
(lp1215
sg4
(lp1216
sg178
(lp1217
sg245
(lp1218
sg20
(lp1219
sg102
(lp1220
sg14
(lp1221
sg16
(lp1222
sg135
(lp1223
I760
asg96
(lp1224
sg350
(lp1225
ssS'wrong'
p1226
(dp1227
g89
(lp1228
sg99
(lp1229
I2490
asg63
(lp1230
sg223
(lp1231
ssS'miix'
p1232
(dp1233
g440
(lp1234
I290
assS'beauti'
p1235
(dp1236
g140
(lp1237
I364
assS'sankt'
p1238
(dp1239
g354
(lp1240
I21
assS'bracher'
p1241
(dp1242
g149
(lp1243
I3044
assS'vicin'
p1244
(dp1245
g42
(lp1246
I1743
asg230
(lp1247
sg44
(lp1248
sg149
(lp1249
ssS'spehar'
p1250
(dp1251
g118
(lp1252
I476
assS'jiii'
p1253
(dp1254
g14
(lp1255
I4311
assS'revolv'
p1256
(dp1257
g484
(lp1258
I837
assS'turb'
p1259
(dp1260
g74
(lp1261
I2776
assS'dispar'
p1262
(dp1263
g118
(lp1264
sg128
(lp1265
I2723
assS'pfbaldi'
p1266
(dp1267
g68
(lp1268
I20
assS'menlo'
p1269
(dp1270
g87
(lp1271
sg44
(lp1272
I39
assS'wing'
p1273
(dp1274
g429
(lp1275
I776
assS'squint'
p1276
(dp1277
g149
(lp1278
I225
assS'interneuron'
p1279
(dp1280
g438
(lp1281
I1129
asg38
(lp1282
ssS'feedback'
p1283
(dp1284
g70
(lp1285
sg116
(lp1286
sg256
(lp1287
sg76
(lp1288
sg12
(lp1289
sg20
(lp1290
sg99
(lp1291
sg350
(lp1292
sg230
(lp1293
sg174
(lp1294
sg245
(lp1295
sg102
(lp1296
sg104
(lp1297
sg110
(lp1298
sg216
(lp1299
sg438
(lp1300
I1650
asg332
(lp1301
sg178
(lp1302
sg128
(lp1303
sg14
(lp1304
sg16
(lp1305
sg135
(lp1306
ssS'neuroimag'
p1307
(dp1308
g4
(lp1309
I1424
assS'telecom'
p1310
(dp1311
g52
(lp1312
I2635
assS'ilum'
p1313
(dp1314
g277
(lp1315
I1996
assS'montaci'
p1316
(dp1317
g96
(lp1318
I377
assS'vara'
p1319
(dp1320
g318
(lp1321
I635
assS'appar'
p1322
(dp1323
g216
(lp1324
sg283
(lp1325
sg4
(lp1326
sg181
(lp1327
sg8
(lp1328
sg85
(lp1329
sg12
(lp1330
sg132
(lp1331
I2715
asg48
(lp1332
sg110
(lp1333
sg52
(lp1334
sg245
(lp1335
ssS'wherein'
p1336
(dp1337
g22
(lp1338
I2131
assS'vari'
p1339
(dp1340
g26
(lp1341
sg163
(lp1342
sg83
(lp1343
sg30
(lp1344
sg74
(lp1345
sg256
(lp1346
sg262
(lp1347
sg183
(lp1348
sg149
(lp1349
sg303
(lp1350
sg87
(lp1351
sg91
(lp1352
sg245
(lp1353
sg46
(lp1354
sg48
(lp1355
sg350
(lp1356
sg329
(lp1357
sg63
(lp1358
sg114
(lp1359
sg438
(lp1360
I810
asg318
(lp1361
sg121
(lp1362
sg181
(lp1363
sg68
(lp1364
sg126
(lp1365
sg128
(lp1366
sg135
(lp1367
sg138
(lp1368
ssS'fir'
p1369
(dp1370
g121
(lp1371
sg22
(lp1372
sg128
(lp1373
I1578
assS'overemphas'
p1374
(dp1375
g34
(lp1376
I2675
assS'fit'
p1377
(dp1378
g68
(lp1379
sg70
(lp1380
sg277
(lp1381
sg30
(lp1382
sg287
(lp1383
sg74
(lp1384
sg295
(lp1385
sg183
(lp1386
sg85
(lp1387
sg89
(lp1388
sg91
(lp1389
sg12
(lp1390
sg46
(lp1391
sg96
(lp1392
sg114
(lp1393
sg221
(lp1394
sg313
(lp1395
sg223
(lp1396
sg350
(lp1397
sg245
(lp1398
sg63
(lp1399
sg4
(lp1400
sg230
(lp1401
sg22
(lp1402
sg8
(lp1403
sg36
(lp1404
sg235
(lp1405
sg72
(lp1406
sg281
(lp1407
sg128
(lp1408
sg132
(lp1409
sg14
(lp1410
sg16
(lp1411
sg138
(lp1412
I59
assS'boundifforal'
p1413
(dp1414
g85
(lp1415
I1960
assS'fix'
p1416
(dp1417
g68
(lp1418
sg70
(lp1419
sg163
(lp1420
sg30
(lp1421
sg287
(lp1422
sg74
(lp1423
sg176
(lp1424
sg145
(lp1425
sg76
(lp1426
sg293
(lp1427
sg295
(lp1428
sg183
(lp1429
sg38
(lp1430
sg85
(lp1431
sg306
(lp1432
sg89
(lp1433
sg91
(lp1434
sg94
(lp1435
sg20
(lp1436
sg18
(lp1437
sg535
(lp1438
sg350
(lp1439
sg174
(lp1440
sg429
(lp1441
sg102
(lp1442
sg104
(lp1443
sg108
(lp1444
sg52
(lp1445
sg22
(lp1446
sg438
(lp1447
I771
asg32
(lp1448
sg121
(lp1449
sg4
(lp1450
sg235
(lp1451
sg34
(lp1452
sg460
(lp1453
sg124
(lp1454
sg72
(lp1455
sg281
(lp1456
sg10
(lp1457
sg40
(lp1458
sg344
(lp1459
sg128
(lp1460
sg130
(lp1461
sg14
(lp1462
sg138
(lp1463
ssS'connel'
p1464
(dp1465
g89
(lp1466
I258
assS'fig'
p1467
(dp1468
g70
(lp1469
sg26
(lp1470
sg72
(lp1471
sg262
(lp1472
sg59
(lp1473
sg484
(lp1474
sg38
(lp1475
sg42
(lp1476
I1668
asg20
(lp1477
sg48
(lp1478
sg99
(lp1479
sg223
(lp1480
sg350
(lp1481
sg118
(lp1482
sg102
(lp1483
sg106
(lp1484
sg108
(lp1485
sg174
(lp1486
sg178
(lp1487
sg6
(lp1488
sg181
(lp1489
sg8
(lp1490
sg36
(lp1491
sg460
(lp1492
sg68
(lp1493
sg126
(lp1494
sg130
(lp1495
sg132
(lp1496
ssS'auditori'
p1497
(dp1498
g116
(lp1499
sg174
(lp1500
I32
asg440
(lp1501
sg332
(lp1502
sg22
(lp1503
sg303
(lp1504
ssS'mogiln'
p1505
(dp1506
g176
(lp1507
I272
assS'bengio'
p1508
(dp1509
g440
(lp1510
I797
asg460
(lp1511
ssS'hidden'
p1512
(dp1513
g68
(lp1514
sg70
(lp1515
sg26
(lp1516
sg277
(lp1517
sg163
(lp1518
sg72
(lp1519
sg283
(lp1520
sg30
(lp1521
sg74
(lp1522
sg76
(lp1523
sg293
(lp1524
sg295
(lp1525
sg183
(lp1526
sg484
(lp1527
sg38
(lp1528
sg83
(lp1529
sg85
(lp1530
sg87
(lp1531
sg89
(lp1532
sg94
(lp1533
sg96
(lp1534
sg178
(lp1535
sg108
(lp1536
sg110
(lp1537
sg63
(lp1538
sg114
(lp1539
sg174
(lp1540
sg440
(lp1541
sg121
(lp1542
sg34
(lp1543
sg36
(lp1544
sg460
(lp1545
sg124
(lp1546
sg126
(lp1547
sg341
(lp1548
sg10
(lp1549
sg344
(lp1550
sg128
(lp1551
sg78
(lp1552
sg132
(lp1553
sg14
(lp1554
sg16
(lp1555
sg135
(lp1556
sg138
(lp1557
sg140
(lp1558
sg354
(lp1559
I1202
assS'fii'
p1560
(dp1561
g174
(lp1562
I2021
asg10
(lp1563
ssS'fin'
p1564
(dp1565
g281
(lp1566
sg350
(lp1567
I1929
assS'easier'
p1568
(dp1569
g332
(lp1570
sg277
(lp1571
sg68
(lp1572
sg10
(lp1573
sg283
(lp1574
sg140
(lp1575
I2785
asg94
(lp1576
sg135
(lp1577
sg63
(lp1578
sg223
(lp1579
ssS'fil'
p1580
(dp1581
g68
(lp1582
I2791
assS'fim'
p1583
(dp1584
g83
(lp1585
I2140
assS'ttepeat'
p1586
(dp1587
g72
(lp1588
I2798
assS'zucker'
p1589
(dp1590
g429
(lp1591
I352
asg70
(lp1592
ssS'neighborclassifi'
p1593
(dp1594
g281
(lp1595
I1489
assS'proce'
p1596
(dp1597
g318
(lp1598
I2830
assS'enrich'
p1599
(dp1600
g74
(lp1601
I2949
assS'slate'
p1602
(dp1603
g104
(lp1604
I2874
assS'losleben'
p1605
(dp1606
g63
(lp1607
I3138
assS'rectangl'
p1608
(dp1609
g12
(lp1610
I1343
asg429
(lp1611
sg484
(lp1612
sg63
(lp1613
ssS'surgic'
p1614
(dp1615
g149
(lp1616
I2177
assS'interrupt'
p1617
(dp1618
g116
(lp1619
sg94
(lp1620
I691
asg332
(lp1621
sg6
(lp1622
sg78
(lp1623
ssS'xe'
p1624
(dp1625
g145
(lp1626
sg535
(lp1627
I1078
assS'subdivis'
p1628
(dp1629
g303
(lp1630
I2783
assS'zipser'
p1631
(dp1632
g178
(lp1633
sg4
(lp1634
I562
assS'demongeot'
p1635
(dp1636
g174
(lp1637
I2402
assS'benjamincum'
p1638
(dp1639
g163
(lp1640
I2191
assS'accommod'
p1641
(dp1642
g132
(lp1643
sg74
(lp1644
sg135
(lp1645
I1349
asg22
(lp1646
ssS'rehder'
p1647
(dp1648
g110
(lp1649
I3060
assS'arrow'
p1650
(dp1651
g216
(lp1652
sg48
(lp1653
I1636
asg350
(lp1654
ssS'debug'
p1655
(dp1656
g10
(lp1657
I1427
assS'klinger'
p1658
(dp1659
g341
(lp1660
I2730
assS'mdl'
p1661
(dp1662
g72
(lp1663
I349
asg85
(lp1664
ssS'laboratori'
p1665
(dp1666
g183
(lp1667
sg59
(lp1668
sg38
(lp1669
sg149
(lp1670
sg42
(lp1671
I2281
asg306
(lp1672
sg91
(lp1673
sg12
(lp1674
sg20
(lp1675
sg48
(lp1676
sg313
(lp1677
sg350
(lp1678
sg18
(lp1679
sg106
(lp1680
sg63
(lp1681
sg114
(lp1682
sg230
(lp1683
sg318
(lp1684
sg178
(lp1685
sg4
(lp1686
sg384
(lp1687
sg68
(lp1688
sg281
(lp1689
sg78
(lp1690
sg14
(lp1691
sg16
(lp1692
sg135
(lp1693
sg50
(lp1694
sg138
(lp1695
ssS'iif'
p1696
(dp1697
g70
(lp1698
sg114
(lp1699
I2133
assS'habetl'
p1700
(dp1701
g78
(lp1702
I855
assS'allan'
p1703
(dp1704
g138
(lp1705
I3237
assS'lxt'
p1706
(dp1707
g76
(lp1708
I2307
assS'efron'
p1709
(dp1710
g484
(lp1711
I512
assS'mds'
p1712
(dp1713
g74
(lp1714
sg130
(lp1715
I366
assS'whit'
p1716
(dp1717
g48
(lp1718
I670
assS'commiss'
p1719
(dp1720
g440
(lp1721
I2496
assS'xp'
p1722
(dp1723
g80
(lp1724
I289
assS'backgammon'
p1725
(dp1726
g132
(lp1727
I232
asg306
(lp1728
sg89
(lp1729
sg83
(lp1730
ssS'rt'
p1731
(dp1732
g176
(lp1733
sg4
(lp1734
I1501
asg34
(lp1735
sg36
(lp1736
sg83
(lp1737
sg245
(lp1738
sg350
(lp1739
ssS'ru'
p1740
(dp1741
g104
(lp1742
I1400
assS'rv'
p1743
(dp1744
g126
(lp1745
sg130
(lp1746
I478
assS'rw'
p1747
(dp1748
g104
(lp1749
sg99
(lp1750
I3183
assS'rp'
p1751
(dp1752
g287
(lp1753
sg6
(lp1754
I1041
assS'rq'
p1755
(dp1756
g14
(lp1757
sg16
(lp1758
I1592
assS'rr'
p1759
(dp1760
g332
(lp1761
sg145
(lp1762
sg235
(lp1763
sg99
(lp1764
sg85
(lp1765
sg429
(lp1766
sg221
(lp1767
sg354
(lp1768
I1266
assS'rs'
p1769
(dp1770
g38
(lp1771
sg384
(lp1772
sg99
(lp1773
I3451
asg10
(lp1774
sg6
(lp1775
ssS'urbana'
p1776
(dp1777
g102
(lp1778
sg36
(lp1779
sg48
(lp1780
I23
assS'ry'
p1781
(dp1782
g130
(lp1783
I702
assS'dsp'
p1784
(dp1785
g114
(lp1786
I2272
assS'mason'
p1787
(dp1788
g460
(lp1789
I309
assS'rd'
p1790
(dp1791
g4
(lp1792
sg72
(lp1793
sg341
(lp1794
sg306
(lp1795
sg104
(lp1796
sg96
(lp1797
I844
asg223
(lp1798
sg22
(lp1799
ssS're'
p1800
(dp1801
g30
(lp1802
sg174
(lp1803
sg440
(lp1804
sg76
(lp1805
sg8
(lp1806
sg74
(lp1807
sg42
(lp1808
I1095
asg130
(lp1809
sg245
(lp1810
sg104
(lp1811
sg96
(lp1812
sg99
(lp1813
sg223
(lp1814
ssS'rf'
p1815
(dp1816
g32
(lp1817
sg176
(lp1818
sg22
(lp1819
sg149
(lp1820
I1164
assS'probabil'
p1821
(dp1822
g72
(lp1823
I3411
assS'ra'
p1824
(dp1825
g116
(lp1826
sg126
(lp1827
sg85
(lp1828
sg42
(lp1829
I3500
asg106
(lp1830
sg48
(lp1831
sg99
(lp1832
ssS'rb'
p1833
(dp1834
g281
(lp1835
I314
assS'rc'
p1836
(dp1837
g20
(lp1838
I1322
asg178
(lp1839
sg40
(lp1840
ssS'rl'
p1841
(dp1842
g230
(lp1843
sg176
(lp1844
sg38
(lp1845
sg341
(lp1846
sg281
(lp1847
sg245
(lp1848
sg102
(lp1849
sg14
(lp1850
I3657
asg313
(lp1851
sg83
(lp1852
ssS'rm'
p1853
(dp1854
g287
(lp1855
sg306
(lp1856
I1199
asg281
(lp1857
sg163
(lp1858
ssS'rn'
p1859
(dp1860
g230
(lp1861
sg287
(lp1862
sg235
(lp1863
sg68
(lp1864
sg281
(lp1865
sg85
(lp1866
sg106
(lp1867
I1883
asg163
(lp1868
sg140
(lp1869
sg149
(lp1870
ssS'ro'
p1871
(dp1872
g329
(lp1873
sg176
(lp1874
sg174
(lp1875
sg68
(lp1876
sg132
(lp1877
sg14
(lp1878
sg16
(lp1879
I904
assS'rh'
p1880
(dp1881
g440
(lp1882
I2729
asg32
(lp1883
ssS'ri'
p1884
(dp1885
g230
(lp1886
sg32
(lp1887
sg256
(lp1888
sg295
(lp1889
sg183
(lp1890
sg20
(lp1891
I1256
asg221
(lp1892
ssS'rj'
p1893
(dp1894
g174
(lp1895
I2047
asg32
(lp1896
sg118
(lp1897
sg295
(lp1898
sg183
(lp1899
sg384
(lp1900
sg293
(lp1901
sg38
(lp1902
sg303
(lp1903
sg18
(lp1904
sg460
(lp1905
ssS'impract'
p1906
(dp1907
g306
(lp1908
sg121
(lp1909
sg138
(lp1910
I1637
asg52
(lp1911
sg114
(lp1912
ssS'sensori'
p1913
(dp1914
g332
(lp1915
I32
asg70
(lp1916
sg80
(lp1917
sg262
(lp1918
sg34
(lp1919
sg303
(lp1920
sg176
(lp1921
sg245
(lp1922
sg18
(lp1923
sg535
(lp1924
sg350
(lp1925
ssS'navig'
p1926
(dp1927
g42
(lp1928
I3474
asg277
(lp1929
sg80
(lp1930
sg293
(lp1931
ssS'andreou'
p1932
(dp1933
g174
(lp1934
I2649
asg22
(lp1935
sg256
(lp1936
ssS'assimil'
p1937
(dp1938
g76
(lp1939
I1276
assS'attern'
p1940
(dp1941
g72
(lp1942
I3515
assS'sheet'
p1943
(dp1944
g216
(lp1945
sg48
(lp1946
sg26
(lp1947
sg149
(lp1948
I624
assS'jexp'
p1949
(dp1950
g460
(lp1951
I2193
assS'knit'
p1952
(dp1953
g181
(lp1954
I1584
assS'finallythefunctionl'
p1955
(dp1956
g341
(lp1957
I1759
assS'ate'
p1958
(dp1959
g18
(lp1960
sg50
(lp1961
sg354
(lp1962
I119
assS'furthermoreletwo'
p1963
(dp1964
g341
(lp1965
I1994
assS'malici'
p1966
(dp1967
g145
(lp1968
I362
assS'uiuj'
p1969
(dp1970
g128
(lp1971
I248
assS'ati'
p1972
(dp1973
g176
(lp1974
I944
asg72
(lp1975
ssS'silicon'
p1976
(dp1977
g174
(lp1978
sg283
(lp1979
sg256
(lp1980
sg245
(lp1981
sg14
(lp1982
I2676
asg20
(lp1983
sg22
(lp1984
ssS'atr'
p1985
(dp1986
g295
(lp1987
sg183
(lp1988
sg18
(lp1989
sg313
(lp1990
I2041
assS'unabl'
p1991
(dp1992
g126
(lp1993
sg108
(lp1994
I2208
asg121
(lp1995
sg63
(lp1996
sg223
(lp1997
ssS'baxt'
p1998
(dp1999
g91
(lp2000
I568
assS'confus'
p2001
(dp2002
g30
(lp2003
sg174
(lp2004
sg74
(lp2005
sg332
(lp2006
sg235
(lp2007
sg72
(lp2008
sg99
(lp2009
I2924
assS'nigel'
p2010
(dp2011
g429
(lp2012
sg52
(lp2013
I7
assS'vertebr'
p2014
(dp2015
g256
(lp2016
I34
assS'subshel'
p2017
(dp2018
g384
(lp2019
I1024
assS'clariti'
p2020
(dp2021
g89
(lp2022
I465
asg76
(lp2023
ssS'vitro'
p2024
(dp2025
g106
(lp2026
I997
asg262
(lp2027
ssS'rustad'
p2028
(dp2029
g438
(lp2030
I2465
assS'misclassif'
p2031
(dp2032
g484
(lp2033
sg460
(lp2034
sg63
(lp2035
sg91
(lp2036
I1701
asg281
(lp2037
ssS'wash'
p2038
(dp2039
g116
(lp2040
sg318
(lp2041
I3084
assS'instruct'
p2042
(dp2043
g32
(lp2044
sg4
(lp2045
sg10
(lp2046
sg42
(lp2047
I2498
asg102
(lp2048
sg99
(lp2049
ssS'ziemeck'
p2050
(dp2051
g59
(lp2052
I3104
assS'knight'
p2053
(dp2054
g132
(lp2055
I854
assS'rnotoneuron'
p2056
(dp2057
g350
(lp2058
I968
assS'xerion'
p2059
(dp2060
g138
(lp2061
I2123
assS'sutherland'
p2062
(dp2063
g216
(lp2064
I584
assS'ronj'
p2065
(dp2066
g116
(lp2067
I1157
assS'lister'
p2068
(dp2069
g14
(lp2070
sg16
(lp2071
I1068
assS'xii'
p2072
(dp2073
g124
(lp2074
I2932
assS'xij'
p2075
(dp2076
g384
(lp2077
sg140
(lp2078
I419
assS'checker'
p2079
(dp2080
g132
(lp2081
I198
asg26
(lp2082
ssS'xin'
p2083
(dp2084
g223
(lp2085
I1997
assS'identi'
p2086
(dp2087
g114
(lp2088
I338
assS'hiyi'
p2089
(dp2090
g313
(lp2091
I1412
assS'master'
p2092
(dp2093
g221
(lp2094
sg384
(lp2095
sg83
(lp2096
sg89
(lp2097
sg78
(lp2098
sg132
(lp2099
I1211
asg99
(lp2100
ssS'lxi'
p2101
(dp2102
g72
(lp2103
I2888
assS'dataj'
p2104
(dp2105
g34
(lp2106
I2439
assS'xiy'
p2107
(dp2108
g72
(lp2109
I1525
assS'gilbert'
p2110
(dp2111
g12
(lp2112
sg74
(lp2113
sg149
(lp2114
I170
assS'rabin'
p2115
(dp2116
g460
(lp2117
sg76
(lp2118
I331
assS'listen'
p2119
(dp2120
g30
(lp2121
sg78
(lp2122
sg332
(lp2123
I1708
assS'logdet'
p2124
(dp2125
g124
(lp2126
I1144
assS'fibrous'
p2127
(dp2128
g283
(lp2129
I750
assS'dichotomi'
p2130
(dp2131
g36
(lp2132
I3151
assS'wisdom'
p2133
(dp2134
g32
(lp2135
I263
assS'jbt'
p2136
(dp2137
g74
(lp2138
I21
assS'modellotheoret'
p2139
(dp2140
g121
(lp2141
I2402
assS'crawl'
p2142
(dp2143
g80
(lp2144
I406
assS'heterosynapt'
p2145
(dp2146
g106
(lp2147
I446
assS'arbjk'
p2148
(dp2149
g46
(lp2150
I2512
assS'tree'
p2151
(dp2152
g74
(lp2153
sg484
(lp2154
sg145
(lp2155
sg181
(lp2156
sg344
(lp2157
sg183
(lp2158
sg68
(lp2159
sg42
(lp2160
I1404
asg429
(lp2161
sg87
(lp2162
sg91
(lp2163
sg94
(lp2164
sg106
(lp2165
sg110
(lp2166
sg138
(lp2167
sg44
(lp2168
ssS'olshausen'
p2169
(dp2170
g178
(lp2171
sg8
(lp2172
I750
assS'kimenat'
p2173
(dp2174
g99
(lp2175
I3481
assS'pneumonia'
p2176
(dp2177
g277
(lp2178
I121
assS'tion'
p2179
(dp2180
g42
(lp2181
I2535
asg230
(lp2182
sg18
(lp2183
sg72
(lp2184
ssS'hayn'
p2185
(dp2186
g14
(lp2187
sg16
(lp2188
I37
assS'retrograd'
p2189
(dp2190
g135
(lp2191
I258
assS'ampa'
p2192
(dp2193
g70
(lp2194
I1277
assS'postnat'
p2195
(dp2196
g438
(lp2197
I1003
assS'valiant'
p2198
(dp2199
g344
(lp2200
sg287
(lp2201
I198
asg110
(lp2202
ssS'runner'
p2203
(dp2204
g63
(lp2205
I2552
assS'grinvald'
p2206
(dp2207
g149
(lp2208
I2901
assS'spectrum'
p2209
(dp2210
g116
(lp2211
sg174
(lp2212
sg145
(lp2213
sg78
(lp2214
sg68
(lp2215
sg102
(lp2216
sg48
(lp2217
I912
assS'craven'
p2218
(dp2219
g344
(lp2220
I23
assS'oldpeak'
p2221
(dp2222
g91
(lp2223
I2095
assS'increment'
p2224
(dp2225
g118
(lp2226
sg440
(lp2227
sg70
(lp2228
sg295
(lp2229
sg183
(lp2230
sg59
(lp2231
sg72
(lp2232
sg83
(lp2233
sg91
(lp2234
I2302
asg245
(lp2235
ssS'incompat'
p2236
(dp2237
g48
(lp2238
I1588
assS'androx'
p2239
(dp2240
g59
(lp2241
I844
assS'that'
p2242
(dp2243
g80
(lp2244
sg293
(lp2245
sg344
(lp2246
sg78
(lp2247
sg59
(lp2248
sg484
(lp2249
sg38
(lp2250
sg83
(lp2251
sg85
(lp2252
sg303
(lp2253
sg438
(lp2254
sg116
(lp2255
sg118
(lp2256
sg34
(lp2257
sg36
(lp2258
sg460
(lp2259
sg68
(lp2260
sg72
(lp2261
sg281
(lp2262
sg10
(lp2263
sg40
(lp2264
sg283
(lp2265
sg70
(lp2266
sg26
(lp2267
sg277
(lp2268
sg163
(lp2269
sg89
(lp2270
sg91
(lp2271
sg12
(lp2272
sg94
(lp2273
sg96
(lp2274
sg48
(lp2275
sg99
(lp2276
sg313
(lp2277
sg44
(lp2278
sg149
(lp2279
sg429
(lp2280
sg102
(lp2281
sg104
(lp2282
sg106
(lp2283
sg108
(lp2284
sg110
(lp2285
sg63
(lp2286
sg52
(lp2287
sg114
(lp2288
sg128
(lp2289
sg130
(lp2290
sg132
(lp2291
sg14
(lp2292
sg16
(lp2293
sg135
(lp2294
sg50
(lp2295
sg138
(lp2296
sg140
(lp2297
sg354
(lp2298
sg306
(lp2299
sg87
(lp2300
sg245
(lp2301
sg46
(lp2302
sg20
(lp2303
sg18
(lp2304
sg221
(lp2305
sg535
(lp2306
sg223
(lp2307
sg350
(lp2308
sg216
(lp2309
sg174
(lp2310
sg440
(lp2311
sg332
(lp2312
sg121
(lp2313
sg4
(lp2314
sg6
(lp2315
sg8
(lp2316
sg126
(lp2317
sg341
(lp2318
sg30
(lp2319
sg287
(lp2320
sg74
(lp2321
sg176
(lp2322
sg145
(lp2323
sg256
(lp2324
sg76
(lp2325
sg262
(lp2326
sg295
(lp2327
sg183
(lp2328
sg42
(lp2329
I86
asg230
(lp2330
sg329
(lp2331
sg32
(lp2332
sg318
(lp2333
sg178
(lp2334
sg22
(lp2335
sg181
(lp2336
sg235
(lp2337
sg384
(lp2338
sg124
(lp2339
ssS'gustaffson'
p2340
(dp2341
g106
(lp2342
I1229
assS'hum'
p2343
(dp2344
g10
(lp2345
I348
assS'simplifi'
p2346
(dp2347
g10
(lp2348
sg116
(lp2349
sg332
(lp2350
sg22
(lp2351
sg181
(lp2352
sg293
(lp2353
sg34
(lp2354
sg163
(lp2355
sg176
(lp2356
sg114
(lp2357
sg281
(lp2358
sg85
(lp2359
sg318
(lp2360
sg48
(lp2361
sg102
(lp2362
sg104
(lp2363
sg20
(lp2364
sg108
(lp2365
sg50
(lp2366
I106
asg91
(lp2367
ssS'shall'
p2368
(dp2369
g230
(lp2370
sg235
(lp2371
sg281
(lp2372
sg128
(lp2373
sg102
(lp2374
sg14
(lp2375
sg16
(lp2376
I861
asg44
(lp2377
ssS'mliller'
p2378
(dp2379
g36
(lp2380
I972
assS'object'
p2381
(dp2382
g70
(lp2383
sg277
(lp2384
sg30
(lp2385
sg287
(lp2386
sg74
(lp2387
sg76
(lp2388
sg293
(lp2389
sg59
(lp2390
sg83
(lp2391
sg303
(lp2392
sg306
(lp2393
sg245
(lp2394
sg46
(lp2395
sg313
(lp2396
sg223
(lp2397
sg350
(lp2398
sg230
(lp2399
sg429
(lp2400
sg110
(lp2401
sg63
(lp2402
sg52
(lp2403
sg114
(lp2404
sg216
(lp2405
sg118
(lp2406
sg32
(lp2407
sg318
(lp2408
sg178
(lp2409
sg181
(lp2410
sg8
(lp2411
sg460
(lp2412
sg126
(lp2413
sg50
(lp2414
sg138
(lp2415
sg354
(lp2416
I3060
assS'landmark'
p2417
(dp2418
g118
(lp2419
I267
asg80
(lp2420
ssS'microsecond'
p2421
(dp2422
g14
(lp2423
sg16
(lp2424
I156
assS'cate'
p2425
(dp2426
g72
(lp2427
I2425
assS'letter'
p2428
(dp2429
g26
(lp2430
sg74
(lp2431
sg318
(lp2432
sg4
(lp2433
sg76
(lp2434
sg72
(lp2435
sg59
(lp2436
sg130
(lp2437
sg126
(lp2438
sg281
(lp2439
sg114
(lp2440
sg124
(lp2441
sg42
(lp2442
I83
asg283
(lp2443
sg94
(lp2444
sg245
(lp2445
sg14
(lp2446
sg135
(lp2447
sg140
(lp2448
sg22
(lp2449
ssS'bradford'
p2450
(dp2451
g76
(lp2452
I3284
assS'muititask'
p2453
(dp2454
g223
(lp2455
I3176
assS'singer'
p2456
(dp2457
g438
(lp2458
I1116
asg70
(lp2459
sg277
(lp2460
sg12
(lp2461
sg106
(lp2462
sg48
(lp2463
sg149
(lp2464
ssS'unexplain'
p2465
(dp2466
g384
(lp2467
sg126
(lp2468
I2783
assS'handcart'
p2469
(dp2470
g42
(lp2471
I2298
assS'camp'
p2472
(dp2473
g245
(lp2474
sg138
(lp2475
I2288
assS'hospit'
p2476
(dp2477
g36
(lp2478
sg48
(lp2479
I1965
asg91
(lp2480
sg484
(lp2481
sg277
(lp2482
ssS'tech'
p2483
(dp2484
g235
(lp2485
sg295
(lp2486
sg183
(lp2487
sg40
(lp2488
sg91
(lp2489
sg36
(lp2490
sg104
(lp2491
sg20
(lp2492
sg313
(lp2493
sg354
(lp2494
I3081
assS'camb'
p2495
(dp2496
g235
(lp2497
I294
assS'came'
p2498
(dp2499
g30
(lp2500
sg176
(lp2501
sg99
(lp2502
I1589
assS'eacher'
p2503
(dp2504
g36
(lp2505
I2400
assS'superset'
p2506
(dp2507
g344
(lp2508
I2715
assS'asid'
p2509
(dp2510
g118
(lp2511
sg128
(lp2512
I2417
assS'cheapli'
p2513
(dp2514
g313
(lp2515
I2019
assS'transcrib'
p2516
(dp2517
g76
(lp2518
I178
assS'seltrt'
p2519
(dp2520
g22
(lp2521
I682
assS'lxn'
p2522
(dp2523
g440
(lp2524
I1288
assS'layout'
p2525
(dp2526
g94
(lp2527
I422
asg48
(lp2528
sg118
(lp2529
ssS'zth'
p2530
(dp2531
g145
(lp2532
I683
assS'ment'
p2533
(dp2534
g4
(lp2535
I1597
assS'buss'
p2536
(dp2537
g106
(lp2538
I114
assS'siev'
p2539
(dp2540
g354
(lp2541
I2038
assS'theme'
p2542
(dp2543
g384
(lp2544
I136
asg63
(lp2545
sg85
(lp2546
ssS'busi'
p2547
(dp2548
g114
(lp2549
I571
assS'bliss'
p2550
(dp2551
g106
(lp2552
I2559
assS'rich'
p2553
(dp2554
g287
(lp2555
sg4
(lp2556
sg277
(lp2557
sg235
(lp2558
sg83
(lp2559
sg132
(lp2560
sg96
(lp2561
sg50
(lp2562
sg138
(lp2563
I3241
assS'mend'
p2564
(dp2565
g46
(lp2566
I3658
assS'spectmgmm'
p2567
(dp2568
g116
(lp2569
I1811
assS'gallag'
p2570
(dp2571
g102
(lp2572
I1464
assS'linsker'
p2573
(dp2574
g12
(lp2575
I202
asg102
(lp2576
sg48
(lp2577
sg72
(lp2578
sg318
(lp2579
ssS'champaign'
p2580
(dp2581
g99
(lp2582
I3378
assS'platt'
p2583
(dp2584
g429
(lp2585
sg96
(lp2586
I529
asg59
(lp2587
ssS'jvfj'
p2588
(dp2589
g130
(lp2590
I2359
assS'intralaminar'
p2591
(dp2592
g303
(lp2593
I1129
assS'ner'
p2594
(dp2595
g230
(lp2596
I1704
assS'patch'
p2597
(dp2598
g30
(lp2599
sg118
(lp2600
sg181
(lp2601
sg149
(lp2602
I94
assS'petsch'
p2603
(dp2604
g78
(lp2605
sg223
(lp2606
I3304
assS'perron'
p2607
(dp2608
g59
(lp2609
sg484
(lp2610
sg221
(lp2611
sg140
(lp2612
I271
asg235
(lp2613
ssS'exdlorati'
p2614
(dp2615
g354
(lp2616
I2695
assS'singhal'
p2617
(dp2618
g108
(lp2619
I10
assS'respond'
p2620
(dp2621
g116
(lp2622
sg438
(lp2623
I1081
asg283
(lp2624
sg70
(lp2625
sg4
(lp2626
sg181
(lp2627
sg256
(lp2628
sg12
(lp2629
sg104
(lp2630
sg110
(lp2631
sg63
(lp2632
sg149
(lp2633
ssS'fair'
p2634
(dp2635
g230
(lp2636
sg438
(lp2637
I93
asg74
(lp2638
sg26
(lp2639
sg277
(lp2640
sg329
(lp2641
sg34
(lp2642
sg460
(lp2643
sg83
(lp2644
sg91
(lp2645
sg128
(lp2646
sg48
(lp2647
sg138
(lp2648
sg44
(lp2649
sg114
(lp2650
ssS'radius'
p2651
(dp2652
g32
(lp2653
sg181
(lp2654
sg8
(lp2655
sg59
(lp2656
sg14
(lp2657
sg16
(lp2658
I959
asg149
(lp2659
ssS'result'
p2660
(dp2661
g80
(lp2662
sg293
(lp2663
sg344
(lp2664
sg78
(lp2665
sg59
(lp2666
sg484
(lp2667
sg38
(lp2668
sg83
(lp2669
sg85
(lp2670
sg303
(lp2671
sg438
(lp2672
sg116
(lp2673
sg118
(lp2674
sg34
(lp2675
sg36
(lp2676
sg460
(lp2677
sg68
(lp2678
sg72
(lp2679
sg281
(lp2680
sg10
(lp2681
sg40
(lp2682
sg283
(lp2683
sg70
(lp2684
sg26
(lp2685
sg277
(lp2686
sg163
(lp2687
sg89
(lp2688
sg91
(lp2689
sg12
(lp2690
sg94
(lp2691
sg96
(lp2692
sg48
(lp2693
sg99
(lp2694
sg313
(lp2695
sg44
(lp2696
sg149
(lp2697
sg429
(lp2698
sg102
(lp2699
sg104
(lp2700
sg106
(lp2701
sg108
(lp2702
sg110
(lp2703
sg63
(lp2704
sg52
(lp2705
sg114
(lp2706
sg128
(lp2707
sg130
(lp2708
sg132
(lp2709
sg14
(lp2710
sg16
(lp2711
sg135
(lp2712
sg50
(lp2713
sg138
(lp2714
sg140
(lp2715
sg354
(lp2716
sg306
(lp2717
sg87
(lp2718
sg245
(lp2719
sg46
(lp2720
sg20
(lp2721
sg18
(lp2722
sg221
(lp2723
sg535
(lp2724
sg223
(lp2725
sg350
(lp2726
sg216
(lp2727
sg174
(lp2728
sg440
(lp2729
sg332
(lp2730
sg121
(lp2731
sg4
(lp2732
sg6
(lp2733
sg8
(lp2734
sg126
(lp2735
sg341
(lp2736
sg30
(lp2737
sg287
(lp2738
sg74
(lp2739
sg176
(lp2740
sg145
(lp2741
sg256
(lp2742
sg76
(lp2743
sg262
(lp2744
sg295
(lp2745
sg183
(lp2746
sg42
(lp2747
I1688
asg230
(lp2748
sg329
(lp2749
sg32
(lp2750
sg318
(lp2751
sg178
(lp2752
sg22
(lp2753
sg181
(lp2754
sg235
(lp2755
sg384
(lp2756
sg124
(lp2757
ssS'respons'
p2758
(dp2759
g329
(lp2760
sg70
(lp2761
sg26
(lp2762
sg116
(lp2763
sg181
(lp2764
sg303
(lp2765
sg287
(lp2766
sg176
(lp2767
sg145
(lp2768
sg256
(lp2769
sg80
(lp2770
sg262
(lp2771
sg78
(lp2772
sg59
(lp2773
sg38
(lp2774
sg83
(lp2775
sg85
(lp2776
sg124
(lp2777
sg42
(lp2778
I3270
asg87
(lp2779
sg12
(lp2780
sg94
(lp2781
sg20
(lp2782
sg48
(lp2783
sg99
(lp2784
sg313
(lp2785
sg149
(lp2786
sg230
(lp2787
sg174
(lp2788
sg18
(lp2789
sg245
(lp2790
sg46
(lp2791
sg102
(lp2792
sg104
(lp2793
sg106
(lp2794
sg110
(lp2795
sg52
(lp2796
sg22
(lp2797
sg216
(lp2798
sg438
(lp2799
sg118
(lp2800
sg332
(lp2801
sg121
(lp2802
sg4
(lp2803
sg6
(lp2804
sg8
(lp2805
sg221
(lp2806
sg384
(lp2807
sg235
(lp2808
sg535
(lp2809
sg128
(lp2810
sg132
(lp2811
sg350
(lp2812
sg354
(lp2813
ssS'blem'
p2814
(dp2815
g42
(lp2816
I3144
assS'hammer'
p2817
(dp2818
g223
(lp2819
I2174
assS'best'
p2820
(dp2821
g283
(lp2822
sg26
(lp2823
sg277
(lp2824
sg163
(lp2825
sg72
(lp2826
sg85
(lp2827
sg30
(lp2828
sg74
(lp2829
sg76
(lp2830
sg183
(lp2831
sg484
(lp2832
sg83
(lp2833
sg114
(lp2834
sg91
(lp2835
sg46
(lp2836
sg96
(lp2837
sg48
(lp2838
sg221
(lp2839
sg313
(lp2840
sg329
(lp2841
sg429
(lp2842
sg94
(lp2843
sg178
(lp2844
sg110
(lp2845
sg63
(lp2846
sg174
(lp2847
sg116
(lp2848
sg438
(lp2849
I1056
asg440
(lp2850
sg318
(lp2851
sg22
(lp2852
sg4
(lp2853
sg181
(lp2854
sg8
(lp2855
sg34
(lp2856
sg99
(lp2857
sg235
(lp2858
sg126
(lp2859
sg281
(lp2860
sg128
(lp2861
sg36
(lp2862
sg135
(lp2863
sg138
(lp2864
sg140
(lp2865
ssS'lnpapp'
p2866
(dp2867
g163
(lp2868
I1037
assS'perceptronl'
p2869
(dp2870
g108
(lp2871
I974
assS'heterogen'
p2872
(dp2873
g104
(lp2874
I610
asg429
(lp2875
ssS'irq'
p2876
(dp2877
g26
(lp2878
sg235
(lp2879
I296
assS'iri'
p2880
(dp2881
g108
(lp2882
I218
asg89
(lp2883
sg341
(lp2884
sg149
(lp2885
ssS'figur'
p2886
(dp2887
g80
(lp2888
sg293
(lp2889
sg344
(lp2890
sg78
(lp2891
sg484
(lp2892
sg38
(lp2893
sg85
(lp2894
sg303
(lp2895
sg438
(lp2896
I163
asg116
(lp2897
sg118
(lp2898
sg34
(lp2899
sg36
(lp2900
sg460
(lp2901
sg68
(lp2902
sg72
(lp2903
sg281
(lp2904
sg10
(lp2905
sg283
(lp2906
sg70
(lp2907
sg26
(lp2908
sg277
(lp2909
sg163
(lp2910
sg89
(lp2911
sg91
(lp2912
sg12
(lp2913
sg94
(lp2914
sg96
(lp2915
sg48
(lp2916
sg99
(lp2917
sg313
(lp2918
sg44
(lp2919
sg149
(lp2920
sg429
(lp2921
sg102
(lp2922
sg104
(lp2923
sg106
(lp2924
sg108
(lp2925
sg110
(lp2926
sg63
(lp2927
sg52
(lp2928
sg114
(lp2929
sg128
(lp2930
sg130
(lp2931
sg132
(lp2932
sg14
(lp2933
sg16
(lp2934
sg135
(lp2935
sg50
(lp2936
sg138
(lp2937
sg140
(lp2938
sg354
(lp2939
sg87
(lp2940
sg245
(lp2941
sg46
(lp2942
sg20
(lp2943
sg18
(lp2944
sg221
(lp2945
sg535
(lp2946
sg223
(lp2947
sg350
(lp2948
sg216
(lp2949
sg174
(lp2950
sg440
(lp2951
sg332
(lp2952
sg121
(lp2953
sg4
(lp2954
sg6
(lp2955
sg8
(lp2956
sg126
(lp2957
sg341
(lp2958
sg30
(lp2959
sg176
(lp2960
sg145
(lp2961
sg256
(lp2962
sg76
(lp2963
sg262
(lp2964
sg295
(lp2965
sg183
(lp2966
sg230
(lp2967
sg329
(lp2968
sg318
(lp2969
sg178
(lp2970
sg22
(lp2971
sg181
(lp2972
sg235
(lp2973
sg384
(lp2974
sg124
(lp2975
ssS'score'
p2976
(dp2977
g6
(lp2978
sg293
(lp2979
sg85
(lp2980
sg130
(lp2981
I2501
asg94
(lp2982
sg18
(lp2983
sg114
(lp2984
ssS'sschaal'
p2985
(dp2986
g295
(lp2987
I15
asg183
(lp2988
ssS'xvl'
p2989
(dp2990
g140
(lp2991
I698
assS'irn'
p2992
(dp2993
g34
(lp2994
sg46
(lp2995
I1792
asg287
(lp2996
ssS'bolouri'
p2997
(dp2998
g283
(lp2999
I45
assS'ire'
p3000
(dp3001
g40
(lp3002
sg114
(lp3003
I2320
assS'serc'
p3004
(dp3005
g14
(lp3006
I4640
assS'inabl'
p3007
(dp3008
g99
(lp3009
I2364
asg4
(lp3010
sg8
(lp3011
ssS'jacm'
p3012
(dp3013
g145
(lp3014
I3143
assS'extend'
p3015
(dp3016
g26
(lp3017
sg287
(lp3018
sg176
(lp3019
sg145
(lp3020
sg262
(lp3021
sg59
(lp3022
sg38
(lp3023
sg87
(lp3024
sg89
(lp3025
sg46
(lp3026
sg96
(lp3027
sg48
(lp3028
sg535
(lp3029
sg223
(lp3030
sg149
(lp3031
sg118
(lp3032
sg116
(lp3033
sg174
(lp3034
sg293
(lp3035
sg102
(lp3036
sg104
(lp3037
sg108
(lp3038
sg22
(lp3039
sg230
(lp3040
sg438
(lp3041
I69
asg32
(lp3042
sg332
(lp3043
sg4
(lp3044
sg36
(lp3045
sg460
(lp3046
sg68
(lp3047
sg72
(lp3048
sg10
(lp3049
sg313
(lp3050
sg44
(lp3051
sg128
(lp3052
sg132
(lp3053
sg50
(lp3054
sg138
(lp3055
ssS'inpul'
p3056
(dp3057
g106
(lp3058
I1302
assS'wtx'
p3059
(dp3060
g460
(lp3061
I2190
assS'sram'
p3062
(dp3063
g14
(lp3064
I4146
asg10
(lp3065
ssS'shidara'
p3066
(dp3067
g99
(lp3068
I2605
assS'juliet'
p3069
(dp3070
g68
(lp3071
I21
assS'extens'
p3072
(dp3073
g283
(lp3074
sg72
(lp3075
sg74
(lp3076
sg145
(lp3077
sg59
(lp3078
sg306
(lp3079
sg12
(lp3080
sg94
(lp3081
sg48
(lp3082
sg99
(lp3083
sg535
(lp3084
sg223
(lp3085
sg350
(lp3086
sg118
(lp3087
sg429
(lp3088
sg52
(lp3089
sg438
(lp3090
I724
asg332
(lp3091
sg181
(lp3092
sg235
(lp3093
sg36
(lp3094
sg68
(lp3095
sg126
(lp3096
sg10
(lp3097
sg130
(lp3098
sg132
(lp3099
sg14
(lp3100
sg16
(lp3101
sg50
(lp3102
ssS'macroscop'
p3103
(dp3104
g384
(lp3105
I188
assS'extent'
p3106
(dp3107
g174
(lp3108
sg118
(lp3109
sg295
(lp3110
sg183
(lp3111
sg484
(lp3112
sg85
(lp3113
sg36
(lp3114
sg354
(lp3115
I1740
asg140
(lp3116
sg149
(lp3117
ssS'toler'
p3118
(dp3119
g102
(lp3120
sg344
(lp3121
sg460
(lp3122
sg138
(lp3123
I1683
asg181
(lp3124
ssS'fiir'
p3125
(dp3126
g313
(lp3127
I2314
assS'accident'
p3128
(dp3129
g332
(lp3130
I204
assS'pitt'
p3131
(dp3132
g484
(lp3133
I20
asg277
(lp3134
ssS'mirolla'
p3135
(dp3136
g174
(lp3137
I1296
assS'logic'
p3138
(dp3139
g283
(lp3140
sg145
(lp3141
sg22
(lp3142
sg295
(lp3143
sg183
(lp3144
sg68
(lp3145
sg10
(lp3146
sg40
(lp3147
sg42
(lp3148
I1632
asg128
(lp3149
sg20
(lp3150
sg223
(lp3151
ssS'countri'
p3152
(dp3153
g74
(lp3154
I374
assS'mirollo'
p3155
(dp3156
g174
(lp3157
I2728
assS'logik'
p3158
(dp3159
g36
(lp3160
I36
assS'compromis'
p3161
(dp3162
g440
(lp3163
I838
asg18
(lp3164
sg178
(lp3165
sg74
(lp3166
ssS'pre'
p3167
(dp3168
g440
(lp3169
sg332
(lp3170
sg121
(lp3171
sg181
(lp3172
sg50
(lp3173
sg68
(lp3174
sg178
(lp3175
sg176
(lp3176
sg52
(lp3177
sg102
(lp3178
sg14
(lp3179
sg106
(lp3180
I1308
asg99
(lp3181
sg96
(lp3182
sg44
(lp3183
ssS'assur'
p3184
(dp3185
g42
(lp3186
I2088
asg245
(lp3187
sg110
(lp3188
sg89
(lp3189
sg230
(lp3190
ssS'munchen'
p3191
(dp3192
g30
(lp3193
sg221
(lp3194
sg313
(lp3195
I2318
assS'hydrophil'
p3196
(dp3197
g26
(lp3198
I195
assS'summat'
p3199
(dp3200
g116
(lp3201
sg332
(lp3202
sg36
(lp3203
sg42
(lp3204
I1890
asg318
(lp3205
sg20
(lp3206
ssS'assum'
p3207
(dp3208
g68
(lp3209
sg78
(lp3210
sg116
(lp3211
sg281
(lp3212
sg283
(lp3213
sg36
(lp3214
sg40
(lp3215
sg26
(lp3216
sg30
(lp3217
sg287
(lp3218
sg74
(lp3219
sg176
(lp3220
sg145
(lp3221
sg76
(lp3222
sg262
(lp3223
sg295
(lp3224
sg183
(lp3225
sg80
(lp3226
sg38
(lp3227
sg83
(lp3228
sg85
(lp3229
sg303
(lp3230
sg42
(lp3231
I207
asg306
(lp3232
sg87
(lp3233
sg89
(lp3234
sg91
(lp3235
sg12
(lp3236
sg46
(lp3237
sg20
(lp3238
sg48
(lp3239
sg99
(lp3240
sg313
(lp3241
sg223
(lp3242
sg149
(lp3243
sg230
(lp3244
sg293
(lp3245
sg460
(lp3246
sg350
(lp3247
sg318
(lp3248
sg18
(lp3249
sg108
(lp3250
sg110
(lp3251
sg63
(lp3252
sg52
(lp3253
sg216
(lp3254
sg438
(lp3255
sg440
(lp3256
sg332
(lp3257
sg178
(lp3258
sg4
(lp3259
sg8
(lp3260
sg34
(lp3261
sg221
(lp3262
sg384
(lp3263
sg235
(lp3264
sg126
(lp3265
sg341
(lp3266
sg535
(lp3267
sg130
(lp3268
sg132
(lp3269
sg135
(lp3270
sg138
(lp3271
sg140
(lp3272
sg354
(lp3273
ssS'summar'
p3274
(dp3275
g230
(lp3276
sg440
(lp3277
sg76
(lp3278
sg8
(lp3279
sg460
(lp3280
sg72
(lp3281
sg32
(lp3282
sg89
(lp3283
sg128
(lp3284
sg130
(lp3285
sg48
(lp3286
sg221
(lp3287
sg44
(lp3288
sg354
(lp3289
I2677
assS'xtwtwx'
p3290
(dp3291
g96
(lp3292
I1030
assS'pertinaci'
p3293
(dp3294
g132
(lp3295
I2683
assS'frg'
p3296
(dp3297
g59
(lp3298
sg48
(lp3299
I34
assS'fri'
p3300
(dp3301
g116
(lp3302
I10
assS'kronland'
p3303
(dp3304
g22
(lp3305
I2432
assS'much'
p3306
(dp3307
g329
(lp3308
sg70
(lp3309
sg78
(lp3310
sg277
(lp3311
sg85
(lp3312
sg303
(lp3313
sg30
(lp3314
sg287
(lp3315
sg74
(lp3316
sg176
(lp3317
sg145
(lp3318
sg76
(lp3319
sg295
(lp3320
sg183
(lp3321
sg59
(lp3322
sg80
(lp3323
sg83
(lp3324
sg114
(lp3325
sg63
(lp3326
sg89
(lp3327
sg12
(lp3328
sg94
(lp3329
sg20
(lp3330
sg48
(lp3331
sg44
(lp3332
sg174
(lp3333
sg429
(lp3334
sg318
(lp3335
sg46
(lp3336
sg102
(lp3337
sg104
(lp3338
sg108
(lp3339
sg110
(lp3340
sg178
(lp3341
sg52
(lp3342
sg22
(lp3343
sg216
(lp3344
sg438
(lp3345
I2245
asg332
(lp3346
sg121
(lp3347
sg4
(lp3348
sg34
(lp3349
sg36
(lp3350
sg384
(lp3351
sg68
(lp3352
sg126
(lp3353
sg281
(lp3354
sg223
(lp3355
sg128
(lp3356
sg130
(lp3357
sg132
(lp3358
sg14
(lp3359
sg16
(lp3360
sg135
(lp3361
sg50
(lp3362
sg138
(lp3363
sg140
(lp3364
sg354
(lp3365
ssS'rnesencephal'
p3366
(dp3367
g350
(lp3368
I1187
assS'hert'
p3369
(dp3370
g283
(lp3371
I18
assS'pro'
p3372
(dp3373
g42
(lp3374
I3143
asg174
(lp3375
sg48
(lp3376
ssS'hepp'
p3377
(dp3378
g32
(lp3379
I473
assS'life'
p3380
(dp3381
g116
(lp3382
sg78
(lp3383
sg176
(lp3384
sg6
(lp3385
I32
asg293
(lp3386
ssS'regul'
p3387
(dp3388
g230
(lp3389
sg116
(lp3390
sg89
(lp3391
sg4
(lp3392
sg149
(lp3393
I1909
assS'eastern'
p3394
(dp3395
g116
(lp3396
I2468
assS'wish'
p3397
(dp3398
g230
(lp3399
sg440
(lp3400
sg124
(lp3401
sg126
(lp3402
sg74
(lp3403
sg306
(lp3404
sg99
(lp3405
sg138
(lp3406
I1114
assS'dave'
p3407
(dp3408
g83
(lp3409
I2774
assS'lift'
p3410
(dp3411
g181
(lp3412
I259
assS'dote'
p3413
(dp3414
g163
(lp3415
I627
assS'spie'
p3416
(dp3417
g245
(lp3418
I2946
asg256
(lp3419
ssS'ank'
p3420
(dp3421
g535
(lp3422
I13
assS'spin'
p3423
(dp3424
g384
(lp3425
I1544
asg63
(lp3426
sg26
(lp3427
ssS'persuas'
p3428
(dp3429
g70
(lp3430
I2171
assS'davi'
p3431
(dp3432
g70
(lp3433
sg277
(lp3434
sg59
(lp3435
sg245
(lp3436
sg14
(lp3437
sg16
(lp3438
I2606
assS'commerci'
p3439
(dp3440
g94
(lp3441
sg135
(lp3442
I141
asg10
(lp3443
sg52
(lp3444
sg78
(lp3445
ssS'castro'
p3446
(dp3447
g14
(lp3448
sg135
(lp3449
I2449
assS'employ'
p3450
(dp3451
g283
(lp3452
sg26
(lp3453
sg277
(lp3454
sg287
(lp3455
sg74
(lp3456
sg118
(lp3457
sg295
(lp3458
sg183
(lp3459
sg83
(lp3460
sg42
(lp3461
I1376
asg306
(lp3462
sg94
(lp3463
sg20
(lp3464
sg223
(lp3465
sg329
(lp3466
sg293
(lp3467
sg52
(lp3468
sg114
(lp3469
sg438
(lp3470
sg32
(lp3471
sg4
(lp3472
sg34
(lp3473
sg384
(lp3474
sg124
(lp3475
sg126
(lp3476
sg10
(lp3477
sg44
(lp3478
sg132
(lp3479
sg50
(lp3480
sg354
(lp3481
ssS'euclidean'
p3482
(dp3483
g116
(lp3484
sg74
(lp3485
sg59
(lp3486
sg281
(lp3487
sg42
(lp3488
I1868
asg306
(lp3489
sg46
(lp3490
sg44
(lp3491
ssS'koiranglip'
p3492
(dp3493
g287
(lp3494
I309
assS'unsolv'
p3495
(dp3496
g48
(lp3497
I1192
asg26
(lp3498
ssS'marriag'
p3499
(dp3500
g72
(lp3501
I2280
assS'viii'
p3502
(dp3503
g102
(lp3504
I2300
asg46
(lp3505
sg181
(lp3506
ssS'hopkin'
p3507
(dp3508
g106
(lp3509
I29
asg99
(lp3510
sg22
(lp3511
sg114
(lp3512
ssS'gerhard'
p3513
(dp3514
g354
(lp3515
I8
assS'laughton'
p3516
(dp3517
g384
(lp3518
I32
assS'kebl'
p3519
(dp3520
g384
(lp3521
I41
assS'player'
p3522
(dp3523
g132
(lp3524
I199
asg306
(lp3525
sg277
(lp3526
ssS'elicit'
p3527
(dp3528
g106
(lp3529
I255
assS'tzj'
p3530
(dp3531
g121
(lp3532
I444
assS'hong'
p3533
(dp3534
g72
(lp3535
I20
assS'dilut'
p3536
(dp3537
g384
(lp3538
I201
assS'credit'
p3539
(dp3540
g329
(lp3541
I3009
asg114
(lp3542
ssS'transpar'
p3543
(dp3544
g216
(lp3545
sg438
(lp3546
I94
assS'split'
p3547
(dp3548
g277
(lp3549
sg183
(lp3550
sg484
(lp3551
sg126
(lp3552
sg85
(lp3553
sg87
(lp3554
sg104
(lp3555
sg135
(lp3556
sg50
(lp3557
I698
asg63
(lp3558
sg52
(lp3559
ssS'horsesho'
p3560
(dp3561
g63
(lp3562
I1719
assS'tegmenti'
p3563
(dp3564
g350
(lp3565
I1302
assS'electro'
p3566
(dp3567
g176
(lp3568
I323
assS'tiefensuch'
p3569
(dp3570
g132
(lp3571
I3710
assS'european'
p3572
(dp3573
g344
(lp3574
sg183
(lp3575
sg440
(lp3576
I2495
asg87
(lp3577
ssS'worgott'
p3578
(dp3579
g48
(lp3580
I290
assS'etworl'
p3581
(dp3582
g116
(lp3583
I1807
assS'prognost'
p3584
(dp3585
g277
(lp3586
I3155
assS'refit'
p3587
(dp3588
g30
(lp3589
I1490
assS'highdimension'
p3590
(dp3591
g181
(lp3592
I2283
assS'saida'
p3593
(dp3594
g332
(lp3595
I2638
assS'sjrin'
p3596
(dp3597
g116
(lp3598
I311
assS'refin'
p3599
(dp3600
g116
(lp3601
sg332
(lp3602
sg70
(lp3603
sg85
(lp3604
sg40
(lp3605
sg91
(lp3606
sg132
(lp3607
I2183
assS'tune'
p3608
(dp3609
g216
(lp3610
sg438
(lp3611
I962
asg118
(lp3612
sg332
(lp3613
sg70
(lp3614
sg22
(lp3615
sg80
(lp3616
sg245
(lp3617
sg78
(lp3618
sg59
(lp3619
sg68
(lp3620
sg85
(lp3621
sg303
(lp3622
sg12
(lp3623
sg306
(lp3624
sg223
(lp3625
sg132
(lp3626
sg44
(lp3627
sg149
(lp3628
ssS'sivilotti'
p3629
(dp3630
g174
(lp3631
I2620
assS'lagrang'
p3632
(dp3633
g102
(lp3634
I1180
assS'blegdamsvej'
p3635
(dp3636
g140
(lp3637
I12
asg235
(lp3638
ssS'nonincreas'
p3639
(dp3640
g85
(lp3641
I1366
assS'unchang'
p3642
(dp3643
g4
(lp3644
sg80
(lp3645
sg38
(lp3646
sg102
(lp3647
sg91
(lp3648
sg12
(lp3649
sg14
(lp3650
sg16
(lp3651
I1302
assS'iiopt'
p3652
(dp3653
g36
(lp3654
I1715
assS'ofneurochess'
p3655
(dp3656
g132
(lp3657
I2992
assS'ctiu'
p3658
(dp3659
g87
(lp3660
I1098
assS'ordin'
p3661
(dp3662
g102
(lp3663
I1885
asg32
(lp3664
sg52
(lp3665
sg163
(lp3666
ssS'lransact'
p3667
(dp3668
g76
(lp3669
I3242
assS'birmingham'
p3670
(dp3671
g14
(lp3672
sg16
(lp3673
sg124
(lp3674
sg38
(lp3675
sg138
(lp3676
I281
assS'previous'
p3677
(dp3678
g68
(lp3679
sg26
(lp3680
sg163
(lp3681
sg72
(lp3682
sg281
(lp3683
sg283
(lp3684
sg303
(lp3685
sg287
(lp3686
sg74
(lp3687
sg176
(lp3688
sg145
(lp3689
sg256
(lp3690
sg76
(lp3691
sg118
(lp3692
sg295
(lp3693
sg183
(lp3694
sg59
(lp3695
sg63
(lp3696
sg306
(lp3697
sg89
(lp3698
sg91
(lp3699
sg245
(lp3700
sg94
(lp3701
sg96
(lp3702
sg18
(lp3703
sg99
(lp3704
sg313
(lp3705
sg223
(lp3706
sg149
(lp3707
sg230
(lp3708
sg329
(lp3709
sg293
(lp3710
sg116
(lp3711
sg32
(lp3712
sg429
(lp3713
sg102
(lp3714
sg104
(lp3715
sg106
(lp3716
sg108
(lp3717
sg110
(lp3718
sg178
(lp3719
sg52
(lp3720
sg114
(lp3721
sg216
(lp3722
sg438
(lp3723
I60
asg440
(lp3724
sg332
(lp3725
sg121
(lp3726
sg4
(lp3727
sg6
(lp3728
sg221
(lp3729
sg460
(lp3730
sg124
(lp3731
sg126
(lp3732
sg341
(lp3733
sg10
(lp3734
sg40
(lp3735
sg344
(lp3736
sg128
(lp3737
sg132
(lp3738
sg14
(lp3739
sg135
(lp3740
sg138
(lp3741
sg354
(lp3742
ssS'atrial'
p3743
(dp3744
g135
(lp3745
I259
assS'han'
p3746
(dp3747
g132
(lp3748
I3428
assS'bns'
p3749
(dp3750
g110
(lp3751
I3031
assS'had'
p3752
(dp3753
g68
(lp3754
sg74
(lp3755
sg176
(lp3756
sg80
(lp3757
sg293
(lp3758
sg295
(lp3759
sg183
(lp3760
sg59
(lp3761
sg91
(lp3762
sg94
(lp3763
sg18
(lp3764
sg99
(lp3765
sg44
(lp3766
sg102
(lp3767
sg106
(lp3768
I1091
asg110
(lp3769
sg63
(lp3770
sg114
(lp3771
sg118
(lp3772
sg32
(lp3773
sg121
(lp3774
sg6
(lp3775
sg124
(lp3776
sg344
(lp3777
sg128
(lp3778
sg78
(lp3779
sg132
(lp3780
sg14
(lp3781
sg16
(lp3782
sg135
(lp3783
sg140
(lp3784
sg354
(lp3785
ssS'satellit'
p3786
(dp3787
g281
(lp3788
I1704
assS'hay'
p3789
(dp3790
g341
(lp3791
I1007
assS'bnn'
p3792
(dp3793
g38
(lp3794
I538
assS'hap'
p3795
(dp3796
g174
(lp3797
I2759
assS'myhil'
p3798
(dp3799
g40
(lp3800
I729
assS'har'
p3801
(dp3802
g14
(lp3803
sg16
(lp3804
I256
assS'has'
p3805
(dp3806
g80
(lp3807
sg293
(lp3808
sg344
(lp3809
sg78
(lp3810
sg59
(lp3811
sg484
(lp3812
sg38
(lp3813
sg83
(lp3814
sg85
(lp3815
sg303
(lp3816
sg438
(lp3817
sg34
(lp3818
sg36
(lp3819
sg460
(lp3820
sg68
(lp3821
sg72
(lp3822
sg281
(lp3823
sg10
(lp3824
sg40
(lp3825
sg283
(lp3826
sg70
(lp3827
sg26
(lp3828
sg277
(lp3829
sg163
(lp3830
sg89
(lp3831
sg91
(lp3832
sg12
(lp3833
sg94
(lp3834
sg96
(lp3835
sg48
(lp3836
sg99
(lp3837
sg313
(lp3838
sg44
(lp3839
sg149
(lp3840
sg429
(lp3841
sg102
(lp3842
sg104
(lp3843
sg106
(lp3844
sg108
(lp3845
sg63
(lp3846
sg52
(lp3847
sg114
(lp3848
sg128
(lp3849
sg130
(lp3850
sg132
(lp3851
sg14
(lp3852
sg16
(lp3853
sg135
(lp3854
sg50
(lp3855
sg138
(lp3856
sg140
(lp3857
sg354
(lp3858
sg306
(lp3859
sg87
(lp3860
sg245
(lp3861
sg46
(lp3862
sg20
(lp3863
sg18
(lp3864
sg221
(lp3865
sg535
(lp3866
sg223
(lp3867
sg350
(lp3868
sg216
(lp3869
sg174
(lp3870
sg440
(lp3871
sg332
(lp3872
sg121
(lp3873
sg4
(lp3874
sg6
(lp3875
sg8
(lp3876
sg126
(lp3877
sg341
(lp3878
sg30
(lp3879
sg287
(lp3880
sg74
(lp3881
sg176
(lp3882
sg145
(lp3883
sg256
(lp3884
sg76
(lp3885
sg262
(lp3886
sg295
(lp3887
sg183
(lp3888
sg42
(lp3889
I914
asg230
(lp3890
sg329
(lp3891
sg32
(lp3892
sg318
(lp3893
sg178
(lp3894
sg22
(lp3895
sg181
(lp3896
sg235
(lp3897
sg384
(lp3898
sg124
(lp3899
ssS'hat'
p3900
(dp3901
g59
(lp3902
sg48
(lp3903
I576
asg223
(lp3904
ssS'exploratori'
p3905
(dp3906
g63
(lp3907
I2366
assS'quirk'
p3908
(dp3909
g216
(lp3910
I239
asg80
(lp3911
ssS'preserv'
p3912
(dp3913
g118
(lp3914
sg440
(lp3915
sg181
(lp3916
sg163
(lp3917
sg72
(lp3918
sg102
(lp3919
sg130
(lp3920
sg12
(lp3921
sg50
(lp3922
I1125
asg63
(lp3923
ssS'twinkl'
p3924
(dp3925
g70
(lp3926
I684
assS'birth'
p3927
(dp3928
g116
(lp3929
sg149
(lp3930
I228
assS'shadow'
p3931
(dp3932
g70
(lp3933
I353
assS'measur'
p3934
(dp3935
g124
(lp3936
sg78
(lp3937
sg277
(lp3938
sg163
(lp3939
sg36
(lp3940
sg181
(lp3941
sg74
(lp3942
sg176
(lp3943
sg145
(lp3944
sg256
(lp3945
sg80
(lp3946
sg118
(lp3947
sg295
(lp3948
sg183
(lp3949
sg59
(lp3950
sg484
(lp3951
sg38
(lp3952
sg83
(lp3953
sg85
(lp3954
sg42
(lp3955
I165
asg91
(lp3956
sg12
(lp3957
sg94
(lp3958
sg96
(lp3959
sg18
(lp3960
sg99
(lp3961
sg313
(lp3962
sg223
(lp3963
sg149
(lp3964
sg329
(lp3965
sg460
(lp3966
sg245
(lp3967
sg429
(lp3968
sg68
(lp3969
sg102
(lp3970
sg20
(lp3971
sg52
(lp3972
sg22
(lp3973
sg230
(lp3974
sg438
(lp3975
sg332
(lp3976
sg178
(lp3977
sg4
(lp3978
sg6
(lp3979
sg8
(lp3980
sg34
(lp3981
sg221
(lp3982
sg384
(lp3983
sg235
(lp3984
sg126
(lp3985
sg10
(lp3986
sg40
(lp3987
sg344
(lp3988
sg128
(lp3989
sg130
(lp3990
sg132
(lp3991
sg14
(lp3992
sg16
(lp3993
sg350
(lp3994
sg50
(lp3995
sg138
(lp3996
sg140
(lp3997
sg354
(lp3998
ssS'specif'
p3999
(dp4000
g68
(lp4001
sg70
(lp4002
sg163
(lp4003
sg287
(lp4004
sg74
(lp4005
sg176
(lp4006
sg145
(lp4007
sg76
(lp4008
sg262
(lp4009
sg344
(lp4010
sg78
(lp4011
sg59
(lp4012
sg38
(lp4013
sg85
(lp4014
sg89
(lp4015
sg91
(lp4016
sg12
(lp4017
sg46
(lp4018
sg96
(lp4019
sg48
(lp4020
sg99
(lp4021
sg223
(lp4022
sg350
(lp4023
sg32
(lp4024
sg429
(lp4025
sg104
(lp4026
sg106
(lp4027
I1408
asg110
(lp4028
sg63
(lp4029
sg116
(lp4030
sg118
(lp4031
sg440
(lp4032
sg318
(lp4033
sg4
(lp4034
sg181
(lp4035
sg8
(lp4036
sg34
(lp4037
sg384
(lp4038
sg124
(lp4039
sg126
(lp4040
sg281
(lp4041
sg128
(lp4042
sg130
(lp4043
sg132
(lp4044
sg149
(lp4045
sg50
(lp4046
sg460
(lp4047
sg354
(lp4048
ssS'sebastian'
p4049
(dp4050
g132
(lp4051
I8
asg223
(lp4052
ssS'underli'
p4053
(dp4054
g74
(lp4055
sg48
(lp4056
I624
assS'right'
p4057
(dp4058
g124
(lp4059
sg26
(lp4060
sg163
(lp4061
sg303
(lp4062
sg145
(lp4063
sg76
(lp4064
sg262
(lp4065
sg344
(lp4066
sg183
(lp4067
sg59
(lp4068
sg80
(lp4069
sg85
(lp4070
sg63
(lp4071
sg87
(lp4072
sg89
(lp4073
sg91
(lp4074
sg245
(lp4075
sg94
(lp4076
sg20
(lp4077
sg48
(lp4078
sg221
(lp4079
sg313
(lp4080
sg223
(lp4081
sg149
(lp4082
sg329
(lp4083
sg293
(lp4084
sg32
(lp4085
sg12
(lp4086
sg318
(lp4087
sg110
(lp4088
sg96
(lp4089
sg52
(lp4090
sg114
(lp4091
sg216
(lp4092
sg438
(lp4093
I1261
asg440
(lp4094
sg18
(lp4095
sg181
(lp4096
sg6
(lp4097
sg8
(lp4098
sg36
(lp4099
sg460
(lp4100
sg235
(lp4101
sg118
(lp4102
sg130
(lp4103
sg132
(lp4104
sg350
(lp4105
sg50
(lp4106
sg140
(lp4107
sg354
(lp4108
ssS'old'
p4109
(dp4110
g245
(lp4111
sg34
(lp4112
sg116
(lp4113
sg63
(lp4114
sg354
(lp4115
I1445
assS'ole'
p4116
(dp4117
g20
(lp4118
I706
assS'oli'
p4119
(dp4120
g68
(lp4121
I2931
assS'crown'
p4122
(dp4123
g116
(lp4124
I580
assS'welch'
p4125
(dp4126
g440
(lp4127
I2026
assS'olr'
p4128
(dp4129
g18
(lp4130
I1102
assS'fcs'
p4131
(dp4132
g118
(lp4133
I825
assS'glove'
p4134
(dp4135
g59
(lp4136
I315
assS'vate'
p4137
(dp4138
g295
(lp4139
I2067
asg183
(lp4140
ssS'olx'
p4141
(dp4142
g126
(lp4143
I386
assS'esusl'
p4144
(dp4145
g341
(lp4146
I1529
assS'noc'
p4147
(dp4148
g283
(lp4149
I40
assS'transmiss'
p4150
(dp4151
g102
(lp4152
sg106
(lp4153
I2609
asg72
(lp4154
ssS'fop'
p4155
(dp4156
g277
(lp4157
I705
assS'peripheri'
p4158
(dp4159
g22
(lp4160
I2245
assS'for'
p4161
(dp4162
g80
(lp4163
sg293
(lp4164
sg344
(lp4165
sg78
(lp4166
sg59
(lp4167
sg484
(lp4168
sg38
(lp4169
sg83
(lp4170
sg85
(lp4171
sg303
(lp4172
sg438
(lp4173
sg116
(lp4174
sg118
(lp4175
sg34
(lp4176
sg36
(lp4177
sg460
(lp4178
sg68
(lp4179
sg72
(lp4180
sg281
(lp4181
sg10
(lp4182
sg40
(lp4183
sg283
(lp4184
sg70
(lp4185
sg26
(lp4186
sg277
(lp4187
sg163
(lp4188
sg89
(lp4189
sg91
(lp4190
sg12
(lp4191
sg94
(lp4192
sg96
(lp4193
sg48
(lp4194
sg99
(lp4195
sg313
(lp4196
sg44
(lp4197
sg149
(lp4198
sg429
(lp4199
sg102
(lp4200
sg104
(lp4201
sg106
(lp4202
sg108
(lp4203
sg110
(lp4204
sg63
(lp4205
sg52
(lp4206
sg114
(lp4207
sg128
(lp4208
sg130
(lp4209
sg132
(lp4210
sg14
(lp4211
sg16
(lp4212
sg135
(lp4213
sg50
(lp4214
sg138
(lp4215
sg140
(lp4216
sg354
(lp4217
sg306
(lp4218
sg87
(lp4219
sg245
(lp4220
sg46
(lp4221
sg20
(lp4222
sg18
(lp4223
sg221
(lp4224
sg535
(lp4225
sg223
(lp4226
sg350
(lp4227
sg216
(lp4228
sg174
(lp4229
sg440
(lp4230
sg332
(lp4231
sg121
(lp4232
sg4
(lp4233
sg6
(lp4234
sg8
(lp4235
sg126
(lp4236
sg341
(lp4237
sg30
(lp4238
sg287
(lp4239
sg74
(lp4240
sg176
(lp4241
sg145
(lp4242
sg256
(lp4243
sg76
(lp4244
sg262
(lp4245
sg295
(lp4246
sg183
(lp4247
sg42
(lp4248
I225
asg230
(lp4249
sg329
(lp4250
sg32
(lp4251
sg318
(lp4252
sg178
(lp4253
sg22
(lp4254
sg181
(lp4255
sg235
(lp4256
sg384
(lp4257
sg124
(lp4258
ssS'bottom'
p4259
(dp4260
g70
(lp4261
sg277
(lp4262
sg262
(lp4263
sg295
(lp4264
sg183
(lp4265
sg484
(lp4266
sg83
(lp4267
sg85
(lp4268
sg42
(lp4269
I2360
asg12
(lp4270
sg48
(lp4271
sg178
(lp4272
sg52
(lp4273
sg22
(lp4274
sg116
(lp4275
sg118
(lp4276
sg318
(lp4277
sg121
(lp4278
sg4
(lp4279
sg181
(lp4280
sg235
(lp4281
sg124
(lp4282
sg138
(lp4283
sg140
(lp4284
ssS'nen'
p4285
(dp4286
g59
(lp4287
I3385
assS'fox'
p4288
(dp4289
g74
(lp4290
I3208
assS'subclass'
p4291
(dp4292
g42
(lp4293
I3306
asg344
(lp4294
sg40
(lp4295
ssS'foa'
p4296
(dp4297
g8
(lp4298
I724
assS'unipi'
p4299
(dp4300
g44
(lp4301
I45
assS'fol'
p4302
(dp4303
g230
(lp4304
I1352
assS'condit'
p4305
(dp4306
g283
(lp4307
sg70
(lp4308
sg277
(lp4309
sg72
(lp4310
sg281
(lp4311
sg36
(lp4312
sg40
(lp4313
sg30
(lp4314
sg287
(lp4315
sg74
(lp4316
sg176
(lp4317
sg145
(lp4318
sg76
(lp4319
sg262
(lp4320
sg460
(lp4321
sg78
(lp4322
sg38
(lp4323
sg85
(lp4324
sg303
(lp4325
sg306
(lp4326
sg87
(lp4327
sg91
(lp4328
sg245
(lp4329
sg46
(lp4330
sg96
(lp4331
sg48
(lp4332
sg99
(lp4333
sg313
(lp4334
sg329
(lp4335
sg32
(lp4336
sg102
(lp4337
sg178
(lp4338
sg106
(lp4339
sg108
(lp4340
sg110
(lp4341
sg52
(lp4342
sg230
(lp4343
sg438
(lp4344
I1076
asg440
(lp4345
sg318
(lp4346
sg121
(lp4347
sg4
(lp4348
sg181
(lp4349
sg34
(lp4350
sg221
(lp4351
sg384
(lp4352
sg124
(lp4353
sg126
(lp4354
sg341
(lp4355
sg535
(lp4356
sg128
(lp4357
sg130
(lp4358
sg14
(lp4359
sg16
(lp4360
sg135
(lp4361
sg138
(lp4362
sg354
(lp4363
ssS'bottou'
p4364
(dp4365
g183
(lp4366
I4289
assS'sensibl'
p4367
(dp4368
g26
(lp4369
sg34
(lp4370
sg124
(lp4371
sg72
(lp4372
sg89
(lp4373
sg138
(lp4374
I2778
assS'annal'
p4375
(dp4376
g72
(lp4377
sg126
(lp4378
I2949
asg281
(lp4379
sg293
(lp4380
ssS'arbeiten'
p4381
(dp4382
g34
(lp4383
I2921
assS'stefan'
p4384
(dp4385
g295
(lp4386
sg183
(lp4387
sg34
(lp4388
sg313
(lp4389
I2052
assS'caricatur'
p4390
(dp4391
g262
(lp4392
I227
assS'cynad'
p4393
(dp4394
g176
(lp4395
I2593
assS'dentat'
p4396
(dp4397
g106
(lp4398
I2574
assS'colleg'
p4399
(dp4400
g176
(lp4401
sg4
(lp4402
sg163
(lp4403
sg295
(lp4404
sg183
(lp4405
sg384
(lp4406
sg128
(lp4407
sg78
(lp4408
sg106
(lp4409
I299
assS'bruhat'
p4410
(dp4411
g32
(lp4412
I1266
assS'tyjl'
p4413
(dp4414
g130
(lp4415
I2769
assS'culat'
p4416
(dp4417
g130
(lp4418
I1985
assS'o'
p4419
(dp4420
g80
(lp4421
sg293
(lp4422
sg344
(lp4423
sg78
(lp4424
sg59
(lp4425
sg484
(lp4426
sg38
(lp4427
sg83
(lp4428
sg85
(lp4429
sg303
(lp4430
sg438
(lp4431
sg116
(lp4432
sg34
(lp4433
sg36
(lp4434
sg460
(lp4435
sg68
(lp4436
sg72
(lp4437
sg281
(lp4438
sg10
(lp4439
sg40
(lp4440
sg283
(lp4441
sg70
(lp4442
sg26
(lp4443
sg277
(lp4444
sg163
(lp4445
sg89
(lp4446
sg91
(lp4447
sg12
(lp4448
sg94
(lp4449
sg96
(lp4450
sg48
(lp4451
sg99
(lp4452
sg313
(lp4453
sg44
(lp4454
sg149
(lp4455
sg429
(lp4456
sg102
(lp4457
sg104
(lp4458
sg106
(lp4459
sg108
(lp4460
sg110
(lp4461
sg63
(lp4462
sg52
(lp4463
sg114
(lp4464
sg128
(lp4465
sg130
(lp4466
sg132
(lp4467
sg14
(lp4468
sg16
(lp4469
sg135
(lp4470
sg138
(lp4471
sg140
(lp4472
sg354
(lp4473
sg306
(lp4474
sg87
(lp4475
sg245
(lp4476
sg46
(lp4477
sg20
(lp4478
sg18
(lp4479
sg221
(lp4480
sg535
(lp4481
sg223
(lp4482
sg350
(lp4483
sg216
(lp4484
sg174
(lp4485
sg332
(lp4486
sg121
(lp4487
sg4
(lp4488
sg6
(lp4489
sg8
(lp4490
sg126
(lp4491
sg341
(lp4492
sg30
(lp4493
sg287
(lp4494
sg74
(lp4495
sg176
(lp4496
sg145
(lp4497
sg256
(lp4498
sg76
(lp4499
sg262
(lp4500
sg295
(lp4501
sg183
(lp4502
sg42
(lp4503
I1027
asg230
(lp4504
sg329
(lp4505
sg32
(lp4506
sg318
(lp4507
sg178
(lp4508
sg22
(lp4509
sg181
(lp4510
sg235
(lp4511
sg384
(lp4512
sg124
(lp4513
ssS'collett'
p4514
(dp4515
g80
(lp4516
I946
assS'systern'
p4517
(dp4518
g350
(lp4519
I1177
assS'coars'
p4520
(dp4521
g178
(lp4522
sg181
(lp4523
sg293
(lp4524
sg8
(lp4525
I888
asg85
(lp4526
sg70
(lp4527
sg63
(lp4528
ssS'saitama'
p4529
(dp4530
g36
(lp4531
I50
assS'movshon'
p4532
(dp4533
g6
(lp4534
I2208
assS'epigenesi'
p4535
(dp4536
g116
(lp4537
I2488
assS'soc'
p4538
(dp4539
g174
(lp4540
sg74
(lp4541
sg176
(lp4542
sg70
(lp4543
sg26
(lp4544
sg293
(lp4545
sg72
(lp4546
sg245
(lp4547
sg104
(lp4548
sg48
(lp4549
sg99
(lp4550
I3242
assS'som'
p4551
(dp4552
g48
(lp4553
I267
assS'soo'
p4554
(dp4555
g59
(lp4556
I2516
assS'son'
p4557
(dp4558
g102
(lp4559
I3624
asg281
(lp4560
sg63
(lp4561
ssS'creativ'
p4562
(dp4563
g332
(lp4564
I268
assS'sos'
p4565
(dp4566
g178
(lp4567
I21
assS'sou'
p4568
(dp4569
g235
(lp4570
I2682
assS'delvin'
p4571
(dp4572
g89
(lp4573
I1354
assS'novo'
p4574
(dp4575
g26
(lp4576
I3221
assS'east'
p4577
(dp4578
g104
(lp4579
I16
asg163
(lp4580
sg80
(lp4581
sg52
(lp4582
sg78
(lp4583
ssS'fabric'
p4584
(dp4585
g438
(lp4586
I2336
asg20
(lp4587
sg256
(lp4588
sg10
(lp4589
sg14
(lp4590
ssS'suffici'
p4591
(dp4592
g124
(lp4593
sg70
(lp4594
sg277
(lp4595
sg281
(lp4596
sg30
(lp4597
sg176
(lp4598
sg293
(lp4599
sg344
(lp4600
sg78
(lp4601
sg59
(lp4602
sg38
(lp4603
sg83
(lp4604
sg85
(lp4605
sg303
(lp4606
sg42
(lp4607
I466
asg87
(lp4608
sg94
(lp4609
sg48
(lp4610
sg99
(lp4611
sg535
(lp4612
sg230
(lp4613
sg118
(lp4614
sg68
(lp4615
sg106
(lp4616
sg108
(lp4617
sg110
(lp4618
sg63
(lp4619
sg216
(lp4620
sg438
(lp4621
sg440
(lp4622
sg318
(lp4623
sg178
(lp4624
sg4
(lp4625
sg8
(lp4626
sg36
(lp4627
sg235
(lp4628
sg126
(lp4629
sg341
(lp4630
sg10
(lp4631
sg128
(lp4632
sg132
(lp4633
sg138
(lp4634
sg354
(lp4635
ssS'panorama'
p4636
(dp4637
g42
(lp4638
I2889
assS'support'
p4639
(dp4640
g68
(lp4641
sg26
(lp4642
sg277
(lp4643
sg281
(lp4644
sg283
(lp4645
sg181
(lp4646
sg287
(lp4647
sg145
(lp4648
sg256
(lp4649
sg80
(lp4650
sg344
(lp4651
sg78
(lp4652
sg59
(lp4653
sg484
(lp4654
sg38
(lp4655
sg83
(lp4656
sg303
(lp4657
sg306
(lp4658
sg89
(lp4659
sg91
(lp4660
sg12
(lp4661
sg94
(lp4662
sg20
(lp4663
sg48
(lp4664
sg313
(lp4665
sg223
(lp4666
sg149
(lp4667
sg230
(lp4668
sg118
(lp4669
sg429
(lp4670
sg318
(lp4671
sg46
(lp4672
sg178
(lp4673
sg106
(lp4674
sg110
(lp4675
sg52
(lp4676
sg22
(lp4677
sg216
(lp4678
sg438
(lp4679
I2365
asg440
(lp4680
sg18
(lp4681
sg121
(lp4682
sg4
(lp4683
sg6
(lp4684
sg8
(lp4685
sg36
(lp4686
sg124
(lp4687
sg72
(lp4688
sg341
(lp4689
sg10
(lp4690
sg40
(lp4691
sg130
(lp4692
sg14
(lp4693
sg16
(lp4694
sg350
(lp4695
sg50
(lp4696
sg140
(lp4697
ssS'lfymax'
p4698
(dp4699
g85
(lp4700
I2400
assS'avail'
p4701
(dp4702
g70
(lp4703
sg26
(lp4704
sg277
(lp4705
sg104
(lp4706
sg287
(lp4707
sg74
(lp4708
sg80
(lp4709
sg76
(lp4710
sg293
(lp4711
sg344
(lp4712
sg78
(lp4713
sg484
(lp4714
sg83
(lp4715
sg63
(lp4716
sg87
(lp4717
sg94
(lp4718
sg20
(lp4719
sg48
(lp4720
sg221
(lp4721
sg223
(lp4722
sg350
(lp4723
sg178
(lp4724
sg108
(lp4725
sg96
(lp4726
sg114
(lp4727
sg216
(lp4728
sg438
(lp4729
I921
asg318
(lp4730
sg121
(lp4731
sg22
(lp4732
sg235
(lp4733
sg36
(lp4734
sg460
(lp4735
sg124
(lp4736
sg126
(lp4737
sg281
(lp4738
sg10
(lp4739
sg132
(lp4740
sg14
(lp4741
sg16
(lp4742
sg138
(lp4743
ssS'width'
p4744
(dp4745
g176
(lp4746
sg22
(lp4747
sg6
(lp4748
sg163
(lp4749
sg384
(lp4750
sg124
(lp4751
sg85
(lp4752
sg42
(lp4753
I2017
asg245
(lp4754
sg68
(lp4755
sg12
(lp4756
sg135
(lp4757
sg63
(lp4758
sg44
(lp4759
sg149
(lp4760
ssS'joseph'
p4761
(dp4762
g114
(lp4763
sg149
(lp4764
I17
assS'acknowledg'
p4765
(dp4766
g68
(lp4767
sg26
(lp4768
sg277
(lp4769
sg72
(lp4770
sg281
(lp4771
sg283
(lp4772
sg30
(lp4773
sg287
(lp4774
sg74
(lp4775
sg176
(lp4776
sg145
(lp4777
sg256
(lp4778
sg76
(lp4779
sg344
(lp4780
sg59
(lp4781
sg484
(lp4782
sg38
(lp4783
sg83
(lp4784
sg85
(lp4785
sg63
(lp4786
sg306
(lp4787
sg89
(lp4788
sg91
(lp4789
sg12
(lp4790
sg94
(lp4791
sg96
(lp4792
sg48
(lp4793
sg313
(lp4794
sg44
(lp4795
sg149
(lp4796
sg230
(lp4797
sg118
(lp4798
sg116
(lp4799
sg32
(lp4800
sg350
(lp4801
sg429
(lp4802
sg318
(lp4803
sg46
(lp4804
sg178
(lp4805
sg110
(lp4806
sg20
(lp4807
sg52
(lp4808
sg216
(lp4809
sg174
(lp4810
sg440
(lp4811
sg18
(lp4812
sg121
(lp4813
sg181
(lp4814
sg6
(lp4815
sg36
(lp4816
sg124
(lp4817
sg126
(lp4818
sg341
(lp4819
sg10
(lp4820
sg128
(lp4821
sg130
(lp4822
sg132
(lp4823
sg14
(lp4824
sg16
(lp4825
sg135
(lp4826
sg50
(lp4827
sg138
(lp4828
sg140
(lp4829
sg354
(lp4830
I2884
assS'overhead'
p4831
(dp4832
g34
(lp4833
sg283
(lp4834
sg89
(lp4835
I2103
asg22
(lp4836
sg10
(lp4837
ssS'subtend'
p4838
(dp4839
g80
(lp4840
I226
assS'fpga'
p4841
(dp4842
g10
(lp4843
I461
assS'offer'
p4844
(dp4845
g283
(lp4846
sg4
(lp4847
sg262
(lp4848
sg295
(lp4849
sg183
(lp4850
sg59
(lp4851
sg293
(lp4852
sg10
(lp4853
sg12
(lp4854
sg306
(lp4855
sg128
(lp4856
sg132
(lp4857
sg14
(lp4858
sg16
(lp4859
I243
asg114
(lp4860
sg344
(lp4861
sg313
(lp4862
sg22
(lp4863
ssS'recommend'
p4864
(dp4865
g91
(lp4866
I2030
assS'sperduli'
p4867
(dp4868
g44
(lp4869
I2304
assS'linearinput'
p4870
(dp4871
g40
(lp4872
I2580
assS'subgoal'
p4873
(dp4874
g460
(lp4875
I367
assS'irnxm'
p4876
(dp4877
g46
(lp4878
I2209
assS'irnxn'
p4879
(dp4880
g46
(lp4881
I2206
assS'dynarn'
p4882
(dp4883
g350
(lp4884
I736
assS'myopic'
p4885
(dp4886
g91
(lp4887
I1863
assS'reiter'
p4888
(dp4889
g106
(lp4890
I2445
asg318
(lp4891
ssS'mber'
p4892
(dp4893
g99
(lp4894
I1822
assS'ijc'
p4895
(dp4896
g283
(lp4897
I466
assS'kohlus'
p4898
(dp4899
g283
(lp4900
I247
assS'grror'
p4901
(dp4902
g85
(lp4903
I4281
assS'beauvoi'
p4904
(dp4905
g332
(lp4906
I634
assS'lwe'
p4907
(dp4908
g36
(lp4909
I1384
assS'grimson'
p4910
(dp4911
g30
(lp4912
sg318
(lp4913
sg138
(lp4914
I1451
assS'tpresent'
p4915
(dp4916
g106
(lp4917
I308
assS'unregular'
p4918
(dp4919
g221
(lp4920
I363
assS'levenbergmarquardt'
p4921
(dp4922
g8
(lp4923
I1528
assS'khz'
p4924
(dp4925
g116
(lp4926
sg174
(lp4927
sg440
(lp4928
sg22
(lp4929
sg14
(lp4930
sg16
(lp4931
I1771
assS'later'
p4932
(dp4933
g70
(lp4934
sg26
(lp4935
sg277
(lp4936
sg303
(lp4937
sg30
(lp4938
sg74
(lp4939
sg176
(lp4940
sg256
(lp4941
sg118
(lp4942
sg344
(lp4943
sg183
(lp4944
sg59
(lp4945
sg85
(lp4946
sg63
(lp4947
sg306
(lp4948
sg12
(lp4949
sg94
(lp4950
sg18
(lp4951
sg99
(lp4952
sg535
(lp4953
sg44
(lp4954
sg350
(lp4955
sg174
(lp4956
sg429
(lp4957
sg102
(lp4958
sg104
(lp4959
sg106
(lp4960
sg178
(lp4961
sg22
(lp4962
sg116
(lp4963
sg438
(lp4964
I455
asg32
(lp4965
sg332
(lp4966
sg121
(lp4967
sg4
(lp4968
sg36
(lp4969
sg78
(lp4970
sg132
(lp4971
sg149
(lp4972
sg50
(lp4973
sg140
(lp4974
sg354
(lp4975
ssS'qii'
p4976
(dp4977
g38
(lp4978
I739
assS'sight'
p4979
(dp4980
g183
(lp4981
I4678
assS'qik'
p4982
(dp4983
g38
(lp4984
I711
assS'qij'
p4985
(dp4986
g26
(lp4987
I2306
assS'proven'
p4988
(dp4989
g318
(lp4990
sg460
(lp4991
sg306
(lp4992
sg91
(lp4993
I186
asg46
(lp4994
sg114
(lp4995
ssS'exist'
p4996
(dp4997
g68
(lp4998
sg70
(lp4999
sg26
(lp5000
sg277
(lp5001
sg72
(lp5002
sg281
(lp5003
sg287
(lp5004
sg74
(lp5005
sg145
(lp5006
sg80
(lp5007
sg293
(lp5008
sg295
(lp5009
sg183
(lp5010
sg484
(lp5011
sg85
(lp5012
sg303
(lp5013
sg42
(lp5014
I620
asg306
(lp5015
sg87
(lp5016
sg91
(lp5017
sg245
(lp5018
sg46
(lp5019
sg48
(lp5020
sg313
(lp5021
sg350
(lp5022
sg329
(lp5023
sg12
(lp5024
sg332
(lp5025
sg110
(lp5026
sg22
(lp5027
sg230
(lp5028
sg438
(lp5029
sg318
(lp5030
sg178
(lp5031
sg181
(lp5032
sg6
(lp5033
sg8
(lp5034
sg34
(lp5035
sg36
(lp5036
sg235
(lp5037
sg126
(lp5038
sg341
(lp5039
sg10
(lp5040
sg40
(lp5041
sg128
(lp5042
sg78
(lp5043
sg50
(lp5044
ssS'floor'
p5045
(dp5046
g135
(lp5047
I619
asg83
(lp5048
sg80
(lp5049
ssS'pasadena'
p5050
(dp5051
g216
(lp5052
sg256
(lp5053
sg6
(lp5054
I48
asg68
(lp5055
sg40
(lp5056
sg221
(lp5057
ssS'role'
p5058
(dp5059
g74
(lp5060
sg145
(lp5061
sg80
(lp5062
sg295
(lp5063
sg183
(lp5064
sg38
(lp5065
sg42
(lp5066
I1578
asg245
(lp5067
sg20
(lp5068
sg223
(lp5069
sg149
(lp5070
sg12
(lp5071
sg102
(lp5072
sg104
(lp5073
sg110
(lp5074
sg116
(lp5075
sg118
(lp5076
sg318
(lp5077
sg4
(lp5078
sg6
(lp5079
sg460
(lp5080
sg124
(lp5081
sg126
(lp5082
ssS'leen'
p5083
(dp5084
g30
(lp5085
sg116
(lp5086
sg440
(lp5087
sg318
(lp5088
sg50
(lp5089
I297
asg460
(lp5090
sg74
(lp5091
sg303
(lp5092
sg132
(lp5093
sg221
(lp5094
sg83
(lp5095
ssS'presum'
p5096
(dp5097
g74
(lp5098
sg6
(lp5099
sg181
(lp5100
sg344
(lp5101
sg83
(lp5102
sg303
(lp5103
sg99
(lp5104
I666
asg44
(lp5105
ssS'roll'
p5106
(dp5107
g22
(lp5108
sg149
(lp5109
I812
assS'runtim'
p5110
(dp5111
g145
(lp5112
I2151
assS'pwout'
p5113
(dp5114
g14
(lp5115
I3656
assS'intend'
p5116
(dp5117
g295
(lp5118
sg183
(lp5119
sg59
(lp5120
sg124
(lp5121
sg354
(lp5122
I71
assS'devot'
p5123
(dp5124
g85
(lp5125
sg121
(lp5126
sg181
(lp5127
sg140
(lp5128
I258
asg26
(lp5129
ssS'intens'
p5130
(dp5131
g438
(lp5132
I1060
asg440
(lp5133
sg318
(lp5134
sg256
(lp5135
sg181
(lp5136
sg8
(lp5137
sg34
(lp5138
sg293
(lp5139
sg83
(lp5140
sg10
(lp5141
sg118
(lp5142
sg245
(lp5143
sg283
(lp5144
sg91
(lp5145
sg132
(lp5146
sg14
(lp5147
sg16
(lp5148
sg135
(lp5149
sg138
(lp5150
sg174
(lp5151
ssS'intent'
p5152
(dp5153
g42
(lp5154
I2289
asg74
(lp5155
sg262
(lp5156
ssS'shibata'
p5157
(dp5158
g20
(lp5159
I13
assS'hurt'
p5160
(dp5161
g132
(lp5162
I2033
assS'annulus'
p5163
(dp5164
g118
(lp5165
I286
assS'bmft'
p5166
(dp5167
g59
(lp5168
sg354
(lp5169
I2903
assS'pepyn'
p5170
(dp5171
g83
(lp5172
I2775
assS'berlin'
p5173
(dp5174
g116
(lp5175
sg287
(lp5176
sg178
(lp5177
sg256
(lp5178
sg34
(lp5179
sg36
(lp5180
sg59
(lp5181
sg132
(lp5182
I3429
assS'geometr'
p5183
(dp5184
g32
(lp5185
sg80
(lp5186
sg34
(lp5187
sg36
(lp5188
sg460
(lp5189
sg126
(lp5190
sg14
(lp5191
sg16
(lp5192
sg110
(lp5193
sg138
(lp5194
I1505
asg44
(lp5195
sg38
(lp5196
ssS'time'
p5197
(dp5198
g80
(lp5199
sg293
(lp5200
sg344
(lp5201
sg78
(lp5202
sg59
(lp5203
sg484
(lp5204
sg38
(lp5205
sg83
(lp5206
sg85
(lp5207
sg303
(lp5208
sg438
(lp5209
sg116
(lp5210
sg118
(lp5211
sg34
(lp5212
sg36
(lp5213
sg460
(lp5214
sg68
(lp5215
sg10
(lp5216
sg40
(lp5217
sg283
(lp5218
sg70
(lp5219
sg26
(lp5220
sg277
(lp5221
sg163
(lp5222
sg91
(lp5223
sg12
(lp5224
sg94
(lp5225
sg96
(lp5226
sg48
(lp5227
sg99
(lp5228
sg44
(lp5229
sg149
(lp5230
sg102
(lp5231
sg104
(lp5232
sg106
(lp5233
sg108
(lp5234
sg110
(lp5235
sg63
(lp5236
sg52
(lp5237
sg114
(lp5238
sg128
(lp5239
sg132
(lp5240
sg14
(lp5241
sg16
(lp5242
sg135
(lp5243
sg50
(lp5244
sg138
(lp5245
sg140
(lp5246
sg306
(lp5247
sg87
(lp5248
sg245
(lp5249
sg46
(lp5250
sg20
(lp5251
sg18
(lp5252
sg535
(lp5253
sg223
(lp5254
sg350
(lp5255
sg216
(lp5256
sg174
(lp5257
sg440
(lp5258
sg332
(lp5259
sg121
(lp5260
sg4
(lp5261
sg6
(lp5262
sg8
(lp5263
sg126
(lp5264
sg341
(lp5265
sg30
(lp5266
sg287
(lp5267
sg74
(lp5268
sg176
(lp5269
sg145
(lp5270
sg256
(lp5271
sg76
(lp5272
sg262
(lp5273
sg295
(lp5274
sg183
(lp5275
sg42
(lp5276
I1355
asg230
(lp5277
sg329
(lp5278
sg318
(lp5279
sg178
(lp5280
sg22
(lp5281
sg181
(lp5282
sg235
(lp5283
sg384
(lp5284
sg124
(lp5285
ssS'push'
p5286
(dp5287
g83
(lp5288
sg6
(lp5289
I1622
assS'nxn'
p5290
(dp5291
g130
(lp5292
I237
assS'chain'
p5293
(dp5294
g74
(lp5295
sg26
(lp5296
sg181
(lp5297
sg293
(lp5298
sg460
(lp5299
sg124
(lp5300
sg91
(lp5301
sg130
(lp5302
sg132
(lp5303
sg354
(lp5304
I79
assS'reverber'
p5305
(dp5306
g216
(lp5307
I2198
assS'oss'
p5308
(dp5309
g341
(lp5310
I259
assS'osr'
p5311
(dp5312
g281
(lp5313
I2217
assS'ost'
p5314
(dp5315
g132
(lp5316
I1297
assS'chair'
p5317
(dp5318
g181
(lp5319
sg140
(lp5320
I268
assS'millisecond'
p5321
(dp5322
g68
(lp5323
sg70
(lp5324
sg52
(lp5325
sg6
(lp5326
I104
asg350
(lp5327
ssS'decid'
p5328
(dp5329
g4
(lp5330
sg277
(lp5331
sg293
(lp5332
sg295
(lp5333
sg183
(lp5334
sg344
(lp5335
sg87
(lp5336
sg138
(lp5337
I1204
asg114
(lp5338
ssS'herebi'
p5339
(dp5340
g384
(lp5341
sg221
(lp5342
I2154
assS'ver'
p5343
(dp5344
g256
(lp5345
I254
assS'sectton'
p5346
(dp5347
g230
(lp5348
I2039
assS'substitut'
p5349
(dp5350
g42
(lp5351
I2898
asg87
(lp5352
sg89
(lp5353
sg128
(lp5354
sg12
(lp5355
sg96
(lp5356
sg63
(lp5357
sg114
(lp5358
ssS'vex'
p5359
(dp5360
g46
(lp5361
I1712
asg535
(lp5362
ssS'vee'
p5363
(dp5364
g12
(lp5365
I1856
assS'burlington'
p5366
(dp5367
g281
(lp5368
I23
assS'decis'
p5369
(dp5370
g277
(lp5371
sg145
(lp5372
sg293
(lp5373
sg344
(lp5374
sg78
(lp5375
sg484
(lp5376
sg83
(lp5377
sg306
(lp5378
sg87
(lp5379
sg89
(lp5380
sg91
(lp5381
sg94
(lp5382
sg221
(lp5383
sg429
(lp5384
sg104
(lp5385
sg108
(lp5386
sg63
(lp5387
sg52
(lp5388
sg230
(lp5389
sg8
(lp5390
sg460
(lp5391
sg126
(lp5392
sg183
(lp5393
sg354
(lp5394
I56
assS'phoeb'
p5395
(dp5396
g116
(lp5397
I2469
assS'ven'
p5398
(dp5399
g344
(lp5400
I3186
assS'larg'
p5401
(dp5402
g124
(lp5403
sg70
(lp5404
sg277
(lp5405
sg163
(lp5406
sg72
(lp5407
sg281
(lp5408
sg283
(lp5409
sg85
(lp5410
sg36
(lp5411
sg181
(lp5412
sg303
(lp5413
sg287
(lp5414
sg74
(lp5415
sg176
(lp5416
sg145
(lp5417
sg256
(lp5418
sg262
(lp5419
sg295
(lp5420
sg183
(lp5421
sg59
(lp5422
sg484
(lp5423
sg38
(lp5424
sg83
(lp5425
sg114
(lp5426
sg63
(lp5427
sg42
(lp5428
I3424
asg306
(lp5429
sg87
(lp5430
sg89
(lp5431
sg91
(lp5432
sg12
(lp5433
sg94
(lp5434
sg96
(lp5435
sg48
(lp5436
sg99
(lp5437
sg535
(lp5438
sg44
(lp5439
sg149
(lp5440
sg230
(lp5441
sg329
(lp5442
sg293
(lp5443
sg178
(lp5444
sg429
(lp5445
sg68
(lp5446
sg46
(lp5447
sg102
(lp5448
sg104
(lp5449
sg108
(lp5450
sg110
(lp5451
sg20
(lp5452
sg52
(lp5453
sg22
(lp5454
sg216
(lp5455
sg438
(lp5456
sg118
(lp5457
sg18
(lp5458
sg121
(lp5459
sg4
(lp5460
sg6
(lp5461
sg8
(lp5462
sg34
(lp5463
sg221
(lp5464
sg460
(lp5465
sg235
(lp5466
sg126
(lp5467
sg341
(lp5468
sg10
(lp5469
sg344
(lp5470
sg223
(lp5471
sg128
(lp5472
sg130
(lp5473
sg132
(lp5474
sg14
(lp5475
sg16
(lp5476
sg135
(lp5477
sg50
(lp5478
sg138
(lp5479
sg140
(lp5480
sg354
(lp5481
ssS'vel'
p5482
(dp5483
g89
(lp5484
I1191
asg256
(lp5485
ssS'vej'
p5486
(dp5487
g230
(lp5488
I1808
assS'multivalu'
p5489
(dp5490
g20
(lp5491
I2438
assS'spati'
p5492
(dp5493
g256
(lp5494
I1593
assS'pek'
p5495
(dp5496
g68
(lp5497
I2819
assS'lrm'
p5498
(dp5499
g306
(lp5500
I1868
assS'redish'
p5501
(dp5502
g80
(lp5503
I14
assS'exact'
p5504
(dp5505
g68
(lp5506
sg70
(lp5507
sg145
(lp5508
sg262
(lp5509
sg295
(lp5510
sg183
(lp5511
sg484
(lp5512
sg38
(lp5513
sg83
(lp5514
sg85
(lp5515
sg303
(lp5516
sg42
(lp5517
I2870
asg306
(lp5518
sg89
(lp5519
sg91
(lp5520
sg245
(lp5521
sg99
(lp5522
sg313
(lp5523
sg350
(lp5524
sg460
(lp5525
sg12
(lp5526
sg429
(lp5527
sg104
(lp5528
sg63
(lp5529
sg22
(lp5530
sg329
(lp5531
sg32
(lp5532
sg318
(lp5533
sg4
(lp5534
sg235
(lp5535
sg34
(lp5536
sg384
(lp5537
sg124
(lp5538
sg72
(lp5539
sg281
(lp5540
sg344
(lp5541
sg130
(lp5542
sg132
(lp5543
sg14
(lp5544
sg16
(lp5545
sg138
(lp5546
sg140
(lp5547
sg354
(lp5548
ssS'semicimtlu'
p5549
(dp5550
g350
(lp5551
I1882
assS'licklid'
p5552
(dp5553
g174
(lp5554
I518
assS'jntplllo'
p5555
(dp5556
g245
(lp5557
I2512
assS'leftov'
p5558
(dp5559
g484
(lp5560
I555
assS'kitso'
p5561
(dp5562
g354
(lp5563
I2968
assS'opstal'
p5564
(dp5565
g32
(lp5566
I416
assS'handzel'
p5567
(dp5568
g32
(lp5569
sg350
(lp5570
I2456
assS'auizona'
p5571
(dp5572
g104
(lp5573
I20
assS'unaccount'
p5574
(dp5575
g74
(lp5576
I2618
assS'exiwij'
p5577
(dp5578
g116
(lp5579
I1141
assS'prevent'
p5580
(dp5581
g176
(lp5582
sg70
(lp5583
sg277
(lp5584
sg181
(lp5585
sg235
(lp5586
sg295
(lp5587
sg183
(lp5588
sg68
(lp5589
sg38
(lp5590
sg303
(lp5591
sg42
(lp5592
I2656
asg102
(lp5593
sg89
(lp5594
sg132
(lp5595
sg223
(lp5596
sg245
(lp5597
ssS'attim'
p5598
(dp5599
g132
(lp5600
I1380
assS'compens'
p5601
(dp5602
g295
(lp5603
sg183
(lp5604
sg59
(lp5605
sg484
(lp5606
sg126
(lp5607
sg89
(lp5608
sg14
(lp5609
sg16
(lp5610
I1912
asg99
(lp5611
sg350
(lp5612
ssS'sign'
p5613
(dp5614
g287
(lp5615
sg32
(lp5616
sg176
(lp5617
sg145
(lp5618
sg262
(lp5619
sg295
(lp5620
sg183
(lp5621
sg68
(lp5622
sg40
(lp5623
sg34
(lp5624
sg91
(lp5625
sg12
(lp5626
sg135
(lp5627
I905
asg344
(lp5628
ssS'bg'
p5629
(dp5630
g230
(lp5631
sg46
(lp5632
I2199
assS'gomi'
p5633
(dp5634
g99
(lp5635
I3413
assS'marroquin'
p5636
(dp5637
g8
(lp5638
I2567
assS'lyngbi'
p5639
(dp5640
g26
(lp5641
sg140
(lp5642
I24
assS'current'
p5643
(dp5644
g68
(lp5645
sg70
(lp5646
sg78
(lp5647
sg277
(lp5648
sg293
(lp5649
sg283
(lp5650
sg460
(lp5651
sg26
(lp5652
sg287
(lp5653
sg74
(lp5654
sg256
(lp5655
sg80
(lp5656
sg262
(lp5657
sg295
(lp5658
sg183
(lp5659
sg59
(lp5660
sg484
(lp5661
sg38
(lp5662
sg83
(lp5663
sg303
(lp5664
sg42
(lp5665
I2485
asg306
(lp5666
sg87
(lp5667
sg89
(lp5668
sg91
(lp5669
sg245
(lp5670
sg94
(lp5671
sg20
(lp5672
sg48
(lp5673
sg99
(lp5674
sg313
(lp5675
sg44
(lp5676
sg118
(lp5677
sg174
(lp5678
sg18
(lp5679
sg32
(lp5680
sg318
(lp5681
sg104
(lp5682
sg106
(lp5683
sg108
(lp5684
sg110
(lp5685
sg178
(lp5686
sg52
(lp5687
sg116
(lp5688
sg438
(lp5689
sg440
(lp5690
sg332
(lp5691
sg121
(lp5692
sg4
(lp5693
sg6
(lp5694
sg8
(lp5695
sg221
(lp5696
sg384
(lp5697
sg124
(lp5698
sg72
(lp5699
sg10
(lp5700
sg535
(lp5701
sg344
(lp5702
sg223
(lp5703
sg128
(lp5704
sg130
(lp5705
sg132
(lp5706
sg14
(lp5707
sg16
(lp5708
sg135
(lp5709
sg138
(lp5710
sg354
(lp5711
ssS'beasley'
p5712
(dp5713
g176
(lp5714
I2617
assS'fifti'
p5715
(dp5716
g183
(lp5717
sg128
(lp5718
I2313
assS'boost'
p5719
(dp5720
g344
(lp5721
sg183
(lp5722
sg223
(lp5723
I484
asg40
(lp5724
ssS'agreement'
p5725
(dp5726
g438
(lp5727
I2270
asg48
(lp5728
sg22
(lp5729
sg8
(lp5730
sg36
(lp5731
sg384
(lp5732
sg329
(lp5733
sg332
(lp5734
sg38
(lp5735
sg130
(lp5736
sg12
(lp5737
sg14
(lp5738
sg16
(lp5739
sg135
(lp5740
sg140
(lp5741
sg350
(lp5742
ssS'poujaud'
p5743
(dp5744
g50
(lp5745
I1644
assS'modif'
p5746
(dp5747
g438
(lp5748
I653
asg4
(lp5749
sg118
(lp5750
sg460
(lp5751
sg38
(lp5752
sg535
(lp5753
sg223
(lp5754
sg104
(lp5755
sg14
(lp5756
sg106
(lp5757
sg16
(lp5758
sg46
(lp5759
sg354
(lp5760
ssS'address'
p5761
(dp5762
g283
(lp5763
sg80
(lp5764
sg293
(lp5765
sg344
(lp5766
sg42
(lp5767
I1372
asg306
(lp5768
sg12
(lp5769
sg48
(lp5770
sg313
(lp5771
sg223
(lp5772
sg230
(lp5773
sg118
(lp5774
sg102
(lp5775
sg106
(lp5776
sg63
(lp5777
sg52
(lp5778
sg22
(lp5779
sg216
(lp5780
sg438
(lp5781
sg318
(lp5782
sg4
(lp5783
sg235
(lp5784
sg36
(lp5785
sg10
(lp5786
sg44
(lp5787
sg130
(lp5788
sg14
(lp5789
sg16
(lp5790
sg138
(lp5791
sg140
(lp5792
sg354
(lp5793
ssS'along'
p5794
(dp5795
g78
(lp5796
sg26
(lp5797
sg80
(lp5798
sg293
(lp5799
sg183
(lp5800
sg59
(lp5801
sg484
(lp5802
sg87
(lp5803
sg46
(lp5804
sg48
(lp5805
sg99
(lp5806
sg313
(lp5807
sg44
(lp5808
sg350
(lp5809
sg429
(lp5810
sg332
(lp5811
sg102
(lp5812
sg230
(lp5813
sg329
(lp5814
sg32
(lp5815
sg318
(lp5816
sg8
(lp5817
sg34
(lp5818
sg384
(lp5819
sg68
(lp5820
sg341
(lp5821
sg128
(lp5822
sg130
(lp5823
sg14
(lp5824
sg16
(lp5825
sg138
(lp5826
I876
assS'bk'
p5827
(dp5828
g230
(lp5829
I1662
assS'gggi'
p5830
(dp5831
g130
(lp5832
I1549
assS'benson'
p5833
(dp5834
g14
(lp5835
I4720
asg4
(lp5836
ssS'appoxim'
p5837
(dp5838
g149
(lp5839
I1225
assS'epanenchnikov'
p5840
(dp5841
g96
(lp5842
I901
assS'queue'
p5843
(dp5844
g306
(lp5845
I2443
asg83
(lp5846
ssS'vectori'
p5847
(dp5848
g216
(lp5849
I2309
assS'throughput'
p5850
(dp5851
g63
(lp5852
sg10
(lp5853
I1838
assS'bp'
p5854
(dp5855
g72
(lp5856
I2070
assS'funder'
p5857
(dp5858
g72
(lp5859
I2092
assS'durbin'
p5860
(dp5861
g26
(lp5862
sg8
(lp5863
sg12
(lp5864
sg48
(lp5865
sg50
(lp5866
sg138
(lp5867
I3240
assS'ourselv'
p5868
(dp5869
g163
(lp5870
sg384
(lp5871
sg68
(lp5872
sg10
(lp5873
sg235
(lp5874
sg104
(lp5875
I278
assS'utdi'
p5876
(dp5877
g256
(lp5878
I967
assS'santa'
p5879
(dp5880
g344
(lp5881
sg341
(lp5882
I19
assS'prefer'
p5883
(dp5884
g216
(lp5885
sg438
(lp5886
I429
asg329
(lp5887
sg4
(lp5888
sg6
(lp5889
sg8
(lp5890
sg344
(lp5891
sg183
(lp5892
sg80
(lp5893
sg116
(lp5894
sg83
(lp5895
sg277
(lp5896
sg110
(lp5897
sg12
(lp5898
sg20
(lp5899
sg48
(lp5900
sg50
(lp5901
sg138
(lp5902
sg223
(lp5903
sg149
(lp5904
ssS'challand'
p5905
(dp5906
g118
(lp5907
I323
assS'naccach'
p5908
(dp5909
g63
(lp5910
I1507
assS'seattl'
p5911
(dp5912
g12
(lp5913
I2697
asg318
(lp5914
sg178
(lp5915
ssS'saccad'
p5916
(dp5917
g32
(lp5918
sg178
(lp5919
sg6
(lp5920
I1713
asg303
(lp5921
sg245
(lp5922
sg350
(lp5923
ssS'instal'
p5924
(dp5925
g78
(lp5926
I2802
assS'myoglobin'
p5927
(dp5928
g130
(lp5929
I1745
assS'spatiotempor'
p5930
(dp5931
g535
(lp5932
I509
asg256
(lp5933
ssS'mumford'
p5934
(dp5935
g70
(lp5936
I2584
assS'alcd'
p5937
(dp5938
g10
(lp5939
I2711
assS'increas'
p5940
(dp5941
g124
(lp5942
sg78
(lp5943
sg163
(lp5944
sg281
(lp5945
sg283
(lp5946
sg181
(lp5947
sg26
(lp5948
sg74
(lp5949
sg176
(lp5950
sg145
(lp5951
sg256
(lp5952
sg262
(lp5953
sg295
(lp5954
sg183
(lp5955
sg59
(lp5956
sg484
(lp5957
sg38
(lp5958
sg85
(lp5959
sg303
(lp5960
sg42
(lp5961
I178
asg87
(lp5962
sg91
(lp5963
sg12
(lp5964
sg94
(lp5965
sg20
(lp5966
sg18
(lp5967
sg99
(lp5968
sg535
(lp5969
sg44
(lp5970
sg350
(lp5971
sg118
(lp5972
sg329
(lp5973
sg32
(lp5974
sg245
(lp5975
sg429
(lp5976
sg318
(lp5977
sg46
(lp5978
sg102
(lp5979
sg178
(lp5980
sg106
(lp5981
sg108
(lp5982
sg110
(lp5983
sg63
(lp5984
sg52
(lp5985
sg22
(lp5986
sg174
(lp5987
sg440
(lp5988
sg332
(lp5989
sg121
(lp5990
sg4
(lp5991
sg6
(lp5992
sg8
(lp5993
sg34
(lp5994
sg221
(lp5995
sg460
(lp5996
sg235
(lp5997
sg126
(lp5998
sg341
(lp5999
sg10
(lp6000
sg40
(lp6001
sg223
(lp6002
sg128
(lp6003
sg36
(lp6004
sg132
(lp6005
sg14
(lp6006
sg135
(lp6007
sg138
(lp6008
sg140
(lp6009
sg354
(lp6010
ssS'scope'
p6011
(dp6012
g230
(lp6013
sg318
(lp6014
sg178
(lp6015
sg22
(lp6016
sg14
(lp6017
I4274
asg108
(lp6018
sg110
(lp6019
ssS'mitchison'
p6020
(dp6021
g12
(lp6022
sg48
(lp6023
sg50
(lp6024
I1605
assS'speciiic'
p6025
(dp6026
g42
(lp6027
I2010
assS'rehovot'
p6028
(dp6029
g32
(lp6030
sg223
(lp6031
I3235
assS'peopl'
p6032
(dp6033
g293
(lp6034
sg83
(lp6035
sg12
(lp6036
I2151
asg94
(lp6037
sg63
(lp6038
sg223
(lp6039
sg114
(lp6040
ssS'oregon'
p6041
(dp6042
g440
(lp6043
I2082
assS'enhanc'
p6044
(dp6045
g216
(lp6046
sg174
(lp6047
sg70
(lp6048
sg332
(lp6049
sg145
(lp6050
sg181
(lp6051
sg36
(lp6052
sg38
(lp6053
sg10
(lp6054
sg14
(lp6055
sg106
(lp6056
I46
asg20
(lp6057
sg99
(lp6058
sg16
(lp6059
ssS'visual'
p6060
(dp6061
g70
(lp6062
sg277
(lp6063
sg30
(lp6064
sg80
(lp6065
sg118
(lp6066
sg183
(lp6067
sg59
(lp6068
sg484
(lp6069
sg303
(lp6070
sg42
(lp6071
I2232
asg245
(lp6072
sg48
(lp6073
sg99
(lp6074
sg223
(lp6075
sg149
(lp6076
sg174
(lp6077
sg12
(lp6078
sg429
(lp6079
sg318
(lp6080
sg104
(lp6081
sg106
(lp6082
sg108
(lp6083
sg63
(lp6084
sg52
(lp6085
sg114
(lp6086
sg216
(lp6087
sg438
(lp6088
sg32
(lp6089
sg18
(lp6090
sg178
(lp6091
sg6
(lp6092
sg181
(lp6093
sg8
(lp6094
sg293
(lp6095
sg130
(lp6096
ssS'appendix'
p6097
(dp6098
g121
(lp6099
sg135
(lp6100
I2526
asg89
(lp6101
ssS'steadyst'
p6102
(dp6103
g176
(lp6104
I1183
assS'behalf'
p6105
(dp6106
g10
(lp6107
I1611
assS'eonelud'
p6108
(dp6109
g63
(lp6110
I1209
assS'strain'
p6111
(dp6112
g104
(lp6113
I370
assS'sayonori'
p6114
(dp6115
g116
(lp6116
I2470
assS'whatev'
p6117
(dp6118
g94
(lp6119
I2759
asg59
(lp6120
ssS'franc'
p6121
(dp6122
g344
(lp6123
sg287
(lp6124
I23
assS'problemat'
p6125
(dp6126
g59
(lp6127
sg89
(lp6128
I2250
assS'munrol'
p6129
(dp6130
g438
(lp6131
I65
assS'preattent'
p6132
(dp6133
g332
(lp6134
I440
assS'retransform'
p6135
(dp6136
g59
(lp6137
I1390
assS'recycl'
p6138
(dp6139
g68
(lp6140
I1092
assS'oculornotor'
p6141
(dp6142
g350
(lp6143
I1176
assS'neurobiologyo'
p6144
(dp6145
g149
(lp6146
I3134
assS'fpe'
p6147
(dp6148
g262
(lp6149
I927
assS'winter'
p6150
(dp6151
g104
(lp6152
I11
assS'azarbayejani'
p6153
(dp6154
g293
(lp6155
I3320
assS'ssu'
p6156
(dp6157
g230
(lp6158
I7
assS'sibl'
p6159
(dp6160
g429
(lp6161
I817
assS'parameter'
p6162
(dp6163
g30
(lp6164
sg329
(lp6165
sg32
(lp6166
sg124
(lp6167
sg181
(lp6168
sg235
(lp6169
sg295
(lp6170
sg183
(lp6171
sg130
(lp6172
sg429
(lp6173
sg306
(lp6174
sg46
(lp6175
sg14
(lp6176
sg16
(lp6177
I890
asg163
(lp6178
sg96
(lp6179
ssS'edinburgh'
p6180
(dp6181
g14
(lp6182
I2695
asg235
(lp6183
ssS'remap'
p6184
(dp6185
g440
(lp6186
I1
assS'vhgfg'
p6187
(dp6188
g178
(lp6189
I1413
assS'stuart'
p6190
(dp6191
g163
(lp6192
I825
assS'spot'
p6193
(dp6194
g114
(lp6195
sg80
(lp6196
sg149
(lp6197
I1345
assS'applicationl'
p6198
(dp6199
g83
(lp6200
I2900
assS'telemekhanika'
p6201
(dp6202
g306
(lp6203
I2994
assS'cardiac'
p6204
(dp6205
g135
(lp6206
I53
assS'such'
p6207
(dp6208
g344
(lp6209
sg329
(lp6210
sg70
(lp6211
sg78
(lp6212
sg277
(lp6213
sg72
(lp6214
sg68
(lp6215
sg293
(lp6216
sg281
(lp6217
sg283
(lp6218
sg85
(lp6219
sg40
(lp6220
sg26
(lp6221
sg30
(lp6222
sg350
(lp6223
sg176
(lp6224
sg145
(lp6225
sg80
(lp6226
sg76
(lp6227
sg118
(lp6228
sg295
(lp6229
sg183
(lp6230
sg59
(lp6231
sg484
(lp6232
sg38
(lp6233
sg83
(lp6234
sg114
(lp6235
sg124
(lp6236
sg42
(lp6237
I190
asg306
(lp6238
sg87
(lp6239
sg89
(lp6240
sg91
(lp6241
sg12
(lp6242
sg94
(lp6243
sg96
(lp6244
sg48
(lp6245
sg99
(lp6246
sg313
(lp6247
sg44
(lp6248
sg149
(lp6249
sg303
(lp6250
sg174
(lp6251
sg18
(lp6252
sg32
(lp6253
sg245
(lp6254
sg429
(lp6255
sg318
(lp6256
sg46
(lp6257
sg102
(lp6258
sg104
(lp6259
sg108
(lp6260
sg110
(lp6261
sg20
(lp6262
sg52
(lp6263
sg22
(lp6264
sg230
(lp6265
sg438
(lp6266
sg440
(lp6267
sg332
(lp6268
sg178
(lp6269
sg4
(lp6270
sg181
(lp6271
sg8
(lp6272
sg36
(lp6273
sg460
(lp6274
sg235
(lp6275
sg126
(lp6276
sg341
(lp6277
sg10
(lp6278
sg535
(lp6279
sg287
(lp6280
sg63
(lp6281
sg223
(lp6282
sg128
(lp6283
sg130
(lp6284
sg132
(lp6285
sg14
(lp6286
sg16
(lp6287
sg135
(lp6288
sg50
(lp6289
sg138
(lp6290
sg140
(lp6291
sg354
(lp6292
ssS'data'
p6293
(dp6294
g124
(lp6295
sg70
(lp6296
sg78
(lp6297
sg277
(lp6298
sg163
(lp6299
sg72
(lp6300
sg281
(lp6301
sg283
(lp6302
sg85
(lp6303
sg36
(lp6304
sg303
(lp6305
sg26
(lp6306
sg30
(lp6307
sg287
(lp6308
sg74
(lp6309
sg176
(lp6310
sg256
(lp6311
sg76
(lp6312
sg118
(lp6313
sg295
(lp6314
sg183
(lp6315
sg59
(lp6316
sg484
(lp6317
sg83
(lp6318
sg114
(lp6319
sg63
(lp6320
sg42
(lp6321
I942
asg306
(lp6322
sg87
(lp6323
sg80
(lp6324
sg91
(lp6325
sg12
(lp6326
sg94
(lp6327
sg96
(lp6328
sg48
(lp6329
sg99
(lp6330
sg313
(lp6331
sg44
(lp6332
sg350
(lp6333
sg116
(lp6334
sg329
(lp6335
sg460
(lp6336
sg245
(lp6337
sg429
(lp6338
sg318
(lp6339
sg46
(lp6340
sg104
(lp6341
sg106
(lp6342
sg108
(lp6343
sg110
(lp6344
sg20
(lp6345
sg22
(lp6346
sg230
(lp6347
sg174
(lp6348
sg440
(lp6349
sg332
(lp6350
sg121
(lp6351
sg4
(lp6352
sg6
(lp6353
sg8
(lp6354
sg34
(lp6355
sg221
(lp6356
sg384
(lp6357
sg235
(lp6358
sg126
(lp6359
sg341
(lp6360
sg10
(lp6361
sg40
(lp6362
sg344
(lp6363
sg223
(lp6364
sg128
(lp6365
sg130
(lp6366
sg132
(lp6367
sg14
(lp6368
sg16
(lp6369
sg135
(lp6370
sg50
(lp6371
sg138
(lp6372
sg140
(lp6373
sg354
(lp6374
ssS'eombin'
p6375
(dp6376
g63
(lp6377
I1195
assS'harmon'
p6378
(dp6379
g116
(lp6380
sg344
(lp6381
sg22
(lp6382
I62
assS'stress'
p6383
(dp6384
g94
(lp6385
I217
asg178
(lp6386
sg104
(lp6387
ssS'fallsiel'
p6388
(dp6389
g108
(lp6390
I217
assS'schurman'
p6391
(dp6392
g163
(lp6393
I309
assS'backpropag'
p6394
(dp6395
g277
(lp6396
sg287
(lp6397
sg76
(lp6398
sg295
(lp6399
sg183
(lp6400
sg83
(lp6401
sg85
(lp6402
sg89
(lp6403
sg94
(lp6404
sg99
(lp6405
sg223
(lp6406
sg178
(lp6407
sg108
(lp6408
sg114
(lp6409
sg121
(lp6410
sg4
(lp6411
sg34
(lp6412
sg124
(lp6413
sg126
(lp6414
sg341
(lp6415
sg10
(lp6416
sg344
(lp6417
sg44
(lp6418
sg78
(lp6419
sg138
(lp6420
I1975
assS'sr'
p6421
(dp6422
g176
(lp6423
I750
assS'sq'
p6424
(dp6425
g85
(lp6426
I4291
assS'qikl'
p6427
(dp6428
g38
(lp6429
I672
assS'sv'
p6430
(dp6431
g14
(lp6432
I3413
assS'su'
p6433
(dp6434
g42
(lp6435
I2375
asg440
(lp6436
sg135
(lp6437
sg110
(lp6438
sg32
(lp6439
ssS'st'
p6440
(dp6441
g174
(lp6442
sg108
(lp6443
sg121
(lp6444
sg76
(lp6445
sg6
(lp6446
sg8
(lp6447
sg344
(lp6448
sg38
(lp6449
sg10
(lp6450
sg91
(lp6451
sg132
(lp6452
sg104
(lp6453
sg20
(lp6454
sg135
(lp6455
I255
asg99
(lp6456
ssS'quadrant'
p6457
(dp6458
g14
(lp6459
sg16
(lp6460
sg135
(lp6461
I962
asg460
(lp6462
sg108
(lp6463
ssS'sj'
p6464
(dp6465
g26
(lp6466
sg36
(lp6467
sg91
(lp6468
sg102
(lp6469
sg104
(lp6470
sg106
(lp6471
I1231
asg535
(lp6472
ssS'si'
p6473
(dp6474
g26
(lp6475
sg72
(lp6476
sg42
(lp6477
I1299
asg87
(lp6478
sg104
(lp6479
sg132
(lp6480
sg14
(lp6481
sg106
(lp6482
sg535
(lp6483
ssS'tailor'
p6484
(dp6485
g96
(lp6486
I259
asg283
(lp6487
ssS'so'
p6488
(dp6489
g329
(lp6490
sg70
(lp6491
sg78
(lp6492
sg277
(lp6493
sg163
(lp6494
sg72
(lp6495
sg68
(lp6496
sg80
(lp6497
sg283
(lp6498
sg40
(lp6499
sg26
(lp6500
sg30
(lp6501
sg287
(lp6502
sg74
(lp6503
sg176
(lp6504
sg145
(lp6505
sg256
(lp6506
sg76
(lp6507
sg262
(lp6508
sg295
(lp6509
sg183
(lp6510
sg59
(lp6511
sg484
(lp6512
sg38
(lp6513
sg83
(lp6514
sg85
(lp6515
sg63
(lp6516
sg42
(lp6517
I791
asg306
(lp6518
sg87
(lp6519
sg89
(lp6520
sg91
(lp6521
sg12
(lp6522
sg94
(lp6523
sg20
(lp6524
sg48
(lp6525
sg99
(lp6526
sg313
(lp6527
sg44
(lp6528
sg149
(lp6529
sg118
(lp6530
sg303
(lp6531
sg174
(lp6532
sg18
(lp6533
sg32
(lp6534
sg350
(lp6535
sg429
(lp6536
sg318
(lp6537
sg46
(lp6538
sg102
(lp6539
sg104
(lp6540
sg178
(lp6541
sg52
(lp6542
sg114
(lp6543
sg230
(lp6544
sg438
(lp6545
sg440
(lp6546
sg332
(lp6547
sg121
(lp6548
sg22
(lp6549
sg6
(lp6550
sg235
(lp6551
sg34
(lp6552
sg221
(lp6553
sg384
(lp6554
sg124
(lp6555
sg126
(lp6556
sg10
(lp6557
sg535
(lp6558
sg344
(lp6559
sg223
(lp6560
sg128
(lp6561
sg36
(lp6562
sg132
(lp6563
sg14
(lp6564
sg16
(lp6565
sg135
(lp6566
sg50
(lp6567
sg138
(lp6568
sg140
(lp6569
ssS'sn'
p6570
(dp6571
g102
(lp6572
I2305
asg38
(lp6573
ssS'sm'
p6574
(dp6575
g102
(lp6576
I1147
assS'sl'
p6577
(dp6578
g74
(lp6579
sg121
(lp6580
sg183
(lp6581
sg59
(lp6582
sg102
(lp6583
sg14
(lp6584
I3988
assS'sc'
p6585
(dp6586
g102
(lp6587
I1112
asg118
(lp6588
sg72
(lp6589
sg256
(lp6590
ssS'sb'
p6591
(dp6592
g38
(lp6593
I1631
assS'sa'
p6594
(dp6595
g42
(lp6596
I1243
asg178
(lp6597
sg235
(lp6598
ssS'sf'
p6599
(dp6600
g132
(lp6601
I614
asg12
(lp6602
sg216
(lp6603
ssS'se'
p6604
(dp6605
g4
(lp6606
sg6
(lp6607
sg78
(lp6608
sg110
(lp6609
sg40
(lp6610
sg130
(lp6611
I2395
asg18
(lp6612
sg99
(lp6613
sg114
(lp6614
ssS'sd'
p6615
(dp6616
g174
(lp6617
sg96
(lp6618
I213
asg6
(lp6619
ssS'jllne'
p6620
(dp6621
g108
(lp6622
I2549
assS'inessenti'
p6623
(dp6624
g102
(lp6625
I902
assS'kong'
p6626
(dp6627
g72
(lp6628
I21
assS'tto'
p6629
(dp6630
g176
(lp6631
I1825
assS'noteworthi'
p6632
(dp6633
g46
(lp6634
I3093
assS'disconnect'
p6635
(dp6636
g108
(lp6637
I1959
asg63
(lp6638
ssS'djt'
p6639
(dp6640
g230
(lp6641
I1418
assS'threelay'
p6642
(dp6643
g36
(lp6644
I506
assS'iliq'
p6645
(dp6646
g99
(lp6647
I1029
assS'jin'
p6648
(dp6649
g38
(lp6650
I402
assS'jij'
p6651
(dp6652
g384
(lp6653
I467
assS'jii'
p6654
(dp6655
g99
(lp6656
I1037
assS'aviv'
p6657
(dp6658
g145
(lp6659
I17
assS'iliit'
p6660
(dp6661
g306
(lp6662
I2297
assS'djk'
p6663
(dp6664
g87
(lp6665
I32
assS'isscc'
p6666
(dp6667
g20
(lp6668
sg135
(lp6669
I2542
assS'xkli'
p6670
(dp6671
g221
(lp6672
I505
assS'hop'
p6673
(dp6674
g429
(lp6675
sg145
(lp6676
I3014
assS'mexampl'
p6677
(dp6678
g85
(lp6679
I994
assS'xkle'
p6680
(dp6681
g221
(lp6682
I501
assS'nation'
p6683
(dp6684
g216
(lp6685
sg230
(lp6686
sg74
(lp6687
sg8
(lp6688
sg183
(lp6689
sg384
(lp6690
sg484
(lp6691
sg72
(lp6692
sg281
(lp6693
sg40
(lp6694
sg36
(lp6695
sg106
(lp6696
I2492
asg354
(lp6697
sg110
(lp6698
sg223
(lp6699
sg149
(lp6700
ssS'retrain'
p6701
(dp6702
g313
(lp6703
I893
asg76
(lp6704
ssS'therebi'
p6705
(dp6706
g70
(lp6707
sg332
(lp6708
sg145
(lp6709
sg384
(lp6710
sg124
(lp6711
sg126
(lp6712
sg10
(lp6713
sg303
(lp6714
sg46
(lp6715
sg14
(lp6716
sg16
(lp6717
sg140
(lp6718
I2925
asg149
(lp6719
ssS'symposium'
p6720
(dp6721
g287
(lp6722
sg256
(lp6723
sg34
(lp6724
sg72
(lp6725
sg344
(lp6726
sg14
(lp6727
sg16
(lp6728
I2596
assS'erlbaum'
p6729
(dp6730
g132
(lp6731
sg438
(lp6732
I2427
asg116
(lp6733
sg83
(lp6734
sg114
(lp6735
ssS'inen'
p6736
(dp6737
g38
(lp6738
I1675
assS'whabpab'
p6739
(dp6740
g178
(lp6741
I1417
assS'margoliash'
p6742
(dp6743
g116
(lp6744
I569
assS'scotomata'
p6745
(dp6746
g48
(lp6747
I618
assS'revers'
p6748
(dp6749
g110
(lp6750
sg4
(lp6751
sg163
(lp6752
sg256
(lp6753
sg85
(lp6754
sg89
(lp6755
sg91
(lp6756
sg12
(lp6757
sg46
(lp6758
sg106
(lp6759
I1865
asg22
(lp6760
sg99
(lp6761
sg149
(lp6762
ssS'revert'
p6763
(dp6764
g50
(lp6765
I1063
assS'separ'
p6766
(dp6767
g26
(lp6768
sg277
(lp6769
sg72
(lp6770
sg30
(lp6771
sg176
(lp6772
sg80
(lp6773
sg76
(lp6774
sg293
(lp6775
sg295
(lp6776
sg183
(lp6777
sg59
(lp6778
sg484
(lp6779
sg63
(lp6780
sg87
(lp6781
sg94
(lp6782
sg99
(lp6783
sg313
(lp6784
sg44
(lp6785
sg332
(lp6786
sg104
(lp6787
sg106
(lp6788
I178
asg178
(lp6789
sg52
(lp6790
sg114
(lp6791
sg116
(lp6792
sg118
(lp6793
sg318
(lp6794
sg121
(lp6795
sg22
(lp6796
sg181
(lp6797
sg235
(lp6798
sg221
(lp6799
sg124
(lp6800
sg126
(lp6801
sg341
(lp6802
sg40
(lp6803
sg130
(lp6804
sg132
(lp6805
sg14
(lp6806
sg16
(lp6807
sg135
(lp6808
sg50
(lp6809
sg138
(lp6810
sg140
(lp6811
ssS'viation'
p6812
(dp6813
g295
(lp6814
I2101
asg183
(lp6815
ssS'reintroduc'
p6816
(dp6817
g50
(lp6818
I441
assS'compil'
p6819
(dp6820
g10
(lp6821
sg74
(lp6822
sg138
(lp6823
I170
asg223
(lp6824
ssS'bellar'
p6825
(dp6826
g145
(lp6827
I2999
assS'rolloutcost'
p6828
(dp6829
g89
(lp6830
I2578
assS'neighbourhood'
p6831
(dp6832
g332
(lp6833
I1327
asg283
(lp6834
sg52
(lp6835
ssS'hcr'
p6836
(dp6837
g163
(lp6838
I795
assS'ovtarget'
p6839
(dp6840
g132
(lp6841
I1344
assS'hct'
p6842
(dp6843
g85
(lp6844
I1972
assS'blu'
p6845
(dp6846
g341
(lp6847
I1077
assS'fu'
p6848
(dp6849
g72
(lp6850
I2550
assS'lagrangian'
p6851
(dp6852
g8
(lp6853
I2621
assS'ltillti'
p6854
(dp6855
g440
(lp6856
I502
assS'internet'
p6857
(dp6858
g80
(lp6859
I34
assS'ble'
p6860
(dp6861
g344
(lp6862
I2726
assS'formula'
p6863
(dp6864
g174
(lp6865
sg32
(lp6866
sg22
(lp6867
sg72
(lp6868
sg281
(lp6869
sg42
(lp6870
I1055
asg130
(lp6871
sg535
(lp6872
sg140
(lp6873
ssS'sheffield'
p6874
(dp6875
g174
(lp6876
I2477
asg332
(lp6877
ssS'million'
p6878
(dp6879
g87
(lp6880
sg140
(lp6881
I320
asg14
(lp6882
sg16
(lp6883
sg52
(lp6884
sg114
(lp6885
ssS'envelop'
p6886
(dp6887
g174
(lp6888
I1005
asg63
(lp6889
ssS'postfilt'
p6890
(dp6891
g22
(lp6892
I1245
assS'unpublish'
p6893
(dp6894
g438
(lp6895
I2399
asg74
(lp6896
sg350
(lp6897
ssS'unpredict'
p6898
(dp6899
g104
(lp6900
sg70
(lp6901
sg99
(lp6902
I1117
assS'mansqur'
p6903
(dp6904
g145
(lp6905
I263
assS'rtest'
p6906
(dp6907
g36
(lp6908
I1936
assS'torqu'
p6909
(dp6910
g99
(lp6911
I297
assS'recod'
p6912
(dp6913
g174
(lp6914
I120
assS'kaufmann'
p6915
(dp6916
g26
(lp6917
sg30
(lp6918
sg287
(lp6919
sg256
(lp6920
sg293
(lp6921
sg295
(lp6922
sg183
(lp6923
sg306
(lp6924
sg89
(lp6925
sg91
(lp6926
sg94
(lp6927
sg20
(lp6928
sg48
(lp6929
sg313
(lp6930
sg223
(lp6931
sg178
(lp6932
sg174
(lp6933
sg440
(lp6934
sg318
(lp6935
sg121
(lp6936
sg34
(lp6937
sg124
(lp6938
sg126
(lp6939
sg344
(lp6940
sg44
(lp6941
sg130
(lp6942
sg132
(lp6943
sg50
(lp6944
sg138
(lp6945
sg140
(lp6946
sg354
(lp6947
I2957
assS'caruana'
p6948
(dp6949
g277
(lp6950
sg223
(lp6951
I3175
assS'modest'
p6952
(dp6953
g484
(lp6954
sg26
(lp6955
sg181
(lp6956
I1017
assS'unavoid'
p6957
(dp6958
g318
(lp6959
I2415
assS'recov'
p6960
(dp6961
g118
(lp6962
sg74
(lp6963
I529
asg145
(lp6964
sg277
(lp6965
sg262
(lp6966
sg303
(lp6967
ssS'gordon'
p6968
(dp6969
g132
(lp6970
I3518
asg110
(lp6971
sg306
(lp6972
sg89
(lp6973
ssS'perfonn'
p6974
(dp6975
g110
(lp6976
I265
assS'nicknam'
p6977
(dp6978
g59
(lp6979
I1069
assS'neglect'
p6980
(dp6981
g176
(lp6982
sg262
(lp6983
sg38
(lp6984
sg460
(lp6985
sg126
(lp6986
sg303
(lp6987
sg108
(lp6988
I717
assS'okabayashi'
p6989
(dp6990
g14
(lp6991
sg16
(lp6992
I2616
assS'oper'
p6993
(dp6994
g68
(lp6995
sg70
(lp6996
sg283
(lp6997
sg104
(lp6998
sg30
(lp6999
sg176
(lp7000
sg145
(lp7001
sg256
(lp7002
sg293
(lp7003
sg344
(lp7004
sg78
(lp7005
sg59
(lp7006
sg83
(lp7007
sg303
(lp7008
sg42
(lp7009
I1797
asg306
(lp7010
sg87
(lp7011
sg245
(lp7012
sg46
(lp7013
sg20
(lp7014
sg114
(lp7015
sg221
(lp7016
sg350
(lp7017
sg102
(lp7018
sg178
(lp7019
sg63
(lp7020
sg52
(lp7021
sg22
(lp7022
sg230
(lp7023
sg174
(lp7024
sg32
(lp7025
sg318
(lp7026
sg121
(lp7027
sg6
(lp7028
sg181
(lp7029
sg124
(lp7030
sg10
(lp7031
sg14
(lp7032
sg135
(lp7033
ssS'onl'
p7034
(dp7035
g108
(lp7036
I526
assS'onm'
p7037
(dp7038
g38
(lp7039
I1032
assS'onb'
p7040
(dp7041
g108
(lp7042
I537
assS'onc'
p7043
(dp7044
g70
(lp7045
sg277
(lp7046
sg30
(lp7047
sg145
(lp7048
sg344
(lp7049
sg183
(lp7050
sg59
(lp7051
sg484
(lp7052
sg38
(lp7053
sg85
(lp7054
sg42
(lp7055
I695
asg306
(lp7056
sg87
(lp7057
sg89
(lp7058
sg94
(lp7059
sg18
(lp7060
sg99
(lp7061
sg223
(lp7062
sg329
(lp7063
sg104
(lp7064
sg63
(lp7065
sg52
(lp7066
sg114
(lp7067
sg230
(lp7068
sg438
(lp7069
sg178
(lp7070
sg34
(lp7071
sg78
(lp7072
sg132
(lp7073
sg14
(lp7074
sg135
(lp7075
ssS'neuroinformatik'
p7076
(dp7077
g59
(lp7078
I19
assS'one'
p7079
(dp7080
g80
(lp7081
sg293
(lp7082
sg344
(lp7083
sg78
(lp7084
sg59
(lp7085
sg484
(lp7086
sg38
(lp7087
sg83
(lp7088
sg85
(lp7089
sg303
(lp7090
sg438
(lp7091
sg116
(lp7092
sg118
(lp7093
sg34
(lp7094
sg36
(lp7095
sg460
(lp7096
sg68
(lp7097
sg72
(lp7098
sg281
(lp7099
sg10
(lp7100
sg40
(lp7101
sg283
(lp7102
sg70
(lp7103
sg26
(lp7104
sg277
(lp7105
sg163
(lp7106
sg89
(lp7107
sg91
(lp7108
sg12
(lp7109
sg94
(lp7110
sg96
(lp7111
sg48
(lp7112
sg99
(lp7113
sg313
(lp7114
sg44
(lp7115
sg149
(lp7116
sg429
(lp7117
sg102
(lp7118
sg104
(lp7119
sg106
(lp7120
sg108
(lp7121
sg110
(lp7122
sg63
(lp7123
sg52
(lp7124
sg114
(lp7125
sg128
(lp7126
sg132
(lp7127
sg14
(lp7128
sg16
(lp7129
sg135
(lp7130
sg50
(lp7131
sg138
(lp7132
sg140
(lp7133
sg354
(lp7134
sg306
(lp7135
sg87
(lp7136
sg245
(lp7137
sg46
(lp7138
sg20
(lp7139
sg18
(lp7140
sg221
(lp7141
sg535
(lp7142
sg223
(lp7143
sg350
(lp7144
sg216
(lp7145
sg174
(lp7146
sg440
(lp7147
sg332
(lp7148
sg121
(lp7149
sg4
(lp7150
sg6
(lp7151
sg8
(lp7152
sg126
(lp7153
sg341
(lp7154
sg30
(lp7155
sg287
(lp7156
sg74
(lp7157
sg176
(lp7158
sg145
(lp7159
sg256
(lp7160
sg76
(lp7161
sg295
(lp7162
sg183
(lp7163
sg42
(lp7164
I854
asg230
(lp7165
sg329
(lp7166
sg32
(lp7167
sg318
(lp7168
sg178
(lp7169
sg22
(lp7170
sg181
(lp7171
sg235
(lp7172
sg384
(lp7173
sg124
(lp7174
ssS'textual'
p7175
(dp7176
g94
(lp7177
I6
assS'symmetri'
p7178
(dp7179
g26
(lp7180
sg126
(lp7181
sg384
(lp7182
sg38
(lp7183
sg130
(lp7184
I252
asg12
(lp7185
sg48
(lp7186
ssS'ajczk'
p7187
(dp7188
g102
(lp7189
I1161
assS'open'
p7190
(dp7191
g230
(lp7192
sg438
(lp7193
I1046
asg256
(lp7194
sg181
(lp7195
sg287
(lp7196
sg344
(lp7197
sg68
(lp7198
sg72
(lp7199
sg341
(lp7200
sg83
(lp7201
sg80
(lp7202
sg132
(lp7203
sg106
(lp7204
sg223
(lp7205
sg350
(lp7206
ssS'accurac'
p7207
(dp7208
g4
(lp7209
I1489
assS'ont'
p7210
(dp7211
g124
(lp7212
I30
assS'breviti'
p7213
(dp7214
g104
(lp7215
sg108
(lp7216
I1080
asg313
(lp7217
ssS'convent'
p7218
(dp7219
g230
(lp7220
sg74
(lp7221
sg108
(lp7222
sg235
(lp7223
sg126
(lp7224
sg83
(lp7225
sg10
(lp7226
sg87
(lp7227
sg89
(lp7228
sg52
(lp7229
sg14
(lp7230
sg16
(lp7231
sg135
(lp7232
sg50
(lp7233
I53
asg223
(lp7234
ssS'concerv'
p7235
(dp7236
g163
(lp7237
I2216
assS'citi'
p7238
(dp7239
g138
(lp7240
I2001
asg181
(lp7241
sg26
(lp7242
ssS'fmm'
p7243
(dp7244
g128
(lp7245
I1529
assS'draft'
p7246
(dp7247
g124
(lp7248
I3019
asg110
(lp7249
ssS'cimiz'
p7250
(dp7251
g318
(lp7252
I2976
assS'pmap'
p7253
(dp7254
g30
(lp7255
I1658
assS'conveni'
p7256
(dp7257
g344
(lp7258
sg32
(lp7259
sg176
(lp7260
sg26
(lp7261
sg235
(lp7262
sg295
(lp7263
sg183
(lp7264
sg68
(lp7265
sg74
(lp7266
sg85
(lp7267
sg102
(lp7268
sg110
(lp7269
sg12
(lp7270
sg104
(lp7271
sg96
(lp7272
sg18
(lp7273
sg50
(lp7274
I1321
asg63
(lp7275
ssS'cite'
p7276
(dp7277
g4
(lp7278
I492
assS'forthcom'
p7279
(dp7280
g306
(lp7281
I2892
asg235
(lp7282
ssS'redescrib'
p7283
(dp7284
g72
(lp7285
I578
assS'squareroot'
p7286
(dp7287
g354
(lp7288
I3004
assS'artifact'
p7289
(dp7290
g287
(lp7291
I874
assS'logist'
p7292
(dp7293
g329
(lp7294
sg50
(lp7295
I189
asg341
(lp7296
ssS'inventor'
p7297
(dp7298
g183
(lp7299
I4197
assS'stiong'
p7300
(dp7301
g106
(lp7302
I984
assS'stlm'
p7303
(dp7304
g106
(lp7305
I2004
assS'rival'
p7306
(dp7307
g440
(lp7308
sg128
(lp7309
I2735
assS'bradtk'
p7310
(dp7311
g89
(lp7312
I281
asg83
(lp7313
ssS'rankprop'
p7314
(dp7315
g277
(lp7316
I9
assS'photomultipli'
p7317
(dp7318
g283
(lp7319
I277
assS'decerebr'
p7320
(dp7321
g174
(lp7322
I2586
assS'grossmann'
p7323
(dp7324
g22
(lp7325
I2430
assS'prospect'
p7326
(dp7327
g30
(lp7328
sg91
(lp7329
I2414
assS'sao'
p7330
(dp7331
g59
(lp7332
I2515
assS'san'
p7333
(dp7334
g26
(lp7335
sg30
(lp7336
sg287
(lp7337
sg256
(lp7338
sg484
(lp7339
sg91
(lp7340
sg12
(lp7341
sg46
(lp7342
sg20
(lp7343
sg18
(lp7344
sg223
(lp7345
sg350
(lp7346
sg94
(lp7347
sg106
(lp7348
I319
asg108
(lp7349
sg110
(lp7350
sg63
(lp7351
sg116
(lp7352
sg440
(lp7353
sg318
(lp7354
sg178
(lp7355
sg4
(lp7356
sg8
(lp7357
sg34
(lp7358
sg124
(lp7359
sg126
(lp7360
sg40
(lp7361
sg132
(lp7362
sg149
(lp7363
sg50
(lp7364
sg140
(lp7365
sg354
(lp7366
ssS'sab'
p7367
(dp7368
g102
(lp7369
I838
assS'argument'
p7370
(dp7371
g438
(lp7372
I786
asg318
(lp7373
sg145
(lp7374
sg22
(lp7375
sg277
(lp7376
sg34
(lp7377
sg38
(lp7378
sg384
(lp7379
sg68
(lp7380
sg72
(lp7381
sg85
(lp7382
sg108
(lp7383
sg313
(lp7384
ssS'say'
p7385
(dp7386
g230
(lp7387
sg174
(lp7388
sg32
(lp7389
sg332
(lp7390
sg26
(lp7391
sg277
(lp7392
sg329
(lp7393
sg34
(lp7394
sg183
(lp7395
sg384
(lp7396
sg126
(lp7397
sg85
(lp7398
sg118
(lp7399
sg42
(lp7400
I168
asg287
(lp7401
sg52
(lp7402
sg36
(lp7403
sg104
(lp7404
sg344
(lp7405
sg63
(lp7406
sg223
(lp7407
ssS'wolpert'
p7408
(dp7409
g235
(lp7410
sg295
(lp7411
sg183
(lp7412
sg484
(lp7413
sg99
(lp7414
sg140
(lp7415
I3074
assS'onchip'
p7416
(dp7417
g10
(lp7418
I962
assS'allen'
p7419
(dp7420
g429
(lp7421
sg178
(lp7422
I2558
assS'turner'
p7423
(dp7424
g104
(lp7425
I51
assS'saw'
p7426
(dp7427
g40
(lp7428
sg235
(lp7429
I2443
assS'halbert'
p7430
(dp7431
g68
(lp7432
I3356
assS'ynlli'
p7433
(dp7434
g306
(lp7435
I1969
assS'fertil'
p7436
(dp7437
g460
(lp7438
I379
asg72
(lp7439
ssS'rckpk'
p7440
(dp7441
g178
(lp7442
I1424
assS'yosef'
p7443
(dp7444
g32
(lp7445
I3102
assS'retroact'
p7446
(dp7447
g99
(lp7448
I3334
assS'neuronbi'
p7449
(dp7450
g4
(lp7451
I2064
assS'note'
p7452
(dp7453
g329
(lp7454
sg277
(lp7455
sg163
(lp7456
sg72
(lp7457
sg281
(lp7458
sg36
(lp7459
sg181
(lp7460
sg287
(lp7461
sg74
(lp7462
sg176
(lp7463
sg145
(lp7464
sg256
(lp7465
sg80
(lp7466
sg262
(lp7467
sg295
(lp7468
sg183
(lp7469
sg59
(lp7470
sg38
(lp7471
sg85
(lp7472
sg306
(lp7473
sg89
(lp7474
sg94
(lp7475
sg20
(lp7476
sg18
(lp7477
sg99
(lp7478
sg313
(lp7479
sg44
(lp7480
sg350
(lp7481
sg118
(lp7482
sg174
(lp7483
sg293
(lp7484
sg429
(lp7485
sg46
(lp7486
sg102
(lp7487
sg104
(lp7488
sg106
(lp7489
sg108
(lp7490
sg178
(lp7491
sg114
(lp7492
sg116
(lp7493
sg438
(lp7494
I327
asg440
(lp7495
sg121
(lp7496
sg4
(lp7497
sg6
(lp7498
sg8
(lp7499
sg34
(lp7500
sg221
(lp7501
sg384
(lp7502
sg68
(lp7503
sg126
(lp7504
sg341
(lp7505
sg40
(lp7506
sg344
(lp7507
sg128
(lp7508
sg130
(lp7509
sg14
(lp7510
sg16
(lp7511
sg135
(lp7512
sg50
(lp7513
sg140
(lp7514
ssS'unlearn'
p7515
(dp7516
g354
(lp7517
I3199
assS'denomin'
p7518
(dp7519
g14
(lp7520
sg16
(lp7521
I1999
assS'take'
p7522
(dp7523
g283
(lp7524
sg70
(lp7525
sg277
(lp7526
sg303
(lp7527
sg40
(lp7528
sg30
(lp7529
sg287
(lp7530
sg74
(lp7531
sg76
(lp7532
sg293
(lp7533
sg295
(lp7534
sg183
(lp7535
sg59
(lp7536
sg38
(lp7537
sg83
(lp7538
sg85
(lp7539
sg63
(lp7540
sg42
(lp7541
I1692
asg306
(lp7542
sg87
(lp7543
sg89
(lp7544
sg460
(lp7545
sg12
(lp7546
sg94
(lp7547
sg96
(lp7548
sg313
(lp7549
sg44
(lp7550
sg350
(lp7551
sg32
(lp7552
sg318
(lp7553
sg46
(lp7554
sg102
(lp7555
sg20
(lp7556
sg52
(lp7557
sg114
(lp7558
sg118
(lp7559
sg440
(lp7560
sg332
(lp7561
sg178
(lp7562
sg22
(lp7563
sg6
(lp7564
sg8
(lp7565
sg36
(lp7566
sg384
(lp7567
sg124
(lp7568
sg126
(lp7569
sg10
(lp7570
sg535
(lp7571
sg344
(lp7572
sg223
(lp7573
sg130
(lp7574
sg132
(lp7575
sg14
(lp7576
sg16
(lp7577
sg135
(lp7578
sg50
(lp7579
sg138
(lp7580
sg140
(lp7581
sg354
(lp7582
ssS'destroy'
p7583
(dp7584
g429
(lp7585
sg99
(lp7586
I2716
assS'noth'
p7587
(dp7588
g30
(lp7589
sg76
(lp7590
sg235
(lp7591
sg183
(lp7592
sg59
(lp7593
sg91
(lp7594
sg138
(lp7595
I2100
asg52
(lp7596
ssS'buffer'
p7597
(dp7598
g22
(lp7599
sg245
(lp7600
sg132
(lp7601
sg14
(lp7602
sg16
(lp7603
sg135
(lp7604
I834
asg20
(lp7605
ssS'nunlber'
p7606
(dp7607
g40
(lp7608
I1233
assS'compress'
p7609
(dp7610
g174
(lp7611
sg121
(lp7612
sg181
(lp7613
sg78
(lp7614
sg42
(lp7615
I2783
asg245
(lp7616
sg63
(lp7617
ssS'abus'
p7618
(dp7619
g287
(lp7620
I1771
assS'servan'
p7621
(dp7622
g4
(lp7623
I31
assS'homepag'
p7624
(dp7625
g121
(lp7626
I233
assS'epri'
p7627
(dp7628
g306
(lp7629
I2911
assS'salj'
p7630
(dp7631
g174
(lp7632
I1815
assS'allevi'
p7633
(dp7634
g135
(lp7635
I578
asg91
(lp7636
ssS'archi'
p7637
(dp7638
g128
(lp7639
I2245
assS'phil'
p7640
(dp7641
g135
(lp7642
I822
asg48
(lp7643
ssS'drive'
p7644
(dp7645
g438
(lp7646
I2281
asg70
(lp7647
sg256
(lp7648
sg277
(lp7649
sg174
(lp7650
sg295
(lp7651
sg183
(lp7652
sg293
(lp7653
sg63
(lp7654
sg78
(lp7655
sg245
(lp7656
sg350
(lp7657
sg313
(lp7658
sg223
(lp7659
sg149
(lp7660
ssS'axe'
p7661
(dp7662
g32
(lp7663
sg318
(lp7664
sg163
(lp7665
sg78
(lp7666
sg59
(lp7667
sg124
(lp7668
sg108
(lp7669
I1681
asg350
(lp7670
ssS'axj'
p7671
(dp7672
g535
(lp7673
sg130
(lp7674
I1434
assS'axi'
p7675
(dp7676
g30
(lp7677
sg32
(lp7678
sg332
(lp7679
sg6
(lp7680
sg8
(lp7681
sg34
(lp7682
sg235
(lp7683
sg126
(lp7684
sg85
(lp7685
sg318
(lp7686
sg89
(lp7687
sg91
(lp7688
sg14
(lp7689
sg16
(lp7690
I454
asg108
(lp7691
sg350
(lp7692
ssS'salt'
p7693
(dp7694
g283
(lp7695
I722
assS'minml'
p7696
(dp7697
g72
(lp7698
I1185
assS'axl'
p7699
(dp7700
g135
(lp7701
I601
assS'minch'
p7702
(dp7703
g256
(lp7704
I2124
assS'madison'
p7705
(dp7706
g344
(lp7707
I30
assS'beha'
p7708
(dp7709
g4
(lp7710
I1533
assS'speakerdepend'
p7711
(dp7712
g96
(lp7713
I84
assS'parsimoni'
p7714
(dp7715
g30
(lp7716
I1535
asg74
(lp7717
sg429
(lp7718
ssS'friction'
p7719
(dp7720
g460
(lp7721
sg99
(lp7722
I291
assS'bright'
p7723
(dp7724
g118
(lp7725
sg70
(lp7726
sg181
(lp7727
sg245
(lp7728
I1263
asg178
(lp7729
sg223
(lp7730
ssS'qoo'
p7731
(dp7732
g262
(lp7733
I1528
assS'federov'
p7734
(dp7735
g281
(lp7736
I2126
assS'slot'
p7737
(dp7738
g429
(lp7739
sg10
(lp7740
I435
assS'slow'
p7741
(dp7742
g438
(lp7743
I1182
asg68
(lp7744
sg70
(lp7745
sg4
(lp7746
sg6
(lp7747
sg329
(lp7748
sg34
(lp7749
sg38
(lp7750
sg384
(lp7751
sg262
(lp7752
sg126
(lp7753
sg281
(lp7754
sg535
(lp7755
sg59
(lp7756
sg245
(lp7757
sg14
(lp7758
sg20
(lp7759
sg135
(lp7760
sg50
(lp7761
sg138
(lp7762
sg256
(lp7763
ssS'transact'
p7764
(dp7765
g230
(lp7766
sg429
(lp7767
sg460
(lp7768
sg68
(lp7769
sg341
(lp7770
sg535
(lp7771
sg63
(lp7772
sg87
(lp7773
sg281
(lp7774
sg245
(lp7775
sg14
(lp7776
sg96
(lp7777
sg40
(lp7778
sg110
(lp7779
sg313
(lp7780
sg140
(lp7781
I3089
asg114
(lp7782
ssS'activ'
p7783
(dp7784
g329
(lp7785
sg70
(lp7786
sg26
(lp7787
sg163
(lp7788
sg293
(lp7789
sg283
(lp7790
sg36
(lp7791
sg303
(lp7792
sg287
(lp7793
sg176
(lp7794
sg256
(lp7795
sg76
(lp7796
sg118
(lp7797
sg295
(lp7798
sg183
(lp7799
sg59
(lp7800
sg80
(lp7801
sg38
(lp7802
sg83
(lp7803
sg63
(lp7804
sg306
(lp7805
sg91
(lp7806
sg12
(lp7807
sg94
(lp7808
sg20
(lp7809
sg48
(lp7810
sg99
(lp7811
sg313
(lp7812
sg149
(lp7813
sg116
(lp7814
sg174
(lp7815
sg18
(lp7816
sg245
(lp7817
sg429
(lp7818
sg102
(lp7819
sg104
(lp7820
sg106
(lp7821
sg110
(lp7822
sg178
(lp7823
sg114
(lp7824
sg216
(lp7825
sg438
(lp7826
I165
asg332
(lp7827
sg121
(lp7828
sg4
(lp7829
sg8
(lp7830
sg221
(lp7831
sg68
(lp7832
sg10
(lp7833
sg535
(lp7834
sg130
(lp7835
sg14
(lp7836
sg16
(lp7837
sg350
(lp7838
sg50
(lp7839
sg138
(lp7840
sg140
(lp7841
sg354
(lp7842
ssS'loopsec'
p7843
(dp7844
g20
(lp7845
I654
assS'moriya'
p7846
(dp7847
g181
(lp7848
I2557
assS'helmholtz'
p7849
(dp7850
g32
(lp7851
I1423
asg72
(lp7852
ssS'hollatz'
p7853
(dp7854
g221
(lp7855
I2719
assS'dissimilar'
p7856
(dp7857
g42
(lp7858
I1073
asg216
(lp7859
sg38
(lp7860
sg277
(lp7861
sg130
(lp7862
ssS'genera'
p7863
(dp7864
g230
(lp7865
I2819
assS'yti'
p7866
(dp7867
g535
(lp7868
I613
assS'erenc'
p7869
(dp7870
g96
(lp7871
I1975
assS'grunewald'
p7872
(dp7873
g216
(lp7874
I12
assS'woodburi'
p7875
(dp7876
g295
(lp7877
I1786
asg183
(lp7878
ssS'zhu'
p7879
(dp7880
g124
(lp7881
I3014
assS'requir'
p7882
(dp7883
g68
(lp7884
sg70
(lp7885
sg78
(lp7886
sg277
(lp7887
sg163
(lp7888
sg303
(lp7889
sg281
(lp7890
sg283
(lp7891
sg85
(lp7892
sg40
(lp7893
sg287
(lp7894
sg74
(lp7895
sg145
(lp7896
sg76
(lp7897
sg293
(lp7898
sg295
(lp7899
sg183
(lp7900
sg59
(lp7901
sg484
(lp7902
sg38
(lp7903
sg83
(lp7904
sg114
(lp7905
sg63
(lp7906
sg306
(lp7907
sg87
(lp7908
sg89
(lp7909
sg460
(lp7910
sg12
(lp7911
sg94
(lp7912
sg96
(lp7913
sg48
(lp7914
sg99
(lp7915
sg313
(lp7916
sg44
(lp7917
sg350
(lp7918
sg116
(lp7919
sg118
(lp7920
sg18
(lp7921
sg32
(lp7922
sg178
(lp7923
sg245
(lp7924
sg429
(lp7925
sg318
(lp7926
sg46
(lp7927
sg102
(lp7928
sg104
(lp7929
sg106
(lp7930
sg108
(lp7931
sg20
(lp7932
sg52
(lp7933
sg22
(lp7934
sg230
(lp7935
sg438
(lp7936
I1627
asg440
(lp7937
sg332
(lp7938
sg121
(lp7939
sg4
(lp7940
sg181
(lp7941
sg8
(lp7942
sg34
(lp7943
sg221
(lp7944
sg384
(lp7945
sg124
(lp7946
sg126
(lp7947
sg341
(lp7948
sg10
(lp7949
sg535
(lp7950
sg223
(lp7951
sg130
(lp7952
sg132
(lp7953
sg14
(lp7954
sg16
(lp7955
sg135
(lp7956
sg50
(lp7957
sg138
(lp7958
sg140
(lp7959
sg354
(lp7960
ssS'prime'
p7961
(dp7962
g332
(lp7963
sg22
(lp7964
sg6
(lp7965
sg8
(lp7966
I497
asg221
(lp7967
sg52
(lp7968
ssS'surrend'
p7969
(dp7970
g176
(lp7971
I2267
assS'vmin'
p7972
(dp7973
g460
(lp7974
I1995
assS'borrow'
p7975
(dp7976
g140
(lp7977
I852
assS'ahmad'
p7978
(dp7979
g178
(lp7980
sg221
(lp7981
sg91
(lp7982
I720
assS'cochlea'
p7983
(dp7984
g174
(lp7985
I234
asg22
(lp7986
ssS'twelv'
p7987
(dp7988
g94
(lp7989
I1765
asg281
(lp7990
ssS'roger'
p7991
(dp7992
g74
(lp7993
sg18
(lp7994
I2596
asg63
(lp7995
ssS'ut'
p7996
(dp7997
g230
(lp7998
sg256
(lp7999
sg34
(lp8000
sg72
(lp8001
sg87
(lp8002
sg46
(lp8003
sg104
(lp8004
sg44
(lp8005
I2717
assS'where'
p8006
(dp8007
g80
(lp8008
sg293
(lp8009
sg344
(lp8010
sg78
(lp8011
sg59
(lp8012
sg484
(lp8013
sg38
(lp8014
sg83
(lp8015
sg85
(lp8016
sg438
(lp8017
sg116
(lp8018
sg118
(lp8019
sg34
(lp8020
sg36
(lp8021
sg460
(lp8022
sg68
(lp8023
sg72
(lp8024
sg281
(lp8025
sg40
(lp8026
sg283
(lp8027
sg70
(lp8028
sg26
(lp8029
sg277
(lp8030
sg163
(lp8031
sg89
(lp8032
sg91
(lp8033
sg12
(lp8034
sg96
(lp8035
sg48
(lp8036
sg99
(lp8037
sg313
(lp8038
sg149
(lp8039
sg429
(lp8040
sg102
(lp8041
sg104
(lp8042
sg108
(lp8043
sg63
(lp8044
sg52
(lp8045
sg128
(lp8046
sg130
(lp8047
sg132
(lp8048
sg14
(lp8049
sg16
(lp8050
sg135
(lp8051
sg50
(lp8052
sg138
(lp8053
sg140
(lp8054
sg354
(lp8055
sg306
(lp8056
sg87
(lp8057
sg245
(lp8058
sg46
(lp8059
sg20
(lp8060
sg18
(lp8061
sg221
(lp8062
sg535
(lp8063
sg223
(lp8064
sg350
(lp8065
sg216
(lp8066
sg174
(lp8067
sg440
(lp8068
sg332
(lp8069
sg121
(lp8070
sg4
(lp8071
sg6
(lp8072
sg8
(lp8073
sg126
(lp8074
sg341
(lp8075
sg30
(lp8076
sg287
(lp8077
sg74
(lp8078
sg176
(lp8079
sg145
(lp8080
sg256
(lp8081
sg76
(lp8082
sg262
(lp8083
sg295
(lp8084
sg183
(lp8085
sg42
(lp8086
I2369
asg230
(lp8087
sg329
(lp8088
sg32
(lp8089
sg318
(lp8090
sg178
(lp8091
sg22
(lp8092
sg181
(lp8093
sg235
(lp8094
sg384
(lp8095
sg124
(lp8096
ssS'vision'
p8097
(dp8098
g104
(lp8099
sg30
(lp8100
sg256
(lp8101
sg293
(lp8102
sg59
(lp8103
sg42
(lp8104
I3452
asg91
(lp8105
sg245
(lp8106
sg223
(lp8107
sg12
(lp8108
sg429
(lp8109
sg178
(lp8110
sg63
(lp8111
sg52
(lp8112
sg22
(lp8113
sg216
(lp8114
sg118
(lp8115
sg32
(lp8116
sg318
(lp8117
sg121
(lp8118
sg6
(lp8119
sg181
(lp8120
sg8
(lp8121
sg138
(lp8122
ssS'noisier'
p8123
(dp8124
g102
(lp8125
I2290
asg70
(lp8126
ssS'cio'
p8127
(dp8128
g135
(lp8129
I1213
assS'surgeri'
p8130
(dp8131
g318
(lp8132
I3048
assS'outpati'
p8133
(dp8134
g277
(lp8135
I343
assS'misdassificaton'
p8136
(dp8137
g460
(lp8138
I2361
assS'wideband'
p8139
(dp8140
g174
(lp8141
I412
assS'um'
p8142
(dp8143
g174
(lp8144
sg135
(lp8145
I181
asg48
(lp8146
sg76
(lp8147
ssS'testset'
p8148
(dp8149
g36
(lp8150
I2487
assS'ubiquit'
p8151
(dp8152
g14
(lp8153
I4574
assS'un'
p8154
(dp8155
g26
(lp8156
sg262
(lp8157
sg344
(lp8158
sg72
(lp8159
sg38
(lp8160
sg341
(lp8161
sg138
(lp8162
I2418
assS'maggioni'
p8163
(dp8164
g59
(lp8165
I3296
assS'spars'
p8166
(dp8167
g30
(lp8168
sg145
(lp8169
sg26
(lp8170
sg181
(lp8171
sg8
(lp8172
sg344
(lp8173
sg68
(lp8174
sg429
(lp8175
sg89
(lp8176
sg94
(lp8177
sg135
(lp8178
sg50
(lp8179
I69
assS'ciu'
p8180
(dp8181
g306
(lp8182
I797
assS'amort'
p8183
(dp8184
g223
(lp8185
I3016
assS'marvin'
p8186
(dp8187
g429
(lp8188
I2498
assS'screen'
p8189
(dp8190
g4
(lp8191
I867
asg293
(lp8192
ssS'uj'
p8193
(dp8194
g293
(lp8195
I2896
assS'fip'
p8196
(dp8197
g429
(lp8198
I1156
assS'orific'
p8199
(dp8200
g116
(lp8201
I1652
assS'sparc'
p8202
(dp8203
g318
(lp8204
sg181
(lp8205
sg295
(lp8206
sg183
(lp8207
sg10
(lp8208
sg132
(lp8209
sg44
(lp8210
sg354
(lp8211
I2589
assS'spare'
p8212
(dp8213
g94
(lp8214
I766
assS'xastmg'
p8215
(dp8216
g72
(lp8217
I2914
assS'nemcek'
p8218
(dp8219
g114
(lp8220
I531
assS's'
p8221
(dp8222
g80
(lp8223
sg293
(lp8224
sg344
(lp8225
sg78
(lp8226
sg59
(lp8227
sg484
(lp8228
sg38
(lp8229
sg83
(lp8230
sg85
(lp8231
sg303
(lp8232
sg438
(lp8233
sg116
(lp8234
sg118
(lp8235
sg34
(lp8236
sg36
(lp8237
sg460
(lp8238
sg72
(lp8239
sg281
(lp8240
sg10
(lp8241
sg40
(lp8242
sg70
(lp8243
sg26
(lp8244
sg277
(lp8245
sg163
(lp8246
sg89
(lp8247
sg91
(lp8248
sg12
(lp8249
sg94
(lp8250
sg96
(lp8251
sg48
(lp8252
sg99
(lp8253
sg313
(lp8254
sg44
(lp8255
sg149
(lp8256
sg429
(lp8257
sg102
(lp8258
sg104
(lp8259
sg106
(lp8260
sg108
(lp8261
sg110
(lp8262
sg63
(lp8263
sg52
(lp8264
sg114
(lp8265
sg128
(lp8266
sg132
(lp8267
sg14
(lp8268
sg16
(lp8269
sg135
(lp8270
sg50
(lp8271
sg138
(lp8272
sg140
(lp8273
sg354
(lp8274
sg306
(lp8275
sg87
(lp8276
sg245
(lp8277
sg46
(lp8278
sg20
(lp8279
sg18
(lp8280
sg221
(lp8281
sg535
(lp8282
sg223
(lp8283
sg350
(lp8284
sg216
(lp8285
sg174
(lp8286
sg440
(lp8287
sg332
(lp8288
sg121
(lp8289
sg4
(lp8290
sg6
(lp8291
sg8
(lp8292
sg126
(lp8293
sg341
(lp8294
sg30
(lp8295
sg287
(lp8296
sg74
(lp8297
sg176
(lp8298
sg145
(lp8299
sg256
(lp8300
sg76
(lp8301
sg262
(lp8302
sg295
(lp8303
sg183
(lp8304
sg42
(lp8305
I260
asg230
(lp8306
sg329
(lp8307
sg32
(lp8308
sg318
(lp8309
sg178
(lp8310
sg22
(lp8311
sg181
(lp8312
sg235
(lp8313
sg384
(lp8314
sg124
(lp8315
ssS'mane'
p8316
(dp8317
g104
(lp8318
I865
assS'mani'
p8319
(dp8320
g329
(lp8321
sg70
(lp8322
sg26
(lp8323
sg277
(lp8324
sg72
(lp8325
sg281
(lp8326
sg181
(lp8327
sg40
(lp8328
sg30
(lp8329
sg74
(lp8330
sg145
(lp8331
sg80
(lp8332
sg262
(lp8333
sg295
(lp8334
sg183
(lp8335
sg59
(lp8336
sg484
(lp8337
sg83
(lp8338
sg85
(lp8339
sg124
(lp8340
sg42
(lp8341
I587
asg306
(lp8342
sg87
(lp8343
sg89
(lp8344
sg91
(lp8345
sg12
(lp8346
sg94
(lp8347
sg48
(lp8348
sg68
(lp8349
sg313
(lp8350
sg44
(lp8351
sg350
(lp8352
sg118
(lp8353
sg174
(lp8354
sg293
(lp8355
sg245
(lp8356
sg429
(lp8357
sg318
(lp8358
sg102
(lp8359
sg104
(lp8360
sg106
(lp8361
sg110
(lp8362
sg178
(lp8363
sg52
(lp8364
sg114
(lp8365
sg230
(lp8366
sg438
(lp8367
sg32
(lp8368
sg18
(lp8369
sg121
(lp8370
sg4
(lp8371
sg6
(lp8372
sg8
(lp8373
sg34
(lp8374
sg36
(lp8375
sg235
(lp8376
sg126
(lp8377
sg341
(lp8378
sg535
(lp8379
sg344
(lp8380
sg63
(lp8381
sg128
(lp8382
sg78
(lp8383
sg14
(lp8384
sg16
(lp8385
sg135
(lp8386
sg138
(lp8387
sg140
(lp8388
sg354
(lp8389
ssS'mann'
p8390
(dp8391
g176
(lp8392
I2613
assS'minima'
p8393
(dp8394
g438
(lp8395
I1779
asg329
(lp8396
sg121
(lp8397
sg8
(lp8398
sg295
(lp8399
sg183
(lp8400
sg124
(lp8401
sg341
(lp8402
sg34
(lp8403
sg429
(lp8404
sg130
(lp8405
sg46
(lp8406
sg138
(lp8407
ssS'exagger'
p8408
(dp8409
g52
(lp8410
I154
assS'anti'
p8411
(dp8412
g438
(lp8413
I743
asg26
(lp8414
sg50
(lp8415
sg384
(lp8416
sg89
(lp8417
sg12
(lp8418
sg106
(lp8419
sg99
(lp8420
ssS'hyperplan'
p8421
(dp8422
g52
(lp8423
I268
assS'comc'
p8424
(dp8425
g70
(lp8426
I1055
assS'lonatmn'
p8427
(dp8428
g4
(lp8429
I3178
assS'ribier'
p8430
(dp8431
g34
(lp8432
I2122
assS'almighti'
p8433
(dp8434
g42
(lp8435
I514
assS'stretch'
p8436
(dp8437
g535
(lp8438
I957
assS'west'
p8439
(dp8440
g344
(lp8441
I31
asg80
(lp8442
ssS'breath'
p8443
(dp8444
g96
(lp8445
I1910
assS'kikini'
p8446
(dp8447
g318
(lp8448
I2314
assS'teddi'
p8449
(dp8450
g181
(lp8451
I1807
assS'pessoa'
p8452
(dp8453
g118
(lp8454
I10
assS'reflex'
p8455
(dp8456
g350
(lp8457
sg4
(lp8458
sg354
(lp8459
I2893
assS'colullln'
p8460
(dp8461
g48
(lp8462
I532
assS'thousand'
p8463
(dp8464
g318
(lp8465
sg94
(lp8466
sg108
(lp8467
I59
asg8
(lp8468
ssS'resolut'
p8469
(dp8470
g174
(lp8471
sg70
(lp8472
sg283
(lp8473
sg178
(lp8474
sg4
(lp8475
sg293
(lp8476
sg59
(lp8477
sg121
(lp8478
sg63
(lp8479
sg44
(lp8480
I459
asg22
(lp8481
ssS'photon'
p8482
(dp8483
g283
(lp8484
I939
assS'datacopi'
p8485
(dp8486
g114
(lp8487
I121
assS'fij'
p8488
(dp8489
g78
(lp8490
I2315
assS'catastroph'
p8491
(dp8492
g295
(lp8493
sg183
(lp8494
sg99
(lp8495
I2723
assS'fik'
p8496
(dp8497
g74
(lp8498
I494
assS'imioqo'
p8499
(dp8500
g70
(lp8501
I1942
assS'former'
p8502
(dp8503
g116
(lp8504
sg174
(lp8505
sg32
(lp8506
sg8
(lp8507
sg460
(lp8508
sg262
(lp8509
sg10
(lp8510
sg42
(lp8511
I2877
asg128
(lp8512
sg14
(lp8513
sg223
(lp8514
ssS'erlang'
p8515
(dp8516
g83
(lp8517
I637
assS'fakultat'
p8518
(dp8519
g30
(lp8520
sg59
(lp8521
sg48
(lp8522
I29
assS'fio'
p8523
(dp8524
g384
(lp8525
sg181
(lp8526
I879
assS'dario'
p8527
(dp8528
g32
(lp8529
I3112
assS'cure'
p8530
(dp8531
g26
(lp8532
I2592
assS'varianceo'
p8533
(dp8534
g313
(lp8535
I1885
assS'inaa'
p8536
(dp8537
g429
(lp8538
I1001
assS'spotlight'
p8539
(dp8540
g8
(lp8541
I731
assS'ive'
p8542
(dp8543
g34
(lp8544
sg48
(lp8545
I704
assS'shepard'
p8546
(dp8547
g74
(lp8548
sg223
(lp8549
I923
assS'jldard'
p8550
(dp8551
g20
(lp8552
I1528
assS'ivl'
p8553
(dp8554
g318
(lp8555
I1580
assS'canon'
p8556
(dp8557
g132
(lp8558
I1908
asg32
(lp8559
sg76
(lp8560
sg181
(lp8561
ssS'convinc'
p8562
(dp8563
g118
(lp8564
sg384
(lp8565
sg4
(lp8566
sg140
(lp8567
I2158
assS'radner'
p8568
(dp8569
g12
(lp8570
I2732
assS'svaizer'
p8571
(dp8572
g96
(lp8573
I2730
assS'ambient'
p8574
(dp8575
g245
(lp8576
sg14
(lp8577
sg16
(lp8578
I2066
assS'telescop'
p8579
(dp8580
g176
(lp8581
I254
assS'pursu'
p8582
(dp8583
g74
(lp8584
sg181
(lp8585
sg295
(lp8586
sg78
(lp8587
sg183
(lp8588
sg245
(lp8589
I2989
assS'lyn'
p8590
(dp8591
g287
(lp8592
I1690
assS'bioelettronica'
p8593
(dp8594
g96
(lp8595
I2695
assS'kperceptron'
p8596
(dp8597
g344
(lp8598
I1355
assS'interstiti'
p8599
(dp8600
g350
(lp8601
I1002
assS'econom'
p8602
(dp8603
g42
(lp8604
I1257
asg91
(lp8605
sg277
(lp8606
ssS'binari'
p8607
(dp8608
g70
(lp8609
sg26
(lp8610
sg72
(lp8611
sg287
(lp8612
sg74
(lp8613
sg176
(lp8614
sg484
(lp8615
sg83
(lp8616
sg42
(lp8617
I1403
asg91
(lp8618
sg94
(lp8619
sg20
(lp8620
sg44
(lp8621
sg429
(lp8622
sg104
(lp8623
sg110
(lp8624
sg63
(lp8625
sg52
(lp8626
sg114
(lp8627
sg318
(lp8628
sg178
(lp8629
sg22
(lp8630
sg8
(lp8631
sg384
(lp8632
sg68
(lp8633
sg126
(lp8634
sg40
(lp8635
sg130
(lp8636
sg14
(lp8637
sg50
(lp8638
sg138
(lp8639
ssS'wilhelm'
p8640
(dp8641
g130
(lp8642
I12
assS'hxt'
p8643
(dp8644
g46
(lp8645
I1967
assS'cardin'
p8646
(dp8647
g287
(lp8648
I2745
assS'subdomain'
p8649
(dp8650
g460
(lp8651
I1284
assS'iibub'
p8652
(dp8653
g42
(lp8654
I2552
assS'breakdown'
p8655
(dp8656
g78
(lp8657
sg332
(lp8658
I400
assS'ligatur'
p8659
(dp8660
g76
(lp8661
I320
assS'unifonn'
p8662
(dp8663
g85
(lp8664
I2117
assS'arden'
p8665
(dp8666
g108
(lp8667
I2544
assS'eavisid'
p8668
(dp8669
g287
(lp8670
I1458
assS'geniculo'
p8671
(dp8672
g438
(lp8673
I1124
assS'wpm'
p8674
(dp8675
g94
(lp8676
I2909
assS'wpn'
p8677
(dp8678
g295
(lp8679
I1679
asg183
(lp8680
ssS'extern'
p8681
(dp8682
g230
(lp8683
sg174
(lp8684
sg332
(lp8685
sg22
(lp8686
sg76
(lp8687
sg78
(lp8688
sg116
(lp8689
sg68
(lp8690
sg26
(lp8691
sg10
(lp8692
sg104
(lp8693
sg293
(lp8694
sg14
(lp8695
sg16
(lp8696
I701
asg18
(lp8697
sg110
(lp8698
sg535
(lp8699
sg149
(lp8700
ssS'earnshaw'
p8701
(dp8702
g59
(lp8703
I3406
assS'trainl'
p8704
(dp8705
g70
(lp8706
I519
assS'dimension'
p8707
(dp8708
g283
(lp8709
sg70
(lp8710
sg163
(lp8711
sg281
(lp8712
sg30
(lp8713
sg287
(lp8714
sg74
(lp8715
sg176
(lp8716
sg256
(lp8717
sg295
(lp8718
sg183
(lp8719
sg59
(lp8720
sg38
(lp8721
sg85
(lp8722
sg306
(lp8723
sg87
(lp8724
sg89
(lp8725
sg91
(lp8726
sg245
(lp8727
sg46
(lp8728
sg96
(lp8729
sg221
(lp8730
sg223
(lp8731
sg149
(lp8732
sg102
(lp8733
sg178
(lp8734
sg108
(lp8735
sg110
(lp8736
sg52
(lp8737
sg32
(lp8738
sg318
(lp8739
sg121
(lp8740
sg22
(lp8741
sg181
(lp8742
sg235
(lp8743
sg34
(lp8744
sg460
(lp8745
sg124
(lp8746
sg126
(lp8747
sg341
(lp8748
sg44
(lp8749
sg130
(lp8750
sg50
(lp8751
sg138
(lp8752
sg354
(lp8753
I1980
assS'summer'
p8754
(dp8755
g80
(lp8756
sg34
(lp8757
sg36
(lp8758
sg83
(lp8759
sg89
(lp8760
sg132
(lp8761
I3879
assS'hippocamp'
p8762
(dp8763
g78
(lp8764
sg106
(lp8765
I186
asg80
(lp8766
ssS'manifold'
p8767
(dp8768
g30
(lp8769
sg32
(lp8770
I1720
assS'lemen'
p8771
(dp8772
g132
(lp8773
I3708
assS'ishii'
p8774
(dp8775
g20
(lp8776
I2536
assS'aelllkl'
p8777
(dp8778
g440
(lp8779
I499
assS'resp'
p8780
(dp8781
g332
(lp8782
I1521
asg70
(lp8783
sg303
(lp8784
ssS'rest'
p8785
(dp8786
g140
(lp8787
sg176
(lp8788
sg26
(lp8789
sg329
(lp8790
sg34
(lp8791
sg124
(lp8792
sg281
(lp8793
sg40
(lp8794
sg42
(lp8795
I2644
asg484
(lp8796
sg91
(lp8797
sg94
(lp8798
sg96
(lp8799
sg63
(lp8800
sg223
(lp8801
sg350
(lp8802
ssS'gdb'
p8803
(dp8804
g10
(lp8805
I1423
assS'concentr'
p8806
(dp8807
g74
(lp8808
sg163
(lp8809
sg283
(lp8810
sg235
(lp8811
sg384
(lp8812
sg124
(lp8813
sg72
(lp8814
sg85
(lp8815
sg87
(lp8816
sg89
(lp8817
sg354
(lp8818
sg106
(lp8819
I1562
asg149
(lp8820
sg221
(lp8821
sg20
(lp8822
sg52
(lp8823
sg350
(lp8824
ssS'esprit'
p8825
(dp8826
g440
(lp8827
I2498
assS'littl'
p8828
(dp8829
g283
(lp8830
sg4
(lp8831
sg6
(lp8832
sg8
(lp8833
sg262
(lp8834
sg183
(lp8835
sg460
(lp8836
sg124
(lp8837
sg52
(lp8838
sg295
(lp8839
sg140
(lp8840
I637
asg128
(lp8841
sg181
(lp8842
sg20
(lp8843
sg293
(lp8844
sg99
(lp8845
sg313
(lp8846
sg44
(lp8847
sg26
(lp8848
ssS'subicular'
p8849
(dp8850
g106
(lp8851
I586
assS'exercis'
p8852
(dp8853
g14
(lp8854
sg135
(lp8855
I303
asg91
(lp8856
ssS'hvc'
p8857
(dp8858
g116
(lp8859
I339
assS'around'
p8860
(dp8861
g26
(lp8862
sg176
(lp8863
sg145
(lp8864
sg76
(lp8865
sg78
(lp8866
sg59
(lp8867
sg38
(lp8868
sg42
(lp8869
I2420
asg12
(lp8870
sg94
(lp8871
sg18
(lp8872
sg350
(lp8873
sg108
(lp8874
sg230
(lp8875
sg32
(lp8876
sg178
(lp8877
sg22
(lp8878
sg181
(lp8879
sg8
(lp8880
sg34
(lp8881
sg460
(lp8882
sg68
(lp8883
sg126
(lp8884
sg14
(lp8885
sg16
(lp8886
sg149
(lp8887
sg138
(lp8888
sg140
(lp8889
ssS'mismatchspik'
p8890
(dp8891
g70
(lp8892
I1683
assS'ixii'
p8893
(dp8894
g130
(lp8895
I510
assS'dark'
p8896
(dp8897
g438
(lp8898
I254
asg256
(lp8899
sg181
(lp8900
sg110
(lp8901
sg80
(lp8902
sg130
(lp8903
sg99
(lp8904
sg350
(lp8905
ssS'regim'
p8906
(dp8907
g48
(lp8908
I421
asg262
(lp8909
sg68
(lp8910
sg38
(lp8911
sg20
(lp8912
sg18
(lp8913
ssS'traffic'
p8914
(dp8915
g83
(lp8916
sg223
(lp8917
I384
assS'welleken'
p8918
(dp8919
g440
(lp8920
I300
assS'vacuum'
p8921
(dp8922
g14
(lp8923
sg16
(lp8924
I420
assS'world'
p8925
(dp8926
g70
(lp8927
sg277
(lp8928
sg30
(lp8929
sg74
(lp8930
sg293
(lp8931
sg295
(lp8932
sg183
(lp8933
sg59
(lp8934
sg484
(lp8935
sg83
(lp8936
sg89
(lp8937
sg245
(lp8938
sg18
(lp8939
sg221
(lp8940
sg223
(lp8941
sg149
(lp8942
sg12
(lp8943
sg429
(lp8944
sg318
(lp8945
sg63
(lp8946
sg114
(lp8947
sg174
(lp8948
sg332
(lp8949
sg178
(lp8950
sg181
(lp8951
sg124
(lp8952
sg126
(lp8953
sg344
(lp8954
sg44
(lp8955
sg130
(lp8956
sg135
(lp8957
I2379
assS'postal'
p8958
(dp8959
g138
(lp8960
I2274
asg76
(lp8961
sg63
(lp8962
ssS'pon'
p8963
(dp8964
g4
(lp8965
I1575
asg350
(lp8966
ssS'expositori'
p8967
(dp8968
g104
(lp8969
I2911
assS'intel'
p8970
(dp8971
g42
(lp8972
I3438
asg74
(lp8973
sg235
(lp8974
ssS'backpropog'
p8975
(dp8976
g85
(lp8977
I3879
assS'intea'
p8978
(dp8979
g72
(lp8980
I977
assS'stationari'
p8981
(dp8982
g124
(lp8983
sg281
(lp8984
sg42
(lp8985
I2554
asg130
(lp8986
sg18
(lp8987
sg350
(lp8988
ssS'integ'
p8989
(dp8990
g287
(lp8991
sg74
(lp8992
sg181
(lp8993
sg293
(lp8994
sg344
(lp8995
sg460
(lp8996
sg68
(lp8997
sg10
(lp8998
sg40
(lp8999
sg42
(lp9000
I3003
asg306
(lp9001
sg102
(lp9002
ssS'caudal'
p9003
(dp9004
g116
(lp9005
I342
assS'updat'
p9006
(dp9007
g124
(lp9008
sg163
(lp9009
sg72
(lp9010
sg30
(lp9011
sg293
(lp9012
sg295
(lp9013
sg183
(lp9014
sg38
(lp9015
sg306
(lp9016
sg91
(lp9017
sg245
(lp9018
sg221
(lp9019
sg116
(lp9020
sg178
(lp9021
sg108
(lp9022
sg114
(lp9023
sg230
(lp9024
sg329
(lp9025
sg318
(lp9026
sg121
(lp9027
sg4
(lp9028
sg8
(lp9029
sg34
(lp9030
sg384
(lp9031
sg235
(lp9032
sg126
(lp9033
sg341
(lp9034
sg10
(lp9035
sg132
(lp9036
sg135
(lp9037
sg50
(lp9038
I1082
asg460
(lp9039
ssS'inter'
p9040
(dp9041
g174
(lp9042
sg6
(lp9043
sg8
(lp9044
sg83
(lp9045
sg130
(lp9046
I1797
asg96
(lp9047
sg44
(lp9048
ssS'manag'
p9049
(dp9050
g332
(lp9051
sg10
(lp9052
sg42
(lp9053
I939
asg87
(lp9054
sg89
(lp9055
sg132
(lp9056
sg52
(lp9057
ssS'unrecord'
p9058
(dp9059
g74
(lp9060
I1880
assS'supplementari'
p9061
(dp9062
g42
(lp9063
I2662
assS'alan'
p9064
(dp9065
g118
(lp9066
I2339
asg429
(lp9067
ssS'ezp'
p9068
(dp9069
g178
(lp9070
I2713
assS'ziijli'
p9071
(dp9072
g121
(lp9073
I756
assS'uih'
p9074
(dp9075
g87
(lp9076
I1246
assS'satisfactori'
p9077
(dp9078
g287
(lp9079
sg74
(lp9080
sg48
(lp9081
I1183
asg76
(lp9082
sg163
(lp9083
ssS'strogatz'
p9084
(dp9085
g174
(lp9086
I1298
assS'tva'
p9087
(dp9088
g44
(lp9089
I914
assS'definit'
p9090
(dp9091
g26
(lp9092
sg163
(lp9093
sg287
(lp9094
sg176
(lp9095
sg76
(lp9096
sg293
(lp9097
sg295
(lp9098
sg183
(lp9099
sg85
(lp9100
sg46
(lp9101
sg350
(lp9102
sg102
(lp9103
sg110
(lp9104
sg230
(lp9105
sg121
(lp9106
sg6
(lp9107
sg181
(lp9108
sg8
(lp9109
sg34
(lp9110
sg36
(lp9111
sg124
(lp9112
sg341
(lp9113
sg40
(lp9114
sg344
(lp9115
sg132
(lp9116
I2006
assS'ddt'
p9117
(dp9118
g384
(lp9119
I1108
assS'evolv'
p9120
(dp9121
g70
(lp9122
sg59
(lp9123
sg178
(lp9124
sg8
(lp9125
sg384
(lp9126
sg38
(lp9127
sg281
(lp9128
sg104
(lp9129
sg14
(lp9130
sg16
(lp9131
I510
asg18
(lp9132
sg460
(lp9133
sg149
(lp9134
ssS'exit'
p9135
(dp9136
g83
(lp9137
I627
assS'runaway'
p9138
(dp9139
g70
(lp9140
I832
assS'ddc'
p9141
(dp9142
g384
(lp9143
I1915
assS'laitial'
p9144
(dp9145
g183
(lp9146
I5171
assS'damag'
p9147
(dp9148
g344
(lp9149
sg36
(lp9150
sg176
(lp9151
sg4
(lp9152
I3532
asg78
(lp9153
ssS'smadar'
p9154
(dp9155
g132
(lp9156
I3601
assS'refer'
p9157
(dp9158
g80
(lp9159
sg293
(lp9160
sg344
(lp9161
sg78
(lp9162
sg59
(lp9163
sg484
(lp9164
sg38
(lp9165
sg83
(lp9166
sg85
(lp9167
sg303
(lp9168
sg438
(lp9169
sg116
(lp9170
sg118
(lp9171
sg34
(lp9172
sg36
(lp9173
sg460
(lp9174
sg68
(lp9175
sg72
(lp9176
sg281
(lp9177
sg10
(lp9178
sg40
(lp9179
sg283
(lp9180
sg70
(lp9181
sg26
(lp9182
sg277
(lp9183
sg163
(lp9184
sg89
(lp9185
sg91
(lp9186
sg12
(lp9187
sg94
(lp9188
sg96
(lp9189
sg48
(lp9190
sg99
(lp9191
sg313
(lp9192
sg44
(lp9193
sg149
(lp9194
sg429
(lp9195
sg102
(lp9196
sg104
(lp9197
sg108
(lp9198
sg110
(lp9199
sg63
(lp9200
sg52
(lp9201
sg128
(lp9202
sg130
(lp9203
sg132
(lp9204
sg14
(lp9205
sg16
(lp9206
sg135
(lp9207
sg50
(lp9208
sg138
(lp9209
sg140
(lp9210
sg354
(lp9211
sg306
(lp9212
sg87
(lp9213
sg245
(lp9214
sg46
(lp9215
sg20
(lp9216
sg18
(lp9217
sg221
(lp9218
sg535
(lp9219
sg223
(lp9220
sg350
(lp9221
sg216
(lp9222
sg174
(lp9223
sg440
(lp9224
sg332
(lp9225
sg121
(lp9226
sg4
(lp9227
sg6
(lp9228
sg8
(lp9229
sg126
(lp9230
sg341
(lp9231
sg30
(lp9232
sg287
(lp9233
sg74
(lp9234
sg176
(lp9235
sg145
(lp9236
sg256
(lp9237
sg76
(lp9238
sg262
(lp9239
sg295
(lp9240
sg183
(lp9241
sg42
(lp9242
I3355
asg230
(lp9243
sg329
(lp9244
sg32
(lp9245
sg318
(lp9246
sg178
(lp9247
sg22
(lp9248
sg181
(lp9249
sg235
(lp9250
sg384
(lp9251
sg124
(lp9252
ssS'turkey'
p9253
(dp9254
g178
(lp9255
I22
assS'pmlyx'
p9256
(dp9257
g72
(lp9258
I1062
assS'power'
p9259
(dp9260
g283
(lp9261
sg287
(lp9262
sg74
(lp9263
sg145
(lp9264
sg344
(lp9265
sg183
(lp9266
sg59
(lp9267
sg83
(lp9268
sg85
(lp9269
sg87
(lp9270
sg91
(lp9271
sg245
(lp9272
sg20
(lp9273
sg48
(lp9274
sg99
(lp9275
sg313
(lp9276
sg116
(lp9277
sg118
(lp9278
sg429
(lp9279
sg102
(lp9280
sg110
(lp9281
sg63
(lp9282
sg230
(lp9283
sg174
(lp9284
sg318
(lp9285
sg121
(lp9286
sg22
(lp9287
sg8
(lp9288
sg34
(lp9289
sg40
(lp9290
sg78
(lp9291
sg132
(lp9292
sg14
(lp9293
sg16
(lp9294
sg135
(lp9295
I65
assS'bivari'
p9296
(dp9297
g329
(lp9298
I2557
asg178
(lp9299
ssS'noton'
p9300
(dp9301
g178
(lp9302
I2632
assS'zoubin'
p9303
(dp9304
g313
(lp9305
I9
assS'standpoint'
p9306
(dp9307
g535
(lp9308
I149
assS'iconip'
p9309
(dp9310
g72
(lp9311
I3551
assS'acc'
p9312
(dp9313
g4
(lp9314
I1546
assS'stone'
p9315
(dp9316
g183
(lp9317
I6623
assS'ace'
p9318
(dp9319
g4
(lp9320
I1498
assS'aco'
p9321
(dp9322
g145
(lp9323
I177
assS'acm'
p9324
(dp9325
g287
(lp9326
sg344
(lp9327
sg83
(lp9328
sg85
(lp9329
sg40
(lp9330
sg110
(lp9331
sg223
(lp9332
I3432
assS'pseudobay'
p9333
(dp9334
g354
(lp9335
I2113
assS'bupa'
p9336
(dp9337
g221
(lp9338
I2200
assS'neighbor'
p9339
(dp9340
g70
(lp9341
sg26
(lp9342
sg277
(lp9343
sg30
(lp9344
sg74
(lp9345
sg293
(lp9346
sg245
(lp9347
sg99
(lp9348
sg223
(lp9349
sg149
(lp9350
sg12
(lp9351
sg429
(lp9352
sg178
(lp9353
sg63
(lp9354
sg174
(lp9355
sg121
(lp9356
sg181
(lp9357
sg8
(lp9358
sg460
(lp9359
sg281
(lp9360
sg44
(lp9361
sg354
(lp9362
I1795
assS'act'
p9363
(dp9364
g216
(lp9365
sg332
(lp9366
sg121
(lp9367
sg4
(lp9368
sg293
(lp9369
sg295
(lp9370
sg183
(lp9371
sg72
(lp9372
sg83
(lp9373
sg10
(lp9374
sg429
(lp9375
sg318
(lp9376
sg89
(lp9377
sg78
(lp9378
sg132
(lp9379
I2147
asg94
(lp9380
sg99
(lp9381
ssS'uhlenbeck'
p9382
(dp9383
g262
(lp9384
I901
assS'industri'
p9385
(dp9386
g14
(lp9387
sg16
(lp9388
I2463
asg124
(lp9389
sg10
(lp9390
sg78
(lp9391
ssS'bond'
p9392
(dp9393
g26
(lp9394
I93
assS'skew'
p9395
(dp9396
g46
(lp9397
sg138
(lp9398
I1005
asg44
(lp9399
sg63
(lp9400
ssS'berger'
p9401
(dp9402
g102
(lp9403
sg354
(lp9404
I348
assS'qrb'
p9405
(dp9406
g46
(lp9407
I2456
assS'effici'
p9408
(dp9409
g283
(lp9410
sg70
(lp9411
sg78
(lp9412
sg163
(lp9413
sg26
(lp9414
sg74
(lp9415
sg145
(lp9416
sg256
(lp9417
sg262
(lp9418
sg344
(lp9419
sg183
(lp9420
sg83
(lp9421
sg85
(lp9422
sg42
(lp9423
I21
asg306
(lp9424
sg87
(lp9425
sg245
(lp9426
sg94
(lp9427
sg96
(lp9428
sg221
(lp9429
sg313
(lp9430
sg44
(lp9431
sg429
(lp9432
sg174
(lp9433
sg440
(lp9434
sg318
(lp9435
sg8
(lp9436
sg34
(lp9437
sg460
(lp9438
sg126
(lp9439
sg281
(lp9440
sg10
(lp9441
sg130
(lp9442
sg135
(lp9443
sg50
(lp9444
sg138
(lp9445
sg140
(lp9446
ssS'bja'
p9447
(dp9448
g176
(lp9449
I1807
assS'utomat'
p9450
(dp9451
g535
(lp9452
I2203
asg293
(lp9453
ssS'surviv'
p9454
(dp9455
g78
(lp9456
sg18
(lp9457
sg149
(lp9458
I1654
assS'qrs'
p9459
(dp9460
g46
(lp9461
sg135
(lp9462
I385
assS'hanson'
p9463
(dp9464
g30
(lp9465
sg174
(lp9466
sg318
(lp9467
sg178
(lp9468
sg6
(lp9469
sg78
(lp9470
sg124
(lp9471
sg429
(lp9472
sg89
(lp9473
sg223
(lp9474
sg132
(lp9475
sg50
(lp9476
sg138
(lp9477
I3351
asg44
(lp9478
ssS'pivot'
p9479
(dp9480
g74
(lp9481
I126
assS'essen'
p9482
(dp9483
g12
(lp9484
I2808
asg178
(lp9485
sg8
(lp9486
ssS'linesearch'
p9487
(dp9488
g36
(lp9489
I2406
assS'her'
p9490
(dp9491
g99
(lp9492
I2936
asg44
(lp9493
ssS'olltpul'
p9494
(dp9495
g108
(lp9496
I1617
assS'hex'
p9497
(dp9498
g344
(lp9499
sg104
(lp9500
sg145
(lp9501
sg313
(lp9502
I1353
assS'philadelphia'
p9503
(dp9504
g245
(lp9505
I32
assS'abiba'
p9506
(dp9507
g332
(lp9508
I2170
assS'gindi'
p9509
(dp9510
g429
(lp9511
I10
assS'italia'
p9512
(dp9513
g44
(lp9514
I21
assS'heb'
p9515
(dp9516
g535
(lp9517
I503
assS'pinwheel'
p9518
(dp9519
g48
(lp9520
I257
assS'goodman'
p9521
(dp9522
g114
(lp9523
I37
assS'witt'
p9524
(dp9525
g32
(lp9526
I3134
assS'ago'
p9527
(dp9528
g102
(lp9529
I1882
assS'bottl'
p9530
(dp9531
g223
(lp9532
I2170
assS'conclud'
p9533
(dp9534
g230
(lp9535
sg145
(lp9536
sg80
(lp9537
sg6
(lp9538
sg235
(lp9539
sg344
(lp9540
sg78
(lp9541
sg384
(lp9542
sg281
(lp9543
sg89
(lp9544
I1430
asg110
(lp9545
sg59
(lp9546
sg350
(lp9547
ssS'bundl'
p9548
(dp9549
g78
(lp9550
sg32
(lp9551
I2848
asg429
(lp9552
ssS'amplifi'
p9553
(dp9554
g132
(lp9555
I2109
asg245
(lp9556
sg78
(lp9557
sg256
(lp9558
sg149
(lp9559
ssS'estimalion'
p9560
(dp9561
g85
(lp9562
I4114
assS'continuit'
p9563
(dp9564
g48
(lp9565
I756
assS'wiluam'
p9566
(dp9567
g124
(lp9568
I1752
assS'with'
p9569
(dp9570
g80
(lp9571
sg293
(lp9572
sg344
(lp9573
sg78
(lp9574
sg59
(lp9575
sg484
(lp9576
sg38
(lp9577
sg83
(lp9578
sg85
(lp9579
sg303
(lp9580
sg438
(lp9581
sg116
(lp9582
sg118
(lp9583
sg34
(lp9584
sg36
(lp9585
sg460
(lp9586
sg68
(lp9587
sg72
(lp9588
sg281
(lp9589
sg10
(lp9590
sg40
(lp9591
sg283
(lp9592
sg70
(lp9593
sg26
(lp9594
sg277
(lp9595
sg163
(lp9596
sg89
(lp9597
sg91
(lp9598
sg12
(lp9599
sg94
(lp9600
sg96
(lp9601
sg48
(lp9602
sg99
(lp9603
sg313
(lp9604
sg44
(lp9605
sg149
(lp9606
sg429
(lp9607
sg102
(lp9608
sg104
(lp9609
sg106
(lp9610
sg108
(lp9611
sg110
(lp9612
sg63
(lp9613
sg52
(lp9614
sg114
(lp9615
sg128
(lp9616
sg130
(lp9617
sg132
(lp9618
sg14
(lp9619
sg16
(lp9620
sg135
(lp9621
sg50
(lp9622
sg138
(lp9623
sg140
(lp9624
sg354
(lp9625
sg306
(lp9626
sg87
(lp9627
sg245
(lp9628
sg46
(lp9629
sg20
(lp9630
sg18
(lp9631
sg221
(lp9632
sg535
(lp9633
sg223
(lp9634
sg350
(lp9635
sg216
(lp9636
sg174
(lp9637
sg440
(lp9638
sg332
(lp9639
sg121
(lp9640
sg4
(lp9641
sg6
(lp9642
sg8
(lp9643
sg126
(lp9644
sg341
(lp9645
sg30
(lp9646
sg287
(lp9647
sg74
(lp9648
sg176
(lp9649
sg145
(lp9650
sg256
(lp9651
sg76
(lp9652
sg262
(lp9653
sg295
(lp9654
sg183
(lp9655
sg42
(lp9656
I31
asg230
(lp9657
sg329
(lp9658
sg32
(lp9659
sg318
(lp9660
sg178
(lp9661
sg22
(lp9662
sg181
(lp9663
sg235
(lp9664
sg384
(lp9665
sg124
(lp9666
ssS'mica'
p9667
(dp9668
g438
(lp9669
I864
assS'kurtzberg'
p9670
(dp9671
g42
(lp9672
I3401
assS'pull'
p9673
(dp9674
g20
(lp9675
I810
asg256
(lp9676
ssS'tripl'
p9677
(dp9678
g104
(lp9679
I896
asg68
(lp9680
sg80
(lp9681
ssS'darken'
p9682
(dp9683
g118
(lp9684
sg121
(lp9685
I1645
asg78
(lp9686
ssS'nisan'
p9687
(dp9688
g145
(lp9689
I3135
assS'riiger'
p9690
(dp9691
g34
(lp9692
I7
assS'assp'
p9693
(dp9694
g460
(lp9695
sg121
(lp9696
I2760
asg76
(lp9697
sg114
(lp9698
ssS'puls'
p9699
(dp9700
g174
(lp9701
sg277
(lp9702
sg245
(lp9703
sg14
(lp9704
sg106
(lp9705
I1397
asg18
(lp9706
sg16
(lp9707
sg114
(lp9708
ssS'darker'
p9709
(dp9710
g118
(lp9711
sg145
(lp9712
I1586
assS'aa'
p9713
(dp9714
g116
(lp9715
sg332
(lp9716
sg121
(lp9717
sg6
(lp9718
sg124
(lp9719
sg140
(lp9720
I550
asg223
(lp9721
ssS'ohm'
p9722
(dp9723
g135
(lp9724
I1011
assS'ac'
p9725
(dp9726
g32
(lp9727
sg68
(lp9728
sg145
(lp9729
sg26
(lp9730
sg76
(lp9731
sg235
(lp9732
sg124
(lp9733
sg38
(lp9734
sg74
(lp9735
sg283
(lp9736
sg87
(lp9737
sg130
(lp9738
sg12
(lp9739
sg14
(lp9740
sg16
(lp9741
I33
asg18
(lp9742
ssS'ab'
p9743
(dp9744
g32
(lp9745
sg332
(lp9746
sg6
(lp9747
sg283
(lp9748
sg48
(lp9749
sg99
(lp9750
I3244
asg149
(lp9751
ssS'ae'
p9752
(dp9753
g12
(lp9754
sg106
(lp9755
I743
asg145
(lp9756
ssS'ad'
p9757
(dp9758
g70
(lp9759
sg26
(lp9760
sg163
(lp9761
sg287
(lp9762
sg74
(lp9763
sg176
(lp9764
sg145
(lp9765
sg76
(lp9766
sg293
(lp9767
sg295
(lp9768
sg183
(lp9769
sg80
(lp9770
sg83
(lp9771
sg85
(lp9772
sg306
(lp9773
sg89
(lp9774
sg91
(lp9775
sg245
(lp9776
sg20
(lp9777
sg114
(lp9778
sg313
(lp9779
sg44
(lp9780
sg460
(lp9781
sg429
(lp9782
sg332
(lp9783
sg108
(lp9784
sg110
(lp9785
sg52
(lp9786
sg22
(lp9787
sg116
(lp9788
sg329
(lp9789
sg440
(lp9790
sg318
(lp9791
sg121
(lp9792
sg4
(lp9793
sg235
(lp9794
sg34
(lp9795
sg384
(lp9796
sg124
(lp9797
sg341
(lp9798
sg10
(lp9799
sg128
(lp9800
sg135
(lp9801
sg50
(lp9802
sg138
(lp9803
sg140
(lp9804
sg354
(lp9805
I1200
assS'creat'
p9806
(dp9807
g116
(lp9808
sg332
(lp9809
sg121
(lp9810
sg22
(lp9811
sg6
(lp9812
sg8
(lp9813
sg295
(lp9814
sg256
(lp9815
sg460
(lp9816
sg124
(lp9817
sg76
(lp9818
sg344
(lp9819
sg429
(lp9820
sg87
(lp9821
sg183
(lp9822
sg59
(lp9823
sg14
(lp9824
sg63
(lp9825
sg354
(lp9826
I1382
assS'af'
p9827
(dp9828
g287
(lp9829
sg140
(lp9830
I2002
assS'ai'
p9831
(dp9832
g283
(lp9833
sg26
(lp9834
sg277
(lp9835
sg163
(lp9836
sg40
(lp9837
sg287
(lp9838
sg176
(lp9839
sg76
(lp9840
sg293
(lp9841
sg460
(lp9842
sg59
(lp9843
sg91
(lp9844
sg245
(lp9845
sg46
(lp9846
sg18
(lp9847
sg99
(lp9848
sg313
(lp9849
sg318
(lp9850
sg106
(lp9851
I65
asg108
(lp9852
sg110
(lp9853
sg63
(lp9854
sg52
(lp9855
sg230
(lp9856
sg118
(lp9857
sg440
(lp9858
sg332
(lp9859
sg121
(lp9860
sg4
(lp9861
sg6
(lp9862
sg8
(lp9863
sg34
(lp9864
sg36
(lp9865
sg384
(lp9866
sg124
(lp9867
sg72
(lp9868
sg535
(lp9869
sg344
(lp9870
sg132
(lp9871
sg50
(lp9872
sg138
(lp9873
ssS'certain'
p9874
(dp9875
g68
(lp9876
sg70
(lp9877
sg163
(lp9878
sg287
(lp9879
sg74
(lp9880
sg176
(lp9881
sg80
(lp9882
sg295
(lp9883
sg183
(lp9884
sg59
(lp9885
sg85
(lp9886
sg306
(lp9887
sg91
(lp9888
sg46
(lp9889
sg18
(lp9890
sg223
(lp9891
sg102
(lp9892
sg63
(lp9893
sg114
(lp9894
sg230
(lp9895
sg174
(lp9896
sg48
(lp9897
sg178
(lp9898
sg6
(lp9899
sg181
(lp9900
sg34
(lp9901
sg384
(lp9902
sg124
(lp9903
sg72
(lp9904
sg344
(lp9905
sg128
(lp9906
sg132
(lp9907
sg50
(lp9908
sg460
(lp9909
sg140
(lp9910
I2190
assS'ak'
p9911
(dp9912
g102
(lp9913
I862
asg245
(lp9914
sg68
(lp9915
sg281
(lp9916
sg235
(lp9917
ssS'aj'
p9918
(dp9919
g145
(lp9920
sg26
(lp9921
sg76
(lp9922
sg295
(lp9923
sg183
(lp9924
sg68
(lp9925
sg40
(lp9926
sg535
(lp9927
sg354
(lp9928
I2907
assS'am'
p9929
(dp9930
g230
(lp9931
sg438
(lp9932
I595
asg74
(lp9933
sg174
(lp9934
sg295
(lp9935
sg183
(lp9936
sg126
(lp9937
sg287
(lp9938
sg245
(lp9939
sg104
(lp9940
sg18
(lp9941
ssS'al'
p9942
(dp9943
g329
(lp9944
sg26
(lp9945
sg277
(lp9946
sg163
(lp9947
sg72
(lp9948
sg80
(lp9949
sg283
(lp9950
sg40
(lp9951
sg30
(lp9952
sg287
(lp9953
sg74
(lp9954
sg176
(lp9955
sg145
(lp9956
sg256
(lp9957
sg76
(lp9958
sg460
(lp9959
sg59
(lp9960
sg484
(lp9961
sg83
(lp9962
sg124
(lp9963
sg42
(lp9964
I3375
asg87
(lp9965
sg89
(lp9966
sg91
(lp9967
sg94
(lp9968
sg96
(lp9969
sg48
(lp9970
sg99
(lp9971
sg313
(lp9972
sg44
(lp9973
sg149
(lp9974
sg174
(lp9975
sg32
(lp9976
sg350
(lp9977
sg429
(lp9978
sg318
(lp9979
sg178
(lp9980
sg106
(lp9981
sg110
(lp9982
sg63
(lp9983
sg438
(lp9984
sg440
(lp9985
sg18
(lp9986
sg121
(lp9987
sg181
(lp9988
sg6
(lp9989
sg8
(lp9990
sg36
(lp9991
sg384
(lp9992
sg235
(lp9993
sg126
(lp9994
sg341
(lp9995
sg535
(lp9996
sg344
(lp9997
sg223
(lp9998
sg128
(lp9999
sg130
(lp10000
sg14
(lp10001
sg16
(lp10002
sg135
(lp10003
sg138
(lp10004
sg303
(lp10005
sg354
(lp10006
ssS'ao'
p10007
(dp10008
g230
(lp10009
sg30
(lp10010
sg332
(lp10011
I1826
asg145
(lp10012
sg76
(lp10013
sg38
(lp10014
sg59
(lp10015
sg124
(lp10016
sg126
(lp10017
sg176
(lp10018
ssS'an'
p10019
(dp10020
g80
(lp10021
sg293
(lp10022
sg344
(lp10023
sg78
(lp10024
sg59
(lp10025
sg484
(lp10026
sg38
(lp10027
sg83
(lp10028
sg85
(lp10029
sg303
(lp10030
sg438
(lp10031
sg116
(lp10032
sg118
(lp10033
sg34
(lp10034
sg36
(lp10035
sg460
(lp10036
sg68
(lp10037
sg72
(lp10038
sg281
(lp10039
sg10
(lp10040
sg40
(lp10041
sg283
(lp10042
sg70
(lp10043
sg26
(lp10044
sg277
(lp10045
sg163
(lp10046
sg89
(lp10047
sg91
(lp10048
sg12
(lp10049
sg94
(lp10050
sg96
(lp10051
sg48
(lp10052
sg99
(lp10053
sg313
(lp10054
sg44
(lp10055
sg149
(lp10056
sg429
(lp10057
sg102
(lp10058
sg104
(lp10059
sg106
(lp10060
sg108
(lp10061
sg110
(lp10062
sg63
(lp10063
sg52
(lp10064
sg114
(lp10065
sg128
(lp10066
sg130
(lp10067
sg132
(lp10068
sg14
(lp10069
sg16
(lp10070
sg135
(lp10071
sg50
(lp10072
sg138
(lp10073
sg140
(lp10074
sg354
(lp10075
sg306
(lp10076
sg87
(lp10077
sg245
(lp10078
sg46
(lp10079
sg20
(lp10080
sg18
(lp10081
sg221
(lp10082
sg535
(lp10083
sg223
(lp10084
sg350
(lp10085
sg216
(lp10086
sg174
(lp10087
sg440
(lp10088
sg332
(lp10089
sg121
(lp10090
sg4
(lp10091
sg6
(lp10092
sg8
(lp10093
sg126
(lp10094
sg341
(lp10095
sg30
(lp10096
sg287
(lp10097
sg74
(lp10098
sg176
(lp10099
sg145
(lp10100
sg256
(lp10101
sg76
(lp10102
sg262
(lp10103
sg295
(lp10104
sg183
(lp10105
sg42
(lp10106
I20
asg230
(lp10107
sg329
(lp10108
sg32
(lp10109
sg318
(lp10110
sg178
(lp10111
sg22
(lp10112
sg181
(lp10113
sg235
(lp10114
sg384
(lp10115
sg124
(lp10116
ssS'ap'
p10117
(dp10118
g106
(lp10119
I1532
asg76
(lp10120
ssS'as'
p10121
(dp10122
g80
(lp10123
sg293
(lp10124
sg344
(lp10125
sg78
(lp10126
sg59
(lp10127
sg484
(lp10128
sg38
(lp10129
sg83
(lp10130
sg85
(lp10131
sg303
(lp10132
sg438
(lp10133
sg116
(lp10134
sg118
(lp10135
sg34
(lp10136
sg36
(lp10137
sg460
(lp10138
sg68
(lp10139
sg72
(lp10140
sg281
(lp10141
sg10
(lp10142
sg40
(lp10143
sg283
(lp10144
sg70
(lp10145
sg26
(lp10146
sg277
(lp10147
sg163
(lp10148
sg89
(lp10149
sg91
(lp10150
sg12
(lp10151
sg94
(lp10152
sg96
(lp10153
sg48
(lp10154
sg99
(lp10155
sg313
(lp10156
sg44
(lp10157
sg149
(lp10158
sg429
(lp10159
sg102
(lp10160
sg104
(lp10161
sg106
(lp10162
sg108
(lp10163
sg110
(lp10164
sg63
(lp10165
sg52
(lp10166
sg114
(lp10167
sg128
(lp10168
sg130
(lp10169
sg132
(lp10170
sg14
(lp10171
sg16
(lp10172
sg135
(lp10173
sg50
(lp10174
sg138
(lp10175
sg140
(lp10176
sg354
(lp10177
sg306
(lp10178
sg87
(lp10179
sg245
(lp10180
sg46
(lp10181
sg20
(lp10182
sg18
(lp10183
sg221
(lp10184
sg535
(lp10185
sg223
(lp10186
sg350
(lp10187
sg216
(lp10188
sg174
(lp10189
sg440
(lp10190
sg332
(lp10191
sg121
(lp10192
sg4
(lp10193
sg6
(lp10194
sg8
(lp10195
sg126
(lp10196
sg341
(lp10197
sg30
(lp10198
sg287
(lp10199
sg74
(lp10200
sg176
(lp10201
sg145
(lp10202
sg256
(lp10203
sg76
(lp10204
sg262
(lp10205
sg295
(lp10206
sg183
(lp10207
sg42
(lp10208
I326
asg230
(lp10209
sg329
(lp10210
sg32
(lp10211
sg318
(lp10212
sg178
(lp10213
sg22
(lp10214
sg181
(lp10215
sg235
(lp10216
sg384
(lp10217
sg124
(lp10218
ssS'ar'
p10219
(dp10220
g295
(lp10221
sg183
(lp10222
sg72
(lp10223
sg341
(lp10224
sg132
(lp10225
I980
asg350
(lp10226
ssS'uiu'
p10227
(dp10228
g6
(lp10229
I1735
assS'at'
p10230
(dp10231
g80
(lp10232
sg293
(lp10233
sg344
(lp10234
sg78
(lp10235
sg59
(lp10236
sg484
(lp10237
sg38
(lp10238
sg83
(lp10239
sg85
(lp10240
sg303
(lp10241
sg438
(lp10242
sg116
(lp10243
sg118
(lp10244
sg34
(lp10245
sg36
(lp10246
sg460
(lp10247
sg68
(lp10248
sg72
(lp10249
sg281
(lp10250
sg10
(lp10251
sg40
(lp10252
sg283
(lp10253
sg70
(lp10254
sg26
(lp10255
sg277
(lp10256
sg163
(lp10257
sg89
(lp10258
sg91
(lp10259
sg12
(lp10260
sg94
(lp10261
sg48
(lp10262
sg99
(lp10263
sg313
(lp10264
sg44
(lp10265
sg149
(lp10266
sg429
(lp10267
sg102
(lp10268
sg104
(lp10269
sg106
(lp10270
sg108
(lp10271
sg110
(lp10272
sg63
(lp10273
sg52
(lp10274
sg114
(lp10275
sg128
(lp10276
sg130
(lp10277
sg132
(lp10278
sg14
(lp10279
sg16
(lp10280
sg135
(lp10281
sg138
(lp10282
sg140
(lp10283
sg354
(lp10284
sg306
(lp10285
sg87
(lp10286
sg245
(lp10287
sg46
(lp10288
sg20
(lp10289
sg18
(lp10290
sg221
(lp10291
sg535
(lp10292
sg223
(lp10293
sg350
(lp10294
sg216
(lp10295
sg174
(lp10296
sg440
(lp10297
sg332
(lp10298
sg121
(lp10299
sg4
(lp10300
sg6
(lp10301
sg8
(lp10302
sg126
(lp10303
sg341
(lp10304
sg30
(lp10305
sg287
(lp10306
sg74
(lp10307
sg176
(lp10308
sg145
(lp10309
sg256
(lp10310
sg76
(lp10311
sg262
(lp10312
sg295
(lp10313
sg183
(lp10314
sg42
(lp10315
I746
asg230
(lp10316
sg329
(lp10317
sg32
(lp10318
sg318
(lp10319
sg178
(lp10320
sg22
(lp10321
sg181
(lp10322
sg235
(lp10323
sg384
(lp10324
sg124
(lp10325
ssS'aw'
p10326
(dp10327
g99
(lp10328
I3323
asg22
(lp10329
sg235
(lp10330
ssS'av'
p10331
(dp10332
g230
(lp10333
sg87
(lp10334
I2132
asg262
(lp10335
ssS'ay'
p10336
(dp10337
g230
(lp10338
sg18
(lp10339
sg4
(lp10340
sg535
(lp10341
sg130
(lp10342
I1433
assS'bicuculin'
p10343
(dp10344
g438
(lp10345
I1575
assS'az'
p10346
(dp10347
g104
(lp10348
I3100
asg85
(lp10349
ssS'vuchkov'
p10350
(dp10351
g354
(lp10352
I3157
assS'abolish'
p10353
(dp10354
g42
(lp10355
I3325
asg149
(lp10356
ssS'tight'
p10357
(dp10358
g287
(lp10359
sg318
(lp10360
sg89
(lp10361
I2906
asg4
(lp10362
sg80
(lp10363
ssS'collis'
p10364
(dp10365
g42
(lp10366
I2652
assS'mdt'
p10367
(dp10368
g384
(lp10369
I1880
assS'spatial'
p10370
(dp10371
g283
(lp10372
sg70
(lp10373
sg74
(lp10374
sg256
(lp10375
sg80
(lp10376
sg293
(lp10377
sg295
(lp10378
sg183
(lp10379
sg303
(lp10380
sg48
(lp10381
sg44
(lp10382
sg149
(lp10383
sg118
(lp10384
sg102
(lp10385
sg106
(lp10386
sg216
(lp10387
sg438
(lp10388
I495
asg318
(lp10389
sg181
(lp10390
sg8
(lp10391
sg460
(lp10392
sg124
(lp10393
ssS'taol'
p10394
(dp10395
g104
(lp10396
I2721
assS'granul'
p10397
(dp10398
g106
(lp10399
I2862
assS'coactiv'
p10400
(dp10401
g106
(lp10402
I1321
assS'congress'
p10403
(dp10404
g12
(lp10405
sg484
(lp10406
sg149
(lp10407
I3167
assS'terri'
p10408
(dp10409
g303
(lp10410
sg50
(lp10411
I8
asg91
(lp10412
sg350
(lp10413
ssS'moller'
p10414
(dp10415
g36
(lp10416
sg313
(lp10417
I266
assS'fukishima'
p10418
(dp10419
g114
(lp10420
I286
assS'slant'
p10421
(dp10422
g42
(lp10423
I1732
asg76
(lp10424
ssS'luca'
p10425
(dp10426
g163
(lp10427
I5
assS'gyrus'
p10428
(dp10429
g106
(lp10430
I2861
assS'vocabulari'
p10431
(dp10432
g440
(lp10433
I2144
asg87
(lp10434
sg76
(lp10435
ssS'geodes'
p10436
(dp10437
g32
(lp10438
I3015
assS'rxi'
p10439
(dp10440
g178
(lp10441
I348
asg281
(lp10442
ssS'ihmsact'
p10443
(dp10444
g283
(lp10445
I1969
assS'adap'
p10446
(dp10447
g42
(lp10448
I1007
assS'mask'
p10449
(dp10450
g59
(lp10451
sg332
(lp10452
I781
asg52
(lp10453
ssS'cfri'
p10454
(dp10455
g116
(lp10456
I11
assS'mimic'
p10457
(dp10458
g245
(lp10459
I1214
asg30
(lp10460
sg484
(lp10461
sg303
(lp10462
ssS'mass'
p10463
(dp10464
g59
(lp10465
sg221
(lp10466
sg130
(lp10467
I384
assS'adam'
p10468
(dp10469
g99
(lp10470
I3312
assS'cps'
p10471
(dp10472
g138
(lp10473
I1913
assS'cpt'
p10474
(dp10475
g4
(lp10476
I824
assS'cpu'
p10477
(dp10478
g126
(lp10479
I2177
asg10
(lp10480
ssS'waiver'
p10481
(dp10482
g114
(lp10483
I195
assS'sci'
p10484
(dp10485
g438
(lp10486
I2443
asg176
(lp10487
sg178
(lp10488
sg287
(lp10489
sg460
(lp10490
sg293
(lp10491
sg38
(lp10492
sg12
(lp10493
sg106
(lp10494
sg48
(lp10495
sg99
(lp10496
ssS'sch'
p10497
(dp10498
g99
(lp10499
I269
assS'mahadevan'
p10500
(dp10501
g89
(lp10502
I256
assS'schizophren'
p10503
(dp10504
g4
(lp10505
I163
assS'respecifi'
p10506
(dp10507
g183
(lp10508
I5464
assS'code'
p10509
(dp10510
g70
(lp10511
sg26
(lp10512
sg277
(lp10513
sg256
(lp10514
sg80
(lp10515
sg262
(lp10516
sg295
(lp10517
sg183
(lp10518
sg63
(lp10519
sg91
(lp10520
sg94
(lp10521
sg20
(lp10522
sg18
(lp10523
sg116
(lp10524
sg118
(lp10525
sg429
(lp10526
sg96
(lp10527
sg52
(lp10528
sg114
(lp10529
sg216
(lp10530
sg174
(lp10531
sg48
(lp10532
sg22
(lp10533
sg181
(lp10534
sg36
(lp10535
sg124
(lp10536
sg72
(lp10537
sg10
(lp10538
sg40
(lp10539
sg344
(lp10540
sg128
(lp10541
sg14
(lp10542
sg16
(lp10543
sg50
(lp10544
sg138
(lp10545
I2004
assS'mnl'
p10546
(dp10547
g262
(lp10548
I17
assS'reson'
p10549
(dp10550
g104
(lp10551
I110
asg318
(lp10552
sg63
(lp10553
sg118
(lp10554
ssS'internist'
p10555
(dp10556
g91
(lp10557
I380
assS'plosiv'
p10558
(dp10559
g174
(lp10560
I2451
asg440
(lp10561
ssS'scr'
p10562
(dp10563
g78
(lp10564
I31
asg163
(lp10565
ssS'unansw'
p10566
(dp10567
g116
(lp10568
I2206
asg83
(lp10569
ssS'tx'
p10570
(dp10571
g149
(lp10572
I3176
assS'ty'
p10573
(dp10574
g178
(lp10575
sg72
(lp10576
sg83
(lp10577
sg42
(lp10578
I1014
asg70
(lp10579
sg18
(lp10580
sg221
(lp10581
ssS'tv'
p10582
(dp10583
g70
(lp10584
sg26
(lp10585
sg235
(lp10586
sg59
(lp10587
sg83
(lp10588
sg40
(lp10589
sg130
(lp10590
sg50
(lp10591
I545
asg44
(lp10592
ssS'tw'
p10593
(dp10594
g104
(lp10595
I1768
assS'tt'
p10596
(dp10597
g287
(lp10598
sg76
(lp10599
sg384
(lp10600
sg124
(lp10601
sg83
(lp10602
sg128
(lp10603
I1558
asg102
(lp10604
ssS'tu'
p10605
(dp10606
g34
(lp10607
sg484
(lp10608
sg221
(lp10609
sg354
(lp10610
I1936
assS'tr'
p10611
(dp10612
g318
(lp10613
sg174
(lp10614
sg440
(lp10615
sg48
(lp10616
sg178
(lp10617
sg256
(lp10618
sg8
(lp10619
sg36
(lp10620
sg484
(lp10621
sg329
(lp10622
sg74
(lp10623
sg293
(lp10624
sg287
(lp10625
sg87
(lp10626
sg12
(lp10627
sg108
(lp10628
sg221
(lp10629
sg223
(lp10630
sg354
(lp10631
I3084
assS'ts'
p10632
(dp10633
g329
(lp10634
sg460
(lp10635
sg18
(lp10636
sg76
(lp10637
sg149
(lp10638
I773
assS'tp'
p10639
(dp10640
g26
(lp10641
sg80
(lp10642
I608
assS'intertwin'
p10643
(dp10644
g149
(lp10645
I93
assS'tn'
p10646
(dp10647
g12
(lp10648
I2574
asg78
(lp10649
ssS'to'
p10650
(dp10651
g80
(lp10652
sg293
(lp10653
sg344
(lp10654
sg78
(lp10655
sg59
(lp10656
sg484
(lp10657
sg38
(lp10658
sg83
(lp10659
sg85
(lp10660
sg303
(lp10661
sg438
(lp10662
sg116
(lp10663
sg118
(lp10664
sg34
(lp10665
sg36
(lp10666
sg460
(lp10667
sg68
(lp10668
sg72
(lp10669
sg281
(lp10670
sg10
(lp10671
sg40
(lp10672
sg283
(lp10673
sg70
(lp10674
sg26
(lp10675
sg277
(lp10676
sg163
(lp10677
sg89
(lp10678
sg91
(lp10679
sg12
(lp10680
sg94
(lp10681
sg96
(lp10682
sg48
(lp10683
sg99
(lp10684
sg313
(lp10685
sg44
(lp10686
sg149
(lp10687
sg429
(lp10688
sg102
(lp10689
sg104
(lp10690
sg106
(lp10691
sg108
(lp10692
sg110
(lp10693
sg63
(lp10694
sg52
(lp10695
sg114
(lp10696
sg128
(lp10697
sg130
(lp10698
sg132
(lp10699
sg14
(lp10700
sg16
(lp10701
sg135
(lp10702
sg50
(lp10703
sg138
(lp10704
sg140
(lp10705
sg354
(lp10706
sg306
(lp10707
sg87
(lp10708
sg245
(lp10709
sg46
(lp10710
sg20
(lp10711
sg18
(lp10712
sg221
(lp10713
sg535
(lp10714
sg223
(lp10715
sg350
(lp10716
sg216
(lp10717
sg174
(lp10718
sg440
(lp10719
sg332
(lp10720
sg121
(lp10721
sg4
(lp10722
sg6
(lp10723
sg8
(lp10724
sg126
(lp10725
sg341
(lp10726
sg30
(lp10727
sg287
(lp10728
sg74
(lp10729
sg176
(lp10730
sg145
(lp10731
sg256
(lp10732
sg76
(lp10733
sg262
(lp10734
sg295
(lp10735
sg183
(lp10736
sg42
(lp10737
I33
asg230
(lp10738
sg329
(lp10739
sg32
(lp10740
sg318
(lp10741
sg178
(lp10742
sg22
(lp10743
sg181
(lp10744
sg235
(lp10745
sg384
(lp10746
sg124
(lp10747
ssS'tail'
p10748
(dp10749
g344
(lp10750
sg14
(lp10751
sg16
(lp10752
I1961
asg163
(lp10753
ssS'tm'
p10754
(dp10755
g116
(lp10756
sg256
(lp10757
I158
asg85
(lp10758
ssS'tj'
p10759
(dp10760
g332
(lp10761
sg121
(lp10762
sg384
(lp10763
sg38
(lp10764
sg341
(lp10765
sg85
(lp10766
sg303
(lp10767
sg87
(lp10768
sg130
(lp10769
sg104
(lp10770
sg106
(lp10771
I2934
asg18
(lp10772
sg83
(lp10773
sg149
(lp10774
ssS'tk'
p10775
(dp10776
g235
(lp10777
sg8
(lp10778
I1544
assS'th'
p10779
(dp10780
g26
(lp10781
sg277
(lp10782
sg163
(lp10783
sg287
(lp10784
sg145
(lp10785
sg80
(lp10786
sg295
(lp10787
sg183
(lp10788
sg59
(lp10789
sg38
(lp10790
sg83
(lp10791
sg42
(lp10792
I2273
asg89
(lp10793
sg94
(lp10794
sg18
(lp10795
sg99
(lp10796
sg223
(lp10797
sg104
(lp10798
sg108
(lp10799
sg114
(lp10800
sg230
(lp10801
sg438
(lp10802
sg22
(lp10803
sg181
(lp10804
sg8
(lp10805
sg34
(lp10806
sg36
(lp10807
sg68
(lp10808
sg72
(lp10809
sg281
(lp10810
sg40
(lp10811
sg344
(lp10812
sg130
(lp10813
sg14
(lp10814
sg16
(lp10815
ssS'ti'
p10816
(dp10817
g116
(lp10818
sg484
(lp10819
sg121
(lp10820
sg80
(lp10821
sg6
(lp10822
sg8
(lp10823
sg295
(lp10824
sg183
(lp10825
sg68
(lp10826
sg126
(lp10827
sg12
(lp10828
sg306
(lp10829
sg132
(lp10830
sg163
(lp10831
sg140
(lp10832
I1111
assS'tf'
p10833
(dp10834
g230
(lp10835
sg329
(lp10836
I1643
asg295
(lp10837
sg183
(lp10838
ssS'smile'
p10839
(dp10840
g138
(lp10841
I1761
asg293
(lp10842
ssS'td'
p10843
(dp10844
g132
(lp10845
I433
asg306
(lp10846
sg89
(lp10847
sg83
(lp10848
ssS'te'
p10849
(dp10850
g42
(lp10851
I2377
asg34
(lp10852
sg106
(lp10853
sg99
(lp10854
sg306
(lp10855
ssS'tb'
p10856
(dp10857
g295
(lp10858
I1365
asg183
(lp10859
ssS'tc'
p10860
(dp10861
g8
(lp10862
I1759
assS'norm'
p10863
(dp10864
g145
(lp10865
sg8
(lp10866
sg34
(lp10867
sg68
(lp10868
sg38
(lp10869
sg306
(lp10870
sg96
(lp10871
I2161
assS'adapt'
p10872
(dp10873
g124
(lp10874
sg72
(lp10875
sg283
(lp10876
sg176
(lp10877
sg256
(lp10878
sg295
(lp10879
sg183
(lp10880
sg59
(lp10881
sg38
(lp10882
sg83
(lp10883
sg303
(lp10884
sg306
(lp10885
sg89
(lp10886
sg91
(lp10887
sg12
(lp10888
sg94
(lp10889
sg96
(lp10890
sg18
(lp10891
sg99
(lp10892
sg313
(lp10893
sg223
(lp10894
sg350
(lp10895
sg230
(lp10896
sg118
(lp10897
sg104
(lp10898
sg110
(lp10899
sg63
(lp10900
sg52
(lp10901
sg114
(lp10902
sg216
(lp10903
sg329
(lp10904
sg121
(lp10905
sg22
(lp10906
sg6
(lp10907
sg8
(lp10908
sg34
(lp10909
sg221
(lp10910
sg460
(lp10911
sg235
(lp10912
sg126
(lp10913
sg10
(lp10914
sg535
(lp10915
sg44
(lp10916
sg128
(lp10917
sg78
(lp10918
sg132
(lp10919
sg149
(lp10920
sg50
(lp10921
sg138
(lp10922
I719
assS'normalis'
p10923
(dp10924
g176
(lp10925
sg126
(lp10926
I2774
asg52
(lp10927
ssS'ppli'
p10928
(dp10929
g313
(lp10930
I1708
assS'candid'
p10931
(dp10932
g230
(lp10933
sg116
(lp10934
sg121
(lp10935
sg42
(lp10936
I202
asg306
(lp10937
sg94
(lp10938
sg18
(lp10939
sg110
(lp10940
sg313
(lp10941
sg44
(lp10942
sg354
(lp10943
ssS'unclassifi'
p10944
(dp10945
g63
(lp10946
I97
assS'orthonorm'
p10947
(dp10948
g32
(lp10949
sg22
(lp10950
sg163
(lp10951
sg295
(lp10952
sg183
(lp10953
sg38
(lp10954
sg12
(lp10955
sg96
(lp10956
sg44
(lp10957
I1002
assS'strand'
p10958
(dp10959
g384
(lp10960
I23
asg176
(lp10961
sg26
(lp10962
ssS'unalt'
p10963
(dp10964
g106
(lp10965
I2161
assS'harri'
p10966
(dp10967
g183
(lp10968
sg106
(lp10969
I1224
asg429
(lp10970
ssS'smc'
p10971
(dp10972
g178
(lp10973
sg535
(lp10974
I2133
asg181
(lp10975
sg63
(lp10976
ssS'colleagu'
p10977
(dp10978
g118
(lp10979
sg176
(lp10980
sg44
(lp10981
I217
assS'pbx'
p10982
(dp10983
g14
(lp10984
sg16
(lp10985
I2636
assS'smale'
p10986
(dp10987
g46
(lp10988
I3612
asg287
(lp10989
ssS'sand'
p10990
(dp10991
g99
(lp10992
I872
asg85
(lp10993
sg44
(lp10994
sg40
(lp10995
ssS'adjust'
p10996
(dp10997
g68
(lp10998
sg70
(lp10999
sg26
(lp11000
sg277
(lp11001
sg76
(lp11002
sg295
(lp11003
sg183
(lp11004
sg149
(lp11005
sg91
(lp11006
sg245
(lp11007
sg20
(lp11008
sg114
(lp11009
sg223
(lp11010
sg350
(lp11011
sg329
(lp11012
sg108
(lp11013
sg4
(lp11014
sg230
(lp11015
sg174
(lp11016
sg318
(lp11017
sg22
(lp11018
sg34
(lp11019
sg124
(lp11020
sg44
(lp11021
sg130
(lp11022
sg14
(lp11023
sg16
(lp11024
sg135
(lp11025
sg138
(lp11026
I3180
assS'small'
p11027
(dp11028
g124
(lp11029
sg70
(lp11030
sg78
(lp11031
sg277
(lp11032
sg163
(lp11033
sg281
(lp11034
sg283
(lp11035
sg85
(lp11036
sg181
(lp11037
sg26
(lp11038
sg350
(lp11039
sg74
(lp11040
sg176
(lp11041
sg145
(lp11042
sg256
(lp11043
sg262
(lp11044
sg295
(lp11045
sg183
(lp11046
sg59
(lp11047
sg484
(lp11048
sg38
(lp11049
sg83
(lp11050
sg114
(lp11051
sg42
(lp11052
I159
asg87
(lp11053
sg89
(lp11054
sg460
(lp11055
sg12
(lp11056
sg94
(lp11057
sg20
(lp11058
sg48
(lp11059
sg221
(lp11060
sg535
(lp11061
sg44
(lp11062
sg149
(lp11063
sg116
(lp11064
sg287
(lp11065
sg245
(lp11066
sg68
(lp11067
sg46
(lp11068
sg102
(lp11069
sg104
(lp11070
sg108
(lp11071
sg110
(lp11072
sg63
(lp11073
sg52
(lp11074
sg22
(lp11075
sg230
(lp11076
sg329
(lp11077
sg118
(lp11078
sg318
(lp11079
sg178
(lp11080
sg4
(lp11081
sg6
(lp11082
sg8
(lp11083
sg34
(lp11084
sg36
(lp11085
sg384
(lp11086
sg235
(lp11087
sg126
(lp11088
sg341
(lp11089
sg10
(lp11090
sg40
(lp11091
sg344
(lp11092
sg223
(lp11093
sg128
(lp11094
sg130
(lp11095
sg132
(lp11096
sg14
(lp11097
sg16
(lp11098
sg135
(lp11099
sg50
(lp11100
sg138
(lp11101
sg140
(lp11102
sg354
(lp11103
ssS'mammal'
p11104
(dp11105
g176
(lp11106
I125
assS'cortifug'
p11107
(dp11108
g70
(lp11109
I2512
assS'quicker'
p11110
(dp11111
g332
(lp11112
I1928
asg76
(lp11113
ssS'bve'
p11114
(dp11115
g48
(lp11116
I252
assS'infinitesim'
p11117
(dp11118
g132
(lp11119
I1322
asg32
(lp11120
ssS'iwi'
p11121
(dp11122
g40
(lp11123
I697
assS'readapt'
p11124
(dp11125
g149
(lp11126
I2514
assS'syna'
p11127
(dp11128
g135
(lp11129
I602
assS'past'
p11130
(dp11131
g230
(lp11132
sg329
(lp11133
sg176
(lp11134
sg121
(lp11135
sg256
(lp11136
sg8
(lp11137
sg83
(lp11138
sg42
(lp11139
I2008
asg87
(lp11140
sg128
(lp11141
sg102
(lp11142
sg94
(lp11143
sg106
(lp11144
sg18
(lp11145
sg50
(lp11146
sg223
(lp11147
ssS'qml'
p11148
(dp11149
g72
(lp11150
I3286
assS'pass'
p11151
(dp11152
g283
(lp11153
sg70
(lp11154
sg277
(lp11155
sg287
(lp11156
sg256
(lp11157
sg76
(lp11158
sg262
(lp11159
sg295
(lp11160
sg183
(lp11161
sg83
(lp11162
sg42
(lp11163
I2351
asg89
(lp11164
sg245
(lp11165
sg46
(lp11166
sg18
(lp11167
sg102
(lp11168
sg178
(lp11169
sg63
(lp11170
sg52
(lp11171
sg116
(lp11172
sg174
(lp11173
sg332
(lp11174
sg121
(lp11175
sg22
(lp11176
sg36
(lp11177
sg293
(lp11178
sg10
(lp11179
ssS'suboptim'
p11180
(dp11181
g329
(lp11182
I1001
asg38
(lp11183
sg85
(lp11184
ssS'ddch'
p11185
(dp11186
g384
(lp11187
I1898
assS'morasso'
p11188
(dp11189
g59
(lp11190
I3374
assS'excitatori'
p11191
(dp11192
g216
(lp11193
sg438
(lp11194
I1189
asg118
(lp11195
sg332
(lp11196
sg70
(lp11197
sg4
(lp11198
sg6
(lp11199
sg174
(lp11200
sg176
(lp11201
sg102
(lp11202
sg106
(lp11203
sg50
(lp11204
sg535
(lp11205
sg149
(lp11206
ssS'systemtim'
p11207
(dp11208
g83
(lp11209
I2317
assS'richard'
p11210
(dp11211
g74
(lp11212
sg26
(lp11213
sg132
(lp11214
sg106
(lp11215
I2509
asg135
(lp11216
sg138
(lp11217
ssS'clock'
p11218
(dp11219
g22
(lp11220
sg8
(lp11221
sg68
(lp11222
sg10
(lp11223
sg20
(lp11224
sg135
(lp11225
I789
assS'section'
p11226
(dp11227
g124
(lp11228
sg277
(lp11229
sg163
(lp11230
sg281
(lp11231
sg40
(lp11232
sg30
(lp11233
sg287
(lp11234
sg74
(lp11235
sg145
(lp11236
sg256
(lp11237
sg76
(lp11238
sg295
(lp11239
sg183
(lp11240
sg83
(lp11241
sg85
(lp11242
sg42
(lp11243
I2838
asg306
(lp11244
sg89
(lp11245
sg12
(lp11246
sg94
(lp11247
sg96
(lp11248
sg18
(lp11249
sg99
(lp11250
sg313
(lp11251
sg44
(lp11252
sg350
(lp11253
sg116
(lp11254
sg68
(lp11255
sg102
(lp11256
sg104
(lp11257
sg108
(lp11258
sg114
(lp11259
sg230
(lp11260
sg329
(lp11261
sg32
(lp11262
sg318
(lp11263
sg22
(lp11264
sg6
(lp11265
sg8
(lp11266
sg34
(lp11267
sg221
(lp11268
sg460
(lp11269
sg235
(lp11270
sg72
(lp11271
sg341
(lp11272
sg10
(lp11273
sg535
(lp11274
sg344
(lp11275
sg36
(lp11276
sg132
(lp11277
sg14
(lp11278
sg16
(lp11279
sg135
(lp11280
sg50
(lp11281
sg138
(lp11282
sg354
(lp11283
ssS'hepatoma'
p11284
(dp11285
g484
(lp11286
I114
assS'higherdimension'
p11287
(dp11288
g68
(lp11289
I805
assS'delet'
p11290
(dp11291
g96
(lp11292
I1993
asg63
(lp11293
ssS'abbrevi'
p11294
(dp11295
g130
(lp11296
I1751
assS'guckenheim'
p11297
(dp11298
g18
(lp11299
I536
assS'otot'
p11300
(dp11301
g230
(lp11302
I1108
assS'method'
p11303
(dp11304
g124
(lp11305
sg70
(lp11306
sg26
(lp11307
sg277
(lp11308
sg163
(lp11309
sg72
(lp11310
sg281
(lp11311
sg283
(lp11312
sg30
(lp11313
sg287
(lp11314
sg74
(lp11315
sg145
(lp11316
sg76
(lp11317
sg293
(lp11318
sg295
(lp11319
sg183
(lp11320
sg59
(lp11321
sg484
(lp11322
sg83
(lp11323
sg85
(lp11324
sg42
(lp11325
I22
asg306
(lp11326
sg87
(lp11327
sg89
(lp11328
sg91
(lp11329
sg12
(lp11330
sg94
(lp11331
sg96
(lp11332
sg99
(lp11333
sg535
(lp11334
sg44
(lp11335
sg230
(lp11336
sg116
(lp11337
sg245
(lp11338
sg429
(lp11339
sg68
(lp11340
sg46
(lp11341
sg102
(lp11342
sg106
(lp11343
sg63
(lp11344
sg52
(lp11345
sg114
(lp11346
sg216
(lp11347
sg438
(lp11348
sg440
(lp11349
sg178
(lp11350
sg22
(lp11351
sg8
(lp11352
sg34
(lp11353
sg221
(lp11354
sg460
(lp11355
sg235
(lp11356
sg126
(lp11357
sg341
(lp11358
sg40
(lp11359
sg344
(lp11360
sg223
(lp11361
sg130
(lp11362
sg132
(lp11363
sg14
(lp11364
sg16
(lp11365
sg135
(lp11366
sg138
(lp11367
sg140
(lp11368
sg354
(lp11369
ssS'contrast'
p11370
(dp11371
g287
(lp11372
sg74
(lp11373
sg256
(lp11374
sg76
(lp11375
sg262
(lp11376
sg295
(lp11377
sg183
(lp11378
sg245
(lp11379
sg20
(lp11380
sg221
(lp11381
sg313
(lp11382
sg44
(lp11383
sg329
(lp11384
sg12
(lp11385
sg429
(lp11386
sg102
(lp11387
sg106
(lp11388
sg63
(lp11389
sg116
(lp11390
sg438
(lp11391
I1500
asg32
(lp11392
sg318
(lp11393
sg6
(lp11394
sg384
(lp11395
sg68
(lp11396
sg341
(lp11397
sg10
(lp11398
sg118
(lp11399
sg78
(lp11400
sg132
(lp11401
sg14
(lp11402
sg354
(lp11403
ssS'hasn'
p11404
(dp11405
g94
(lp11406
I706
assS'full'
p11407
(dp11408
g26
(lp11409
sg30
(lp11410
sg287
(lp11411
sg293
(lp11412
sg295
(lp11413
sg183
(lp11414
sg38
(lp11415
sg85
(lp11416
sg306
(lp11417
sg87
(lp11418
sg91
(lp11419
sg44
(lp11420
sg149
(lp11421
sg32
(lp11422
sg118
(lp11423
sg440
(lp11424
sg22
(lp11425
sg235
(lp11426
sg384
(lp11427
sg124
(lp11428
sg126
(lp11429
sg341
(lp11430
sg10
(lp11431
sg78
(lp11432
sg14
(lp11433
sg16
(lp11434
sg138
(lp11435
sg140
(lp11436
sg354
(lp11437
I2098
assS'unmodifi'
p11438
(dp11439
g96
(lp11440
I2092
assS'bledso'
p11441
(dp11442
g52
(lp11443
I963
assS'sy'
p11444
(dp11445
g145
(lp11446
I3209
asg83
(lp11447
ssS'inher'
p11448
(dp11449
g145
(lp11450
sg22
(lp11451
sg235
(lp11452
sg303
(lp11453
sg20
(lp11454
sg135
(lp11455
I91
asg52
(lp11456
ssS'prevlous'
p11457
(dp11458
g440
(lp11459
I685
assS'soraku'
p11460
(dp11461
g295
(lp11462
sg183
(lp11463
sg18
(lp11464
I42
assS'joliot'
p11465
(dp11466
g176
(lp11467
I2610
assS'persu'
p11468
(dp11469
g14
(lp11470
sg16
(lp11471
I2423
assS'prior'
p11472
(dp11473
g277
(lp11474
sg30
(lp11475
sg74
(lp11476
sg76
(lp11477
sg262
(lp11478
sg295
(lp11479
sg183
(lp11480
sg83
(lp11481
sg306
(lp11482
sg87
(lp11483
sg91
(lp11484
sg221
(lp11485
sg313
(lp11486
sg223
(lp11487
sg116
(lp11488
sg293
(lp11489
sg106
(lp11490
I1776
asg230
(lp11491
sg329
(lp11492
sg318
(lp11493
sg4
(lp11494
sg181
(lp11495
sg460
(lp11496
sg124
(lp11497
sg126
(lp11498
sg281
(lp11499
sg132
(lp11500
sg14
(lp11501
sg16
(lp11502
sg50
(lp11503
sg138
(lp11504
sg354
(lp11505
ssS'perso'
p11506
(dp11507
g44
(lp11508
I43
assS'social'
p11509
(dp11510
g4
(lp11511
I482
assS'action'
p11512
(dp11513
g329
(lp11514
sg18
(lp11515
sg4
(lp11516
sg6
(lp11517
sg293
(lp11518
sg429
(lp11519
sg181
(lp11520
sg68
(lp11521
sg83
(lp11522
sg277
(lp11523
sg350
(lp11524
sg306
(lp11525
sg89
(lp11526
sg59
(lp11527
sg14
(lp11528
sg106
(lp11529
I750
asg20
(lp11530
sg313
(lp11531
sg52
(lp11532
sg354
(lp11533
ssS'introductori'
p11534
(dp11535
g535
(lp11536
I1334
assS'eberman'
p11537
(dp11538
g460
(lp11539
I1800
assS'diamet'
p11540
(dp11541
g36
(lp11542
sg106
(lp11543
I1065
asg283
(lp11544
ssS'via'
p11545
(dp11546
g74
(lp11547
sg145
(lp11548
sg293
(lp11549
sg344
(lp11550
sg59
(lp11551
sg83
(lp11552
sg303
(lp11553
sg42
(lp11554
I1955
asg91
(lp11555
sg46
(lp11556
sg20
(lp11557
sg99
(lp11558
sg313
(lp11559
sg102
(lp11560
sg104
(lp11561
sg96
(lp11562
sg440
(lp11563
sg318
(lp11564
sg8
(lp11565
sg36
(lp11566
sg460
(lp11567
sg124
(lp11568
sg72
(lp11569
sg10
(lp11570
sg221
(lp11571
sg132
(lp11572
sg14
(lp11573
sg16
(lp11574
sg135
(lp11575
sg50
(lp11576
ssS'lehmkuhl'
p11577
(dp11578
g438
(lp11579
I2426
assS'vie'
p11580
(dp11581
g306
(lp11582
I899
assS'vij'
p11583
(dp11584
g130
(lp11585
I2159
assS'vik'
p11586
(dp11587
g130
(lp11588
I2927
assS'vii'
p11589
(dp11590
g12
(lp11591
I870
asg46
(lp11592
sg121
(lp11593
ssS'corti'
p11594
(dp11595
g174
(lp11596
I271
assS'vis'
p11597
(dp11598
g12
(lp11599
I2774
assS'vip'
p11600
(dp11601
g303
(lp11602
I277
assS'captor'
p11603
(dp11604
g332
(lp11605
I2000
assS'vit'
p11606
(dp11607
g12
(lp11608
I1078
assS'unbias'
p11609
(dp11610
g235
(lp11611
I825
assS'decrement'
p11612
(dp11613
g118
(lp11614
sg70
(lp11615
sg99
(lp11616
I2136
assS'select'
p11617
(dp11618
g283
(lp11619
sg78
(lp11620
sg277
(lp11621
sg163
(lp11622
sg72
(lp11623
sg26
(lp11624
sg30
(lp11625
sg350
(lp11626
sg176
(lp11627
sg145
(lp11628
sg293
(lp11629
sg295
(lp11630
sg183
(lp11631
sg83
(lp11632
sg85
(lp11633
sg303
(lp11634
sg42
(lp11635
I861
asg87
(lp11636
sg91
(lp11637
sg12
(lp11638
sg94
(lp11639
sg96
(lp11640
sg18
(lp11641
sg221
(lp11642
sg313
(lp11643
sg44
(lp11644
sg149
(lp11645
sg230
(lp11646
sg329
(lp11647
sg116
(lp11648
sg245
(lp11649
sg429
(lp11650
sg46
(lp11651
sg102
(lp11652
sg104
(lp11653
sg106
(lp11654
sg110
(lp11655
sg20
(lp11656
sg114
(lp11657
sg216
(lp11658
sg438
(lp11659
sg318
(lp11660
sg178
(lp11661
sg4
(lp11662
sg8
(lp11663
sg460
(lp11664
sg235
(lp11665
sg126
(lp11666
sg281
(lp11667
sg10
(lp11668
sg344
(lp11669
sg223
(lp11670
sg130
(lp11671
sg132
(lp11672
sg14
(lp11673
sg16
(lp11674
sg135
(lp11675
sg140
(lp11676
sg354
(lp11677
ssS'silva'
p11678
(dp11679
g34
(lp11680
I2946
assS'vlsi'
p11681
(dp11682
g283
(lp11683
sg22
(lp11684
sg10
(lp11685
sg245
(lp11686
sg132
(lp11687
sg14
(lp11688
sg20
(lp11689
sg135
(lp11690
I7
asg63
(lp11691
sg256
(lp11692
ssS'objectcent'
p11693
(dp11694
g303
(lp11695
I862
assS'jnflnli'
p11696
(dp11697
g174
(lp11698
I2052
assS'pearl'
p11699
(dp11700
g91
(lp11701
I433
assS'automata'
p11702
(dp11703
g329
(lp11704
sg145
(lp11705
sg128
(lp11706
I2825
asg104
(lp11707
sg96
(lp11708
sg178
(lp11709
ssS'mori'
p11710
(dp11711
g96
(lp11712
I2723
asg440
(lp11713
sg63
(lp11714
ssS'hirsch'
p11715
(dp11716
g46
(lp11717
I3609
assS'more'
p11718
(dp11719
g80
(lp11720
sg293
(lp11721
sg344
(lp11722
sg78
(lp11723
sg59
(lp11724
sg38
(lp11725
sg83
(lp11726
sg85
(lp11727
sg303
(lp11728
sg438
(lp11729
I1082
asg116
(lp11730
sg118
(lp11731
sg34
(lp11732
sg460
(lp11733
sg68
(lp11734
sg72
(lp11735
sg281
(lp11736
sg10
(lp11737
sg40
(lp11738
sg70
(lp11739
sg26
(lp11740
sg277
(lp11741
sg163
(lp11742
sg89
(lp11743
sg91
(lp11744
sg12
(lp11745
sg94
(lp11746
sg96
(lp11747
sg48
(lp11748
sg99
(lp11749
sg313
(lp11750
sg44
(lp11751
sg149
(lp11752
sg429
(lp11753
sg102
(lp11754
sg104
(lp11755
sg106
(lp11756
sg108
(lp11757
sg110
(lp11758
sg63
(lp11759
sg52
(lp11760
sg114
(lp11761
sg128
(lp11762
sg130
(lp11763
sg132
(lp11764
sg14
(lp11765
sg16
(lp11766
sg135
(lp11767
sg50
(lp11768
sg138
(lp11769
sg140
(lp11770
sg354
(lp11771
sg306
(lp11772
sg87
(lp11773
sg46
(lp11774
sg20
(lp11775
sg18
(lp11776
sg221
(lp11777
sg223
(lp11778
sg350
(lp11779
sg216
(lp11780
sg174
(lp11781
sg440
(lp11782
sg332
(lp11783
sg121
(lp11784
sg4
(lp11785
sg8
(lp11786
sg126
(lp11787
sg341
(lp11788
sg30
(lp11789
sg287
(lp11790
sg74
(lp11791
sg176
(lp11792
sg145
(lp11793
sg256
(lp11794
sg76
(lp11795
sg262
(lp11796
sg295
(lp11797
sg183
(lp11798
sg230
(lp11799
sg329
(lp11800
sg32
(lp11801
sg318
(lp11802
sg178
(lp11803
sg22
(lp11804
sg181
(lp11805
sg235
(lp11806
sg384
(lp11807
sg124
(lp11808
ssS'reachabl'
p11809
(dp11810
g460
(lp11811
sg89
(lp11812
I534
assS'cact'
p11813
(dp11814
g14
(lp11815
I3577
assS'door'
p11816
(dp11817
g42
(lp11818
I2846
asg83
(lp11819
ssS'hundr'
p11820
(dp11821
g318
(lp11822
sg8
(lp11823
sg124
(lp11824
sg94
(lp11825
sg132
(lp11826
sg14
(lp11827
sg16
(lp11828
sg135
(lp11829
I996
asg99
(lp11830
sg223
(lp11831
ssS'sect'
p11832
(dp11833
g59
(lp11834
I674
assS'visuomotor'
p11835
(dp11836
g99
(lp11837
I3301
assS'uncov'
p11838
(dp11839
g34
(lp11840
I1975
assS'multiset'
p11841
(dp11842
g72
(lp11843
I388
assS'teet'
p11844
(dp11845
g128
(lp11846
I2246
assS'cach'
p11847
(dp11848
g10
(lp11849
I658
assS'endpoint'
p11850
(dp11851
g99
(lp11852
I854
assS'reformul'
p11853
(dp11854
g295
(lp11855
I1765
asg183
(lp11856
ssS'underw'
p11857
(dp11858
g145
(lp11859
I2908
assS'phillip'
p11860
(dp11861
g135
(lp11862
I2425
assS'malt'
p11863
(dp11864
g174
(lp11865
I1061
assS'bias'
p11866
(dp11867
g70
(lp11868
sg277
(lp11869
sg30
(lp11870
sg287
(lp11871
sg74
(lp11872
sg256
(lp11873
sg76
(lp11874
sg295
(lp11875
sg183
(lp11876
sg484
(lp11877
sg83
(lp11878
sg303
(lp11879
sg89
(lp11880
sg20
(lp11881
sg48
(lp11882
sg99
(lp11883
sg313
(lp11884
sg223
(lp11885
sg18
(lp11886
sg110
(lp11887
sg114
(lp11888
sg329
(lp11889
sg318
(lp11890
sg121
(lp11891
sg4
(lp11892
sg235
(lp11893
sg221
(lp11894
sg124
(lp11895
sg126
(lp11896
sg341
(lp11897
sg344
(lp11898
sg128
(lp11899
sg36
(lp11900
sg132
(lp11901
sg135
(lp11902
sg50
(lp11903
sg138
(lp11904
sg140
(lp11905
I394
assS'morgan'
p11906
(dp11907
g283
(lp11908
sg26
(lp11909
sg30
(lp11910
sg256
(lp11911
sg76
(lp11912
sg293
(lp11913
sg295
(lp11914
sg183
(lp11915
sg59
(lp11916
sg306
(lp11917
sg87
(lp11918
sg89
(lp11919
sg91
(lp11920
sg12
(lp11921
sg94
(lp11922
sg20
(lp11923
sg48
(lp11924
sg221
(lp11925
sg313
(lp11926
sg223
(lp11927
sg178
(lp11928
sg174
(lp11929
sg440
(lp11930
sg318
(lp11931
sg121
(lp11932
sg4
(lp11933
sg34
(lp11934
sg124
(lp11935
sg126
(lp11936
sg10
(lp11937
sg344
(lp11938
sg44
(lp11939
sg130
(lp11940
sg132
(lp11941
sg135
(lp11942
sg50
(lp11943
sg138
(lp11944
sg140
(lp11945
sg354
(lp11946
I2956
assS'learn'
p11947
(dp11948
g68
(lp11949
sg70
(lp11950
sg78
(lp11951
sg277
(lp11952
sg163
(lp11953
sg72
(lp11954
sg303
(lp11955
sg283
(lp11956
sg40
(lp11957
sg26
(lp11958
sg30
(lp11959
sg287
(lp11960
sg74
(lp11961
sg176
(lp11962
sg145
(lp11963
sg80
(lp11964
sg76
(lp11965
sg293
(lp11966
sg295
(lp11967
sg183
(lp11968
sg59
(lp11969
sg484
(lp11970
sg38
(lp11971
sg83
(lp11972
sg85
(lp11973
sg63
(lp11974
sg42
(lp11975
I121
asg306
(lp11976
sg89
(lp11977
sg91
(lp11978
sg12
(lp11979
sg94
(lp11980
sg96
(lp11981
sg48
(lp11982
sg99
(lp11983
sg313
(lp11984
sg44
(lp11985
sg149
(lp11986
sg116
(lp11987
sg329
(lp11988
sg350
(lp11989
sg318
(lp11990
sg46
(lp11991
sg104
(lp11992
sg106
(lp11993
sg108
(lp11994
sg110
(lp11995
sg178
(lp11996
sg52
(lp11997
sg114
(lp11998
sg230
(lp11999
sg438
(lp12000
sg18
(lp12001
sg121
(lp12002
sg4
(lp12003
sg181
(lp12004
sg235
(lp12005
sg34
(lp12006
sg221
(lp12007
sg460
(lp12008
sg124
(lp12009
sg126
(lp12010
sg341
(lp12011
sg10
(lp12012
sg535
(lp12013
sg344
(lp12014
sg223
(lp12015
sg128
(lp12016
sg36
(lp12017
sg132
(lp12018
sg14
(lp12019
sg16
(lp12020
sg135
(lp12021
sg50
(lp12022
sg138
(lp12023
sg140
(lp12024
sg354
(lp12025
ssS'male'
p12026
(dp12027
g96
(lp12028
I1297
asg72
(lp12029
sg91
(lp12030
ssS'pick'
p12031
(dp12032
g116
(lp12033
sg287
(lp12034
sg440
(lp12035
sg68
(lp12036
sg70
(lp12037
sg8
(lp12038
sg183
(lp12039
sg460
(lp12040
sg124
(lp12041
sg126
(lp12042
sg83
(lp12043
sg91
(lp12044
sg14
(lp12045
sg16
(lp12046
I846
asg59
(lp12047
sg44
(lp12048
ssS'multiway'
p12049
(dp12050
g59
(lp12051
I1629
assS'scan'
p12052
(dp12053
g318
(lp12054
sg178
(lp12055
sg76
(lp12056
sg42
(lp12057
I1727
asg176
(lp12058
sg20
(lp12059
sg140
(lp12060
sg114
(lp12061
ssS'challeng'
p12062
(dp12063
g174
(lp12064
sg74
(lp12065
sg68
(lp12066
sg277
(lp12067
sg118
(lp12068
sg344
(lp12069
sg59
(lp12070
sg124
(lp12071
sg83
(lp12072
sg10
(lp12073
sg130
(lp12074
sg132
(lp12075
sg14
(lp12076
sg16
(lp12077
I266
assS'ponti'
p12078
(dp12079
g350
(lp12080
I1199
assS'cajal'
p12081
(dp12082
g350
(lp12083
I1005
assS'zouhin'
p12084
(dp12085
g313
(lp12086
I1965
assS'accept'
p12087
(dp12088
g283
(lp12089
sg277
(lp12090
sg72
(lp12091
sg76
(lp12092
sg293
(lp12093
sg183
(lp12094
sg85
(lp12095
sg42
(lp12096
I2611
asg91
(lp12097
sg94
(lp12098
sg96
(lp12099
sg223
(lp12100
sg104
(lp12101
sg63
(lp12102
sg114
(lp12103
sg8
(lp12104
sg124
(lp12105
sg126
(lp12106
sg10
(lp12107
sg14
(lp12108
sg16
(lp12109
sg135
(lp12110
ssS'statel'
p12111
(dp12112
g14
(lp12113
I4232
assS'auer'
p12114
(dp12115
g341
(lp12116
I9
assS'nsms'
p12117
(dp12118
g34
(lp12119
I2957
assS'station'
p12120
(dp12121
g132
(lp12122
I2614
asg10
(lp12123
ssS'ist'
p12124
(dp12125
g110
(lp12126
I3039
assS'huge'
p12127
(dp12128
g132
(lp12129
I1929
asg94
(lp12130
sg306
(lp12131
sg34
(lp12132
sg126
(lp12133
ssS'interconnect'
p12134
(dp12135
g174
(lp12136
sg26
(lp12137
sg68
(lp12138
sg40
(lp12139
sg429
(lp12140
sg14
(lp12141
sg16
(lp12142
I2108
asg149
(lp12143
ssS'antiparallel'
p12144
(dp12145
g26
(lp12146
I480
assS'hugh'
p12147
(dp12148
g74
(lp12149
sg318
(lp12150
sg6
(lp12151
I11
asg303
(lp12152
sg176
(lp12153
sg350
(lp12154
ssS'invertebr'
p12155
(dp12156
g256
(lp12157
I36
assS'voltag'
p12158
(dp12159
g70
(lp12160
sg22
(lp12161
sg262
(lp12162
sg245
(lp12163
sg14
(lp12164
sg16
(lp12165
sg135
(lp12166
I1142
asg20
(lp12167
sg256
(lp12168
ssS'simpl'
p12169
(dp12170
g329
(lp12171
sg70
(lp12172
sg277
(lp12173
sg163
(lp12174
sg68
(lp12175
sg85
(lp12176
sg303
(lp12177
sg30
(lp12178
sg287
(lp12179
sg74
(lp12180
sg176
(lp12181
sg145
(lp12182
sg256
(lp12183
sg76
(lp12184
sg262
(lp12185
sg295
(lp12186
sg183
(lp12187
sg484
(lp12188
sg38
(lp12189
sg114
(lp12190
sg124
(lp12191
sg42
(lp12192
I1128
asg306
(lp12193
sg89
(lp12194
sg91
(lp12195
sg245
(lp12196
sg18
(lp12197
sg221
(lp12198
sg44
(lp12199
sg149
(lp12200
sg174
(lp12201
sg293
(lp12202
sg429
(lp12203
sg318
(lp12204
sg102
(lp12205
sg104
(lp12206
sg108
(lp12207
sg110
(lp12208
sg178
(lp12209
sg52
(lp12210
sg22
(lp12211
sg438
(lp12212
sg440
(lp12213
sg332
(lp12214
sg121
(lp12215
sg4
(lp12216
sg181
(lp12217
sg8
(lp12218
sg34
(lp12219
sg384
(lp12220
sg235
(lp12221
sg126
(lp12222
sg341
(lp12223
sg10
(lp12224
sg118
(lp12225
sg344
(lp12226
sg63
(lp12227
sg223
(lp12228
sg128
(lp12229
sg78
(lp12230
sg14
(lp12231
sg16
(lp12232
sg135
(lp12233
sg50
(lp12234
sg138
(lp12235
sg140
(lp12236
ssS'referene'
p12237
(dp12238
g114
(lp12239
I2215
assS'plant'
p12240
(dp12241
g230
(lp12242
I801
assS'seoul'
p12243
(dp12244
g20
(lp12245
I2652
assS'referenc'
p12246
(dp12247
g59
(lp12248
I122
assS'wiley'
p12249
(dp12250
g32
(lp12251
sg318
(lp12252
sg295
(lp12253
sg183
(lp12254
sg124
(lp12255
sg281
(lp12256
sg40
(lp12257
sg245
(lp12258
sg63
(lp12259
sg102
(lp12260
sg354
(lp12261
I3052
asg535
(lp12262
sg149
(lp12263
ssS'variant'
p12264
(dp12265
g4
(lp12266
sg344
(lp12267
sg460
(lp12268
sg72
(lp12269
sg85
(lp12270
sg306
(lp12271
sg130
(lp12272
sg132
(lp12273
sg221
(lp12274
sg354
(lp12275
I424
assS'plane'
p12276
(dp12277
g30
(lp12278
sg32
(lp12279
sg181
(lp12280
sg293
(lp12281
sg384
(lp12282
sg68
(lp12283
sg429
(lp12284
sg46
(lp12285
sg245
(lp12286
sg14
(lp12287
sg16
(lp12288
I963
asg350
(lp12289
ssS'saxena'
p12290
(dp12291
g277
(lp12292
I3244
assS'thalamocort'
p12293
(dp12294
g6
(lp12295
I2268
assS'rissanen'
p12296
(dp12297
g36
(lp12298
I3290
assS'varianc'
p12299
(dp12300
g163
(lp12301
sg30
(lp12302
sg74
(lp12303
sg262
(lp12304
sg484
(lp12305
sg38
(lp12306
sg91
(lp12307
sg221
(lp12308
sg313
(lp12309
sg102
(lp12310
sg329
(lp12311
sg318
(lp12312
sg6
(lp12313
sg181
(lp12314
sg235
(lp12315
sg36
(lp12316
sg460
(lp12317
sg124
(lp12318
sg126
(lp12319
sg281
(lp12320
sg130
(lp12321
sg138
(lp12322
sg140
(lp12323
sg354
(lp12324
I1705
assS'greenwalt'
p12325
(dp12326
g116
(lp12327
I1702
assS'circumst'
p12328
(dp12329
g140
(lp12330
I1708
assS'eindhoven'
p12331
(dp12332
g332
(lp12333
I2740
assS'frasconi'
p12334
(dp12335
g440
(lp12336
sg460
(lp12337
sg128
(lp12338
I2800
assS'crystallin'
p12339
(dp12340
g14
(lp12341
I4506
assS'ortho'
p12342
(dp12343
g44
(lp12344
I911
assS'syltem'
p12345
(dp12346
g83
(lp12347
I2903
assS'itn'
p12348
(dp12349
g59
(lp12350
I3116
assS'ito'
p12351
(dp12352
g181
(lp12353
I2421
assS'constitu'
p12354
(dp12355
g59
(lp12356
I2953
assS'mexican'
p12357
(dp12358
g48
(lp12359
I889
assS'ith'
p12360
(dp12361
g438
(lp12362
I168
asg145
(lp12363
sg181
(lp12364
sg306
(lp12365
sg128
(lp12366
sg104
(lp12367
sg46
(lp12368
sg52
(lp12369
ssS'iti'
p12370
(dp12371
g221
(lp12372
sg91
(lp12373
I1483
asg163
(lp12374
ssS'pmjylx'
p12375
(dp12376
g72
(lp12377
I3170
assS'trade'
p12378
(dp12379
g283
(lp12380
sg178
(lp12381
sg34
(lp12382
sg460
(lp12383
sg484
(lp12384
sg126
(lp12385
sg10
(lp12386
sg40
(lp12387
sg132
(lp12388
sg14
(lp12389
sg16
(lp12390
I2461
asg20
(lp12391
ssS'paper'
p12392
(dp12393
g344
(lp12394
sg124
(lp12395
sg26
(lp12396
sg277
(lp12397
sg163
(lp12398
sg72
(lp12399
sg281
(lp12400
sg283
(lp12401
sg40
(lp12402
sg30
(lp12403
sg350
(lp12404
sg74
(lp12405
sg176
(lp12406
sg145
(lp12407
sg256
(lp12408
sg76
(lp12409
sg262
(lp12410
sg295
(lp12411
sg183
(lp12412
sg59
(lp12413
sg80
(lp12414
sg38
(lp12415
sg83
(lp12416
sg85
(lp12417
sg63
(lp12418
sg42
(lp12419
I1147
asg306
(lp12420
sg87
(lp12421
sg89
(lp12422
sg91
(lp12423
sg12
(lp12424
sg94
(lp12425
sg96
(lp12426
sg18
(lp12427
sg221
(lp12428
sg313
(lp12429
sg223
(lp12430
sg149
(lp12431
sg118
(lp12432
sg329
(lp12433
sg293
(lp12434
sg460
(lp12435
sg245
(lp12436
sg429
(lp12437
sg318
(lp12438
sg46
(lp12439
sg102
(lp12440
sg104
(lp12441
sg108
(lp12442
sg110
(lp12443
sg20
(lp12444
sg52
(lp12445
sg114
(lp12446
sg230
(lp12447
sg438
(lp12448
sg440
(lp12449
sg332
(lp12450
sg178
(lp12451
sg4
(lp12452
sg8
(lp12453
sg34
(lp12454
sg36
(lp12455
sg384
(lp12456
sg235
(lp12457
sg126
(lp12458
sg341
(lp12459
sg10
(lp12460
sg535
(lp12461
sg287
(lp12462
sg128
(lp12463
sg130
(lp12464
sg132
(lp12465
sg14
(lp12466
sg16
(lp12467
sg135
(lp12468
sg50
(lp12469
sg138
(lp12470
sg140
(lp12471
sg354
(lp12472
ssS'scott'
p12473
(dp12474
g295
(lp12475
sg183
(lp12476
sg89
(lp12477
I2320
asg181
(lp12478
ssS'broadway'
p12479
(dp12480
g104
(lp12481
I17
assS'gerald'
p12482
(dp12483
g132
(lp12484
I3569
assS'coarser'
p12485
(dp12486
g70
(lp12487
sg8
(lp12488
I615
assS'allj'
p12489
(dp12490
g78
(lp12491
I2320
assS'alli'
p12492
(dp12493
g221
(lp12494
I1656
asg83
(lp12495
ssS'bypass'
p12496
(dp12497
g135
(lp12498
I2291
assS'superflu'
p12499
(dp12500
g145
(lp12501
I2280
assS'polytechniqu'
p12502
(dp12503
g40
(lp12504
I2478
assS'tri'
p12505
(dp12506
g124
(lp12507
sg70
(lp12508
sg26
(lp12509
sg277
(lp12510
sg283
(lp12511
sg30
(lp12512
sg145
(lp12513
sg80
(lp12514
sg183
(lp12515
sg59
(lp12516
sg83
(lp12517
sg85
(lp12518
sg89
(lp12519
sg91
(lp12520
sg94
(lp12521
sg221
(lp12522
sg313
(lp12523
sg44
(lp12524
sg104
(lp12525
sg63
(lp12526
sg174
(lp12527
sg34
(lp12528
sg384
(lp12529
sg68
(lp12530
sg126
(lp12531
sg128
(lp12532
sg130
(lp12533
sg135
(lp12534
sg50
(lp12535
sg354
(lp12536
I1179
assS'loooa'
p12537
(dp12538
g14
(lp12539
I2723
assS'veco'
p12540
(dp12541
g183
(lp12542
I5174
assS'wast'
p12543
(dp12544
g94
(lp12545
I502
asg235
(lp12546
ssS'thereof'
p12547
(dp12548
g460
(lp12549
I2631
asg429
(lp12550
ssS'nlogn'
p12551
(dp12552
g287
(lp12553
I3373
assS'tro'
p12554
(dp12555
g329
(lp12556
I1259
assS'tra'
p12557
(dp12558
g128
(lp12559
I1588
asg149
(lp12560
ssS'achiev'
p12561
(dp12562
g68
(lp12563
sg277
(lp12564
sg283
(lp12565
sg74
(lp12566
sg145
(lp12567
sg262
(lp12568
sg295
(lp12569
sg183
(lp12570
sg59
(lp12571
sg484
(lp12572
sg38
(lp12573
sg83
(lp12574
sg85
(lp12575
sg63
(lp12576
sg42
(lp12577
I380
asg87
(lp12578
sg89
(lp12579
sg91
(lp12580
sg12
(lp12581
sg46
(lp12582
sg96
(lp12583
sg48
(lp12584
sg221
(lp12585
sg535
(lp12586
sg44
(lp12587
sg350
(lp12588
sg116
(lp12589
sg118
(lp12590
sg245
(lp12591
sg429
(lp12592
sg94
(lp12593
sg102
(lp12594
sg110
(lp12595
sg20
(lp12596
sg52
(lp12597
sg230
(lp12598
sg174
(lp12599
sg32
(lp12600
sg18
(lp12601
sg178
(lp12602
sg22
(lp12603
sg181
(lp12604
sg235
(lp12605
sg34
(lp12606
sg460
(lp12607
sg124
(lp12608
sg126
(lp12609
sg281
(lp12610
sg10
(lp12611
sg40
(lp12612
sg223
(lp12613
sg128
(lp12614
sg130
(lp12615
sg14
(lp12616
sg16
(lp12617
sg135
(lp12618
sg50
(lp12619
sg138
(lp12620
ssS'sk'
p12621
(dp12622
g535
(lp12623
I747
assS'genom'
p12624
(dp12625
g130
(lp12626
I3032
assS'discontinuitit'
p12627
(dp12628
g70
(lp12629
I1793
assS'found'
p12630
(dp12631
g329
(lp12632
sg70
(lp12633
sg78
(lp12634
sg277
(lp12635
sg163
(lp12636
sg283
(lp12637
sg36
(lp12638
sg40
(lp12639
sg26
(lp12640
sg74
(lp12641
sg176
(lp12642
sg145
(lp12643
sg76
(lp12644
sg293
(lp12645
sg295
(lp12646
sg183
(lp12647
sg59
(lp12648
sg80
(lp12649
sg83
(lp12650
sg124
(lp12651
sg42
(lp12652
I735
asg87
(lp12653
sg91
(lp12654
sg245
(lp12655
sg303
(lp12656
sg48
(lp12657
sg99
(lp12658
sg313
(lp12659
sg44
(lp12660
sg116
(lp12661
sg174
(lp12662
sg18
(lp12663
sg460
(lp12664
sg429
(lp12665
sg104
(lp12666
sg106
(lp12667
sg108
(lp12668
sg110
(lp12669
sg178
(lp12670
sg52
(lp12671
sg22
(lp12672
sg216
(lp12673
sg438
(lp12674
sg440
(lp12675
sg332
(lp12676
sg121
(lp12677
sg4
(lp12678
sg6
(lp12679
sg8
(lp12680
sg34
(lp12681
sg221
(lp12682
sg384
(lp12683
sg235
(lp12684
sg126
(lp12685
sg535
(lp12686
sg344
(lp12687
sg63
(lp12688
sg223
(lp12689
sg128
(lp12690
sg130
(lp12691
sg132
(lp12692
sg14
(lp12693
sg16
(lp12694
sg50
(lp12695
sg138
(lp12696
sg140
(lp12697
sg354
(lp12698
ssS'subnetwork'
p12699
(dp12700
g104
(lp12701
sg50
(lp12702
I142
assS'hermit'
p12703
(dp12704
g163
(lp12705
I793
assS'status'
p12706
(dp12707
g48
(lp12708
I1683
asg72
(lp12709
ssS'england'
p12710
(dp12711
g174
(lp12712
sg87
(lp12713
sg91
(lp12714
I3052
asg52
(lp12715
sg76
(lp12716
ssS'interstic'
p12717
(dp12718
g181
(lp12719
I649
assS'osaka'
p12720
(dp12721
g42
(lp12722
I14
assS'monoton'
p12723
(dp12724
g230
(lp12725
sg235
(lp12726
sg36
(lp12727
sg38
(lp12728
sg341
(lp12729
I2529
asg303
(lp12730
sg110
(lp12731
sg535
(lp12732
ssS'procedur'
p12733
(dp12734
g283
(lp12735
sg26
(lp12736
sg163
(lp12737
sg72
(lp12738
sg30
(lp12739
sg74
(lp12740
sg145
(lp12741
sg76
(lp12742
sg295
(lp12743
sg183
(lp12744
sg59
(lp12745
sg484
(lp12746
sg303
(lp12747
sg42
(lp12748
I571
asg87
(lp12749
sg89
(lp12750
sg91
(lp12751
sg46
(lp12752
sg96
(lp12753
sg221
(lp12754
sg116
(lp12755
sg110
(lp12756
sg230
(lp12757
sg329
(lp12758
sg440
(lp12759
sg318
(lp12760
sg121
(lp12761
sg4
(lp12762
sg6
(lp12763
sg8
(lp12764
sg99
(lp12765
sg460
(lp12766
sg124
(lp12767
sg126
(lp12768
sg281
(lp12769
sg130
(lp12770
sg132
(lp12771
sg14
(lp12772
sg135
(lp12773
sg138
(lp12774
sg354
(lp12775
ssS'realli'
p12776
(dp12777
g287
(lp12778
sg178
(lp12779
sg34
(lp12780
sg138
(lp12781
I1689
asg104
(lp12782
sg63
(lp12783
ssS'mume'
p12784
(dp12785
g135
(lp12786
I1665
assS'benda'
p12787
(dp12788
g99
(lp12789
I3229
assS'operation'
p12790
(dp12791
g94
(lp12792
I789
assS'reduct'
p12793
(dp12794
g163
(lp12795
sg118
(lp12796
sg295
(lp12797
sg183
(lp12798
sg484
(lp12799
sg87
(lp12800
sg89
(lp12801
sg99
(lp12802
sg329
(lp12803
sg178
(lp12804
sg106
(lp12805
sg108
(lp12806
sg52
(lp12807
sg438
(lp12808
I125
asg440
(lp12809
sg318
(lp12810
sg121
(lp12811
sg4
(lp12812
sg8
(lp12813
sg130
(lp12814
sg14
(lp12815
sg16
(lp12816
sg354
(lp12817
ssS'ftp'
p12818
(dp12819
g126
(lp12820
sg484
(lp12821
sg145
(lp12822
sg121
(lp12823
I1482
asg76
(lp12824
ssS'agre'
p12825
(dp12826
g287
(lp12827
sg4
(lp12828
sg80
(lp12829
sg8
(lp12830
sg344
(lp12831
sg235
(lp12832
sg38
(lp12833
sg12
(lp12834
sg48
(lp12835
sg221
(lp12836
sg138
(lp12837
sg140
(lp12838
I933
assS'dxdi'
p12839
(dp12840
g72
(lp12841
sg354
(lp12842
I1151
assS'bfmap'
p12843
(dp12844
g303
(lp12845
I1252
assS'ftx'
p12846
(dp12847
g295
(lp12848
I2451
asg183
(lp12849
ssS'research'
p12850
(dp12851
g70
(lp12852
sg277
(lp12853
sg163
(lp12854
sg181
(lp12855
sg30
(lp12856
sg350
(lp12857
sg176
(lp12858
sg145
(lp12859
sg80
(lp12860
sg76
(lp12861
sg118
(lp12862
sg344
(lp12863
sg78
(lp12864
sg484
(lp12865
sg83
(lp12866
sg85
(lp12867
sg303
(lp12868
sg42
(lp12869
I684
asg306
(lp12870
sg89
(lp12871
sg91
(lp12872
sg12
(lp12873
sg46
(lp12874
sg96
(lp12875
sg18
(lp12876
sg221
(lp12877
sg313
(lp12878
sg44
(lp12879
sg149
(lp12880
sg230
(lp12881
sg174
(lp12882
sg32
(lp12883
sg245
(lp12884
sg429
(lp12885
sg318
(lp12886
sg102
(lp12887
sg178
(lp12888
sg106
(lp12889
sg108
(lp12890
sg110
(lp12891
sg20
(lp12892
sg52
(lp12893
sg114
(lp12894
sg216
(lp12895
sg438
(lp12896
sg440
(lp12897
sg332
(lp12898
sg121
(lp12899
sg4
(lp12900
sg6
(lp12901
sg8
(lp12902
sg124
(lp12903
sg126
(lp12904
sg341
(lp12905
sg10
(lp12906
sg40
(lp12907
sg287
(lp12908
sg223
(lp12909
sg128
(lp12910
sg130
(lp12911
sg132
(lp12912
sg14
(lp12913
sg16
(lp12914
sg135
(lp12915
sg138
(lp12916
sg354
(lp12917
ssS'bubbl'
p12918
(dp12919
g149
(lp12920
I1030
assS'submod'
p12921
(dp12922
g181
(lp12923
I326
assS'dxdv'
p12924
(dp12925
g460
(lp12926
I933
assS'ftj'
p12927
(dp12928
g36
(lp12929
I3099
assS'occurr'
p12930
(dp12931
g216
(lp12932
sg118
(lp12933
sg26
(lp12934
sg6
(lp12935
I1292
asg34
(lp12936
sg341
(lp12937
sg85
(lp12938
ssS'belief'
p12939
(dp12940
g74
(lp12941
sg140
(lp12942
I506
asg235
(lp12943
ssS'dissoci'
p12944
(dp12945
g4
(lp12946
I3130
assS'siegel'
p12947
(dp12948
g303
(lp12949
I2775
assS'believ'
p12950
(dp12951
g318
(lp12952
sg4
(lp12953
sg344
(lp12954
sg124
(lp12955
sg10
(lp12956
sg245
(lp12957
sg68
(lp12958
sg89
(lp12959
sg85
(lp12960
sg12
(lp12961
sg138
(lp12962
I3190
asg26
(lp12963
ssS'ziij'
p12964
(dp12965
g121
(lp12966
I712
assS'driven'
p12967
(dp12968
g438
(lp12969
I1552
asg118
(lp12970
sg332
(lp12971
sg70
(lp12972
sg22
(lp12973
sg80
(lp12974
sg174
(lp12975
sg384
(lp12976
sg293
(lp12977
sg38
(lp12978
sg59
(lp12979
sg104
(lp12980
sg313
(lp12981
ssS'iglrl'
p12982
(dp12983
g108
(lp12984
I1115
assS'fruit'
p12985
(dp12986
g32
(lp12987
sg354
(lp12988
I2922
assS'w'
p12989
(dp12990
g344
(lp12991
sg329
(lp12992
sg70
(lp12993
sg277
(lp12994
sg163
(lp12995
sg72
(lp12996
sg303
(lp12997
sg293
(lp12998
sg281
(lp12999
sg40
(lp13000
sg30
(lp13001
sg350
(lp13002
sg74
(lp13003
sg176
(lp13004
sg145
(lp13005
sg76
(lp13006
sg262
(lp13007
sg295
(lp13008
sg183
(lp13009
sg484
(lp13010
sg38
(lp13011
sg114
(lp13012
sg124
(lp13013
sg42
(lp13014
I3361
asg306
(lp13015
sg89
(lp13016
sg91
(lp13017
sg12
(lp13018
sg94
(lp13019
sg96
(lp13020
sg48
(lp13021
sg221
(lp13022
sg313
(lp13023
sg44
(lp13024
sg149
(lp13025
sg118
(lp13026
sg230
(lp13027
sg174
(lp13028
sg18
(lp13029
sg116
(lp13030
sg460
(lp13031
sg178
(lp13032
sg245
(lp13033
sg429
(lp13034
sg318
(lp13035
sg46
(lp13036
sg102
(lp13037
sg104
(lp13038
sg106
(lp13039
sg108
(lp13040
sg110
(lp13041
sg20
(lp13042
sg52
(lp13043
sg22
(lp13044
sg216
(lp13045
sg438
(lp13046
sg32
(lp13047
sg332
(lp13048
sg121
(lp13049
sg181
(lp13050
sg6
(lp13051
sg8
(lp13052
sg34
(lp13053
sg36
(lp13054
sg384
(lp13055
sg235
(lp13056
sg126
(lp13057
sg341
(lp13058
sg10
(lp13059
sg535
(lp13060
sg287
(lp13061
sg63
(lp13062
sg223
(lp13063
sg128
(lp13064
sg132
(lp13065
sg14
(lp13066
sg135
(lp13067
sg50
(lp13068
sg138
(lp13069
sg140
(lp13070
sg354
(lp13071
ssS'zittartz'
p13072
(dp13073
g384
(lp13074
I2385
assS'clump'
p13075
(dp13076
g484
(lp13077
I1755
assS'integrand'
p13078
(dp13079
g138
(lp13080
I377
asg262
(lp13081
ssS'grossman'
p13082
(dp13083
g176
(lp13084
sg350
(lp13085
I364
assS'major'
p13086
(dp13087
g116
(lp13088
sg74
(lp13089
sg145
(lp13090
sg295
(lp13091
sg183
(lp13092
sg59
(lp13093
sg72
(lp13094
sg10
(lp13095
sg114
(lp13096
sg40
(lp13097
sg42
(lp13098
I1806
asg344
(lp13099
sg223
(lp13100
sg44
(lp13101
sg132
(lp13102
sg14
(lp13103
sg16
(lp13104
sg350
(lp13105
sg138
(lp13106
sg140
(lp13107
sg91
(lp13108
ssS'number'
p13109
(dp13110
g80
(lp13111
sg293
(lp13112
sg344
(lp13113
sg59
(lp13114
sg484
(lp13115
sg38
(lp13116
sg83
(lp13117
sg85
(lp13118
sg303
(lp13119
sg438
(lp13120
sg116
(lp13121
sg34
(lp13122
sg36
(lp13123
sg460
(lp13124
sg68
(lp13125
sg72
(lp13126
sg281
(lp13127
sg10
(lp13128
sg40
(lp13129
sg283
(lp13130
sg26
(lp13131
sg277
(lp13132
sg163
(lp13133
sg89
(lp13134
sg91
(lp13135
sg94
(lp13136
sg96
(lp13137
sg48
(lp13138
sg99
(lp13139
sg313
(lp13140
sg44
(lp13141
sg149
(lp13142
sg429
(lp13143
sg102
(lp13144
sg104
(lp13145
sg108
(lp13146
sg110
(lp13147
sg63
(lp13148
sg52
(lp13149
sg114
(lp13150
sg128
(lp13151
sg130
(lp13152
sg132
(lp13153
sg14
(lp13154
sg16
(lp13155
sg135
(lp13156
sg138
(lp13157
sg140
(lp13158
sg354
(lp13159
sg306
(lp13160
sg87
(lp13161
sg46
(lp13162
sg20
(lp13163
sg18
(lp13164
sg221
(lp13165
sg535
(lp13166
sg223
(lp13167
sg350
(lp13168
sg174
(lp13169
sg440
(lp13170
sg332
(lp13171
sg121
(lp13172
sg4
(lp13173
sg6
(lp13174
sg8
(lp13175
sg126
(lp13176
sg341
(lp13177
sg30
(lp13178
sg287
(lp13179
sg74
(lp13180
sg145
(lp13181
sg256
(lp13182
sg76
(lp13183
sg295
(lp13184
sg183
(lp13185
sg42
(lp13186
I181
asg230
(lp13187
sg329
(lp13188
sg32
(lp13189
sg318
(lp13190
sg178
(lp13191
sg22
(lp13192
sg181
(lp13193
sg235
(lp13194
sg384
(lp13195
sg124
(lp13196
ssS'evolut'
p13197
(dp13198
g116
(lp13199
sg438
(lp13200
I645
asg262
(lp13201
sg384
(lp13202
sg68
(lp13203
sg38
(lp13204
sg460
(lp13205
sg14
(lp13206
sg12
(lp13207
sg104
(lp13208
sg16
(lp13209
sg18
(lp13210
sg163
(lp13211
sg535
(lp13212
ssS'hnakahar'
p13213
(dp13214
g18
(lp13215
I288
assS'horribl'
p13216
(dp13217
g14
(lp13218
I2910
assS'vedelsbi'
p13219
(dp13220
g140
(lp13221
I16
asg235
(lp13222
ssS'gennari'
p13223
(dp13224
g91
(lp13225
I2305
assS'guess'
p13226
(dp13227
g287
(lp13228
sg145
(lp13229
sg183
(lp13230
sg306
(lp13231
sg138
(lp13232
I987
asg223
(lp13233
ssS'fuller'
p13234
(dp13235
g138
(lp13236
I625
assS'lakwik'
p13237
(dp13238
g76
(lp13239
I1097
assS'jet'
p13240
(dp13241
g429
(lp13242
sg68
(lp13243
I259
assS'exponenti'
p13244
(dp13245
g30
(lp13246
sg174
(lp13247
sg32
(lp13248
sg163
(lp13249
sg256
(lp13250
sg329
(lp13251
sg34
(lp13252
sg262
(lp13253
sg38
(lp13254
sg341
(lp13255
sg40
(lp13256
sg344
(lp13257
sg306
(lp13258
sg91
(lp13259
sg132
(lp13260
sg14
(lp13261
sg16
(lp13262
sg50
(lp13263
I674
asg83
(lp13264
sg52
(lp13265
sg114
(lp13266
ssS'equa'
p13267
(dp13268
g20
(lp13269
I1740
assS'promot'
p13270
(dp13271
g344
(lp13272
sg4
(lp13273
I1227
assS'underlin'
p13274
(dp13275
g221
(lp13276
I2106
assS'protocol'
p13277
(dp13278
g176
(lp13279
I2360
assS'molecular'
p13280
(dp13281
g344
(lp13282
sg26
(lp13283
sg130
(lp13284
I2945
assS'contro'
p13285
(dp13286
g230
(lp13287
I2585
assS'nearsight'
p13288
(dp13289
g91
(lp13290
I1861
assS'analyllil'
p13291
(dp13292
g318
(lp13293
I552
assS'relationship'
p13294
(dp13295
g176
(lp13296
sg80
(lp13297
sg295
(lp13298
sg183
(lp13299
sg484
(lp13300
sg87
(lp13301
sg245
(lp13302
sg48
(lp13303
sg44
(lp13304
sg149
(lp13305
sg12
(lp13306
sg429
(lp13307
sg178
(lp13308
sg108
(lp13309
sg118
(lp13310
sg332
(lp13311
sg121
(lp13312
sg4
(lp13313
sg6
(lp13314
sg460
(lp13315
sg124
(lp13316
sg72
(lp13317
sg344
(lp13318
sg50
(lp13319
sg138
(lp13320
I3415
assS'mijkmi'
p13321
(dp13322
g74
(lp13323
I1943
assS'rtrain'
p13324
(dp13325
g36
(lp13326
I744
assS'suppcf'
p13327
(dp13328
g281
(lp13329
I322
assS'consult'
p13330
(dp13331
g91
(lp13332
I3046
asg281
(lp13333
ssS'scatterplot'
p13334
(dp13335
g295
(lp13336
sg183
(lp13337
sg8
(lp13338
I2222
assS'oiolith'
p13339
(dp13340
g350
(lp13341
I1765
assS'aam'
p13342
(dp13343
g174
(lp13344
I2758
assS'ohmi'
p13345
(dp13346
g20
(lp13347
I24
assS'vocal'
p13348
(dp13349
g116
(lp13350
I357
assS'aaj'
p13351
(dp13352
g40
(lp13353
I2045
assS'nikolaus'
p13354
(dp13355
g132
(lp13356
I3719
assS'ranck'
p13357
(dp13358
g80
(lp13359
I2664
assS'reus'
p13360
(dp13361
g116
(lp13362
sg91
(lp13363
I425
asg10
(lp13364
ssS'yuill'
p13365
(dp13366
g48
(lp13367
I271
asg8
(lp13368
ssS'arrang'
p13369
(dp13370
g118
(lp13371
sg176
(lp13372
sg22
(lp13373
sg6
(lp13374
sg221
(lp13375
sg59
(lp13376
sg68
(lp13377
sg42
(lp13378
I2291
asg102
(lp13379
sg130
(lp13380
sg12
(lp13381
sg94
(lp13382
sg108
(lp13383
sg50
(lp13384
sg149
(lp13385
ssS'hyperpol'
p13386
(dp13387
g106
(lp13388
I2005
assS'bhi'
p13389
(dp13390
g384
(lp13391
I395
assS'comput'
p13392
(dp13393
g80
(lp13394
sg293
(lp13395
sg344
(lp13396
sg78
(lp13397
sg59
(lp13398
sg484
(lp13399
sg83
(lp13400
sg85
(lp13401
sg303
(lp13402
sg438
(lp13403
sg116
(lp13404
sg118
(lp13405
sg34
(lp13406
sg36
(lp13407
sg460
(lp13408
sg68
(lp13409
sg72
(lp13410
sg281
(lp13411
sg10
(lp13412
sg40
(lp13413
sg283
(lp13414
sg70
(lp13415
sg26
(lp13416
sg277
(lp13417
sg163
(lp13418
sg89
(lp13419
sg91
(lp13420
sg12
(lp13421
sg94
(lp13422
sg96
(lp13423
sg48
(lp13424
sg99
(lp13425
sg313
(lp13426
sg44
(lp13427
sg149
(lp13428
sg429
(lp13429
sg102
(lp13430
sg104
(lp13431
sg106
(lp13432
sg108
(lp13433
sg110
(lp13434
sg63
(lp13435
sg52
(lp13436
sg114
(lp13437
sg128
(lp13438
sg130
(lp13439
sg132
(lp13440
sg14
(lp13441
sg16
(lp13442
sg135
(lp13443
sg50
(lp13444
sg138
(lp13445
sg140
(lp13446
sg354
(lp13447
sg306
(lp13448
sg87
(lp13449
sg245
(lp13450
sg46
(lp13451
sg20
(lp13452
sg18
(lp13453
sg221
(lp13454
sg535
(lp13455
sg223
(lp13456
sg350
(lp13457
sg174
(lp13458
sg440
(lp13459
sg332
(lp13460
sg121
(lp13461
sg4
(lp13462
sg6
(lp13463
sg8
(lp13464
sg126
(lp13465
sg341
(lp13466
sg30
(lp13467
sg287
(lp13468
sg74
(lp13469
sg176
(lp13470
sg145
(lp13471
sg256
(lp13472
sg262
(lp13473
sg295
(lp13474
sg183
(lp13475
sg42
(lp13476
I149
asg329
(lp13477
sg32
(lp13478
sg318
(lp13479
sg178
(lp13480
sg22
(lp13481
sg181
(lp13482
sg124
(lp13483
ssS'defect'
p13484
(dp13485
g42
(lp13486
I724
asg91
(lp13487
ssS'merzenich'
p13488
(dp13489
g176
(lp13490
I218
assS'packag'
p13491
(dp13492
g94
(lp13493
sg135
(lp13494
I1658
asg26
(lp13495
sg10
(lp13496
ssS'sontag'
p13497
(dp13498
g287
(lp13499
sg341
(lp13500
I2733
assS'requisit'
p13501
(dp13502
g44
(lp13503
I2095
assS'wafa'
p13504
(dp13505
g140
(lp13506
I630
assS'equival'
p13507
(dp13508
g283
(lp13509
sg163
(lp13510
sg287
(lp13511
sg74
(lp13512
sg256
(lp13513
sg76
(lp13514
sg262
(lp13515
sg80
(lp13516
sg38
(lp13517
sg303
(lp13518
sg42
(lp13519
I275
asg87
(lp13520
sg46
(lp13521
sg18
(lp13522
sg44
(lp13523
sg230
(lp13524
sg118
(lp13525
sg293
(lp13526
sg32
(lp13527
sg102
(lp13528
sg104
(lp13529
sg52
(lp13530
sg216
(lp13531
sg329
(lp13532
sg440
(lp13533
sg318
(lp13534
sg4
(lp13535
sg8
(lp13536
sg36
(lp13537
sg384
(lp13538
sg124
(lp13539
sg72
(lp13540
sg128
(lp13541
sg130
(lp13542
sg14
(lp13543
sg16
(lp13544
sg460
(lp13545
ssS'odd'
p13546
(dp13547
g118
(lp13548
sg74
(lp13549
sg145
(lp13550
sg68
(lp13551
sg40
(lp13552
sg20
(lp13553
I858
assS'self'
p13554
(dp13555
g116
(lp13556
sg318
(lp13557
sg145
(lp13558
sg4
(lp13559
sg235
(lp13560
sg34
(lp13561
sg384
(lp13562
sg72
(lp13563
sg83
(lp13564
sg535
(lp13565
sg42
(lp13566
I1
asg12
(lp13567
sg59
(lp13568
sg130
(lp13569
sg132
(lp13570
sg104
(lp13571
sg106
(lp13572
sg18
(lp13573
sg20
(lp13574
sg52
(lp13575
sg149
(lp13576
ssS'also'
p13577
(dp13578
g80
(lp13579
sg293
(lp13580
sg344
(lp13581
sg78
(lp13582
sg59
(lp13583
sg484
(lp13584
sg38
(lp13585
sg83
(lp13586
sg85
(lp13587
sg303
(lp13588
sg438
(lp13589
sg118
(lp13590
sg34
(lp13591
sg36
(lp13592
sg460
(lp13593
sg68
(lp13594
sg72
(lp13595
sg281
(lp13596
sg10
(lp13597
sg40
(lp13598
sg70
(lp13599
sg26
(lp13600
sg277
(lp13601
sg163
(lp13602
sg89
(lp13603
sg91
(lp13604
sg12
(lp13605
sg94
(lp13606
sg96
(lp13607
sg48
(lp13608
sg313
(lp13609
sg44
(lp13610
sg149
(lp13611
sg429
(lp13612
sg102
(lp13613
sg104
(lp13614
sg106
(lp13615
sg108
(lp13616
sg110
(lp13617
sg63
(lp13618
sg52
(lp13619
sg114
(lp13620
sg128
(lp13621
sg130
(lp13622
sg132
(lp13623
sg14
(lp13624
sg16
(lp13625
sg135
(lp13626
sg138
(lp13627
sg140
(lp13628
sg354
(lp13629
sg306
(lp13630
sg87
(lp13631
sg245
(lp13632
sg46
(lp13633
sg20
(lp13634
sg18
(lp13635
sg221
(lp13636
sg535
(lp13637
sg223
(lp13638
sg350
(lp13639
sg216
(lp13640
sg174
(lp13641
sg440
(lp13642
sg332
(lp13643
sg121
(lp13644
sg4
(lp13645
sg6
(lp13646
sg8
(lp13647
sg126
(lp13648
sg341
(lp13649
sg30
(lp13650
sg287
(lp13651
sg74
(lp13652
sg176
(lp13653
sg145
(lp13654
sg256
(lp13655
sg76
(lp13656
sg262
(lp13657
sg295
(lp13658
sg183
(lp13659
sg42
(lp13660
I1563
asg230
(lp13661
sg329
(lp13662
sg32
(lp13663
sg318
(lp13664
sg178
(lp13665
sg22
(lp13666
sg181
(lp13667
sg235
(lp13668
sg384
(lp13669
sg124
(lp13670
ssS'etienn'
p13671
(dp13672
g245
(lp13673
I9
assS'analogu'
p13674
(dp13675
g4
(lp13676
sg8
(lp13677
sg384
(lp13678
sg46
(lp13679
sg14
(lp13680
sg106
(lp13681
I2634
asg135
(lp13682
sg99
(lp13683
sg16
(lp13684
sg52
(lp13685
ssS'pipelin'
p13686
(dp13687
g135
(lp13688
I2539
asg283
(lp13689
sg10
(lp13690
ssS'tcurr'
p13691
(dp13692
g354
(lp13693
I1878
assS'nasal'
p13694
(dp13695
g74
(lp13696
sg87
(lp13697
I221
assS'airborn'
p13698
(dp13699
g283
(lp13700
I3
assS'xtqx'
p13701
(dp13702
g230
(lp13703
I2513
assS'plan'
p13704
(dp13705
g4
(lp13706
sg295
(lp13707
sg183
(lp13708
sg460
(lp13709
sg344
(lp13710
sg132
(lp13711
sg135
(lp13712
I620
asg99
(lp13713
ssS'altom'
p13714
(dp13715
g110
(lp13716
I3143
assS'otest'
p13717
(dp13718
g30
(lp13719
I1027
assS'noncartesian'
p13720
(dp13721
g303
(lp13722
I2596
assS'modem'
p13723
(dp13724
g384
(lp13725
I816
assS'apparatus'
p13726
(dp13727
g116
(lp13728
sg176
(lp13729
I294
assS'asum'
p13730
(dp13731
g438
(lp13732
I598
assS'tpn'
p13733
(dp13734
g295
(lp13735
I1686
asg183
(lp13736
ssS'torus'
p13737
(dp13738
g14
(lp13739
sg16
(lp13740
I856
asg18
(lp13741
ssS'cover'
p13742
(dp13743
g230
(lp13744
sg287
(lp13745
sg32
(lp13746
sg22
(lp13747
sg235
(lp13748
sg34
(lp13749
sg281
(lp13750
sg85
(lp13751
sg303
(lp13752
sg42
(lp13753
I518
asg18
(lp13754
sg110
(lp13755
sg138
(lp13756
ssS'exk'
p13757
(dp13758
g223
(lp13759
I1211
assS'rais'
p13760
(dp13761
g116
(lp13762
I160
assS'hamid'
p13763
(dp13764
g283
(lp13765
I44
assS'ext'
p13766
(dp13767
g20
(lp13768
sg108
(lp13769
I1150
assS'saddlenod'
p13770
(dp13771
g18
(lp13772
I97
assS'exp'
p13773
(dp13774
g70
(lp13775
sg74
(lp13776
sg262
(lp13777
sg295
(lp13778
sg183
(lp13779
sg12
(lp13780
sg18
(lp13781
sg221
(lp13782
sg313
(lp13783
sg149
(lp13784
sg329
(lp13785
sg99
(lp13786
sg102
(lp13787
sg174
(lp13788
sg32
(lp13789
sg178
(lp13790
sg22
(lp13791
sg235
(lp13792
sg36
(lp13793
sg384
(lp13794
sg124
(lp13795
sg126
(lp13796
sg130
(lp13797
I413
assS'anton'
p13798
(dp13799
g132
(lp13800
I3851
assS'barrel'
p13801
(dp13802
g6
(lp13803
I2275
assS'microsoft'
p13804
(dp13805
g114
(lp13806
I931
assS'gold'
p13807
(dp13808
g104
(lp13809
I866
assS'session'
p13810
(dp13811
g176
(lp13812
sg80
(lp13813
sg78
(lp13814
sg91
(lp13815
sg99
(lp13816
sg140
(lp13817
I256
assS'oja'
p13818
(dp13819
g318
(lp13820
sg52
(lp13821
I518
assS'affin'
p13822
(dp13823
g30
(lp13824
sg287
(lp13825
sg32
(lp13826
sg121
(lp13827
sg306
(lp13828
sg96
(lp13829
sg138
(lp13830
I602
assS'murata'
p13831
(dp13832
g36
(lp13833
I13
assS'abund'
p13834
(dp13835
g114
(lp13836
I647
assS'impact'
p13837
(dp13838
g94
(lp13839
sg384
(lp13840
sg135
(lp13841
I85
assS'conju'
p13842
(dp13843
g34
(lp13844
I2150
assS'writer'
p13845
(dp13846
g63
(lp13847
sg76
(lp13848
sg114
(lp13849
I323
assS'solut'
p13850
(dp13851
g124
(lp13852
sg26
(lp13853
sg30
(lp13854
sg74
(lp13855
sg176
(lp13856
sg145
(lp13857
sg76
(lp13858
sg262
(lp13859
sg460
(lp13860
sg78
(lp13861
sg59
(lp13862
sg38
(lp13863
sg42
(lp13864
I504
asg306
(lp13865
sg91
(lp13866
sg46
(lp13867
sg48
(lp13868
sg99
(lp13869
sg535
(lp13870
sg293
(lp13871
sg429
(lp13872
sg102
(lp13873
sg104
(lp13874
sg106
(lp13875
sg108
(lp13876
sg110
(lp13877
sg230
(lp13878
sg329
(lp13879
sg440
(lp13880
sg332
(lp13881
sg80
(lp13882
sg8
(lp13883
sg34
(lp13884
sg221
(lp13885
sg384
(lp13886
sg235
(lp13887
sg281
(lp13888
sg10
(lp13889
sg128
(lp13890
sg130
(lp13891
sg14
(lp13892
sg16
(lp13893
sg50
(lp13894
sg138
(lp13895
sg140
(lp13896
ssS'technic'
p13897
(dp13898
g26
(lp13899
sg30
(lp13900
sg74
(lp13901
sg295
(lp13902
sg183
(lp13903
sg83
(lp13904
sg306
(lp13905
sg89
(lp13906
sg12
(lp13907
sg94
(lp13908
sg20
(lp13909
sg221
(lp13910
sg535
(lp13911
sg223
(lp13912
sg429
(lp13913
sg102
(lp13914
sg110
(lp13915
sg63
(lp13916
sg114
(lp13917
sg230
(lp13918
sg174
(lp13919
sg440
(lp13920
sg384
(lp13921
sg124
(lp13922
sg341
(lp13923
sg40
(lp13924
sg132
(lp13925
sg14
(lp13926
sg16
(lp13927
sg135
(lp13928
sg460
(lp13929
sg140
(lp13930
sg354
(lp13931
I144
assS'factor'
p13932
(dp13933
g78
(lp13934
sg163
(lp13935
sg74
(lp13936
sg176
(lp13937
sg145
(lp13938
sg295
(lp13939
sg183
(lp13940
sg59
(lp13941
sg484
(lp13942
sg83
(lp13943
sg85
(lp13944
sg303
(lp13945
sg42
(lp13946
I2245
asg306
(lp13947
sg87
(lp13948
sg91
(lp13949
sg245
(lp13950
sg18
(lp13951
sg223
(lp13952
sg118
(lp13953
sg102
(lp13954
sg108
(lp13955
sg63
(lp13956
sg438
(lp13957
sg32
(lp13958
sg178
(lp13959
sg4
(lp13960
sg6
(lp13961
sg235
(lp13962
sg344
(lp13963
sg44
(lp13964
sg130
(lp13965
sg132
(lp13966
sg14
(lp13967
sg16
(lp13968
sg135
(lp13969
sg50
(lp13970
sg138
(lp13971
sg354
(lp13972
ssS'bernstein'
p13973
(dp13974
g74
(lp13975
I3017
asg26
(lp13976
ssS'cowan'
p13977
(dp13978
g30
(lp13979
sg174
(lp13980
sg318
(lp13981
sg295
(lp13982
sg183
(lp13983
sg59
(lp13984
sg124
(lp13985
sg306
(lp13986
sg89
(lp13987
sg223
(lp13988
sg132
(lp13989
sg48
(lp13990
sg50
(lp13991
sg313
(lp13992
sg44
(lp13993
sg354
(lp13994
I2951
assS'remedi'
p13995
(dp13996
g145
(lp13997
I2192
asg281
(lp13998
sg293
(lp13999
ssS'adcl'
p14000
(dp14001
g74
(lp14002
I29
assS'shortdelay'
p14003
(dp14004
g4
(lp14005
I1502
assS'mainten'
p14006
(dp14007
g18
(lp14008
sg130
(lp14009
I3030
assS'compass'
p14010
(dp14011
g14
(lp14012
sg16
(lp14013
I352
asg89
(lp14014
ssS'rfs'
p14015
(dp14016
g176
(lp14017
sg149
(lp14018
I697
assS'respif'
p14019
(dp14020
g332
(lp14021
I1538
assS'synthet'
p14022
(dp14023
g102
(lp14024
I357
asg484
(lp14025
sg281
(lp14026
ssS'synthes'
p14027
(dp14028
g68
(lp14029
I409
assS'coord'
p14030
(dp14031
g59
(lp14032
sg48
(lp14033
I2004
assS'acut'
p14034
(dp14035
g91
(lp14036
I2872
assS'crc'
p14037
(dp14038
g44
(lp14039
I48
assS'lllusion'
p14040
(dp14041
g118
(lp14042
I1764
assS'crg'
p14043
(dp14044
g74
(lp14045
sg354
(lp14046
I3083
assS'weaver'
p14047
(dp14048
g102
(lp14049
I3657
assS'crl'
p14050
(dp14051
g341
(lp14052
I2781
assS'universiuit'
p14053
(dp14054
g132
(lp14055
I3715
assS'set'
p14056
(dp14057
g80
(lp14058
sg293
(lp14059
sg344
(lp14060
sg78
(lp14061
sg59
(lp14062
sg484
(lp14063
sg38
(lp14064
sg83
(lp14065
sg85
(lp14066
sg303
(lp14067
sg438
(lp14068
sg116
(lp14069
sg34
(lp14070
sg36
(lp14071
sg460
(lp14072
sg68
(lp14073
sg72
(lp14074
sg281
(lp14075
sg10
(lp14076
sg40
(lp14077
sg283
(lp14078
sg26
(lp14079
sg277
(lp14080
sg163
(lp14081
sg89
(lp14082
sg91
(lp14083
sg12
(lp14084
sg94
(lp14085
sg96
(lp14086
sg48
(lp14087
sg99
(lp14088
sg313
(lp14089
sg44
(lp14090
sg149
(lp14091
sg429
(lp14092
sg102
(lp14093
sg104
(lp14094
sg108
(lp14095
sg110
(lp14096
sg63
(lp14097
sg52
(lp14098
sg114
(lp14099
sg128
(lp14100
sg130
(lp14101
sg132
(lp14102
sg14
(lp14103
sg16
(lp14104
sg135
(lp14105
sg50
(lp14106
sg138
(lp14107
sg140
(lp14108
sg354
(lp14109
sg306
(lp14110
sg87
(lp14111
sg245
(lp14112
sg46
(lp14113
sg18
(lp14114
sg221
(lp14115
sg535
(lp14116
sg223
(lp14117
sg350
(lp14118
sg174
(lp14119
sg440
(lp14120
sg332
(lp14121
sg121
(lp14122
sg4
(lp14123
sg6
(lp14124
sg8
(lp14125
sg126
(lp14126
sg341
(lp14127
sg30
(lp14128
sg287
(lp14129
sg74
(lp14130
sg145
(lp14131
sg76
(lp14132
sg262
(lp14133
sg295
(lp14134
sg183
(lp14135
sg42
(lp14136
I111
asg230
(lp14137
sg329
(lp14138
sg32
(lp14139
sg318
(lp14140
sg178
(lp14141
sg22
(lp14142
sg181
(lp14143
sg235
(lp14144
sg384
(lp14145
sg124
(lp14146
ssS'sep'
p14147
(dp14148
g114
(lp14149
I2235
assS'overwhelm'
p14150
(dp14151
g145
(lp14152
I1801
assS'sex'
p14153
(dp14154
g91
(lp14155
I2087
asg277
(lp14156
ssS'see'
p14157
(dp14158
g329
(lp14159
sg70
(lp14160
sg26
(lp14161
sg277
(lp14162
sg163
(lp14163
sg40
(lp14164
sg287
(lp14165
sg176
(lp14166
sg145
(lp14167
sg256
(lp14168
sg80
(lp14169
sg118
(lp14170
sg295
(lp14171
sg183
(lp14172
sg59
(lp14173
sg484
(lp14174
sg83
(lp14175
sg85
(lp14176
sg63
(lp14177
sg42
(lp14178
I1142
asg306
(lp14179
sg87
(lp14180
sg89
(lp14181
sg91
(lp14182
sg12
(lp14183
sg303
(lp14184
sg20
(lp14185
sg48
(lp14186
sg68
(lp14187
sg313
(lp14188
sg44
(lp14189
sg149
(lp14190
sg174
(lp14191
sg293
(lp14192
sg32
(lp14193
sg350
(lp14194
sg429
(lp14195
sg318
(lp14196
sg102
(lp14197
sg18
(lp14198
sg110
(lp14199
sg178
(lp14200
sg22
(lp14201
sg216
(lp14202
sg438
(lp14203
sg440
(lp14204
sg332
(lp14205
sg121
(lp14206
sg181
(lp14207
sg6
(lp14208
sg235
(lp14209
sg36
(lp14210
sg384
(lp14211
sg124
(lp14212
sg341
(lp14213
sg535
(lp14214
sg344
(lp14215
sg223
(lp14216
sg78
(lp14217
sg132
(lp14218
sg135
(lp14219
sg138
(lp14220
sg140
(lp14221
sg354
(lp14222
ssS'grjj'
p14223
(dp14224
g124
(lp14225
I3027
assS'sec'
p14226
(dp14227
g116
(lp14228
sg332
(lp14229
sg256
(lp14230
sg6
(lp14231
sg295
(lp14232
sg183
(lp14233
sg59
(lp14234
sg83
(lp14235
sg42
(lp14236
I3219
asg14
(lp14237
sg16
(lp14238
sg96
(lp14239
ssS'sea'
p14240
(dp14241
g59
(lp14242
I1044
assS'sel'
p14243
(dp14244
g110
(lp14245
I1255
assS'contour'
p14246
(dp14247
g216
(lp14248
sg118
(lp14249
sg283
(lp14250
sg70
(lp14251
sg181
(lp14252
sg295
(lp14253
sg183
(lp14254
sg124
(lp14255
sg138
(lp14256
I878
assS'analog'
p14257
(dp14258
g70
(lp14259
sg26
(lp14260
sg287
(lp14261
sg256
(lp14262
sg76
(lp14263
sg118
(lp14264
sg78
(lp14265
sg59
(lp14266
sg80
(lp14267
sg85
(lp14268
sg303
(lp14269
sg42
(lp14270
I1137
asg87
(lp14271
sg245
(lp14272
sg46
(lp14273
sg20
(lp14274
sg221
(lp14275
sg329
(lp14276
sg429
(lp14277
sg318
(lp14278
sg102
(lp14279
sg178
(lp14280
sg63
(lp14281
sg174
(lp14282
sg32
(lp14283
sg332
(lp14284
sg121
(lp14285
sg22
(lp14286
sg8
(lp14287
sg341
(lp14288
sg130
(lp14289
sg14
(lp14290
sg16
(lp14291
ssS'eman'
p14292
(dp14293
g183
(lp14294
I5119
assS'project'
p14295
(dp14296
g30
(lp14297
sg256
(lp14298
sg293
(lp14299
sg78
(lp14300
sg484
(lp14301
sg303
(lp14302
sg42
(lp14303
I1858
asg306
(lp14304
sg91
(lp14305
sg245
(lp14306
sg48
(lp14307
sg99
(lp14308
sg44
(lp14309
sg149
(lp14310
sg32
(lp14311
sg332
(lp14312
sg63
(lp14313
sg52
(lp14314
sg114
(lp14315
sg116
(lp14316
sg440
(lp14317
sg318
(lp14318
sg181
(lp14319
sg8
(lp14320
sg34
(lp14321
sg281
(lp14322
sg130
(lp14323
sg354
(lp14324
ssS'objectpart'
p14325
(dp14326
g429
(lp14327
I606
assS'zki'
p14328
(dp14329
g121
(lp14330
I722
assS'jenkin'
p14331
(dp14332
g135
(lp14333
I2490
asg176
(lp14334
ssS'nopt'
p14335
(dp14336
g36
(lp14337
I1152
assS'telectron'
p14338
(dp14339
g135
(lp14340
I2437
assS'nopd'
p14341
(dp14342
g36
(lp14343
I1172
assS'compliment'
p14344
(dp14345
g72
(lp14346
I793
assS'ehip'
p14347
(dp14348
g63
(lp14349
I1309
assS'blow'
p14350
(dp14351
g181
(lp14352
I1383
assS'oculocentr'
p14353
(dp14354
g303
(lp14355
I263
assS'candor'
p14356
(dp14357
g78
(lp14358
I2573
assS'cgote'
p14359
(dp14360
g20
(lp14361
I1356
assS'ctest'
p14362
(dp14363
g235
(lp14364
I840
assS'bodi'
p14365
(dp14366
g176
(lp14367
sg70
(lp14368
sg293
(lp14369
sg72
(lp14370
sg303
(lp14371
sg318
(lp14372
sg18
(lp14373
I2195
assS'last'
p14374
(dp14375
g124
(lp14376
sg26
(lp14377
sg287
(lp14378
sg145
(lp14379
sg76
(lp14380
sg78
(lp14381
sg80
(lp14382
sg83
(lp14383
sg85
(lp14384
sg306
(lp14385
sg245
(lp14386
sg94
(lp14387
sg20
(lp14388
sg99
(lp14389
sg223
(lp14390
sg440
(lp14391
sg102
(lp14392
sg104
(lp14393
sg106
(lp14394
I119
asg108
(lp14395
sg110
(lp14396
sg178
(lp14397
sg114
(lp14398
sg116
(lp14399
sg329
(lp14400
sg32
(lp14401
sg121
(lp14402
sg4
(lp14403
sg181
(lp14404
sg8
(lp14405
sg235
(lp14406
sg341
(lp14407
sg132
(lp14408
sg140
(lp14409
sg354
(lp14410
ssS'synapsespecif'
p14411
(dp14412
g106
(lp14413
I2037
assS'retent'
p14414
(dp14415
g99
(lp14416
I1762
assS'pdt'
p14417
(dp14418
g221
(lp14419
I606
assS'jlilft'
p14420
(dp14421
g221
(lp14422
I952
assS'tuwien'
p14423
(dp14424
g68
(lp14425
I37
assS'pdp'
p14426
(dp14427
g36
(lp14428
I3303
assS'pdf'
p14429
(dp14430
g102
(lp14431
I231
assS'whole'
p14432
(dp14433
g30
(lp14434
sg116
(lp14435
sg70
(lp14436
sg26
(lp14437
sg76
(lp14438
sg235
(lp14439
sg59
(lp14440
sg68
(lp14441
sg429
(lp14442
sg87
(lp14443
sg14
(lp14444
I3832
asg96
(lp14445
sg63
(lp14446
sg223
(lp14447
ssS'hierach'
p14448
(dp14449
g44
(lp14450
I221
assS'load'
p14451
(dp14452
g78
(lp14453
sg83
(lp14454
sg10
(lp14455
sg14
(lp14456
sg16
(lp14457
I1865
asg20
(lp14458
ssS'llsing'
p14459
(dp14460
g108
(lp14461
I581
asg48
(lp14462
ssS'episod'
p14463
(dp14464
g30
(lp14465
I48
assS'bell'
p14466
(dp14467
g183
(lp14468
sg318
(lp14469
sg145
(lp14470
sg295
(lp14471
sg50
(lp14472
I302
asg72
(lp14473
sg221
(lp14474
sg14
(lp14475
sg16
(lp14476
sg108
(lp14477
sg99
(lp14478
sg63
(lp14479
sg38
(lp14480
ssS'schedul'
p14481
(dp14482
g121
(lp14483
sg293
(lp14484
sg34
(lp14485
sg38
(lp14486
sg10
(lp14487
sg138
(lp14488
I1103
asg114
(lp14489
ssS'northeastern'
p14490
(dp14491
g89
(lp14492
I2921
assS'leon'
p14493
(dp14494
g438
(lp14495
I2348
asg128
(lp14496
sg140
(lp14497
ssS'communiti'
p14498
(dp14499
g145
(lp14500
sg235
(lp14501
sg384
(lp14502
sg221
(lp14503
sg138
(lp14504
sg140
(lp14505
I175
assS'axisymmetr'
p14506
(dp14507
g14
(lp14508
sg16
(lp14509
I450
assS'xax'
p14510
(dp14511
g145
(lp14512
I704
assS'extrema'
p14513
(dp14514
g341
(lp14515
sg8
(lp14516
I74
assS'devic'
p14517
(dp14518
g283
(lp14519
sg59
(lp14520
sg10
(lp14521
sg94
(lp14522
sg14
(lp14523
sg16
(lp14524
sg135
(lp14525
I233
asg20
(lp14526
sg114
(lp14527
ssS'photocathod'
p14528
(dp14529
g283
(lp14530
I645
assS'imbed'
p14531
(dp14532
g181
(lp14533
I2268
assS'implicit'
p14534
(dp14535
g287
(lp14536
sg68
(lp14537
sg70
(lp14538
sg235
(lp14539
sg295
(lp14540
sg183
(lp14541
sg8
(lp14542
sg38
(lp14543
sg42
(lp14544
I278
asg34
(lp14545
sg163
(lp14546
sg108
(lp14547
sg344
(lp14548
sg52
(lp14549
ssS'geoffrey'
p14550
(dp14551
g138
(lp14552
I18
assS'boston'
p14553
(dp14554
g287
(lp14555
sg118
(lp14556
sg22
(lp14557
sg223
(lp14558
sg149
(lp14559
I3143
assS'unambigu'
p14560
(dp14561
g174
(lp14562
I1454
assS'xab'
p14563
(dp14564
g332
(lp14565
I2252
assS'devis'
p14566
(dp14567
g295
(lp14568
sg183
(lp14569
sg114
(lp14570
sg145
(lp14571
sg8
(lp14572
I1336
assS'blob'
p14573
(dp14574
g176
(lp14575
sg181
(lp14576
I1079
assS'lipofski'
p14577
(dp14578
g181
(lp14579
I2328
assS'fire'
p14580
(dp14581
g216
(lp14582
sg174
(lp14583
sg332
(lp14584
sg70
(lp14585
sg4
(lp14586
sg6
(lp14587
sg262
(lp14588
sg116
(lp14589
sg80
(lp14590
sg176
(lp14591
sg78
(lp14592
sg102
(lp14593
sg106
(lp14594
I259
asg350
(lp14595
sg99
(lp14596
sg149
(lp14597
ssS'notori'
p14598
(dp14599
g344
(lp14600
sg329
(lp14601
sg8
(lp14602
I63
assS'fund'
p14603
(dp14604
g178
(lp14605
sg181
(lp14606
sg6
(lp14607
sg126
(lp14608
sg313
(lp14609
sg14
(lp14610
sg135
(lp14611
sg138
(lp14612
I3224
asg52
(lp14613
sg114
(lp14614
ssS'fung'
p14615
(dp14616
g484
(lp14617
I2578
assS'edul'
p14618
(dp14619
g223
(lp14620
I31
assS'ctp'
p14621
(dp14622
g76
(lp14623
I2525
assS'educ'
p14624
(dp14625
g20
(lp14626
I2472
assS'uncertain'
p14627
(dp14628
g230
(lp14629
sg70
(lp14630
sg354
(lp14631
I3099
assS'straight'
p14632
(dp14633
g32
(lp14634
sg181
(lp14635
sg38
(lp14636
sg303
(lp14637
sg12
(lp14638
sg135
(lp14639
sg99
(lp14640
sg138
(lp14641
I1492
asg350
(lp14642
ssS'erron'
p14643
(dp14644
g132
(lp14645
I1464
asg20
(lp14646
sg89
(lp14647
ssS'histor'
p14648
(dp14649
g76
(lp14650
I156
assS'durat'
p14651
(dp14652
g116
(lp14653
sg174
(lp14654
sg440
(lp14655
sg4
(lp14656
sg6
(lp14657
sg76
(lp14658
sg295
(lp14659
sg183
(lp14660
sg68
(lp14661
sg14
(lp14662
sg106
(lp14663
I1463
assS'mlp'
p14664
(dp14665
g440
(lp14666
sg121
(lp14667
sg344
(lp14668
sg10
(lp14669
sg87
(lp14670
sg89
(lp14671
sg128
(lp14672
sg178
(lp14673
sg354
(lp14674
I2222
assS'mlv'
p14675
(dp14676
g130
(lp14677
I1162
assS'versi'
p14678
(dp14679
g18
(lp14680
I19
asg178
(lp14681
ssS'error'
p14682
(dp14683
g68
(lp14684
sg70
(lp14685
sg78
(lp14686
sg277
(lp14687
sg72
(lp14688
sg281
(lp14689
sg283
(lp14690
sg36
(lp14691
sg181
(lp14692
sg30
(lp14693
sg74
(lp14694
sg145
(lp14695
sg76
(lp14696
sg262
(lp14697
sg295
(lp14698
sg183
(lp14699
sg59
(lp14700
sg484
(lp14701
sg38
(lp14702
sg83
(lp14703
sg85
(lp14704
sg42
(lp14705
I162
asg306
(lp14706
sg87
(lp14707
sg89
(lp14708
sg91
(lp14709
sg12
(lp14710
sg94
(lp14711
sg96
(lp14712
sg99
(lp14713
sg313
(lp14714
sg44
(lp14715
sg350
(lp14716
sg116
(lp14717
sg293
(lp14718
sg32
(lp14719
sg245
(lp14720
sg46
(lp14721
sg102
(lp14722
sg108
(lp14723
sg110
(lp14724
sg63
(lp14725
sg114
(lp14726
sg230
(lp14727
sg329
(lp14728
sg440
(lp14729
sg121
(lp14730
sg4
(lp14731
sg6
(lp14732
sg235
(lp14733
sg34
(lp14734
sg221
(lp14735
sg460
(lp14736
sg124
(lp14737
sg126
(lp14738
sg341
(lp14739
sg10
(lp14740
sg344
(lp14741
sg223
(lp14742
sg128
(lp14743
sg130
(lp14744
sg14
(lp14745
sg16
(lp14746
sg135
(lp14747
sg138
(lp14748
sg140
(lp14749
sg354
(lp14750
ssS'fllgiil'
p14751
(dp14752
g149
(lp14753
I811
assS'lomtioftof'
p14754
(dp14755
g350
(lp14756
I2135
assS'real'
p14757
(dp14758
g124
(lp14759
sg70
(lp14760
sg26
(lp14761
sg277
(lp14762
sg72
(lp14763
sg283
(lp14764
sg85
(lp14765
sg287
(lp14766
sg74
(lp14767
sg145
(lp14768
sg262
(lp14769
sg295
(lp14770
sg183
(lp14771
sg59
(lp14772
sg484
(lp14773
sg83
(lp14774
sg114
(lp14775
sg63
(lp14776
sg42
(lp14777
I1026
asg306
(lp14778
sg89
(lp14779
sg245
(lp14780
sg46
(lp14781
sg20
(lp14782
sg48
(lp14783
sg221
(lp14784
sg44
(lp14785
sg350
(lp14786
sg293
(lp14787
sg429
(lp14788
sg68
(lp14789
sg94
(lp14790
sg102
(lp14791
sg104
(lp14792
sg110
(lp14793
sg96
(lp14794
sg52
(lp14795
sg22
(lp14796
sg116
(lp14797
sg174
(lp14798
sg32
(lp14799
sg318
(lp14800
sg178
(lp14801
sg4
(lp14802
sg181
(lp14803
sg8
(lp14804
sg460
(lp14805
sg235
(lp14806
sg126
(lp14807
sg40
(lp14808
sg344
(lp14809
sg130
(lp14810
sg14
(lp14811
sg16
(lp14812
sg135
(lp14813
sg140
(lp14814
sg354
(lp14815
ssS'voc'
p14816
(dp14817
g20
(lp14818
I729
assS'vol'
p14819
(dp14820
g163
(lp14821
sg72
(lp14822
sg74
(lp14823
sg176
(lp14824
sg262
(lp14825
sg183
(lp14826
sg87
(lp14827
sg91
(lp14828
sg245
(lp14829
sg20
(lp14830
sg99
(lp14831
sg223
(lp14832
sg149
(lp14833
sg429
(lp14834
sg108
(lp14835
sg114
(lp14836
sg116
(lp14837
sg174
(lp14838
sg22
(lp14839
sg36
(lp14840
sg124
(lp14841
sg126
(lp14842
sg281
(lp14843
sg40
(lp14844
sg44
(lp14845
sg130
(lp14846
sg135
(lp14847
I2476
assS'moti'
p14848
(dp14849
g429
(lp14850
I2208
assS'von'
p14851
(dp14852
g429
(lp14853
sg176
(lp14854
sg91
(lp14855
sg262
(lp14856
sg149
(lp14857
I322
assS'belmont'
p14858
(dp14859
g230
(lp14860
sg306
(lp14861
I2929
assS'hover'
p14862
(dp14863
g262
(lp14864
I1737
assS'vor'
p14865
(dp14866
g350
(lp14867
I67
assS'mccelland'
p14868
(dp14869
g108
(lp14870
I2573
assS'vanish'
p14871
(dp14872
g332
(lp14873
sg235
(lp14874
sg295
(lp14875
sg183
(lp14876
sg68
(lp14877
sg126
(lp14878
sg34
(lp14879
sg46
(lp14880
sg48
(lp14881
I1398
asg535
(lp14882
ssS'chase'
p14883
(dp14884
g132
(lp14885
I2702
asg72
(lp14886
ssS'irrelev'
p14887
(dp14888
g295
(lp14889
sg183
(lp14890
sg124
(lp14891
sg72
(lp14892
sg85
(lp14893
sg91
(lp14894
sg44
(lp14895
I1744
assS'hypothesi'
p14896
(dp14897
g116
(lp14898
sg287
(lp14899
sg118
(lp14900
sg121
(lp14901
sg4
(lp14902
sg6
(lp14903
sg8
(lp14904
sg344
(lp14905
sg183
(lp14906
sg384
(lp14907
sg85
(lp14908
sg303
(lp14909
sg42
(lp14910
I3387
asg12
(lp14911
sg70
(lp14912
sg132
(lp14913
sg145
(lp14914
sg106
(lp14915
sg18
(lp14916
sg350
(lp14917
ssS'shorten'
p14918
(dp14919
g138
(lp14920
I2250
assS'shorter'
p14921
(dp14922
g102
(lp14923
sg174
(lp14924
sg126
(lp14925
sg76
(lp14926
sg149
(lp14927
I1849
assS'ama'
p14928
(dp14929
g535
(lp14930
I318
assS'decod'
p14931
(dp14932
g87
(lp14933
I126
asg72
(lp14934
sg22
(lp14935
ssS'bethesda'
p14936
(dp14937
g484
(lp14938
sg70
(lp14939
sg4
(lp14940
I3517
assS'outermost'
p14941
(dp14942
g344
(lp14943
I863
assS'anatom'
p14944
(dp14945
g303
(lp14946
sg32
(lp14947
sg350
(lp14948
sg256
(lp14949
sg149
(lp14950
I693
assS'dooley'
p14951
(dp14952
g341
(lp14953
I2728
assS'weinberg'
p14954
(dp14955
g106
(lp14956
I2921
assS'cigar'
p14957
(dp14958
g50
(lp14959
I873
assS'aixz'
p14960
(dp14961
g145
(lp14962
I795
assS'alert'
p14963
(dp14964
g332
(lp14965
I293
assS'lxe'
p14966
(dp14967
g295
(lp14968
I1680
asg183
(lp14969
ssS'lxd'
p14970
(dp14971
g76
(lp14972
I2261
assS'infomax'
p14973
(dp14974
g102
(lp14975
I160
assS'stack'
p14976
(dp14977
g295
(lp14978
sg183
(lp14979
sg484
(lp14980
sg140
(lp14981
I3100
assS'focal'
p14982
(dp14983
g245
(lp14984
I121
asg59
(lp14985
sg181
(lp14986
ssS'recent'
p14987
(dp14988
g70
(lp14989
sg287
(lp14990
sg74
(lp14991
sg176
(lp14992
sg256
(lp14993
sg80
(lp14994
sg118
(lp14995
sg344
(lp14996
sg83
(lp14997
sg85
(lp14998
sg303
(lp14999
sg42
(lp15000
I593
asg306
(lp15001
sg87
(lp15002
sg89
(lp15003
sg12
(lp15004
sg94
(lp15005
sg48
(lp15006
sg99
(lp15007
sg44
(lp15008
sg149
(lp15009
sg174
(lp15010
sg32
(lp15011
sg318
(lp15012
sg102
(lp15013
sg108
(lp15014
sg216
(lp15015
sg438
(lp15016
sg440
(lp15017
sg332
(lp15018
sg4
(lp15019
sg8
(lp15020
sg221
(lp15021
sg384
(lp15022
sg293
(lp15023
sg72
(lp15024
sg281
(lp15025
sg40
(lp15026
sg36
(lp15027
sg132
(lp15028
sg14
(lp15029
sg16
(lp15030
sg138
(lp15031
sg354
(lp15032
ssS'knn'
p15033
(dp15034
g63
(lp15035
sg223
(lp15036
I939
assS'elec'
p15037
(dp15038
g460
(lp15039
I13
assS'eleg'
p15040
(dp15041
g14
(lp15042
sg16
(lp15043
I2007
asg318
(lp15044
sg313
(lp15045
sg44
(lp15046
ssS'qbik'
p15047
(dp15048
g38
(lp15049
I2647
assS'pickl'
p15050
(dp15051
g174
(lp15052
I334
assS'person'
p15053
(dp15054
g74
(lp15055
sg70
(lp15056
sg76
(lp15057
sg235
(lp15058
sg59
(lp15059
sg293
(lp15060
sg83
(lp15061
sg429
(lp15062
sg94
(lp15063
I75
asg223
(lp15064
sg114
(lp15065
ssS'elet'
p15066
(dp15067
g22
(lp15068
I709
assS'expens'
p15069
(dp15070
g178
(lp15071
sg78
(lp15072
sg277
(lp15073
sg163
(lp15074
sg221
(lp15075
sg10
(lp15076
sg87
(lp15077
sg89
(lp15078
sg91
(lp15079
sg130
(lp15080
sg132
(lp15081
sg94
(lp15082
sg50
(lp15083
sg313
(lp15084
sg140
(lp15085
I2242
assS'elev'
p15086
(dp15087
g83
(lp15088
I2
asg277
(lp15089
ssS'feasabl'
p15090
(dp15091
g59
(lp15092
I2807
assS'stentz'
p15093
(dp15094
g42
(lp15095
I3467
asg277
(lp15096
ssS'academ'
p15097
(dp15098
g174
(lp15099
sg440
(lp15100
sg48
(lp15101
sg145
(lp15102
sg59
(lp15103
sg74
(lp15104
sg10
(lp15105
sg281
(lp15106
sg46
(lp15107
sg135
(lp15108
I2615
asg313
(lp15109
sg223
(lp15110
ssS'leong'
p15111
(dp15112
g135
(lp15113
I273
assS'tim'
p15114
(dp15115
g116
(lp15116
sg26
(lp15117
sg256
(lp15118
sg20
(lp15119
I1566
asg63
(lp15120
sg22
(lp15121
ssS'untt'
p15122
(dp15123
g76
(lp15124
I1028
assS'sympo'
p15125
(dp15126
g318
(lp15127
I3038
asg145
(lp15128
ssS'simd'
p15129
(dp15130
g178
(lp15131
I2409
asg10
(lp15132
ssS'globin'
p15133
(dp15134
g130
(lp15135
I1583
assS'signal'
p15136
(dp15137
g329
(lp15138
sg70
(lp15139
sg163
(lp15140
sg116
(lp15141
sg283
(lp15142
sg30
(lp15143
sg74
(lp15144
sg256
(lp15145
sg262
(lp15146
sg78
(lp15147
sg59
(lp15148
sg38
(lp15149
sg83
(lp15150
sg114
(lp15151
sg303
(lp15152
sg87
(lp15153
sg12
(lp15154
sg46
(lp15155
sg20
(lp15156
sg18
(lp15157
sg535
(lp15158
sg350
(lp15159
sg230
(lp15160
sg174
(lp15161
sg245
(lp15162
sg318
(lp15163
sg94
(lp15164
sg102
(lp15165
sg104
(lp15166
sg96
(lp15167
sg22
(lp15168
sg216
(lp15169
sg438
(lp15170
I151
asg440
(lp15171
sg332
(lp15172
sg121
(lp15173
sg4
(lp15174
sg384
(lp15175
sg293
(lp15176
sg72
(lp15177
sg10
(lp15178
sg118
(lp15179
sg128
(lp15180
sg14
(lp15181
sg16
(lp15182
sg135
(lp15183
sg138
(lp15184
ssS'paclearn'
p15185
(dp15186
g344
(lp15187
I1106
assS'chest'
p15188
(dp15189
g91
(lp15190
I2108
assS'rear'
p15191
(dp15192
g438
(lp15193
I1075
assS'sydney'
p15194
(dp15195
g135
(lp15196
I32
assS'input'
p15197
(dp15198
g80
(lp15199
sg344
(lp15200
sg78
(lp15201
sg59
(lp15202
sg484
(lp15203
sg38
(lp15204
sg83
(lp15205
sg85
(lp15206
sg303
(lp15207
sg438
(lp15208
sg116
(lp15209
sg118
(lp15210
sg34
(lp15211
sg36
(lp15212
sg460
(lp15213
sg68
(lp15214
sg72
(lp15215
sg281
(lp15216
sg10
(lp15217
sg40
(lp15218
sg283
(lp15219
sg70
(lp15220
sg26
(lp15221
sg277
(lp15222
sg163
(lp15223
sg89
(lp15224
sg91
(lp15225
sg12
(lp15226
sg94
(lp15227
sg96
(lp15228
sg48
(lp15229
sg99
(lp15230
sg313
(lp15231
sg44
(lp15232
sg149
(lp15233
sg429
(lp15234
sg102
(lp15235
sg104
(lp15236
sg106
(lp15237
sg108
(lp15238
sg110
(lp15239
sg63
(lp15240
sg52
(lp15241
sg114
(lp15242
sg128
(lp15243
sg132
(lp15244
sg14
(lp15245
sg16
(lp15246
sg135
(lp15247
sg50
(lp15248
sg138
(lp15249
sg140
(lp15250
sg354
(lp15251
sg87
(lp15252
sg245
(lp15253
sg46
(lp15254
sg20
(lp15255
sg18
(lp15256
sg221
(lp15257
sg535
(lp15258
sg223
(lp15259
sg350
(lp15260
sg216
(lp15261
sg174
(lp15262
sg440
(lp15263
sg332
(lp15264
sg121
(lp15265
sg4
(lp15266
sg6
(lp15267
sg126
(lp15268
sg341
(lp15269
sg287
(lp15270
sg176
(lp15271
sg145
(lp15272
sg256
(lp15273
sg76
(lp15274
sg262
(lp15275
sg295
(lp15276
sg183
(lp15277
sg42
(lp15278
I43
asg230
(lp15279
sg329
(lp15280
sg178
(lp15281
sg22
(lp15282
sg181
(lp15283
sg235
(lp15284
sg124
(lp15285
ssS'australia'
p15286
(dp15287
g135
(lp15288
I33
asg121
(lp15289
ssS'format'
p15290
(dp15291
g116
(lp15292
sg332
(lp15293
sg121
(lp15294
sg26
(lp15295
sg80
(lp15296
sg10
(lp15297
sg350
(lp15298
sg87
(lp15299
sg91
(lp15300
sg130
(lp15301
sg18
(lp15302
sg106
(lp15303
I2778
asg48
(lp15304
sg99
(lp15305
sg535
(lp15306
sg149
(lp15307
ssS'intuit'
p15308
(dp15309
g484
(lp15310
sg32
(lp15311
sg318
(lp15312
sg121
(lp15313
sg235
(lp15314
sg59
(lp15315
sg68
(lp15316
sg126
(lp15317
sg306
(lp15318
sg46
(lp15319
sg48
(lp15320
sg50
(lp15321
I492
asg313
(lp15322
ssS'oddisord'
p15323
(dp15324
g48
(lp15325
I2267
assS'bitpad'
p15326
(dp15327
g63
(lp15328
I2718
assS'parameteris'
p15329
(dp15330
g124
(lp15331
I522
assS'lathrop'
p15332
(dp15333
g26
(lp15334
I3288
assS'formal'
p15335
(dp15336
g438
(lp15337
I1728
asg32
(lp15338
sg176
(lp15339
sg145
(lp15340
sg4
(lp15341
sg329
(lp15342
sg293
(lp15343
sg384
(lp15344
sg124
(lp15345
sg341
(lp15346
sg85
(lp15347
sg118
(lp15348
sg287
(lp15349
sg306
(lp15350
sg223
(lp15351
sg91
(lp15352
sg132
(lp15353
sg46
(lp15354
sg163
(lp15355
sg138
(lp15356
sg44
(lp15357
ssS'd'
p15358
(dp15359
g80
(lp15360
sg293
(lp15361
sg344
(lp15362
sg78
(lp15363
sg59
(lp15364
sg484
(lp15365
sg38
(lp15366
sg83
(lp15367
sg85
(lp15368
sg303
(lp15369
sg438
(lp15370
sg116
(lp15371
sg118
(lp15372
sg34
(lp15373
sg36
(lp15374
sg460
(lp15375
sg68
(lp15376
sg72
(lp15377
sg281
(lp15378
sg10
(lp15379
sg40
(lp15380
sg283
(lp15381
sg70
(lp15382
sg26
(lp15383
sg277
(lp15384
sg163
(lp15385
sg89
(lp15386
sg91
(lp15387
sg12
(lp15388
sg94
(lp15389
sg96
(lp15390
sg48
(lp15391
sg99
(lp15392
sg313
(lp15393
sg44
(lp15394
sg149
(lp15395
sg429
(lp15396
sg102
(lp15397
sg104
(lp15398
sg106
(lp15399
sg108
(lp15400
sg110
(lp15401
sg63
(lp15402
sg52
(lp15403
sg114
(lp15404
sg128
(lp15405
sg130
(lp15406
sg132
(lp15407
sg14
(lp15408
sg16
(lp15409
sg135
(lp15410
sg50
(lp15411
sg138
(lp15412
sg140
(lp15413
sg354
(lp15414
sg306
(lp15415
sg87
(lp15416
sg245
(lp15417
sg46
(lp15418
sg20
(lp15419
sg18
(lp15420
sg221
(lp15421
sg535
(lp15422
sg223
(lp15423
sg350
(lp15424
sg216
(lp15425
sg174
(lp15426
sg440
(lp15427
sg332
(lp15428
sg121
(lp15429
sg4
(lp15430
sg6
(lp15431
sg8
(lp15432
sg126
(lp15433
sg341
(lp15434
sg30
(lp15435
sg287
(lp15436
sg74
(lp15437
sg176
(lp15438
sg145
(lp15439
sg256
(lp15440
sg76
(lp15441
sg262
(lp15442
sg295
(lp15443
sg183
(lp15444
sg42
(lp15445
I1022
asg230
(lp15446
sg329
(lp15447
sg32
(lp15448
sg318
(lp15449
sg178
(lp15450
sg22
(lp15451
sg181
(lp15452
sg235
(lp15453
sg384
(lp15454
sg124
(lp15455
ssS'zemel'
p15456
(dp15457
g72
(lp15458
sg50
(lp15459
sg138
(lp15460
I1724
assS'consensus'
p15461
(dp15462
g4
(lp15463
I199
assS'mcgraw'
p15464
(dp15465
g429
(lp15466
sg128
(lp15467
I2841
asg163
(lp15468
ssS'encount'
p15469
(dp15470
g121
(lp15471
sg68
(lp15472
sg306
(lp15473
sg135
(lp15474
I1053
asg99
(lp15475
sg223
(lp15476
ssS'canst'
p15477
(dp15478
g163
(lp15479
I545
assS'ttthe'
p15480
(dp15481
g128
(lp15482
I1689
assS'dempster'
p15483
(dp15484
g74
(lp15485
sg460
(lp15486
sg72
(lp15487
sg440
(lp15488
sg91
(lp15489
sg221
(lp15490
sg313
(lp15491
I949
assS'spring'
p15492
(dp15493
g149
(lp15494
I3053
assS'horst'
p15495
(dp15496
g132
(lp15497
I3463
assS'ifloss'
p15498
(dp15499
g341
(lp15500
I1467
assS'limb'
p15501
(dp15502
g32
(lp15503
I3090
asg176
(lp15504
ssS'mooney'
p15505
(dp15506
g132
(lp15507
I3573
assS'sampl'
p15508
(dp15509
g78
(lp15510
sg277
(lp15511
sg163
(lp15512
sg30
(lp15513
sg287
(lp15514
sg74
(lp15515
sg145
(lp15516
sg76
(lp15517
sg295
(lp15518
sg183
(lp15519
sg59
(lp15520
sg484
(lp15521
sg85
(lp15522
sg42
(lp15523
I130
asg306
(lp15524
sg89
(lp15525
sg245
(lp15526
sg46
(lp15527
sg221
(lp15528
sg313
(lp15529
sg223
(lp15530
sg116
(lp15531
sg118
(lp15532
sg318
(lp15533
sg110
(lp15534
sg52
(lp15535
sg114
(lp15536
sg230
(lp15537
sg329
(lp15538
sg440
(lp15539
sg332
(lp15540
sg178
(lp15541
sg22
(lp15542
sg235
(lp15543
sg460
(lp15544
sg124
(lp15545
sg126
(lp15546
sg281
(lp15547
sg344
(lp15548
sg128
(lp15549
sg130
(lp15550
sg132
(lp15551
sg135
(lp15552
sg140
(lp15553
sg354
(lp15554
ssS'pall'
p15555
(dp15556
g116
(lp15557
I307
assS'palo'
p15558
(dp15559
g245
(lp15560
I747
assS'carryov'
p15561
(dp15562
g235
(lp15563
I3012
assS'ijk'
p15564
(dp15565
g46
(lp15566
I2586
assS'ijl'
p15567
(dp15568
g221
(lp15569
I1280
assS'broadband'
p15570
(dp15571
g174
(lp15572
I2202
assS'multitask'
p15573
(dp15574
g277
(lp15575
I11
assS'recognit'
p15576
(dp15577
g283
(lp15578
sg70
(lp15579
sg104
(lp15580
sg30
(lp15581
sg287
(lp15582
sg74
(lp15583
sg76
(lp15584
sg293
(lp15585
sg183
(lp15586
sg59
(lp15587
sg42
(lp15588
I84
asg306
(lp15589
sg87
(lp15590
sg94
(lp15591
sg96
(lp15592
sg99
(lp15593
sg223
(lp15594
sg429
(lp15595
sg178
(lp15596
sg108
(lp15597
sg110
(lp15598
sg63
(lp15599
sg52
(lp15600
sg114
(lp15601
sg174
(lp15602
sg440
(lp15603
sg318
(lp15604
sg121
(lp15605
sg181
(lp15606
sg8
(lp15607
sg72
(lp15608
sg281
(lp15609
sg10
(lp15610
sg40
(lp15611
sg44
(lp15612
sg128
(lp15613
sg130
(lp15614
sg50
(lp15615
sg138
(lp15616
ssS'recognis'
p15617
(dp15618
g332
(lp15619
I189
asg52
(lp15620
ssS'novelti'
p15621
(dp15622
g78
(lp15623
sg96
(lp15624
I921
asg4
(lp15625
ssS'recogniz'
p15626
(dp15627
g295
(lp15628
sg183
(lp15629
sg429
(lp15630
sg181
(lp15631
I673
assS'machin'
p15632
(dp15633
g283
(lp15634
sg26
(lp15635
sg277
(lp15636
sg281
(lp15637
sg287
(lp15638
sg76
(lp15639
sg293
(lp15640
sg344
(lp15641
sg183
(lp15642
sg59
(lp15643
sg484
(lp15644
sg38
(lp15645
sg83
(lp15646
sg42
(lp15647
I122
asg306
(lp15648
sg87
(lp15649
sg89
(lp15650
sg91
(lp15651
sg245
(lp15652
sg46
(lp15653
sg48
(lp15654
sg313
(lp15655
sg223
(lp15656
sg429
(lp15657
sg104
(lp15658
sg63
(lp15659
sg52
(lp15660
sg329
(lp15661
sg178
(lp15662
sg36
(lp15663
sg124
(lp15664
sg72
(lp15665
sg341
(lp15666
sg10
(lp15667
sg535
(lp15668
sg44
(lp15669
sg128
(lp15670
sg78
(lp15671
sg132
(lp15672
sg14
(lp15673
sg138
(lp15674
sg140
(lp15675
ssS'be'
p15676
(dp15677
g80
(lp15678
sg293
(lp15679
sg344
(lp15680
sg78
(lp15681
sg59
(lp15682
sg484
(lp15683
sg38
(lp15684
sg83
(lp15685
sg85
(lp15686
sg303
(lp15687
sg438
(lp15688
sg116
(lp15689
sg118
(lp15690
sg34
(lp15691
sg36
(lp15692
sg460
(lp15693
sg68
(lp15694
sg72
(lp15695
sg281
(lp15696
sg10
(lp15697
sg40
(lp15698
sg283
(lp15699
sg70
(lp15700
sg26
(lp15701
sg277
(lp15702
sg163
(lp15703
sg89
(lp15704
sg91
(lp15705
sg12
(lp15706
sg94
(lp15707
sg96
(lp15708
sg48
(lp15709
sg99
(lp15710
sg313
(lp15711
sg44
(lp15712
sg149
(lp15713
sg429
(lp15714
sg102
(lp15715
sg104
(lp15716
sg106
(lp15717
sg108
(lp15718
sg110
(lp15719
sg63
(lp15720
sg52
(lp15721
sg114
(lp15722
sg128
(lp15723
sg130
(lp15724
sg132
(lp15725
sg14
(lp15726
sg16
(lp15727
sg135
(lp15728
sg50
(lp15729
sg138
(lp15730
sg140
(lp15731
sg354
(lp15732
sg306
(lp15733
sg87
(lp15734
sg245
(lp15735
sg46
(lp15736
sg20
(lp15737
sg18
(lp15738
sg221
(lp15739
sg535
(lp15740
sg223
(lp15741
sg350
(lp15742
sg216
(lp15743
sg174
(lp15744
sg440
(lp15745
sg332
(lp15746
sg121
(lp15747
sg4
(lp15748
sg6
(lp15749
sg8
(lp15750
sg126
(lp15751
sg341
(lp15752
sg30
(lp15753
sg287
(lp15754
sg74
(lp15755
sg176
(lp15756
sg145
(lp15757
sg256
(lp15758
sg76
(lp15759
sg262
(lp15760
sg295
(lp15761
sg183
(lp15762
sg42
(lp15763
I102
asg230
(lp15764
sg329
(lp15765
sg32
(lp15766
sg318
(lp15767
sg178
(lp15768
sg22
(lp15769
sg181
(lp15770
sg235
(lp15771
sg384
(lp15772
sg124
(lp15773
ssS'bf'
p15774
(dp15775
g32
(lp15776
I1643
asg38
(lp15777
sg303
(lp15778
ssS'limitless'
p15779
(dp15780
g14
(lp15781
sg16
(lp15782
I249
assS'ba'
p15783
(dp15784
g32
(lp15785
sg145
(lp15786
sg4
(lp15787
I1583
assS'bc'
p15788
(dp15789
g176
(lp15790
I1860
assS'bl'
p15791
(dp15792
g102
(lp15793
I2884
asg46
(lp15794
ssS'bm'
p15795
(dp15796
g230
(lp15797
I827
asg74
(lp15798
sg38
(lp15799
sg85
(lp15800
sg10
(lp15801
ssS'bn'
p15802
(dp15803
g38
(lp15804
I535
assS'bo'
p15805
(dp15806
g295
(lp15807
sg183
(lp15808
sg38
(lp15809
sg341
(lp15810
sg102
(lp15811
sg48
(lp15812
I1610
assS'bi'
p15813
(dp15814
g216
(lp15815
sg30
(lp15816
sg70
(lp15817
sg230
(lp15818
sg76
(lp15819
sg384
(lp15820
sg102
(lp15821
I3409
asg221
(lp15822
ssS'bj'
p15823
(dp15824
g216
(lp15825
sg535
(lp15826
sg6
(lp15827
I2327
assS'dissimilari'
p15828
(dp15829
g42
(lp15830
I1013
assS'bt'
p15831
(dp15832
g216
(lp15833
sg14
(lp15834
I4636
asg99
(lp15835
sg230
(lp15836
sg245
(lp15837
ssS'bu'
p15838
(dp15839
g230
(lp15840
sg118
(lp15841
sg135
(lp15842
I612
assS'bw'
p15843
(dp15844
g104
(lp15845
sg341
(lp15846
sg6
(lp15847
I2261
assS'veloso'
p15848
(dp15849
g80
(lp15850
sg223
(lp15851
I3377
assS'br'
p15852
(dp15853
g245
(lp15854
sg118
(lp15855
sg138
(lp15856
I1993
asg52
(lp15857
ssS'bs'
p15858
(dp15859
g230
(lp15860
sg138
(lp15861
I2029
assS'bx'
p15862
(dp15863
g32
(lp15864
sg4
(lp15865
I1511
asg535
(lp15866
ssS'by'
p15867
(dp15868
g80
(lp15869
sg293
(lp15870
sg344
(lp15871
sg78
(lp15872
sg59
(lp15873
sg484
(lp15874
sg38
(lp15875
sg83
(lp15876
sg85
(lp15877
sg303
(lp15878
sg438
(lp15879
sg116
(lp15880
sg118
(lp15881
sg34
(lp15882
sg36
(lp15883
sg460
(lp15884
sg68
(lp15885
sg72
(lp15886
sg281
(lp15887
sg10
(lp15888
sg40
(lp15889
sg283
(lp15890
sg70
(lp15891
sg26
(lp15892
sg277
(lp15893
sg163
(lp15894
sg89
(lp15895
sg91
(lp15896
sg12
(lp15897
sg94
(lp15898
sg96
(lp15899
sg48
(lp15900
sg99
(lp15901
sg313
(lp15902
sg44
(lp15903
sg149
(lp15904
sg429
(lp15905
sg102
(lp15906
sg104
(lp15907
sg106
(lp15908
sg108
(lp15909
sg110
(lp15910
sg63
(lp15911
sg52
(lp15912
sg114
(lp15913
sg128
(lp15914
sg130
(lp15915
sg132
(lp15916
sg14
(lp15917
sg16
(lp15918
sg135
(lp15919
sg50
(lp15920
sg138
(lp15921
sg140
(lp15922
sg354
(lp15923
sg306
(lp15924
sg87
(lp15925
sg245
(lp15926
sg46
(lp15927
sg20
(lp15928
sg18
(lp15929
sg221
(lp15930
sg535
(lp15931
sg223
(lp15932
sg350
(lp15933
sg216
(lp15934
sg174
(lp15935
sg440
(lp15936
sg332
(lp15937
sg121
(lp15938
sg4
(lp15939
sg6
(lp15940
sg8
(lp15941
sg126
(lp15942
sg341
(lp15943
sg30
(lp15944
sg287
(lp15945
sg74
(lp15946
sg176
(lp15947
sg145
(lp15948
sg256
(lp15949
sg76
(lp15950
sg262
(lp15951
sg295
(lp15952
sg183
(lp15953
sg42
(lp15954
I541
asg230
(lp15955
sg329
(lp15956
sg32
(lp15957
sg318
(lp15958
sg178
(lp15959
sg22
(lp15960
sg181
(lp15961
sg235
(lp15962
sg384
(lp15963
sg124
(lp15964
ssS'blackburn'
p15965
(dp15966
g174
(lp15967
I561
assS'coexist'
p15968
(dp15969
g303
(lp15970
I2470
assS'cmaj'
p15971
(dp15972
g145
(lp15973
I2637
assS'materi'
p15974
(dp15975
g132
(lp15976
I1846
asg96
(lp15977
sg281
(lp15978
ssS'furui'
p15979
(dp15980
g96
(lp15981
I446
assS'jjwij'
p15982
(dp15983
g68
(lp15984
I350
assS'iilp'
p15985
(dp15986
g68
(lp15987
I2421
assS'iilw'
p15988
(dp15989
g104
(lp15990
I1459
assS'anterior'
p15991
(dp15992
g116
(lp15993
sg80
(lp15994
sg350
(lp15995
I838
assS'repair'
p15996
(dp15997
g26
(lp15998
sg149
(lp15999
I2587
assS'primarili'
p16000
(dp16001
g216
(lp16002
sg438
(lp16003
I1200
asg332
(lp16004
sg256
(lp16005
sg8
(lp16006
sg183
(lp16007
sg484
(lp16008
sg30
(lp16009
sg78
(lp16010
sg20
(lp16011
sg149
(lp16012
ssS'intl'
p16013
(dp16014
g10
(lp16015
I2835
assS'into'
p16016
(dp16017
g344
(lp16018
sg329
(lp16019
sg70
(lp16020
sg78
(lp16021
sg277
(lp16022
sg163
(lp16023
sg72
(lp16024
sg68
(lp16025
sg80
(lp16026
sg281
(lp16027
sg283
(lp16028
sg85
(lp16029
sg460
(lp16030
sg181
(lp16031
sg30
(lp16032
sg350
(lp16033
sg74
(lp16034
sg145
(lp16035
sg256
(lp16036
sg76
(lp16037
sg262
(lp16038
sg295
(lp16039
sg183
(lp16040
sg59
(lp16041
sg484
(lp16042
sg38
(lp16043
sg83
(lp16044
sg114
(lp16045
sg42
(lp16046
I2725
asg87
(lp16047
sg89
(lp16048
sg91
(lp16049
sg12
(lp16050
sg94
(lp16051
sg96
(lp16052
sg48
(lp16053
sg221
(lp16054
sg313
(lp16055
sg44
(lp16056
sg149
(lp16057
sg118
(lp16058
sg174
(lp16059
sg18
(lp16060
sg32
(lp16061
sg245
(lp16062
sg429
(lp16063
sg318
(lp16064
sg46
(lp16065
sg102
(lp16066
sg104
(lp16067
sg110
(lp16068
sg63
(lp16069
sg52
(lp16070
sg22
(lp16071
sg116
(lp16072
sg438
(lp16073
sg440
(lp16074
sg332
(lp16075
sg121
(lp16076
sg4
(lp16077
sg6
(lp16078
sg8
(lp16079
sg34
(lp16080
sg36
(lp16081
sg384
(lp16082
sg235
(lp16083
sg126
(lp16084
sg341
(lp16085
sg10
(lp16086
sg40
(lp16087
sg287
(lp16088
sg223
(lp16089
sg128
(lp16090
sg130
(lp16091
sg132
(lp16092
sg14
(lp16093
sg16
(lp16094
sg135
(lp16095
sg50
(lp16096
sg138
(lp16097
sg140
(lp16098
sg354
(lp16099
ssS'inti'
p16100
(dp16101
g318
(lp16102
sg293
(lp16103
sg72
(lp16104
sg10
(lp16105
sg104
(lp16106
I2999
asg63
(lp16107
ssS'convexifi'
p16108
(dp16109
g8
(lp16110
I548
assS'cho'
p16111
(dp16112
g295
(lp16113
I55
asg183
(lp16114
ssS'occupi'
p16115
(dp16116
g20
(lp16117
I1602
asg72
(lp16118
ssS'tolhurst'
p16119
(dp16120
g12
(lp16121
I2762
assS'span'
p16122
(dp16123
g32
(lp16124
sg332
(lp16125
sg38
(lp16126
sg245
(lp16127
sg14
(lp16128
sg16
(lp16129
sg138
(lp16130
I1756
asg44
(lp16131
ssS'segev'
p16132
(dp16133
g12
(lp16134
I2825
assS'rudow'
p16135
(dp16136
g36
(lp16137
I248
assS'bioi'
p16138
(dp16139
g26
(lp16140
sg48
(lp16141
I2162
asg176
(lp16142
sg8
(lp16143
ssS'sock'
p16144
(dp16145
g181
(lp16146
I1576
assS'submit'
p16147
(dp16148
g230
(lp16149
sg332
(lp16150
I2682
asg26
(lp16151
sg277
(lp16152
sg235
(lp16153
sg295
(lp16154
sg183
(lp16155
sg59
(lp16156
sg163
(lp16157
sg281
(lp16158
sg318
(lp16159
sg36
(lp16160
sg46
(lp16161
sg110
(lp16162
ssS'custom'
p16163
(dp16164
g283
(lp16165
sg78
(lp16166
sg10
(lp16167
sg306
(lp16168
sg14
(lp16169
sg16
(lp16170
I1827
asg63
(lp16171
ssS'suit'
p16172
(dp16173
g329
(lp16174
sg176
(lp16175
sg76
(lp16176
sg295
(lp16177
sg183
(lp16178
sg59
(lp16179
sg124
(lp16180
sg83
(lp16181
sg10
(lp16182
sg34
(lp16183
sg429
(lp16184
sg87
(lp16185
sg128
(lp16186
sg104
(lp16187
sg135
(lp16188
I1671
asg221
(lp16189
sg63
(lp16190
sg223
(lp16191
ssS'toeplitz'
p16192
(dp16193
g102
(lp16194
I857
assS'blueprint'
p16195
(dp16196
g104
(lp16197
I2620
assS'saecking'
p16198
(dp16199
g44
(lp16200
I385
assS'retina'
p16201
(dp16202
g256
(lp16203
sg181
(lp16204
sg303
(lp16205
sg245
(lp16206
sg149
(lp16207
I85
asg350
(lp16208
ssS'premultipl'
p16209
(dp16210
g22
(lp16211
I1092
assS'caudel'
p16212
(dp16213
g46
(lp16214
I3581
assS'decomposit'
p16215
(dp16216
g46
(lp16217
sg96
(lp16218
I1218
asg68
(lp16219
sg32
(lp16220
sg22
(lp16221
ssS'imprecis'
p16222
(dp16223
g460
(lp16224
I2486
asg281
(lp16225
ssS'gatech'
p16226
(dp16227
g295
(lp16228
I17
asg183
(lp16229
ssS'link'
p16230
(dp16231
g440
(lp16232
sg332
(lp16233
sg76
(lp16234
sg181
(lp16235
sg262
(lp16236
sg384
(lp16237
sg10
(lp16238
sg429
(lp16239
sg132
(lp16240
I2171
asg99
(lp16241
sg44
(lp16242
sg149
(lp16243
ssS'nerod'
p16244
(dp16245
g104
(lp16246
I661
assS'asanov'
p16247
(dp16248
g10
(lp16249
I19
assS'line'
p16250
(dp16251
g124
(lp16252
sg26
(lp16253
sg163
(lp16254
sg283
(lp16255
sg460
(lp16256
sg30
(lp16257
sg74
(lp16258
sg76
(lp16259
sg295
(lp16260
sg183
(lp16261
sg59
(lp16262
sg484
(lp16263
sg38
(lp16264
sg114
(lp16265
sg303
(lp16266
sg42
(lp16267
I1779
asg91
(lp16268
sg245
(lp16269
sg46
(lp16270
sg96
(lp16271
sg48
(lp16272
sg99
(lp16273
sg44
(lp16274
sg350
(lp16275
sg118
(lp16276
sg32
(lp16277
sg12
(lp16278
sg429
(lp16279
sg68
(lp16280
sg94
(lp16281
sg102
(lp16282
sg110
(lp16283
sg63
(lp16284
sg22
(lp16285
sg230
(lp16286
sg174
(lp16287
sg440
(lp16288
sg318
(lp16289
sg121
(lp16290
sg181
(lp16291
sg6
(lp16292
sg8
(lp16293
sg34
(lp16294
sg36
(lp16295
sg384
(lp16296
sg235
(lp16297
sg341
(lp16298
sg10
(lp16299
sg344
(lp16300
sg128
(lp16301
sg78
(lp16302
sg50
(lp16303
sg138
(lp16304
sg140
(lp16305
ssS'heinrich'
p16306
(dp16307
g10
(lp16308
I2845
assS'squaredwait'
p16309
(dp16310
g83
(lp16311
I2315
assS'cia'
p16312
(dp16313
g130
(lp16314
I1359
assS'uy'
p16315
(dp16316
g313
(lp16317
I1422
asg262
(lp16318
ssS'ux'
p16319
(dp16320
g313
(lp16321
I1399
assS'charmel'
p16322
(dp16323
g174
(lp16324
I1307
assS'uv'
p16325
(dp16326
g287
(lp16327
I2285
assS'uq'
p16328
(dp16329
g121
(lp16330
I20
assS'up'
p16331
(dp16332
g124
(lp16333
sg26
(lp16334
sg163
(lp16335
sg72
(lp16336
sg281
(lp16337
sg287
(lp16338
sg145
(lp16339
sg76
(lp16340
sg293
(lp16341
sg295
(lp16342
sg183
(lp16343
sg83
(lp16344
sg63
(lp16345
sg42
(lp16346
I508
asg306
(lp16347
sg89
(lp16348
sg91
(lp16349
sg94
(lp16350
sg20
(lp16351
sg18
(lp16352
sg149
(lp16353
sg329
(lp16354
sg68
(lp16355
sg106
(lp16356
sg108
(lp16357
sg96
(lp16358
sg52
(lp16359
sg114
(lp16360
sg116
(lp16361
sg174
(lp16362
sg440
(lp16363
sg318
(lp16364
sg178
(lp16365
sg4
(lp16366
sg181
(lp16367
sg8
(lp16368
sg34
(lp16369
sg384
(lp16370
sg235
(lp16371
sg126
(lp16372
sg341
(lp16373
sg10
(lp16374
sg40
(lp16375
sg78
(lp16376
sg132
(lp16377
sg14
(lp16378
sg16
(lp16379
ssS'us'
p16380
(dp16381
g68
(lp16382
sg26
(lp16383
sg277
(lp16384
sg72
(lp16385
sg287
(lp16386
sg74
(lp16387
sg176
(lp16388
sg145
(lp16389
sg80
(lp16390
sg262
(lp16391
sg183
(lp16392
sg59
(lp16393
sg85
(lp16394
sg42
(lp16395
I223
asg306
(lp16396
sg87
(lp16397
sg91
(lp16398
sg12
(lp16399
sg46
(lp16400
sg18
(lp16401
sg99
(lp16402
sg32
(lp16403
sg102
(lp16404
sg110
(lp16405
sg63
(lp16406
sg230
(lp16407
sg440
(lp16408
sg48
(lp16409
sg22
(lp16410
sg235
(lp16411
sg36
(lp16412
sg384
(lp16413
sg124
(lp16414
sg126
(lp16415
sg10
(lp16416
sg40
(lp16417
sg78
(lp16418
sg14
(lp16419
sg50
(lp16420
sg460
(lp16421
sg140
(lp16422
sg354
(lp16423
ssS'ur'
p16424
(dp16425
g295
(lp16426
I4032
asg183
(lp16427
ssS'cis'
p16428
(dp16429
g14
(lp16430
sg16
(lp16431
I1378
assS'ul'
p16432
(dp16433
g132
(lp16434
I3320
asg46
(lp16435
sg87
(lp16436
sg18
(lp16437
sg68
(lp16438
ssS'uo'
p16439
(dp16440
g230
(lp16441
I1970
assS'indicat'
p16442
(dp16443
g48
(lp16444
I1637
assS'ui'
p16445
(dp16446
g230
(lp16447
sg30
(lp16448
sg8
(lp16449
sg68
(lp16450
sg341
(lp16451
sg40
(lp16452
sg287
(lp16453
sg128
(lp16454
I207
asg46
(lp16455
sg18
(lp16456
ssS'civ'
p16457
(dp16458
g20
(lp16459
sg130
(lp16460
I1361
assS'uk'
p16461
(dp16462
g174
(lp16463
sg176
(lp16464
sg283
(lp16465
sg26
(lp16466
sg76
(lp16467
sg235
(lp16468
sg124
(lp16469
sg38
(lp16470
sg87
(lp16471
sg14
(lp16472
sg16
(lp16473
sg138
(lp16474
I284
assS'cit'
p16475
(dp16476
g36
(lp16477
I2728
assS'horner'
p16478
(dp16479
g384
(lp16480
I765
assS'ug'
p16481
(dp16482
g178
(lp16483
I1142
assS'ua'
p16484
(dp16485
g14
(lp16486
I3676
assS'uc'
p16487
(dp16488
g344
(lp16489
sg20
(lp16490
sg484
(lp16491
sg221
(lp16492
sg91
(lp16493
I2186
assS'iit'
p16494
(dp16495
g306
(lp16496
I1022
assS'dof'
p16497
(dp16498
g295
(lp16499
I3301
asg183
(lp16500
ssS'inputlay'
p16501
(dp16502
g70
(lp16503
I1548
assS'chao'
p16504
(dp16505
g384
(lp16506
sg121
(lp16507
I2680
assS'chap'
p16508
(dp16509
g102
(lp16510
I3612
assS'parsec'
p16511
(dp16512
g94
(lp16513
I3526
assS'phantom'
p16514
(dp16515
g176
(lp16516
I252
assS'paradox'
p16517
(dp16518
g354
(lp16519
I2125
assS'jcx'
p16520
(dp16521
g281
(lp16522
I324
assS'leftward'
p16523
(dp16524
g245
(lp16525
I2097
assS'invalid'
p16526
(dp16527
g181
(lp16528
I2231
assS'slaney'
p16529
(dp16530
g174
(lp16531
I455
assS'ferent'
p16532
(dp16533
g30
(lp16534
I298
assS'brul'
p16535
(dp16536
g10
(lp16537
I2861
assS'luhmann'
p16538
(dp16539
g12
(lp16540
sg106
(lp16541
I2644
assS'fonn'
p16542
(dp16543
g22
(lp16544
I1254
assS'freez'
p16545
(dp16546
g344
(lp16547
sg135
(lp16548
I1510
assS'arcslll'
p16549
(dp16550
g38
(lp16551
I729
assS'drastal'
p16552
(dp16553
g30
(lp16554
I2738
assS'lucia'
p16555
(dp16556
g121
(lp16557
I33
assS'polyak'
p16558
(dp16559
g306
(lp16560
I2236
assS'du'
p16561
(dp16562
g287
(lp16563
I13
assS'lxed'
p16564
(dp16565
g18
(lp16566
I773
assS'mjoj'
p16567
(dp16568
g130
(lp16569
I1368
assS'lpf'
p16570
(dp16571
g22
(lp16572
I299
assS'heileman'
p16573
(dp16574
g46
(lp16575
I19
assS'lang'
p16576
(dp16577
g94
(lp16578
sg108
(lp16579
sg121
(lp16580
sg277
(lp16581
sg128
(lp16582
I2846
assS'land'
p16583
(dp16584
g118
(lp16585
sg235
(lp16586
sg83
(lp16587
sg42
(lp16588
I3532
asg102
(lp16589
sg12
(lp16590
ssS'lanc'
p16591
(dp16592
g108
(lp16593
I12
assS'algorithm'
p16594
(dp16595
g68
(lp16596
sg70
(lp16597
sg78
(lp16598
sg163
(lp16599
sg72
(lp16600
sg281
(lp16601
sg283
(lp16602
sg26
(lp16603
sg30
(lp16604
sg287
(lp16605
sg74
(lp16606
sg145
(lp16607
sg76
(lp16608
sg293
(lp16609
sg295
(lp16610
sg183
(lp16611
sg59
(lp16612
sg484
(lp16613
sg83
(lp16614
sg85
(lp16615
sg63
(lp16616
sg42
(lp16617
I55
asg306
(lp16618
sg87
(lp16619
sg89
(lp16620
sg91
(lp16621
sg94
(lp16622
sg96
(lp16623
sg48
(lp16624
sg99
(lp16625
sg313
(lp16626
sg44
(lp16627
sg149
(lp16628
sg230
(lp16629
sg318
(lp16630
sg46
(lp16631
sg102
(lp16632
sg178
(lp16633
sg106
(lp16634
sg108
(lp16635
sg110
(lp16636
sg20
(lp16637
sg52
(lp16638
sg114
(lp16639
sg216
(lp16640
sg329
(lp16641
sg440
(lp16642
sg18
(lp16643
sg121
(lp16644
sg8
(lp16645
sg34
(lp16646
sg221
(lp16647
sg460
(lp16648
sg124
(lp16649
sg126
(lp16650
sg341
(lp16651
sg10
(lp16652
sg40
(lp16653
sg344
(lp16654
sg223
(lp16655
sg128
(lp16656
sg130
(lp16657
sg132
(lp16658
sg14
(lp16659
sg16
(lp16660
sg135
(lp16661
sg50
(lp16662
sg138
(lp16663
sg354
(lp16664
ssS'psychiatri'
p16665
(dp16666
g4
(lp16667
I35
assS'age'
p16668
(dp16669
g94
(lp16670
I937
asg72
(lp16671
sg91
(lp16672
sg277
(lp16673
sg4
(lp16674
ssS'dow'
p16675
(dp16676
g48
(lp16677
I283
assS'discrimin'
p16678
(dp16679
g277
(lp16680
sg30
(lp16681
sg256
(lp16682
sg293
(lp16683
sg344
(lp16684
sg59
(lp16685
sg87
(lp16686
sg20
(lp16687
sg221
(lp16688
sg429
(lp16689
sg318
(lp16690
sg102
(lp16691
sg110
(lp16692
sg114
(lp16693
sg440
(lp16694
sg332
(lp16695
sg178
(lp16696
sg181
(lp16697
sg72
(lp16698
sg281
(lp16699
sg132
(lp16700
sg50
(lp16701
sg138
(lp16702
I1659
assS'train'
p16703
(dp16704
g68
(lp16705
sg70
(lp16706
sg78
(lp16707
sg277
(lp16708
sg163
(lp16709
sg281
(lp16710
sg283
(lp16711
sg181
(lp16712
sg26
(lp16713
sg30
(lp16714
sg287
(lp16715
sg176
(lp16716
sg80
(lp16717
sg76
(lp16718
sg262
(lp16719
sg295
(lp16720
sg183
(lp16721
sg59
(lp16722
sg484
(lp16723
sg38
(lp16724
sg83
(lp16725
sg85
(lp16726
sg63
(lp16727
sg42
(lp16728
I1629
asg306
(lp16729
sg87
(lp16730
sg89
(lp16731
sg91
(lp16732
sg245
(lp16733
sg94
(lp16734
sg96
(lp16735
sg99
(lp16736
sg313
(lp16737
sg44
(lp16738
sg149
(lp16739
sg116
(lp16740
sg329
(lp16741
sg293
(lp16742
sg46
(lp16743
sg104
(lp16744
sg106
(lp16745
sg108
(lp16746
sg110
(lp16747
sg178
(lp16748
sg52
(lp16749
sg114
(lp16750
sg230
(lp16751
sg174
(lp16752
sg440
(lp16753
sg121
(lp16754
sg4
(lp16755
sg6
(lp16756
sg235
(lp16757
sg221
(lp16758
sg460
(lp16759
sg124
(lp16760
sg126
(lp16761
sg341
(lp16762
sg10
(lp16763
sg344
(lp16764
sg223
(lp16765
sg128
(lp16766
sg36
(lp16767
sg132
(lp16768
sg14
(lp16769
sg16
(lp16770
sg135
(lp16771
sg138
(lp16772
sg140
(lp16773
sg354
(lp16774
ssS'dins'
p16775
(dp16776
g176
(lp16777
I2649
assS'fresh'
p16778
(dp16779
g12
(lp16780
I2873
assS'predoctor'
p16781
(dp16782
g74
(lp16783
I3040
assS'agr'
p16784
(dp16785
g293
(lp16786
I1280
assS'essay'
p16787
(dp16788
g116
(lp16789
I2491
assS'bvr'
p16790
(dp16791
g306
(lp16792
I35
assS'extrapol'
p16793
(dp16794
g89
(lp16795
sg99
(lp16796
I422
assS'partial'
p16797
(dp16798
g104
(lp16799
sg80
(lp16800
sg293
(lp16801
sg344
(lp16802
sg42
(lp16803
I1975
asg12
(lp16804
sg20
(lp16805
sg149
(lp16806
sg118
(lp16807
sg178
(lp16808
sg108
(lp16809
sg114
(lp16810
sg230
(lp16811
sg438
(lp16812
sg121
(lp16813
sg181
(lp16814
sg124
(lp16815
sg281
(lp16816
sg130
(lp16817
sg132
(lp16818
sg14
(lp16819
sg16
(lp16820
ssS'egocentr'
p16821
(dp16822
g80
(lp16823
sg303
(lp16824
I330
assS'jgij'
p16825
(dp16826
g63
(lp16827
I380
assS'scratch'
p16828
(dp16829
g295
(lp16830
I2829
asg183
(lp16831
ssS'broaden'
p16832
(dp16833
g102
(lp16834
I2594
assS'legal'
p16835
(dp16836
g440
(lp16837
I352
asg277
(lp16838
sg114
(lp16839
ssS'broader'
p16840
(dp16841
g216
(lp16842
sg329
(lp16843
sg18
(lp16844
sg6
(lp16845
I1693
asg114
(lp16846
ssS'unfulfil'
p16847
(dp16848
g70
(lp16849
I1170
assS'jdepart'
p16850
(dp16851
g174
(lp16852
I9
assS'byrn'
p16853
(dp16854
g72
(lp16855
I261
assS'young'
p16856
(dp16857
g116
(lp16858
sg87
(lp16859
I1755
asg40
(lp16860
ssS'send'
p16861
(dp16862
g216
(lp16863
I550
asg30
(lp16864
sg287
(lp16865
ssS'sens'
p16866
(dp16867
g70
(lp16868
sg287
(lp16869
sg176
(lp16870
sg256
(lp16871
sg344
(lp16872
sg42
(lp16873
I361
asg46
(lp16874
sg221
(lp16875
sg535
(lp16876
sg44
(lp16877
sg350
(lp16878
sg104
(lp16879
sg108
(lp16880
sg440
(lp16881
sg318
(lp16882
sg8
(lp16883
sg34
(lp16884
sg99
(lp16885
sg384
(lp16886
sg68
(lp16887
sg341
(lp16888
sg130
(lp16889
sg460
(lp16890
ssS'sent'
p16891
(dp16892
g329
(lp16893
sg4
(lp16894
I1587
asg63
(lp16895
ssS'mcup'
p16896
(dp16897
g10
(lp16898
I2778
assS'recip'
p16899
(dp16900
g34
(lp16901
sg96
(lp16902
I1192
asg70
(lp16903
ssS'northwestern'
p16904
(dp16905
g99
(lp16906
I266
assS'magic'
p16907
(dp16908
g94
(lp16909
I573
assS'inclic'
p16910
(dp16911
g12
(lp16912
I474
assS'counterproduct'
p16913
(dp16914
g235
(lp16915
I2422
assS'harbor'
p16916
(dp16917
g149
(lp16918
I3054
assS'inclin'
p16919
(dp16920
g132
(lp16921
I1988
assS'eve'
p16922
(dp16923
g484
(lp16924
I871
assS'fewer'
p16925
(dp16926
g74
(lp16927
sg178
(lp16928
sg34
(lp16929
sg344
(lp16930
sg306
(lp16931
sg87
(lp16932
sg91
(lp16933
sg94
(lp16934
sg108
(lp16935
I2058
asg44
(lp16936
ssS'hemiretina'
p16937
(dp16938
g303
(lp16939
I1451
assS'race'
p16940
(dp16941
g78
(lp16942
I1294
assS'vertic'
p16943
(dp16944
g116
(lp16945
sg174
(lp16946
sg32
(lp16947
sg318
(lp16948
sg26
(lp16949
sg76
(lp16950
sg36
(lp16951
sg460
(lp16952
sg59
(lp16953
sg42
(lp16954
I1608
asg429
(lp16955
sg12
(lp16956
sg14
(lp16957
sg16
(lp16958
sg48
(lp16959
sg50
(lp16960
sg63
(lp16961
sg350
(lp16962
ssS'marri'
p16963
(dp16964
g72
(lp16965
I964
assS'rack'
p16966
(dp16967
g14
(lp16968
sg16
(lp16969
I2099
assS'vehicl'
p16970
(dp16971
g42
(lp16972
I3491
asg277
(lp16973
ssS'impli'
p16974
(dp16975
g329
(lp16976
sg26
(lp16977
sg163
(lp16978
sg287
(lp16979
sg145
(lp16980
sg80
(lp16981
sg344
(lp16982
sg85
(lp16983
sg42
(lp16984
I3275
asg87
(lp16985
sg18
(lp16986
sg174
(lp16987
sg68
(lp16988
sg114
(lp16989
sg230
(lp16990
sg438
(lp16991
sg32
(lp16992
sg178
(lp16993
sg22
(lp16994
sg235
(lp16995
sg460
(lp16996
sg124
(lp16997
sg281
(lp16998
sg40
(lp16999
sg14
(lp17000
sg135
(lp17001
sg140
(lp17002
ssS'natur'
p17003
(dp17004
g283
(lp17005
sg70
(lp17006
sg281
(lp17007
sg181
(lp17008
sg30
(lp17009
sg74
(lp17010
sg176
(lp17011
sg145
(lp17012
sg256
(lp17013
sg118
(lp17014
sg344
(lp17015
sg59
(lp17016
sg83
(lp17017
sg85
(lp17018
sg42
(lp17019
I931
asg306
(lp17020
sg89
(lp17021
sg20
(lp17022
sg48
(lp17023
sg99
(lp17024
sg44
(lp17025
sg174
(lp17026
sg293
(lp17027
sg32
(lp17028
sg318
(lp17029
sg102
(lp17030
sg178
(lp17031
sg106
(lp17032
sg110
(lp17033
sg22
(lp17034
sg116
(lp17035
sg438
(lp17036
sg440
(lp17037
sg332
(lp17038
sg121
(lp17039
sg4
(lp17040
sg6
(lp17041
sg8
(lp17042
sg384
(lp17043
sg68
(lp17044
sg341
(lp17045
sg40
(lp17046
sg223
(lp17047
sg132
(lp17048
sg14
(lp17049
sg16
(lp17050
sg50
(lp17051
sg138
(lp17052
ssS'imple'
p17053
(dp17054
g295
(lp17055
I2122
asg183
(lp17056
ssS'cambridg'
p17057
(dp17058
g26
(lp17059
sg30
(lp17060
sg287
(lp17061
sg74
(lp17062
sg76
(lp17063
sg262
(lp17064
sg295
(lp17065
sg183
(lp17066
sg83
(lp17067
sg303
(lp17068
sg306
(lp17069
sg87
(lp17070
sg89
(lp17071
sg12
(lp17072
sg96
(lp17073
sg99
(lp17074
sg313
(lp17075
sg350
(lp17076
sg116
(lp17077
sg20
(lp17078
sg230
(lp17079
sg329
(lp17080
sg440
(lp17081
sg318
(lp17082
sg181
(lp17083
sg34
(lp17084
sg460
(lp17085
sg293
(lp17086
sg138
(lp17087
sg354
(lp17088
I3233
assS'video'
p17089
(dp17090
g42
(lp17091
I3526
asg59
(lp17092
sg318
(lp17093
sg181
(lp17094
sg293
(lp17095
ssS'cumbersom'
p17096
(dp17097
g384
(lp17098
I762
asg283
(lp17099
ssS'odo'
p17100
(dp17101
g36
(lp17102
I1446
assS'ueu'
p17103
(dp17104
g306
(lp17105
I897
assS'icsi'
p17106
(dp17107
g440
(lp17108
sg121
(lp17109
I1484
asg10
(lp17110
ssS'victor'
p17111
(dp17112
g83
(lp17113
I2778
assS'index'
p17114
(dp17115
g230
(lp17116
sg440
(lp17117
sg283
(lp17118
sg277
(lp17119
sg181
(lp17120
sg8
(lp17121
sg293
(lp17122
sg460
(lp17123
sg124
(lp17124
sg429
(lp17125
sg10
(lp17126
sg40
(lp17127
sg102
(lp17128
sg306
(lp17129
sg85
(lp17130
sg104
(lp17131
sg12
(lp17132
sg14
(lp17133
I4003
asg96
(lp17134
sg114
(lp17135
ssS'bandera'
p17136
(dp17137
g245
(lp17138
I235
assS'indep'
p17139
(dp17140
g48
(lp17141
I2002
assS'rbjkt'
p17142
(dp17143
g46
(lp17144
I2452
assS'multimedia'
p17145
(dp17146
g10
(lp17147
I353
assS'ukit'
p17148
(dp17149
g52
(lp17150
I2590
assS'richmond'
p17151
(dp17152
g438
(lp17153
I36
asg6
(lp17154
ssS'icassp'
p17155
(dp17156
g96
(lp17157
I2827
asg87
(lp17158
ssS'kershaw'
p17159
(dp17160
g87
(lp17161
I14
assS'reestim'
p17162
(dp17163
g76
(lp17164
I2204
assS'dsino'
p17165
(dp17166
g36
(lp17167
I1514
assS'bird'
p17168
(dp17169
g116
(lp17170
I101
assS'led'
p17171
(dp17172
g174
(lp17173
sg32
(lp17174
sg283
(lp17175
sg256
(lp17176
sg181
(lp17177
sg221
(lp17178
sg59
(lp17179
sg74
(lp17180
sg245
(lp17181
sg78
(lp17182
sg102
(lp17183
sg106
(lp17184
I1971
asg99
(lp17185
sg44
(lp17186
sg354
(lp17187
ssS'lee'
p17188
(dp17189
g174
(lp17190
sg440
(lp17191
sg87
(lp17192
sg89
(lp17193
sg128
(lp17194
I13
asg245
(lp17195
sg48
(lp17196
sg221
(lp17197
ssS'larson'
p17198
(dp17199
g106
(lp17200
I562
assS'lei'
p17201
(dp17202
g72
(lp17203
I10
assS'lem'
p17204
(dp17205
g245
(lp17206
I2536
assS'openmg'
p17207
(dp17208
g132
(lp17209
I3296
assS'sceneri'
p17210
(dp17211
g59
(lp17212
I748
assS'let'
p17213
(dp17214
g281
(lp17215
sg30
(lp17216
sg287
(lp17217
sg176
(lp17218
sg145
(lp17219
sg344
(lp17220
sg183
(lp17221
sg484
(lp17222
sg85
(lp17223
sg42
(lp17224
I96
asg306
(lp17225
sg460
(lp17226
sg94
(lp17227
sg96
(lp17228
sg48
(lp17229
sg99
(lp17230
sg535
(lp17231
sg44
(lp17232
sg102
(lp17233
sg108
(lp17234
sg230
(lp17235
sg32
(lp17236
sg18
(lp17237
sg34
(lp17238
sg36
(lp17239
sg384
(lp17240
sg68
(lp17241
sg72
(lp17242
sg341
(lp17243
sg40
(lp17244
sg132
(lp17245
sg50
(lp17246
sg138
(lp17247
sg354
(lp17248
ssS'lex'
p17249
(dp17250
g42
(lp17251
I1285
asg235
(lp17252
ssS'rasmus'
p17253
(dp17254
g176
(lp17255
I8
assS'twostep'
p17256
(dp17257
g74
(lp17258
I1234
assS'ctt'
p17259
(dp17260
g34
(lp17261
I2124
assS'great'
p17262
(dp17263
g438
(lp17264
I2194
asg318
(lp17265
sg145
(lp17266
sg256
(lp17267
sg262
(lp17268
sg295
(lp17269
sg183
(lp17270
sg59
(lp17271
sg429
(lp17272
sg85
(lp17273
sg313
(lp17274
sg34
(lp17275
sg63
(lp17276
sg94
(lp17277
sg138
(lp17278
sg140
(lp17279
sg114
(lp17280
ssS'psychometrika'
p17281
(dp17282
g74
(lp17283
I3058
assS'ctr'
p17284
(dp17285
g76
(lp17286
I2558
assS'dembo'
p17287
(dp17288
g438
(lp17289
I2493
assS'technolog'
p17290
(dp17291
g74
(lp17292
sg145
(lp17293
sg256
(lp17294
sg183
(lp17295
sg59
(lp17296
sg306
(lp17297
sg87
(lp17298
sg94
(lp17299
sg20
(lp17300
sg221
(lp17301
sg313
(lp17302
sg329
(lp17303
sg96
(lp17304
sg230
(lp17305
sg174
(lp17306
sg318
(lp17307
sg22
(lp17308
sg6
(lp17309
sg460
(lp17310
sg68
(lp17311
sg40
(lp17312
sg78
(lp17313
sg132
(lp17314
sg14
(lp17315
sg16
(lp17316
sg135
(lp17317
sg138
(lp17318
sg354
(lp17319
I2902
assS'rdx'
p17320
(dp17321
g176
(lp17322
I1101
assS'peke'
p17323
(dp17324
g72
(lp17325
I26
assS'survey'
p17326
(dp17327
g42
(lp17328
I2866
asg287
(lp17329
sg293
(lp17330
sg145
(lp17331
sg354
(lp17332
ssS'defeat'
p17333
(dp17334
g145
(lp17335
I2818
assS'skin'
p17336
(dp17337
g176
(lp17338
I201
assS'opinion'
p17339
(dp17340
g70
(lp17341
sg262
(lp17342
I2093
assS'cth'
p17343
(dp17344
g283
(lp17345
I444
assS'maker'
p17346
(dp17347
g91
(lp17348
I1680
assS'fake'
p17349
(dp17350
g295
(lp17351
I1881
asg183
(lp17352
ssS'calibr'
p17353
(dp17354
g42
(lp17355
I3447
asg59
(lp17356
sg293
(lp17357
ssS'rockefel'
p17358
(dp17359
g12
(lp17360
I169
assS'grupen'
p17361
(dp17362
g83
(lp17363
I2781
assS'disciplin'
p17364
(dp17365
g429
(lp17366
I2047
assS'schmidhub'
p17367
(dp17368
g30
(lp17369
sg50
(lp17370
I301
asg313
(lp17371
ssS'sgn'
p17372
(dp17373
g295
(lp17374
sg183
(lp17375
sg384
(lp17376
I427
asg40
(lp17377
ssS'sperduti'
p17378
(dp17379
g44
(lp17380
I13
assS'corkela'
p17381
(dp17382
g106
(lp17383
I957
assS'zip'
p17384
(dp17385
g94
(lp17386
sg63
(lp17387
sg138
(lp17388
I2003
asg181
(lp17389
sg114
(lp17390
ssS'commun'
p17391
(dp17392
g32
(lp17393
I3159
assS'doubl'
p17394
(dp17395
g32
(lp17396
sg318
(lp17397
sg4
(lp17398
I3129
asg80
(lp17399
sg68
(lp17400
sg440
(lp17401
sg10
(lp17402
sg20
(lp17403
ssS'next'
p17404
(dp17405
g26
(lp17406
sg277
(lp17407
sg281
(lp17408
sg30
(lp17409
sg287
(lp17410
sg256
(lp17411
sg76
(lp17412
sg344
(lp17413
sg183
(lp17414
sg59
(lp17415
sg83
(lp17416
sg303
(lp17417
sg42
(lp17418
I1515
asg306
(lp17419
sg89
(lp17420
sg91
(lp17421
sg12
(lp17422
sg94
(lp17423
sg20
(lp17424
sg48
(lp17425
sg99
(lp17426
sg313
(lp17427
sg44
(lp17428
sg329
(lp17429
sg18
(lp17430
sg32
(lp17431
sg245
(lp17432
sg102
(lp17433
sg104
(lp17434
sg63
(lp17435
sg174
(lp17436
sg440
(lp17437
sg332
(lp17438
sg178
(lp17439
sg80
(lp17440
sg8
(lp17441
sg34
(lp17442
sg460
(lp17443
sg68
(lp17444
sg341
(lp17445
sg14
(lp17446
sg16
(lp17447
sg135
(lp17448
sg50
(lp17449
sg354
(lp17450
ssS'zif'
p17451
(dp17452
g48
(lp17453
I1959
assS'doubt'
p17454
(dp17455
g6
(lp17456
I1179
assS'cornplet'
p17457
(dp17458
g350
(lp17459
I913
assS'lesli'
p17460
(dp17461
g174
(lp17462
I5
assS'zim'
p17463
(dp17464
g138
(lp17465
I367
assS'zij'
p17466
(dp17467
g118
(lp17468
sg91
(lp17469
I1120
assS'commut'
p17470
(dp17471
g96
(lp17472
I1077
asg32
(lp17473
ssS'zii'
p17474
(dp17475
g138
(lp17476
I486
assS'pfa'
p17477
(dp17478
g20
(lp17479
I785
assS'pfc'
p17480
(dp17481
g4
(lp17482
I57
assS'xci'
p17483
(dp17484
g124
(lp17485
I549
assS'sharpli'
p17486
(dp17487
g438
(lp17488
I961
asg149
(lp17489
sg354
(lp17490
ssS'floatingpoint'
p17491
(dp17492
g10
(lp17493
I2471
assS'indoor'
p17494
(dp17495
g245
(lp17496
I2629
asg181
(lp17497
ssS'oi'
p17498
(dp17499
g30
(lp17500
I913
asg344
(lp17501
sg429
(lp17502
sg63
(lp17503
sg303
(lp17504
ssS'hessian'
p17505
(dp17506
g8
(lp17507
sg295
(lp17508
sg183
(lp17509
sg126
(lp17510
sg341
(lp17511
sg34
(lp17512
sg138
(lp17513
I527
assS'this'
p17514
(dp17515
g80
(lp17516
sg293
(lp17517
sg344
(lp17518
sg78
(lp17519
sg59
(lp17520
sg484
(lp17521
sg38
(lp17522
sg83
(lp17523
sg85
(lp17524
sg303
(lp17525
sg438
(lp17526
sg116
(lp17527
sg118
(lp17528
sg34
(lp17529
sg36
(lp17530
sg460
(lp17531
sg68
(lp17532
sg72
(lp17533
sg281
(lp17534
sg10
(lp17535
sg40
(lp17536
sg283
(lp17537
sg70
(lp17538
sg26
(lp17539
sg277
(lp17540
sg163
(lp17541
sg89
(lp17542
sg91
(lp17543
sg12
(lp17544
sg94
(lp17545
sg96
(lp17546
sg48
(lp17547
sg99
(lp17548
sg313
(lp17549
sg44
(lp17550
sg149
(lp17551
sg429
(lp17552
sg102
(lp17553
sg104
(lp17554
sg106
(lp17555
sg108
(lp17556
sg110
(lp17557
sg63
(lp17558
sg52
(lp17559
sg114
(lp17560
sg128
(lp17561
sg130
(lp17562
sg132
(lp17563
sg14
(lp17564
sg16
(lp17565
sg135
(lp17566
sg50
(lp17567
sg138
(lp17568
sg140
(lp17569
sg354
(lp17570
sg306
(lp17571
sg87
(lp17572
sg245
(lp17573
sg46
(lp17574
sg20
(lp17575
sg18
(lp17576
sg221
(lp17577
sg535
(lp17578
sg223
(lp17579
sg350
(lp17580
sg216
(lp17581
sg174
(lp17582
sg440
(lp17583
sg332
(lp17584
sg121
(lp17585
sg4
(lp17586
sg6
(lp17587
sg8
(lp17588
sg126
(lp17589
sg341
(lp17590
sg30
(lp17591
sg287
(lp17592
sg74
(lp17593
sg176
(lp17594
sg145
(lp17595
sg256
(lp17596
sg76
(lp17597
sg262
(lp17598
sg295
(lp17599
sg183
(lp17600
sg42
(lp17601
I220
asg230
(lp17602
sg329
(lp17603
sg32
(lp17604
sg318
(lp17605
sg178
(lp17606
sg22
(lp17607
sg181
(lp17608
sg235
(lp17609
sg384
(lp17610
sg124
(lp17611
ssS'irlput'
p17612
(dp17613
g124
(lp17614
I430
assS'pour'
p17615
(dp17616
g102
(lp17617
I1375
assS'sartana'
p17618
(dp17619
g8
(lp17620
I2509
assS'thin'
p17621
(dp17622
g14
(lp17623
I2722
asg63
(lp17624
sg6
(lp17625
sg44
(lp17626
sg76
(lp17627
ssS'eould'
p17628
(dp17629
g63
(lp17630
I1329
assS'liegroup'
p17631
(dp17632
g32
(lp17633
I3148
assS'scatter'
p17634
(dp17635
g283
(lp17636
sg50
(lp17637
sg80
(lp17638
sg140
(lp17639
I2777
assS'sabe'
p17640
(dp17641
g329
(lp17642
sg89
(lp17643
I1578
assS'weaker'
p17644
(dp17645
g4
(lp17646
sg34
(lp17647
sg38
(lp17648
sg85
(lp17649
sg303
(lp17650
sg102
(lp17651
sg140
(lp17652
I1605
asg149
(lp17653
ssS'slip'
p17654
(dp17655
g245
(lp17656
I806
assS'pawn'
p17657
(dp17658
g132
(lp17659
I987
assS'plate'
p17660
(dp17661
g138
(lp17662
I2290
assS'process'
p17663
(dp17664
g80
(lp17665
sg293
(lp17666
sg344
(lp17667
sg78
(lp17668
sg59
(lp17669
sg38
(lp17670
sg85
(lp17671
sg303
(lp17672
sg116
(lp17673
sg118
(lp17674
sg34
(lp17675
sg36
(lp17676
sg460
(lp17677
sg68
(lp17678
sg72
(lp17679
sg281
(lp17680
sg10
(lp17681
sg40
(lp17682
sg283
(lp17683
sg70
(lp17684
sg26
(lp17685
sg277
(lp17686
sg163
(lp17687
sg89
(lp17688
sg91
(lp17689
sg12
(lp17690
sg94
(lp17691
sg96
(lp17692
sg48
(lp17693
sg99
(lp17694
sg313
(lp17695
sg44
(lp17696
sg149
(lp17697
sg429
(lp17698
sg102
(lp17699
sg104
(lp17700
sg108
(lp17701
sg110
(lp17702
sg63
(lp17703
sg52
(lp17704
sg114
(lp17705
sg128
(lp17706
sg130
(lp17707
sg132
(lp17708
sg14
(lp17709
sg135
(lp17710
sg50
(lp17711
sg138
(lp17712
sg140
(lp17713
sg354
(lp17714
sg306
(lp17715
sg87
(lp17716
sg245
(lp17717
sg46
(lp17718
sg20
(lp17719
sg18
(lp17720
sg221
(lp17721
sg535
(lp17722
sg223
(lp17723
sg350
(lp17724
sg216
(lp17725
sg174
(lp17726
sg440
(lp17727
sg332
(lp17728
sg121
(lp17729
sg4
(lp17730
sg6
(lp17731
sg8
(lp17732
sg126
(lp17733
sg341
(lp17734
sg30
(lp17735
sg74
(lp17736
sg176
(lp17737
sg145
(lp17738
sg256
(lp17739
sg76
(lp17740
sg262
(lp17741
sg295
(lp17742
sg183
(lp17743
sg42
(lp17744
I1136
asg318
(lp17745
sg178
(lp17746
sg22
(lp17747
sg181
(lp17748
sg235
(lp17749
sg384
(lp17750
sg124
(lp17751
ssS'srn'
p17752
(dp17753
g128
(lp17754
I662
assS'loci'
p17755
(dp17756
g48
(lp17757
I2141
asg293
(lp17758
ssS'hauser'
p17759
(dp17760
g10
(lp17761
I2644
assS'high'
p17762
(dp17763
g124
(lp17764
sg70
(lp17765
sg26
(lp17766
sg277
(lp17767
sg283
(lp17768
sg303
(lp17769
sg80
(lp17770
sg287
(lp17771
sg74
(lp17772
sg176
(lp17773
sg145
(lp17774
sg256
(lp17775
sg76
(lp17776
sg262
(lp17777
sg295
(lp17778
sg183
(lp17779
sg59
(lp17780
sg484
(lp17781
sg85
(lp17782
sg63
(lp17783
sg306
(lp17784
sg87
(lp17785
sg89
(lp17786
sg68
(lp17787
sg12
(lp17788
sg94
(lp17789
sg20
(lp17790
sg48
(lp17791
sg221
(lp17792
sg44
(lp17793
sg149
(lp17794
sg116
(lp17795
sg329
(lp17796
sg293
(lp17797
sg460
(lp17798
sg245
(lp17799
sg429
(lp17800
sg318
(lp17801
sg104
(lp17802
sg106
(lp17803
I92
asg110
(lp17804
sg178
(lp17805
sg52
(lp17806
sg22
(lp17807
sg216
(lp17808
sg174
(lp17809
sg332
(lp17810
sg121
(lp17811
sg4
(lp17812
sg181
(lp17813
sg8
(lp17814
sg34
(lp17815
sg384
(lp17816
sg235
(lp17817
sg126
(lp17818
sg281
(lp17819
sg10
(lp17820
sg344
(lp17821
sg128
(lp17822
sg130
(lp17823
sg132
(lp17824
sg14
(lp17825
sg16
(lp17826
sg135
(lp17827
sg50
(lp17828
sg138
(lp17829
sg140
(lp17830
sg354
(lp17831
ssS'slit'
p17832
(dp17833
g42
(lp17834
I1609
asg438
(lp17835
ssS'bend'
p17836
(dp17837
g89
(lp17838
I1012
assS'technisch'
p17839
(dp17840
g30
(lp17841
sg34
(lp17842
sg68
(lp17843
sg48
(lp17844
sg221
(lp17845
sg313
(lp17846
I2316
assS'schemat'
p17847
(dp17848
g440
(lp17849
sg76
(lp17850
sg8
(lp17851
sg68
(lp17852
sg245
(lp17853
sg14
(lp17854
sg16
(lp17855
I300
asg20
(lp17856
ssS'weaken'
p17857
(dp17858
g216
(lp17859
sg287
(lp17860
sg106
(lp17861
I425
asg332
(lp17862
sg38
(lp17863
ssS'laird'
p17864
(dp17865
g74
(lp17866
sg460
(lp17867
sg72
(lp17868
sg440
(lp17869
sg91
(lp17870
sg221
(lp17871
sg313
(lp17872
I2128
assS'patter'
p17873
(dp17874
g70
(lp17875
I1192
assS'edwa'
p17876
(dp17877
g22
(lp17878
I257
assS'delay'
p17879
(dp17880
g230
(lp17881
sg350
(lp17882
sg121
(lp17883
sg4
(lp17884
sg76
(lp17885
sg116
(lp17886
sg68
(lp17887
sg83
(lp17888
sg85
(lp17889
sg303
(lp17890
sg102
(lp17891
sg87
(lp17892
sg89
(lp17893
sg128
(lp17894
sg132
(lp17895
sg70
(lp17896
sg20
(lp17897
sg108
(lp17898
sg22
(lp17899
sg140
(lp17900
I392
asg245
(lp17901
ssS'infeas'
p17902
(dp17903
g354
(lp17904
I1291
assS'zone'
p17905
(dp17906
g230
(lp17907
sg48
(lp17908
I1211
assS'sucess'
p17909
(dp17910
g178
(lp17911
I467
assS'stand'
p17912
(dp17913
g287
(lp17914
sg440
(lp17915
sg318
(lp17916
sg26
(lp17917
sg277
(lp17918
sg293
(lp17919
sg72
(lp17920
sg32
(lp17921
sg42
(lp17922
I2294
asg99
(lp17923
sg138
(lp17924
sg149
(lp17925
ssS'fitj'
p17926
(dp17927
g89
(lp17928
I2356
assS'overridden'
p17929
(dp17930
g223
(lp17931
I2073
assS'singular'
p17932
(dp17933
g329
(lp17934
sg34
(lp17935
sg96
(lp17936
I1216
asg48
(lp17937
sg221
(lp17938
sg535
(lp17939
ssS'await'
p17940
(dp17941
g14
(lp17942
I4089
asg4
(lp17943
ssS'epsrc'
p17944
(dp17945
g14
(lp17946
I4638
asg124
(lp17947
sg283
(lp17948
ssS'spse'
p17949
(dp17950
g256
(lp17951
I2180
assS'decisionmak'
p17952
(dp17953
g91
(lp17954
I2868
assS'npoli'
p17955
(dp17956
g145
(lp17957
I186
assS'alloc'
p17958
(dp17959
g178
(lp17960
sg96
(lp17961
I7
asg89
(lp17962
sg281
(lp17963
sg76
(lp17964
ssS'essenti'
p17965
(dp17966
g70
(lp17967
sg287
(lp17968
sg74
(lp17969
sg145
(lp17970
sg293
(lp17971
sg295
(lp17972
sg183
(lp17973
sg59
(lp17974
sg85
(lp17975
sg303
(lp17976
sg42
(lp17977
I648
asg306
(lp17978
sg20
(lp17979
sg18
(lp17980
sg221
(lp17981
sg223
(lp17982
sg63
(lp17983
sg32
(lp17984
sg332
(lp17985
sg4
(lp17986
sg384
(lp17987
sg68
(lp17988
sg126
(lp17989
sg341
(lp17990
sg132
(lp17991
sg138
(lp17992
sg140
(lp17993
ssS'mja'
p17994
(dp17995
g130
(lp17996
I1358
assS'psychophys'
p17997
(dp17998
g216
(lp17999
sg118
(lp18000
sg332
(lp18001
sg6
(lp18002
sg245
(lp18003
sg12
(lp18004
I104
asg63
(lp18005
ssS'logzt'
p18006
(dp18007
g329
(lp18008
I682
assS'counter'
p18009
(dp18010
g42
(lp18011
I3028
asg20
(lp18012
sg22
(lp18013
sg44
(lp18014
sg10
(lp18015
ssS'robot'
p18016
(dp18017
g216
(lp18018
sg318
(lp18019
sg59
(lp18020
sg277
(lp18021
sg8
(lp18022
sg295
(lp18023
sg183
(lp18024
sg460
(lp18025
sg124
(lp18026
sg126
(lp18027
sg42
(lp18028
I34
asg68
(lp18029
sg89
(lp18030
sg132
(lp18031
sg104
(lp18032
sg48
(lp18033
sg99
(lp18034
sg313
(lp18035
sg223
(lp18036
ssS'element'
p18037
(dp18038
g26
(lp18039
sg163
(lp18040
sg287
(lp18041
sg145
(lp18042
sg256
(lp18043
sg293
(lp18044
sg295
(lp18045
sg183
(lp18046
sg59
(lp18047
sg38
(lp18048
sg85
(lp18049
sg42
(lp18050
I1275
asg245
(lp18051
sg46
(lp18052
sg20
(lp18053
sg48
(lp18054
sg221
(lp18055
sg230
(lp18056
sg429
(lp18057
sg102
(lp18058
sg104
(lp18059
sg106
(lp18060
sg108
(lp18061
sg63
(lp18062
sg52
(lp18063
sg216
(lp18064
sg174
(lp18065
sg32
(lp18066
sg318
(lp18067
sg181
(lp18068
sg235
(lp18069
sg34
(lp18070
sg99
(lp18071
sg10
(lp18072
sg40
(lp18073
sg344
(lp18074
sg14
(lp18075
sg135
(lp18076
ssS'issu'
p18077
(dp18078
g70
(lp18079
sg78
(lp18080
sg287
(lp18081
sg145
(lp18082
sg262
(lp18083
sg183
(lp18084
sg59
(lp18085
sg484
(lp18086
sg83
(lp18087
sg306
(lp18088
sg89
(lp18089
sg48
(lp18090
sg99
(lp18091
sg44
(lp18092
sg350
(lp18093
sg102
(lp18094
sg216
(lp18095
sg329
(lp18096
sg32
(lp18097
sg318
(lp18098
sg460
(lp18099
sg68
(lp18100
sg10
(lp18101
sg128
(lp18102
sg130
(lp18103
sg132
(lp18104
sg14
(lp18105
I4703
assS'mjv'
p18106
(dp18107
g130
(lp18108
I2299
assS'mjp'
p18109
(dp18110
g230
(lp18111
I2782
assS'unaccept'
p18112
(dp18113
g14
(lp18114
I4596
asg96
(lp18115
ssS'allow'
p18116
(dp18117
g329
(lp18118
sg277
(lp18119
sg163
(lp18120
sg283
(lp18121
sg303
(lp18122
sg287
(lp18123
sg256
(lp18124
sg76
(lp18125
sg118
(lp18126
sg344
(lp18127
sg59
(lp18128
sg38
(lp18129
sg83
(lp18130
sg114
(lp18131
sg124
(lp18132
sg306
(lp18133
sg87
(lp18134
sg89
(lp18135
sg460
(lp18136
sg245
(lp18137
sg94
(lp18138
sg96
(lp18139
sg48
(lp18140
sg99
(lp18141
sg313
(lp18142
sg350
(lp18143
sg174
(lp18144
sg293
(lp18145
sg429
(lp18146
sg68
(lp18147
sg46
(lp18148
sg102
(lp18149
sg104
(lp18150
sg106
(lp18151
sg108
(lp18152
sg63
(lp18153
sg52
(lp18154
sg22
(lp18155
sg216
(lp18156
sg438
(lp18157
I1829
asg32
(lp18158
sg332
(lp18159
sg121
(lp18160
sg4
(lp18161
sg181
(lp18162
sg8
(lp18163
sg34
(lp18164
sg384
(lp18165
sg235
(lp18166
sg126
(lp18167
sg10
(lp18168
sg40
(lp18169
sg128
(lp18170
sg132
(lp18171
sg14
(lp18172
sg16
(lp18173
sg135
(lp18174
sg50
(lp18175
sg138
(lp18176
ssS'vme'
p18177
(dp18178
g14
(lp18179
sg16
(lp18180
I1847
assS'evolutionari'
p18181
(dp18182
g20
(lp18183
I254
asg18
(lp18184
ssS'vmp'
p18185
(dp18186
g10
(lp18187
I1079
assS'perfect'
p18188
(dp18189
g174
(lp18190
sg74
(lp18191
sg18
(lp18192
sg145
(lp18193
sg26
(lp18194
sg293
(lp18195
sg384
(lp18196
sg484
(lp18197
sg38
(lp18198
sg114
(lp18199
sg89
(lp18200
sg85
(lp18201
sg128
(lp18202
sg245
(lp18203
sg354
(lp18204
I2823
asg99
(lp18205
sg460
(lp18206
sg140
(lp18207
sg149
(lp18208
ssS'decay'
p18209
(dp18210
g216
(lp18211
sg332
(lp18212
sg70
(lp18213
sg4
(lp18214
sg235
(lp18215
sg295
(lp18216
sg183
(lp18217
sg124
(lp18218
sg126
(lp18219
sg83
(lp18220
sg85
(lp18221
sg12
(lp18222
sg163
(lp18223
sg132
(lp18224
I780
asg34
(lp18225
sg48
(lp18226
sg221
(lp18227
sg38
(lp18228
ssS'chosen'
p18229
(dp18230
g124
(lp18231
sg163
(lp18232
sg283
(lp18233
sg30
(lp18234
sg287
(lp18235
sg74
(lp18236
sg145
(lp18237
sg76
(lp18238
sg262
(lp18239
sg295
(lp18240
sg183
(lp18241
sg59
(lp18242
sg484
(lp18243
sg38
(lp18244
sg85
(lp18245
sg306
(lp18246
sg87
(lp18247
sg89
(lp18248
sg91
(lp18249
sg245
(lp18250
sg94
(lp18251
sg18
(lp18252
sg221
(lp18253
sg44
(lp18254
sg116
(lp18255
sg293
(lp18256
sg12
(lp18257
sg318
(lp18258
sg46
(lp18259
sg102
(lp18260
sg178
(lp18261
sg108
(lp18262
sg230
(lp18263
sg329
(lp18264
sg32
(lp18265
sg48
(lp18266
sg121
(lp18267
sg22
(lp18268
sg181
(lp18269
sg8
(lp18270
sg34
(lp18271
sg460
(lp18272
sg235
(lp18273
sg281
(lp18274
sg344
(lp18275
sg128
(lp18276
sg78
(lp18277
sg132
(lp18278
sg14
(lp18279
sg16
(lp18280
sg135
(lp18281
sg140
(lp18282
I1890
assS'gari'
p18283
(dp18284
g78
(lp18285
I19
assS'laminar'
p18286
(dp18287
g256
(lp18288
I113
assS'tenenbaum'
p18289
(dp18290
g74
(lp18291
I8
assS'oognit'
p18292
(dp18293
g114
(lp18294
I2389
assS'decad'
p18295
(dp18296
g132
(lp18297
I89
asg256
(lp18298
sg72
(lp18299
sg4
(lp18300
sg8
(lp18301
ssS'therefor'
p18302
(dp18303
g68
(lp18304
sg70
(lp18305
sg26
(lp18306
sg163
(lp18307
sg72
(lp18308
sg283
(lp18309
sg36
(lp18310
sg181
(lp18311
sg30
(lp18312
sg287
(lp18313
sg145
(lp18314
sg80
(lp18315
sg262
(lp18316
sg295
(lp18317
sg183
(lp18318
sg59
(lp18319
sg484
(lp18320
sg38
(lp18321
sg83
(lp18322
sg303
(lp18323
sg42
(lp18324
I2201
asg306
(lp18325
sg94
(lp18326
sg20
(lp18327
sg48
(lp18328
sg99
(lp18329
sg535
(lp18330
sg149
(lp18331
sg118
(lp18332
sg318
(lp18333
sg46
(lp18334
sg102
(lp18335
sg18
(lp18336
sg106
(lp18337
sg110
(lp18338
sg63
(lp18339
sg52
(lp18340
sg114
(lp18341
sg216
(lp18342
sg438
(lp18343
sg32
(lp18344
sg332
(lp18345
sg121
(lp18346
sg4
(lp18347
sg6
(lp18348
sg235
(lp18349
sg221
(lp18350
sg124
(lp18351
sg126
(lp18352
sg10
(lp18353
sg40
(lp18354
sg344
(lp18355
sg128
(lp18356
sg130
(lp18357
sg132
(lp18358
sg14
(lp18359
sg16
(lp18360
sg350
(lp18361
sg50
(lp18362
sg140
(lp18363
sg354
(lp18364
ssS'underfit'
p18365
(dp18366
g126
(lp18367
I1702
assS'recept'
p18368
(dp18369
g118
(lp18370
sg318
(lp18371
sg70
(lp18372
sg256
(lp18373
sg181
(lp18374
sg295
(lp18375
sg183
(lp18376
sg303
(lp18377
sg102
(lp18378
sg176
(lp18379
sg12
(lp18380
sg106
(lp18381
I2382
asg48
(lp18382
sg63
(lp18383
sg149
(lp18384
ssS'overal'
p18385
(dp18386
g68
(lp18387
sg74
(lp18388
sg145
(lp18389
sg59
(lp18390
sg303
(lp18391
sg94
(lp18392
sg221
(lp18393
sg44
(lp18394
sg118
(lp18395
sg32
(lp18396
sg102
(lp18397
sg104
(lp18398
sg110
(lp18399
sg63
(lp18400
sg52
(lp18401
sg230
(lp18402
sg329
(lp18403
sg440
(lp18404
sg332
(lp18405
sg6
(lp18406
sg384
(lp18407
sg124
(lp18408
sg126
(lp18409
sg50
(lp18410
sg460
(lp18411
sg140
(lp18412
I1057
assS'strateg'
p18413
(dp18414
g132
(lp18415
I2697
asg40
(lp18416
ssS'empirica'
p18417
(dp18418
g4
(lp18419
I1600
assS'lzj'
p18420
(dp18421
g32
(lp18422
I2388
assS'facilit'
p18423
(dp18424
g332
(lp18425
sg4
(lp18426
sg295
(lp18427
sg183
(lp18428
sg281
(lp18429
sg344
(lp18430
sg130
(lp18431
sg102
(lp18432
sg94
(lp18433
sg135
(lp18434
sg354
(lp18435
I1728
assS'hershowitz'
p18436
(dp18437
g74
(lp18438
I3014
assS'anyth'
p18439
(dp18440
g52
(lp18441
sg140
(lp18442
I442
assS'ebel'
p18443
(dp18444
g132
(lp18445
I3525
assS'mnemon'
p18446
(dp18447
g80
(lp18448
I2635
assS'beneath'
p18449
(dp18450
g52
(lp18451
I2206
assS'wkwl'
p18452
(dp18453
g74
(lp18454
I1468
assS'subset'
p18455
(dp18456
g78
(lp18457
sg287
(lp18458
sg74
(lp18459
sg183
(lp18460
sg484
(lp18461
sg42
(lp18462
I288
asg87
(lp18463
sg235
(lp18464
sg94
(lp18465
sg96
(lp18466
sg221
(lp18467
sg318
(lp18468
sg110
(lp18469
sg174
(lp18470
sg32
(lp18471
sg332
(lp18472
sg181
(lp18473
sg8
(lp18474
sg124
(lp18475
sg126
(lp18476
sg281
(lp18477
sg40
(lp18478
sg130
(lp18479
sg14
(lp18480
sg16
(lp18481
ssS'societi'
p18482
(dp18483
g174
(lp18484
sg440
(lp18485
sg318
(lp18486
sg121
(lp18487
sg4
(lp18488
sg80
(lp18489
sg118
(lp18490
sg110
(lp18491
sg460
(lp18492
sg124
(lp18493
sg32
(lp18494
sg283
(lp18495
sg91
(lp18496
sg130
(lp18497
sg96
(lp18498
sg114
(lp18499
sg221
(lp18500
sg313
(lp18501
sg354
(lp18502
I2996
assS'hathaway'
p18503
(dp18504
g245
(lp18505
I2906
asg72
(lp18506
ssS'bump'
p18507
(dp18508
g295
(lp18509
I982
asg183
(lp18510
sg70
(lp18511
ssS'sabisch'
p18512
(dp18513
g283
(lp18514
I22
assS'meta'
p18515
(dp18516
g30
(lp18517
I450
assS'static'
p18518
(dp18519
g121
(lp18520
sg293
(lp18521
sg460
(lp18522
sg8
(lp18523
sg429
(lp18524
sg128
(lp18525
I853
asg70
(lp18526
sg114
(lp18527
ssS'thirteen'
p18528
(dp18529
g78
(lp18530
sg91
(lp18531
I2209
assS'meti'
p18532
(dp18533
g429
(lp18534
I647
assS'tenth'
p18535
(dp18536
g132
(lp18537
I3637
asg110
(lp18538
sg223
(lp18539
ssS'shannon'
p18540
(dp18541
g102
(lp18542
I78
assS'variabl'
p18543
(dp18544
g124
(lp18545
sg26
(lp18546
sg163
(lp18547
sg283
(lp18548
sg460
(lp18549
sg287
(lp18550
sg74
(lp18551
sg145
(lp18552
sg80
(lp18553
sg293
(lp18554
sg295
(lp18555
sg183
(lp18556
sg59
(lp18557
sg484
(lp18558
sg38
(lp18559
sg83
(lp18560
sg63
(lp18561
sg42
(lp18562
I1443
asg306
(lp18563
sg91
(lp18564
sg12
(lp18565
sg94
(lp18566
sg96
(lp18567
sg535
(lp18568
sg68
(lp18569
sg350
(lp18570
sg32
(lp18571
sg429
(lp18572
sg318
(lp18573
sg46
(lp18574
sg102
(lp18575
sg104
(lp18576
sg108
(lp18577
sg20
(lp18578
sg114
(lp18579
sg230
(lp18580
sg118
(lp18581
sg440
(lp18582
sg332
(lp18583
sg121
(lp18584
sg8
(lp18585
sg36
(lp18586
sg384
(lp18587
sg235
(lp18588
sg126
(lp18589
sg341
(lp18590
sg40
(lp18591
sg128
(lp18592
sg130
(lp18593
sg132
(lp18594
sg14
(lp18595
sg16
(lp18596
sg135
(lp18597
sg50
(lp18598
sg138
(lp18599
sg140
(lp18600
ssS'matrix'
p18601
(dp18602
g124
(lp18603
sg163
(lp18604
sg36
(lp18605
sg40
(lp18606
sg74
(lp18607
sg295
(lp18608
sg183
(lp18609
sg38
(lp18610
sg306
(lp18611
sg91
(lp18612
sg12
(lp18613
sg46
(lp18614
sg96
(lp18615
sg18
(lp18616
sg221
(lp18617
sg313
(lp18618
sg223
(lp18619
sg329
(lp18620
sg429
(lp18621
sg68
(lp18622
sg102
(lp18623
sg104
(lp18624
sg108
(lp18625
sg63
(lp18626
sg114
(lp18627
sg230
(lp18628
sg438
(lp18629
I206
asg32
(lp18630
sg8
(lp18631
sg34
(lp18632
sg99
(lp18633
sg460
(lp18634
sg235
(lp18635
sg10
(lp18636
sg535
(lp18637
sg130
(lp18638
sg14
(lp18639
sg16
(lp18640
sg50
(lp18641
sg138
(lp18642
sg140
(lp18643
sg354
(lp18644
ssS'contigu'
p18645
(dp18646
g78
(lp18647
sg106
(lp18648
I2788
asg63
(lp18649
ssS'yoshizawa'
p18650
(dp18651
g36
(lp18652
I3242
assS'iclkcmmux'
p18653
(dp18654
g135
(lp18655
I610
assS'cess'
p18656
(dp18657
g48
(lp18658
I323
assS'matric'
p18659
(dp18660
g230
(lp18661
sg32
(lp18662
sg318
(lp18663
sg26
(lp18664
sg8
(lp18665
sg36
(lp18666
sg10
(lp18667
sg46
(lp18668
sg12
(lp18669
sg104
(lp18670
sg96
(lp18671
sg18
(lp18672
sg99
(lp18673
I878
assS'liporac'
p18674
(dp18675
g440
(lp18676
I69
assS'unipolar'
p18677
(dp18678
g245
(lp18679
I1991
assS'allinson'
p18680
(dp18681
g52
(lp18682
I9
assS'gardner'
p18683
(dp18684
g384
(lp18685
I2339
asg63
(lp18686
ssS'tempt'
p18687
(dp18688
g235
(lp18689
I900
assS'ofeq'
p18690
(dp18691
g130
(lp18692
I1260
assS'greedi'
p18693
(dp18694
g26
(lp18695
sg8
(lp18696
sg295
(lp18697
sg183
(lp18698
sg34
(lp18699
sg89
(lp18700
I405
assS'pomerlea'
p18701
(dp18702
g94
(lp18703
I17
assS'unflip'
p18704
(dp18705
g80
(lp18706
I1300
assS'jeer'
p18707
(dp18708
g110
(lp18709
I2395
assS'could'
p18710
(dp18711
g283
(lp18712
sg70
(lp18713
sg26
(lp18714
sg163
(lp18715
sg181
(lp18716
sg30
(lp18717
sg287
(lp18718
sg76
(lp18719
sg293
(lp18720
sg295
(lp18721
sg183
(lp18722
sg59
(lp18723
sg80
(lp18724
sg83
(lp18725
sg303
(lp18726
sg87
(lp18727
sg89
(lp18728
sg91
(lp18729
sg12
(lp18730
sg94
(lp18731
sg96
(lp18732
sg48
(lp18733
sg99
(lp18734
sg44
(lp18735
sg149
(lp18736
sg329
(lp18737
sg32
(lp18738
sg350
(lp18739
sg429
(lp18740
sg318
(lp18741
sg102
(lp18742
sg18
(lp18743
sg106
(lp18744
sg110
(lp18745
sg63
(lp18746
sg52
(lp18747
sg114
(lp18748
sg116
(lp18749
sg438
(lp18750
I1584
asg440
(lp18751
sg332
(lp18752
sg121
(lp18753
sg4
(lp18754
sg6
(lp18755
sg235
(lp18756
sg34
(lp18757
sg36
(lp18758
sg68
(lp18759
sg10
(lp18760
sg40
(lp18761
sg344
(lp18762
sg223
(lp18763
sg128
(lp18764
sg130
(lp18765
sg132
(lp18766
sg14
(lp18767
sg16
(lp18768
sg135
(lp18769
sg50
(lp18770
sg138
(lp18771
sg140
(lp18772
ssS'scarv'
p18773
(dp18774
g181
(lp18775
I1570
assS'david'
p18776
(dp18777
g484
(lp18778
sg4
(lp18779
sg80
(lp18780
sg124
(lp18781
sg38
(lp18782
sg10
(lp18783
sg283
(lp18784
sg44
(lp18785
sg14
(lp18786
sg94
(lp18787
sg16
(lp18788
sg18
(lp18789
sg313
(lp18790
sg140
(lp18791
I3073
assS'chelazzi'
p18792
(dp18793
g18
(lp18794
I2613
assS'length'
p18795
(dp18796
g283
(lp18797
sg26
(lp18798
sg76
(lp18799
sg344
(lp18800
sg38
(lp18801
sg83
(lp18802
sg85
(lp18803
sg42
(lp18804
I2213
asg306
(lp18805
sg89
(lp18806
sg12
(lp18807
sg94
(lp18808
sg118
(lp18809
sg429
(lp18810
sg102
(lp18811
sg104
(lp18812
sg114
(lp18813
sg116
(lp18814
sg174
(lp18815
sg440
(lp18816
sg121
(lp18817
sg22
(lp18818
sg235
(lp18819
sg34
(lp18820
sg460
(lp18821
sg124
(lp18822
sg10
(lp18823
sg128
(lp18824
sg135
(lp18825
sg138
(lp18826
ssS'enforc'
p18827
(dp18828
g429
(lp18829
sg440
(lp18830
I2326
asg221
(lp18831
sg63
(lp18832
sg26
(lp18833
ssS'outsid'
p18834
(dp18835
g438
(lp18836
I153
asg32
(lp18837
sg121
(lp18838
sg140
(lp18839
sg130
(lp18840
sg70
(lp18841
sg18
(lp18842
sg44
(lp18843
sg350
(lp18844
ssS'scarc'
p18845
(dp18846
g223
(lp18847
I419
assS'llllllllllll'
p18848
(dp18849
g277
(lp18850
I2374
assS'scarf'
p18851
(dp18852
g181
(lp18853
I1943
assS'softwar'
p18854
(dp18855
g121
(lp18856
sg59
(lp18857
sg126
(lp18858
sg10
(lp18859
sg94
(lp18860
sg132
(lp18861
sg14
(lp18862
sg16
(lp18863
sg135
(lp18864
I1657
asg20
(lp18865
sg223
(lp18866
sg114
(lp18867
ssS'manuscript'
p18868
(dp18869
g18
(lp18870
I2602
assS'blown'
p18871
(dp18872
g118
(lp18873
sg124
(lp18874
I2782
assS'scene'
p18875
(dp18876
g174
(lp18877
I2467
asg332
(lp18878
sg277
(lp18879
sg181
(lp18880
sg118
(lp18881
sg59
(lp18882
sg281
(lp18883
sg429
(lp18884
sg318
(lp18885
sg245
(lp18886
sg63
(lp18887
ssS'unobserv'
p18888
(dp18889
g460
(lp18890
sg104
(lp18891
sg74
(lp18892
sg91
(lp18893
I1474
assS'ozeki'
p18894
(dp18895
g384
(lp18896
I638
assS'kiorp'
p18897
(dp18898
g48
(lp18899
I2509
assS'fatigu'
p18900
(dp18901
g216
(lp18902
I537
assS'stark'
p18903
(dp18904
g329
(lp18905
I2536
asg178
(lp18906
ssS'ying'
p18907
(dp18908
g72
(lp18909
I7
assS'los'
p18910
(dp18911
g181
(lp18912
I25
asg293
(lp18913
sg295
(lp18914
sg183
(lp18915
sg384
(lp18916
sg303
(lp18917
ssS'applicalion'
p18918
(dp18919
g85
(lp18920
I4148
assS'amputationldenerv'
p18921
(dp18922
g176
(lp18923
I417
assS'system'
p18924
(dp18925
g329
(lp18926
sg70
(lp18927
sg78
(lp18928
sg277
(lp18929
sg116
(lp18930
sg303
(lp18931
sg293
(lp18932
sg283
(lp18933
sg460
(lp18934
sg36
(lp18935
sg181
(lp18936
sg40
(lp18937
sg26
(lp18938
sg30
(lp18939
sg74
(lp18940
sg176
(lp18941
sg256
(lp18942
sg76
(lp18943
sg262
(lp18944
sg295
(lp18945
sg183
(lp18946
sg59
(lp18947
sg484
(lp18948
sg38
(lp18949
sg83
(lp18950
sg114
(lp18951
sg63
(lp18952
sg42
(lp18953
I36
asg306
(lp18954
sg87
(lp18955
sg80
(lp18956
sg91
(lp18957
sg12
(lp18958
sg94
(lp18959
sg96
(lp18960
sg48
(lp18961
sg99
(lp18962
sg313
(lp18963
sg44
(lp18964
sg350
(lp18965
sg118
(lp18966
sg230
(lp18967
sg174
(lp18968
sg18
(lp18969
sg32
(lp18970
sg178
(lp18971
sg245
(lp18972
sg429
(lp18973
sg68
(lp18974
sg46
(lp18975
sg102
(lp18976
sg104
(lp18977
sg108
(lp18978
sg110
(lp18979
sg20
(lp18980
sg52
(lp18981
sg22
(lp18982
sg216
(lp18983
sg438
(lp18984
sg440
(lp18985
sg332
(lp18986
sg121
(lp18987
sg4
(lp18988
sg6
(lp18989
sg8
(lp18990
sg34
(lp18991
sg221
(lp18992
sg384
(lp18993
sg124
(lp18994
sg126
(lp18995
sg341
(lp18996
sg10
(lp18997
sg535
(lp18998
sg344
(lp18999
sg223
(lp19000
sg128
(lp19001
sg130
(lp19002
sg132
(lp19003
sg14
(lp19004
sg16
(lp19005
sg135
(lp19006
sg50
(lp19007
sg138
(lp19008
sg140
(lp19009
ssS'ttansform'
p19010
(dp19011
g110
(lp19012
I1022
assS'twoparamet'
p19013
(dp19014
g30
(lp19015
I1980
assS'termin'
p19016
(dp19017
g8
(lp19018
sg344
(lp19019
sg183
(lp19020
sg42
(lp19021
I1478
asg89
(lp19022
sg14
(lp19023
sg106
(lp19024
sg108
(lp19025
sg20
(lp19026
sg52
(lp19027
ssS'suguru'
p19028
(dp19029
g42
(lp19030
I12
assS'rfor'
p19031
(dp19032
g295
(lp19033
I2635
asg183
(lp19034
ssS'rivest'
p19035
(dp19036
g30
(lp19037
I2741
assS'accompani'
p19038
(dp19039
g106
(lp19040
I1669
asg91
(lp19041
ssS'multistag'
p19042
(dp19043
g332
(lp19044
I434
assS'haven'
p19045
(dp19046
g295
(lp19047
sg183
(lp19048
sg429
(lp19049
sg8
(lp19050
I19
assS'clarkson'
p19051
(dp19052
g283
(lp19053
I1959
assS'exptl'
p19054
(dp19055
g332
(lp19056
I2641
assS'recuiient'
p19057
(dp19058
g76
(lp19059
I3292
assS'bother'
p19060
(dp19061
g34
(lp19062
I1100
assS'cli'
p19063
(dp19064
g85
(lp19065
I1138
assS'exptj'
p19066
(dp19067
g332
(lp19068
I2659
assS'psycholog'
p19069
(dp19070
g216
(lp19071
sg174
(lp19072
sg74
(lp19073
sg178
(lp19074
sg4
(lp19075
sg118
(lp19076
sg110
(lp19077
sg429
(lp19078
sg130
(lp19079
I114
asg18
(lp19080
sg99
(lp19081
sg223
(lp19082
ssS'steep'
p19083
(dp19084
g283
(lp19085
I541
assS'torrent'
p19086
(dp19087
g10
(lp19088
I554
assS'ailxd'
p19089
(dp19090
g76
(lp19091
I2266
assS'pmxboopm'
p19092
(dp19093
g20
(lp19094
I1872
assS'diffusor'
p19095
(dp19096
g256
(lp19097
I711
assS'bleick'
p19098
(dp19099
g32
(lp19100
I3138
assS'mji'
p19101
(dp19102
g535
(lp19103
I1465
assS'fail'
p19104
(dp19105
g216
(lp19106
sg329
(lp19107
sg74
(lp19108
sg48
(lp19109
sg178
(lp19110
sg163
(lp19111
sg110
(lp19112
sg384
(lp19113
sg341
(lp19114
sg303
(lp19115
sg89
(lp19116
sg108
(lp19117
sg78
(lp19118
sg132
(lp19119
sg145
(lp19120
sg135
(lp19121
sg50
(lp19122
I719
asg70
(lp19123
ssS'fourteenth'
p19124
(dp19125
g78
(lp19126
I3088
assS'sondhi'
p19127
(dp19128
g96
(lp19129
I448
assS'optimis'
p19130
(dp19131
g283
(lp19132
sg135
(lp19133
I2189
asg126
(lp19134
ssS'lwiwmclm'
p19135
(dp19136
g235
(lp19137
I1608
assS'depict'
p19138
(dp19139
g329
(lp19140
sg145
(lp19141
sg181
(lp19142
sg8
(lp19143
sg295
(lp19144
sg183
(lp19145
sg460
(lp19146
sg341
(lp19147
sg132
(lp19148
sg94
(lp19149
sg20
(lp19150
sg50
(lp19151
I213
asg114
(lp19152
ssS'sendai'
p19153
(dp19154
g20
(lp19155
I35
assS'accuraci'
p19156
(dp19157
g70
(lp19158
sg26
(lp19159
sg277
(lp19160
sg145
(lp19161
sg76
(lp19162
sg293
(lp19163
sg295
(lp19164
sg183
(lp19165
sg59
(lp19166
sg87
(lp19167
sg89
(lp19168
sg94
(lp19169
sg20
(lp19170
sg221
(lp19171
sg223
(lp19172
sg96
(lp19173
sg22
(lp19174
sg116
(lp19175
sg178
(lp19176
sg4
(lp19177
sg8
(lp19178
sg460
(lp19179
sg126
(lp19180
sg344
(lp19181
sg44
(lp19182
sg14
(lp19183
sg16
(lp19184
sg140
(lp19185
I40
assS'chiba'
p19186
(dp19187
g96
(lp19188
I1511
assS'discret'
p19189
(dp19190
g70
(lp19191
sg74
(lp19192
sg344
(lp19193
sg59
(lp19194
sg83
(lp19195
sg42
(lp19196
I1624
asg89
(lp19197
sg245
(lp19198
sg20
(lp19199
sg99
(lp19200
sg102
(lp19201
sg108
(lp19202
sg22
(lp19203
sg230
(lp19204
sg118
(lp19205
sg121
(lp19206
sg4
(lp19207
sg8
(lp19208
sg460
(lp19209
sg124
(lp19210
sg126
(lp19211
sg281
(lp19212
sg128
(lp19213
ssS'solla'
p19214
(dp19215
g344
(lp19216
sg36
(lp19217
sg38
(lp19218
sg341
(lp19219
I2906
assS'discrep'
p19220
(dp19221
g145
(lp19222
I2252
asg80
(lp19223
sg76
(lp19224
ssS'aell'
p19225
(dp19226
g440
(lp19227
I495
assS'biiv'
p19228
(dp19229
g306
(lp19230
I1028
assS'vtomatika'
p19231
(dp19232
g306
(lp19233
I2992
assS'calculus'
p19234
(dp19235
g32
(lp19236
I347
assS'xor'
p19237
(dp19238
g245
(lp19239
sg108
(lp19240
I1714
asg110
(lp19241
sg341
(lp19242
ssS'foilow'
p19243
(dp19244
g80
(lp19245
I2052
assS'oracl'
p19246
(dp19247
g344
(lp19248
I874
asg293
(lp19249
ssS'poetal'
p19250
(dp19251
g63
(lp19252
I592
assS'rollout'
p19253
(dp19254
g89
(lp19255
I400
assS'segment'
p19256
(dp19257
g30
(lp19258
sg76
(lp19259
sg293
(lp19260
sg59
(lp19261
sg85
(lp19262
sg303
(lp19263
sg87
(lp19264
sg245
(lp19265
sg96
(lp19266
sg118
(lp19267
sg429
(lp19268
sg318
(lp19269
sg63
(lp19270
sg116
(lp19271
sg174
(lp19272
sg440
(lp19273
sg332
(lp19274
sg178
(lp19275
sg80
(lp19276
sg181
(lp19277
sg8
(lp19278
sg341
(lp19279
sg135
(lp19280
sg138
(lp19281
I829
assS'similado'
p19282
(dp19283
g74
(lp19284
I119
assS'hillsdal'
p19285
(dp19286
g80
(lp19287
sg36
(lp19288
sg68
(lp19289
sg83
(lp19290
sg132
(lp19291
I3881
asg114
(lp19292
ssS'placement'
p19293
(dp19294
g295
(lp19295
I1140
asg183
(lp19296
ssS'ampam'
p19297
(dp19298
g230
(lp19299
I1046
assS'brei'
p19300
(dp19301
g183
(lp19302
I5434
assS'ckl'
p19303
(dp19304
g235
(lp19305
I1787
assS'stronger'
p19306
(dp19307
g216
(lp19308
sg174
(lp19309
sg118
(lp19310
sg235
(lp19311
sg48
(lp19312
sg149
(lp19313
I2089
assS'osinm'
p19314
(dp19315
g36
(lp19316
I1554
assS'ckk'
p19317
(dp19318
g235
(lp19319
I1605
assS'ststalll'
p19320
(dp19321
g174
(lp19322
I1816
assS'irp'
p19323
(dp19324
g287
(lp19325
I1762
assS'univari'
p19326
(dp19327
g313
(lp19328
I671
assS'chollet'
p19329
(dp19330
g96
(lp19331
I2920
assS'fact'
p19332
(dp19333
g68
(lp19334
sg26
(lp19335
sg277
(lp19336
sg163
(lp19337
sg287
(lp19338
sg74
(lp19339
sg176
(lp19340
sg80
(lp19341
sg262
(lp19342
sg344
(lp19343
sg183
(lp19344
sg85
(lp19345
sg303
(lp19346
sg42
(lp19347
I2714
asg306
(lp19348
sg89
(lp19349
sg12
(lp19350
sg94
(lp19351
sg18
(lp19352
sg221
(lp19353
sg223
(lp19354
sg118
(lp19355
sg329
(lp19356
sg318
(lp19357
sg46
(lp19358
sg102
(lp19359
sg104
(lp19360
sg110
(lp19361
sg438
(lp19362
sg32
(lp19363
sg48
(lp19364
sg121
(lp19365
sg4
(lp19366
sg181
(lp19367
sg235
(lp19368
sg34
(lp19369
sg384
(lp19370
sg124
(lp19371
sg72
(lp19372
sg341
(lp19373
sg40
(lp19374
sg44
(lp19375
sg128
(lp19376
sg78
(lp19377
sg14
(lp19378
sg16
(lp19379
sg50
(lp19380
ssS'anlauf'
p19381
(dp19382
g10
(lp19383
I2859
assS'bestfit'
p19384
(dp19385
g30
(lp19386
I1266
assS'bidirect'
p19387
(dp19388
g70
(lp19389
I2639
assS'indispens'
p19390
(dp19391
g145
(lp19392
I1537
assS'tractabl'
p19393
(dp19394
g281
(lp19395
sg91
(lp19396
I360
asg83
(lp19397
sg4
(lp19398
ssS'rough'
p19399
(dp19400
g329
(lp19401
sg74
(lp19402
sg176
(lp19403
sg121
(lp19404
sg181
(lp19405
sg6
(lp19406
sg287
(lp19407
sg484
(lp19408
sg126
(lp19409
sg10
(lp19410
sg42
(lp19411
I2850
asg87
(lp19412
sg85
(lp19413
sg128
(lp19414
sg12
(lp19415
sg178
(lp19416
sg48
(lp19417
sg221
(lp19418
ssS'intracardiac'
p19419
(dp19420
g135
(lp19421
I116
assS'trivial'
p19422
(dp19423
g59
(lp19424
sg70
(lp19425
sg26
(lp19426
sg384
(lp19427
sg281
(lp19428
sg429
(lp19429
sg89
(lp19430
sg140
(lp19431
sg221
(lp19432
sg138
(lp19433
sg52
(lp19434
sg354
(lp19435
I1294
assS'szu'
p19436
(dp19437
g22
(lp19438
I208
assS'redirect'
p19439
(dp19440
g8
(lp19441
I779
assS'nois'
p19442
(dp19443
g70
(lp19444
sg163
(lp19445
sg181
(lp19446
sg30
(lp19447
sg74
(lp19448
sg145
(lp19449
sg262
(lp19450
sg344
(lp19451
sg484
(lp19452
sg85
(lp19453
sg460
(lp19454
sg245
(lp19455
sg48
(lp19456
sg313
(lp19457
sg223
(lp19458
sg116
(lp19459
sg174
(lp19460
sg12
(lp19461
sg429
(lp19462
sg102
(lp19463
sg108
(lp19464
sg63
(lp19465
sg52
(lp19466
sg216
(lp19467
sg438
(lp19468
I1354
asg318
(lp19469
sg4
(lp19470
sg6
(lp19471
sg235
(lp19472
sg34
(lp19473
sg384
(lp19474
sg124
(lp19475
sg126
(lp19476
sg128
(lp19477
sg135
(lp19478
sg138
(lp19479
sg354
(lp19480
ssS'neurobiologist'
p19481
(dp19482
g116
(lp19483
I2457
assS'should'
p19484
(dp19485
g68
(lp19486
sg70
(lp19487
sg78
(lp19488
sg277
(lp19489
sg163
(lp19490
sg26
(lp19491
sg74
(lp19492
sg176
(lp19493
sg80
(lp19494
sg293
(lp19495
sg295
(lp19496
sg183
(lp19497
sg59
(lp19498
sg484
(lp19499
sg83
(lp19500
sg85
(lp19501
sg303
(lp19502
sg42
(lp19503
I211
asg91
(lp19504
sg245
(lp19505
sg94
(lp19506
sg20
(lp19507
sg18
(lp19508
sg99
(lp19509
sg535
(lp19510
sg44
(lp19511
sg118
(lp19512
sg32
(lp19513
sg12
(lp19514
sg429
(lp19515
sg318
(lp19516
sg102
(lp19517
sg106
(lp19518
sg63
(lp19519
sg52
(lp19520
sg114
(lp19521
sg230
(lp19522
sg438
(lp19523
sg440
(lp19524
sg332
(lp19525
sg178
(lp19526
sg4
(lp19527
sg235
(lp19528
sg34
(lp19529
sg36
(lp19530
sg460
(lp19531
sg124
(lp19532
sg126
(lp19533
sg10
(lp19534
sg344
(lp19535
sg223
(lp19536
sg130
(lp19537
sg14
(lp19538
sg16
(lp19539
sg138
(lp19540
sg140
(lp19541
sg354
(lp19542
ssS'jan'
p19543
(dp19544
g245
(lp19545
sg140
(lp19546
I3149
asg114
(lp19547
ssS'gestur'
p19548
(dp19549
g59
(lp19550
I2
asg293
(lp19551
ssS'jam'
p19552
(dp19553
g460
(lp19554
I1000
assS'suppos'
p19555
(dp19556
g230
(lp19557
sg176
(lp19558
sg277
(lp19559
sg262
(lp19560
sg110
(lp19561
sg85
(lp19562
sg40
(lp19563
sg42
(lp19564
I463
asg12
(lp19565
sg63
(lp19566
sg130
(lp19567
sg132
(lp19568
sg102
(lp19569
sg535
(lp19570
sg223
(lp19571
sg354
(lp19572
ssS'intrigu'
p19573
(dp19574
g223
(lp19575
I2428
assS'irl'
p19576
(dp19577
g344
(lp19578
I3628
assS'anneal'
p19579
(dp19580
g74
(lp19581
sg26
(lp19582
sg8
(lp19583
sg295
(lp19584
sg183
(lp19585
sg34
(lp19586
sg130
(lp19587
sg138
(lp19588
I1102
assS'jab'
p19589
(dp19590
g89
(lp19591
I25
assS'hope'
p19592
(dp19593
g30
(lp19594
sg74
(lp19595
sg145
(lp19596
sg183
(lp19597
sg124
(lp19598
sg126
(lp19599
sg89
(lp19600
sg78
(lp19601
sg48
(lp19602
I1853
asg110
(lp19603
sg63
(lp19604
ssS'irm'
p19605
(dp19606
g287
(lp19607
I1865
asg281
(lp19608
ssS'meant'
p19609
(dp19610
g87
(lp19611
sg429
(lp19612
sg135
(lp19613
I217
asg128
(lp19614
sg350
(lp19615
ssS'nonneglig'
p19616
(dp19617
g102
(lp19618
I1797
assS'move'
p19619
(dp19620
g70
(lp19621
sg26
(lp19622
sg277
(lp19623
sg104
(lp19624
sg256
(lp19625
sg80
(lp19626
sg293
(lp19627
sg295
(lp19628
sg183
(lp19629
sg59
(lp19630
sg83
(lp19631
sg303
(lp19632
sg42
(lp19633
I2419
asg245
(lp19634
sg94
(lp19635
sg18
(lp19636
sg99
(lp19637
sg535
(lp19638
sg329
(lp19639
sg46
(lp19640
sg102
(lp19641
sg178
(lp19642
sg52
(lp19643
sg216
(lp19644
sg174
(lp19645
sg32
(lp19646
sg121
(lp19647
sg8
(lp19648
sg460
(lp19649
sg341
(lp19650
sg10
(lp19651
sg78
(lp19652
sg132
(lp19653
ssS'changeux'
p19654
(dp19655
g4
(lp19656
I575
assS'familiar'
p19657
(dp19658
g74
(lp19659
sg80
(lp19660
sg181
(lp19661
sg384
(lp19662
sg354
(lp19663
I427
asg99
(lp19664
sg149
(lp19665
ssS'passeng'
p19666
(dp19667
g83
(lp19668
I99
assS'autom'
p19669
(dp19670
g78
(lp19671
sg42
(lp19672
I3480
asg91
(lp19673
sg135
(lp19674
sg223
(lp19675
sg114
(lp19676
ssS'piecewis'
p19677
(dp19678
g287
(lp19679
sg295
(lp19680
sg183
(lp19681
sg460
(lp19682
sg10
(lp19683
sg96
(lp19684
sg149
(lp19685
I895
assS'epitheli'
p19686
(dp19687
g256
(lp19688
I537
assS'sinus'
p19689
(dp19690
g135
(lp19691
I284
assS'lucki'
p19692
(dp19693
g89
(lp19694
I674
assS'antagonist'
p19695
(dp19696
g106
(lp19697
I1527
asg256
(lp19698
sg350
(lp19699
ssS'arcsm'
p19700
(dp19701
g38
(lp19702
I1889
assS'amid'
p19703
(dp19704
g429
(lp19705
I1991
assS'nisn'
p19706
(dp19707
g183
(lp19708
I5640
assS'wengerek'
p19709
(dp19710
g59
(lp19711
I3095
assS'denmark'
p19712
(dp19713
g38
(lp19714
sg26
(lp19715
sg140
(lp19716
I14
asg235
(lp19717
ssS'frequ'
p19718
(dp19719
g4
(lp19720
I1223
assS'aea'
p19721
(dp19722
g14
(lp19723
sg16
(lp19724
I52
asg89
(lp19725
sg293
(lp19726
ssS'swain'
p19727
(dp19728
g181
(lp19729
I483
assS'ael'
p19730
(dp19731
g440
(lp19732
I496
assS'nist'
p19733
(dp19734
g183
(lp19735
sg178
(lp19736
sg44
(lp19737
I592
assS'ition'
p19738
(dp19739
g108
(lp19740
I1873
asg178
(lp19741
ssS'benari'
p19742
(dp19743
g118
(lp19744
I134
assS'unimport'
p19745
(dp19746
g438
(lp19747
I2266
asg126
(lp19748
sg52
(lp19749
ssS'argmax'
p19750
(dp19751
g460
(lp19752
I1661
assS'frame'
p19753
(dp19754
g440
(lp19755
sg283
(lp19756
sg121
(lp19757
sg76
(lp19758
sg303
(lp19759
sg42
(lp19760
I3216
asg429
(lp19761
sg87
(lp19762
sg96
(lp19763
sg20
(lp19764
sg138
(lp19765
sg44
(lp19766
ssS'toyonaka'
p19767
(dp19768
g42
(lp19769
I16
assS'alessandro'
p19770
(dp19771
g44
(lp19772
I12
assS'temporarili'
p19773
(dp19774
g42
(lp19775
I2863
assS'gralmi'
p19776
(dp19777
g295
(lp19778
I3431
asg183
(lp19779
ssS'mth'
p19780
(dp19781
g124
(lp19782
I1084
assS'jardin'
p19783
(dp19784
g14
(lp19785
sg16
(lp19786
I2610
assS'kalaba'
p19787
(dp19788
g89
(lp19789
I2633
assS'lyon'
p19790
(dp19791
g287
(lp19792
sg22
(lp19793
I242
assS'wire'
p19794
(dp19795
g78
(lp19796
sg106
(lp19797
I1060
assS'spackman'
p19798
(dp19799
g344
(lp19800
I355
assS'nuclear'
p19801
(dp19802
g12
(lp19803
sg14
(lp19804
sg16
(lp19805
I2667
assS'email'
p19806
(dp19807
g230
(lp19808
sg26
(lp19809
sg87
(lp19810
sg130
(lp19811
sg94
(lp19812
sg135
(lp19813
sg140
(lp19814
I238
asg149
(lp19815
ssS'byli'
p19816
(dp19817
g32
(lp19818
I1450
assS'mitchi'
p19819
(dp19820
g429
(lp19821
I2395
assS'olduvai'
p19822
(dp19823
g94
(lp19824
I581
assS'darpa'
p19825
(dp19826
g429
(lp19827
sg87
(lp19828
I406
assS'dislik'
p19829
(dp19830
g26
(lp19831
I190
assS'meilii'
p19832
(dp19833
g460
(lp19834
I10
assS'fullyconnect'
p19835
(dp19836
g114
(lp19837
I1069
assS'ttram'
p19838
(dp19839
g230
(lp19840
I1747
assS'disput'
p19841
(dp19842
g114
(lp19843
I241
assS'wessel'
p19844
(dp19845
g10
(lp19846
I2866
assS'tls'
p19847
(dp19848
g262
(lp19849
I868
assS'netwrok'
p19850
(dp19851
g440
(lp19852
I2723
assS'tlv'
p19853
(dp19854
g230
(lp19855
I3108
assS'ett'
p19856
(dp19857
g34
(lp19858
I2158
assS'tlt'
p19859
(dp19860
g6
(lp19861
I941
assS'stensmo'
p19862
(dp19863
g91
(lp19864
I14
assS'etc'
p19865
(dp19866
g30
(lp19867
sg70
(lp19868
sg178
(lp19869
sg26
(lp19870
sg181
(lp19871
sg34
(lp19872
sg484
(lp19873
sg42
(lp19874
I1643
asg104
(lp19875
sg94
(lp19876
sg20
(lp19877
sg350
(lp19878
sg44
(lp19879
sg354
(lp19880
ssS'labium'
p19881
(dp19882
g116
(lp19883
I1661
assS'etj'
p19884
(dp19885
g116
(lp19886
I1140
assS'volley'
p19887
(dp19888
g106
(lp19889
I2688
assS'tli'
p19890
(dp19891
g42
(lp19892
I1606
assS'eti'
p19893
(dp19894
g429
(lp19895
sg72
(lp19896
I3441
assS'tln'
p19897
(dp19898
g104
(lp19899
I1809
assS'tll'
p19900
(dp19901
g6
(lp19902
I1734
assS'ck'
p19903
(dp19904
g102
(lp19905
I1111
asg344
(lp19906
sg235
(lp19907
ssS'cj'
p19908
(dp19909
g438
(lp19910
I295
asg121
(lp19911
sg4
(lp19912
sg287
(lp19913
sg68
(lp19914
sg87
(lp19915
sg132
(lp19916
sg20
(lp19917
ssS'ci'
p19918
(dp19919
g438
(lp19920
I303
asg332
(lp19921
sg121
(lp19922
sg287
(lp19923
sg110
(lp19924
sg87
(lp19925
sg78
(lp19926
sg18
(lp19927
sg99
(lp19928
sg52
(lp19929
ssS'ch'
p19930
(dp19931
g36
(lp19932
sg178
(lp19933
sg181
(lp19934
I2183
assS'co'
p19935
(dp19936
g287
(lp19937
sg32
(lp19938
sg332
(lp19939
sg256
(lp19940
sg118
(lp19941
sg124
(lp19942
sg114
(lp19943
sg87
(lp19944
sg52
(lp19945
sg130
(lp19946
sg245
(lp19947
sg18
(lp19948
sg99
(lp19949
sg140
(lp19950
I1563
asg149
(lp19951
ssS'cn'
p19952
(dp19953
g281
(lp19954
I1115
assS'cm'
p19955
(dp19956
g245
(lp19957
sg36
(lp19958
sg59
(lp19959
sg48
(lp19960
I1970
asg350
(lp19961
ssS'cl'
p19962
(dp19963
g32
(lp19964
sg121
(lp19965
sg256
(lp19966
sg68
(lp19967
sg303
(lp19968
sg102
(lp19969
sg132
(lp19970
I2388
asg535
(lp19971
sg245
(lp19972
ssS'cc'
p19973
(dp19974
g295
(lp19975
I16
asg183
(lp19976
sg70
(lp19977
ssS'kriegman'
p19978
(dp19979
g42
(lp19980
I3507
assS'ca'
p19981
(dp19982
g68
(lp19983
sg26
(lp19984
sg116
(lp19985
sg30
(lp19986
sg287
(lp19987
sg256
(lp19988
sg262
(lp19989
sg303
(lp19990
sg87
(lp19991
sg91
(lp19992
sg12
(lp19993
sg46
(lp19994
sg20
(lp19995
sg18
(lp19996
sg223
(lp19997
sg350
(lp19998
sg230
(lp19999
sg293
(lp20000
sg94
(lp20001
sg106
(lp20002
I321
asg110
(lp20003
sg114
(lp20004
sg216
(lp20005
sg440
(lp20006
sg318
(lp20007
sg178
(lp20008
sg181
(lp20009
sg6
(lp20010
sg8
(lp20011
sg34
(lp20012
sg124
(lp20013
sg341
(lp20014
sg10
(lp20015
sg40
(lp20016
sg44
(lp20017
sg130
(lp20018
sg132
(lp20019
sg149
(lp20020
sg50
(lp20021
sg140
(lp20022
ssS'cg'
p20023
(dp20024
g178
(lp20025
sg535
(lp20026
I382
assS'cf'
p20027
(dp20028
g42
(lp20029
I3010
asg102
(lp20030
sg36
(lp20031
sg281
(lp20032
sg104
(lp20033
ssS'ce'
p20034
(dp20035
g116
(lp20036
sg89
(lp20037
I1233
assS'cd'
p20038
(dp20039
g216
(lp20040
sg116
(lp20041
sg283
(lp20042
sg70
(lp20043
sg256
(lp20044
sg10
(lp20045
sg87
(lp20046
sg12
(lp20047
sg14
(lp20048
sg16
(lp20049
I1383
asg114
(lp20050
sg221
(lp20051
sg149
(lp20052
ssS'distil'
p20053
(dp20054
g63
(lp20055
I2508
assS'cy'
p20056
(dp20057
g4
(lp20058
I1548
assS'escudi'
p20059
(dp20060
g174
(lp20061
I2835
assS'ucb'
p20062
(dp20063
g440
(lp20064
I2493
assS'sextupl'
p20065
(dp20066
g104
(lp20067
I199
assS'carbon'
p20068
(dp20069
g132
(lp20070
I24
assS'cs'
p20071
(dp20072
g174
(lp20073
sg332
(lp20074
sg145
(lp20075
sg277
(lp20076
sg8
(lp20077
sg34
(lp20078
sg124
(lp20079
sg126
(lp20080
sg341
(lp20081
sg10
(lp20082
sg281
(lp20083
sg344
(lp20084
sg306
(lp20085
sg89
(lp20086
sg429
(lp20087
sg130
(lp20088
sg132
(lp20089
I3504
asg94
(lp20090
sg313
(lp20091
sg223
(lp20092
sg149
(lp20093
ssS'cr'
p20094
(dp20095
g83
(lp20096
sg332
(lp20097
sg313
(lp20098
I1162
asg8
(lp20099
ssS'sanger'
p20100
(dp20101
g26
(lp20102
sg235
(lp20103
I291
assS'cp'
p20104
(dp20105
g91
(lp20106
I2088
asg26
(lp20107
ssS'chromium'
p20108
(dp20109
g14
(lp20110
I2734
assS'cv'
p20111
(dp20112
g295
(lp20113
sg183
(lp20114
sg85
(lp20115
sg140
(lp20116
I1568
assS'uci'
p20117
(dp20118
g484
(lp20119
I1784
assS'ct'
p20120
(dp20121
g118
(lp20122
sg145
(lp20123
sg8
(lp20124
sg295
(lp20125
sg183
(lp20126
sg38
(lp20127
sg429
(lp20128
sg87
(lp20129
sg149
(lp20130
I1117
assS'eklundh'
p20131
(dp20132
g59
(lp20133
I3163
assS'obstacl'
p20134
(dp20135
g42
(lp20136
I2134
assS'insuffici'
p20137
(dp20138
g118
(lp20139
sg281
(lp20140
I1981
assS'rhode'
p20141
(dp20142
g438
(lp20143
I31
assS'trajectori'
p20144
(dp20145
g230
(lp20146
sg32
(lp20147
sg332
(lp20148
sg4
(lp20149
sg8
(lp20150
sg295
(lp20151
sg183
(lp20152
sg384
(lp20153
sg68
(lp20154
sg83
(lp20155
sg42
(lp20156
I3141
asg34
(lp20157
sg460
(lp20158
sg36
(lp20159
sg46
(lp20160
sg99
(lp20161
sg535
(lp20162
ssS'immedi'
p20163
(dp20164
g174
(lp20165
sg178
(lp20166
sg277
(lp20167
sg262
(lp20168
sg295
(lp20169
sg183
(lp20170
sg68
(lp20171
sg341
(lp20172
sg83
(lp20173
sg12
(lp20174
sg145
(lp20175
sg99
(lp20176
sg140
(lp20177
I997
assS'lgi'
p20178
(dp20179
g329
(lp20180
I1561
assS'lgn'
p20181
(dp20182
g12
(lp20183
sg438
(lp20184
I251
assS'intrapariet'
p20185
(dp20186
g303
(lp20187
I252
assS'eduardo'
p20188
(dp20189
g287
(lp20190
sg341
(lp20191
I2732
assS'ibob'
p20192
(dp20193
g223
(lp20194
I676
assS'neighbour'
p20195
(dp20196
g460
(lp20197
I725
asg283
(lp20198
ssS'togeth'
p20199
(dp20200
g277
(lp20201
sg30
(lp20202
sg287
(lp20203
sg74
(lp20204
sg256
(lp20205
sg295
(lp20206
sg183
(lp20207
sg59
(lp20208
sg85
(lp20209
sg42
(lp20210
I30
asg306
(lp20211
sg89
(lp20212
sg91
(lp20213
sg149
(lp20214
sg116
(lp20215
sg429
(lp20216
sg318
(lp20217
sg178
(lp20218
sg63
(lp20219
sg52
(lp20220
sg22
(lp20221
sg216
(lp20222
sg32
(lp20223
sg332
(lp20224
sg121
(lp20225
sg4
(lp20226
sg181
(lp20227
sg68
(lp20228
sg72
(lp20229
sg341
(lp20230
sg10
(lp20231
sg128
(lp20232
sg78
(lp20233
sg14
(lp20234
sg16
(lp20235
sg140
(lp20236
sg354
(lp20237
ssS'atlanta'
p20238
(dp20239
g295
(lp20240
I47
asg183
(lp20241
ssS'videowal'
p20242
(dp20243
g293
(lp20244
I908
assS'laser'
p20245
(dp20246
g283
(lp20247
I72
assS'britain'
p20248
(dp20249
g59
(lp20250
I3317
assS'ljef'
p20251
(dp20252
g104
(lp20253
I1454
assS'weber'
p20254
(dp20255
g313
(lp20256
sg354
(lp20257
I2919
assS'rbi'
p20258
(dp20259
g281
(lp20260
I891
assS'rochest'
p20261
(dp20262
g178
(lp20263
I2704
assS'rbf'
p20264
(dp20265
g96
(lp20266
I533
asg293
(lp20267
ssS'ultra'
p20268
(dp20269
g20
(lp20270
I405
assS'snjql'
p20271
(dp20272
g102
(lp20273
I1144
assS'va'
p20274
(dp20275
g22
(lp20276
sg124
(lp20277
sg429
(lp20278
sg130
(lp20279
sg12
(lp20280
sg20
(lp20281
sg140
(lp20282
I471
assS'vb'
p20283
(dp20284
g20
(lp20285
I1690
asg256
(lp20286
ssS'vc'
p20287
(dp20288
g287
(lp20289
sg20
(lp20290
I1692
asg85
(lp20291
sg262
(lp20292
ssS'vd'
p20293
(dp20294
g230
(lp20295
I1265
assS've'
p20296
(dp20297
g121
(lp20298
sg262
(lp20299
sg34
(lp20300
sg341
(lp20301
sg40
(lp20302
sg42
(lp20303
I729
asg429
(lp20304
sg130
(lp20305
sg245
(lp20306
ssS'cusp'
p20307
(dp20308
g18
(lp20309
I690
assS'vh'
p20310
(dp20311
g32
(lp20312
I2016
asg256
(lp20313
ssS'vi'
p20314
(dp20315
g70
(lp20316
sg74
(lp20317
sg256
(lp20318
sg293
(lp20319
sg303
(lp20320
sg306
(lp20321
sg89
(lp20322
sg12
(lp20323
sg46
(lp20324
sg96
(lp20325
sg230
(lp20326
sg318
(lp20327
sg20
(lp20328
sg216
(lp20329
sg329
(lp20330
sg32
(lp20331
sg332
(lp20332
sg8
(lp20333
sg36
(lp20334
sg124
(lp20335
sg130
(lp20336
sg354
(lp20337
I1749
assS'vj'
p20338
(dp20339
g12
(lp20340
I1017
asg306
(lp20341
sg332
(lp20342
sg40
(lp20343
sg262
(lp20344
ssS'vk'
p20345
(dp20346
g440
(lp20347
sg8
(lp20348
sg38
(lp20349
sg130
(lp20350
I1112
assS'vl'
p20351
(dp20352
g130
(lp20353
I2294
assS'vm'
p20354
(dp20355
g135
(lp20356
I1144
asg38
(lp20357
ssS'vn'
p20358
(dp20359
g287
(lp20360
sg38
(lp20361
sg130
(lp20362
I1163
assS'vo'
p20363
(dp20364
g124
(lp20365
I820
asg72
(lp20366
sg262
(lp20367
ssS'lust'
p20368
(dp20369
g91
(lp20370
I352
assS'momentarili'
p20371
(dp20372
g94
(lp20373
I2749
assS'vr'
p20374
(dp20375
g116
(lp20376
sg256
(lp20377
sg262
(lp20378
sg245
(lp20379
sg20
(lp20380
sg354
(lp20381
I1750
assS'vs'
p20382
(dp20383
g329
(lp20384
sg484
(lp20385
sg70
(lp20386
sg22
(lp20387
sg181
(lp20388
sg344
(lp20389
sg40
(lp20390
sg114
(lp20391
sg313
(lp20392
sg245
(lp20393
sg85
(lp20394
sg132
(lp20395
I2993
asg94
(lp20396
sg96
(lp20397
sg48
(lp20398
sg20
(lp20399
sg223
(lp20400
sg350
(lp20401
ssS'vt'
p20402
(dp20403
g230
(lp20404
sg108
(lp20405
sg36
(lp20406
sg281
(lp20407
sg132
(lp20408
sg20
(lp20409
sg135
(lp20410
I257
asg149
(lp20411
ssS'vv'
p20412
(dp20413
g12
(lp20414
I1108
asg76
(lp20415
ssS'vw'
p20416
(dp20417
g14
(lp20418
I3424
asg306
(lp20419
ssS'vx'
p20420
(dp20421
g245
(lp20422
I1712
asg460
(lp20423
sg40
(lp20424
ssS'vy'
p20425
(dp20426
g174
(lp20427
I935
asg460
(lp20428
sg535
(lp20429
ssS'situ'
p20430
(dp20431
g14
(lp20432
I2894
assS'hubbard'
p20433
(dp20434
g26
(lp20435
sg181
(lp20436
sg94
(lp20437
sg14
(lp20438
I4650
asg63
(lp20439
sg44
(lp20440
sg114
(lp20441
ssS'insight'
p20442
(dp20443
g440
(lp20444
sg235
(lp20445
sg59
(lp20446
sg262
(lp20447
sg126
(lp20448
sg74
(lp20449
sg140
(lp20450
I349
asg102
(lp20451
sg46
(lp20452
sg44
(lp20453
sg350
(lp20454
ssS'mechanismss'
p20455
(dp20456
g80
(lp20457
I497
assS'bishop'
p20458
(dp20459
g344
(lp20460
sg14
(lp20461
sg16
(lp20462
I13
asg124
(lp20463
ssS'fpms'
p20464
(dp20465
g440
(lp20466
I2504
assS'competit'
p20467
(dp20468
g116
(lp20469
sg183
(lp20470
sg332
(lp20471
sg4
(lp20472
sg295
(lp20473
sg221
(lp20474
sg303
(lp20475
sg72
(lp20476
sg176
(lp20477
sg535
(lp20478
sg429
(lp20479
sg87
(lp20480
sg91
(lp20481
sg130
(lp20482
sg104
(lp20483
sg20
(lp20484
sg48
(lp20485
sg50
(lp20486
I3
asg313
(lp20487
sg149
(lp20488
ssS'phi'
p20489
(dp20490
g135
(lp20491
I824
assS'ball'
p20492
(dp20493
g85
(lp20494
I1726
assS'raymond'
p20495
(dp20496
g132
(lp20497
I3572
asg216
(lp20498
ssS'expans'
p20499
(dp20500
g32
(lp20501
sg8
(lp20502
sg295
(lp20503
sg183
(lp20504
sg384
(lp20505
sg262
(lp20506
sg38
(lp20507
sg281
(lp20508
sg40
(lp20509
sg34
(lp20510
sg96
(lp20511
sg163
(lp20512
sg149
(lp20513
I2576
assS'grafton'
p20514
(dp20515
g99
(lp20516
I3053
assS'upon'
p20517
(dp20518
g329
(lp20519
sg118
(lp20520
sg176
(lp20521
sg4
(lp20522
sg76
(lp20523
sg287
(lp20524
sg50
(lp20525
sg384
(lp20526
sg80
(lp20527
sg281
(lp20528
sg85
(lp20529
sg429
(lp20530
sg89
(lp20531
sg104
(lp20532
sg106
(lp20533
I224
asg99
(lp20534
sg44
(lp20535
ssS'phd'
p20536
(dp20537
g329
(lp20538
sg318
(lp20539
sg283
(lp20540
sg262
(lp20541
sg34
(lp20542
sg221
(lp20543
sg460
(lp20544
sg124
(lp20545
sg126
(lp20546
sg83
(lp20547
sg344
(lp20548
sg484
(lp20549
sg89
(lp20550
sg91
(lp20551
sg132
(lp20552
sg135
(lp20553
sg99
(lp20554
sg138
(lp20555
I3509
assS'kushilevitz'
p20556
(dp20557
g145
(lp20558
I3114
assS'dayan'
p20559
(dp20560
g74
(lp20561
sg70
(lp20562
sg293
(lp20563
sg124
(lp20564
sg72
(lp20565
sg89
(lp20566
sg91
(lp20567
sg132
(lp20568
sg138
(lp20569
I3244
assS'expand'
p20570
(dp20571
g230
(lp20572
sg438
(lp20573
I264
asg176
(lp20574
sg22
(lp20575
sg4
(lp20576
sg262
(lp20577
sg36
(lp20578
sg384
(lp20579
sg68
(lp20580
sg38
(lp20581
sg114
(lp20582
sg89
(lp20583
sg104
(lp20584
sg293
(lp20585
sg108
(lp20586
sg163
(lp20587
sg138
(lp20588
sg149
(lp20589
ssS'figllt'
p20590
(dp20591
g110
(lp20592
I1360
assS'ofa'
p20593
(dp20594
g80
(lp20595
sg10
(lp20596
I2890
assS'off'
p20597
(dp20598
g283
(lp20599
sg70
(lp20600
sg116
(lp20601
sg30
(lp20602
sg256
(lp20603
sg76
(lp20604
sg484
(lp20605
sg38
(lp20606
sg83
(lp20607
sg245
(lp20608
sg94
(lp20609
sg20
(lp20610
sg99
(lp20611
sg313
(lp20612
sg149
(lp20613
sg230
(lp20614
sg118
(lp20615
sg63
(lp20616
sg52
(lp20617
sg114
(lp20618
sg216
(lp20619
sg329
(lp20620
sg178
(lp20621
sg22
(lp20622
sg181
(lp20623
sg235
(lp20624
sg34
(lp20625
sg126
(lp20626
sg10
(lp20627
sg132
(lp20628
sg14
(lp20629
sg135
(lp20630
I867
assS'uml'
p20631
(dp20632
g174
(lp20633
I1805
assS'subtree'
p20634
(dp20635
g145
(lp20636
I2133
assS'ofm'
p20637
(dp20638
g344
(lp20639
I1834
assS'hartj'
p20640
(dp20641
g303
(lp20642
I2850
assS'exampl'
p20643
(dp20644
g329
(lp20645
sg70
(lp20646
sg78
(lp20647
sg277
(lp20648
sg163
(lp20649
sg116
(lp20650
sg68
(lp20651
sg281
(lp20652
sg283
(lp20653
sg40
(lp20654
sg26
(lp20655
sg30
(lp20656
sg287
(lp20657
sg74
(lp20658
sg145
(lp20659
sg80
(lp20660
sg118
(lp20661
sg295
(lp20662
sg183
(lp20663
sg59
(lp20664
sg484
(lp20665
sg38
(lp20666
sg83
(lp20667
sg85
(lp20668
sg124
(lp20669
sg42
(lp20670
I1061
asg306
(lp20671
sg87
(lp20672
sg89
(lp20673
sg91
(lp20674
sg12
(lp20675
sg94
(lp20676
sg96
(lp20677
sg48
(lp20678
sg99
(lp20679
sg313
(lp20680
sg44
(lp20681
sg149
(lp20682
sg230
(lp20683
sg174
(lp20684
sg32
(lp20685
sg178
(lp20686
sg429
(lp20687
sg318
(lp20688
sg46
(lp20689
sg102
(lp20690
sg104
(lp20691
sg108
(lp20692
sg110
(lp20693
sg20
(lp20694
sg52
(lp20695
sg216
(lp20696
sg438
(lp20697
sg440
(lp20698
sg18
(lp20699
sg121
(lp20700
sg181
(lp20701
sg6
(lp20702
sg8
(lp20703
sg34
(lp20704
sg221
(lp20705
sg460
(lp20706
sg235
(lp20707
sg126
(lp20708
sg341
(lp20709
sg535
(lp20710
sg344
(lp20711
sg63
(lp20712
sg223
(lp20713
sg36
(lp20714
sg132
(lp20715
sg14
(lp20716
sg16
(lp20717
sg135
(lp20718
sg138
(lp20719
sg140
(lp20720
sg354
(lp20721
ssS'command'
p20722
(dp20723
g178
(lp20724
sg293
(lp20725
sg295
(lp20726
sg183
(lp20727
sg281
(lp20728
sg18
(lp20729
I1533
asg350
(lp20730
ssS'oft'
p20731
(dp20732
g287
(lp20733
sg281
(lp20734
sg44
(lp20735
I1177
assS'figlll'
p20736
(dp20737
g110
(lp20738
I2146
assS'audio'
p20739
(dp20740
g174
(lp20741
I2498
asg22
(lp20742
sg78
(lp20743
ssS'subgroup'
p20744
(dp20745
g96
(lp20746
I2605
asg332
(lp20747
sg32
(lp20748
ssS'lest'
p20749
(dp20750
g63
(lp20751
I713
assS'paus'
p20752
(dp20753
g96
(lp20754
I1908
assS'less'
p20755
(dp20756
g283
(lp20757
sg26
(lp20758
sg277
(lp20759
sg72
(lp20760
sg287
(lp20761
sg74
(lp20762
sg176
(lp20763
sg145
(lp20764
sg293
(lp20765
sg295
(lp20766
sg183
(lp20767
sg484
(lp20768
sg83
(lp20769
sg85
(lp20770
sg303
(lp20771
sg42
(lp20772
I2250
asg87
(lp20773
sg89
(lp20774
sg91
(lp20775
sg245
(lp20776
sg94
(lp20777
sg20
(lp20778
sg535
(lp20779
sg223
(lp20780
sg329
(lp20781
sg106
(lp20782
sg108
(lp20783
sg110
(lp20784
sg63
(lp20785
sg52
(lp20786
sg22
(lp20787
sg216
(lp20788
sg174
(lp20789
sg32
(lp20790
sg178
(lp20791
sg4
(lp20792
sg6
(lp20793
sg235
(lp20794
sg34
(lp20795
sg384
(lp20796
sg68
(lp20797
sg126
(lp20798
sg281
(lp20799
sg10
(lp20800
sg344
(lp20801
sg44
(lp20802
sg128
(lp20803
sg78
(lp20804
sg132
(lp20805
sg14
(lp20806
sg135
(lp20807
sg50
(lp20808
sg460
(lp20809
sg140
(lp20810
ssS'kramer'
p20811
(dp20812
g34
(lp20813
sg384
(lp20814
I844
assS'nxiit'
p20815
(dp20816
g116
(lp20817
I352
assS'electrocardiogr'
p20818
(dp20819
g91
(lp20820
I2119
assS'xes'
p20821
(dp20822
g344
(lp20823
sg287
(lp20824
I2400
assS'lesl'
p20825
(dp20826
g440
(lp20827
I604
assS'paul'
p20828
(dp20829
g318
(lp20830
sg484
(lp20831
sg245
(lp20832
sg283
(lp20833
sg132
(lp20834
sg14
(lp20835
sg16
(lp20836
sg50
(lp20837
I1566
asg350
(lp20838
ssS'multiclass'
p20839
(dp20840
g281
(lp20841
I40
assS'mussaivaldi'
p20842
(dp20843
g99
(lp20844
I626
assS'demograph'
p20845
(dp20846
g121
(lp20847
I1383
assS'web'
p20848
(dp20849
g132
(lp20850
sg223
(lp20851
sg354
(lp20852
I1340
assS'generous'
p20853
(dp20854
g74
(lp20855
sg18
(lp20856
I2576
asg124
(lp20857
ssS'sachusett'
p20858
(dp20859
g89
(lp20860
I2926
assS'igtllv'
p20861
(dp20862
g34
(lp20863
I1221
assS'abarea'
p20864
(dp20865
g6
(lp20866
I645
assS'wen'
p20867
(dp20868
g74
(lp20869
I1270
assS'wel'
p20870
(dp20871
g230
(lp20872
I1178
assS'wer'
p20873
(dp20874
g87
(lp20875
I2012
assS'inaijm'
p20876
(dp20877
g429
(lp20878
I1003
assS'valid'
p20879
(dp20880
g124
(lp20881
sg26
(lp20882
sg277
(lp20883
sg163
(lp20884
sg287
(lp20885
sg262
(lp20886
sg295
(lp20887
sg183
(lp20888
sg484
(lp20889
sg85
(lp20890
sg87
(lp20891
sg94
(lp20892
sg96
(lp20893
sg221
(lp20894
sg44
(lp20895
sg32
(lp20896
sg178
(lp20897
sg230
(lp20898
sg440
(lp20899
sg318
(lp20900
sg121
(lp20901
sg8
(lp20902
sg36
(lp20903
sg235
(lp20904
sg126
(lp20905
sg281
(lp20906
sg40
(lp20907
sg344
(lp20908
sg138
(lp20909
sg140
(lp20910
sg354
(lp20911
I339
assS'cdfs'
p20912
(dp20913
g114
(lp20914
I1425
assS'wet'
p20915
(dp20916
g287
(lp20917
I2113
assS'stsegment'
p20918
(dp20919
g91
(lp20920
I2139
assS'tict'
p20921
(dp20922
g306
(lp20923
I1595
assS'piec'
p20924
(dp20925
g174
(lp20926
sg32
(lp20927
sg256
(lp20928
sg91
(lp20929
I537
asg63
(lp20930
sg114
(lp20931
ssS'smithsonian'
p20932
(dp20933
g116
(lp20934
I2416
assS'cruz'
p20935
(dp20936
g344
(lp20937
sg341
(lp20938
I20
assS'cedar'
p20939
(dp20940
g138
(lp20941
I1997
assS'magnifi'
p20942
(dp20943
g89
(lp20944
I741
assS'belgium'
p20945
(dp20946
g440
(lp20947
I232
assS'recurs'
p20948
(dp20949
g230
(lp20950
sg287
(lp20951
sg440
(lp20952
sg145
(lp20953
sg76
(lp20954
sg295
(lp20955
sg183
(lp20956
sg460
(lp20957
sg68
(lp20958
sg42
(lp20959
I732
asg245
(lp20960
sg89
(lp20961
sg132
(lp20962
sg34
(lp20963
sg96
(lp20964
sg108
(lp20965
ssS'recurr'
p20966
(dp20967
g216
(lp20968
sg438
(lp20969
I245
asg440
(lp20970
sg484
(lp20971
sg121
(lp20972
sg4
(lp20973
sg76
(lp20974
sg163
(lp20975
sg384
(lp20976
sg68
(lp20977
sg178
(lp20978
sg87
(lp20979
sg128
(lp20980
sg46
(lp20981
sg18
(lp20982
sg535
(lp20983
sg149
(lp20984
ssS'sparsiti'
p20985
(dp20986
g68
(lp20987
sg8
(lp20988
I1831
assS'depolaris'
p20989
(dp20990
g256
(lp20991
I350
assS'emma'
p20992
(dp20993
g318
(lp20994
I106
assS'corpor'
p20995
(dp20996
g104
(lp20997
sg163
(lp20998
sg313
(lp20999
I2048
asg78
(lp21000
ssS'mhz'
p21001
(dp21002
g135
(lp21003
I2537
asg126
(lp21004
sg22
(lp21005
sg10
(lp21006
ssS'almaden'
p21007
(dp21008
g40
(lp21009
I2415
assS'unsuccess'
p21010
(dp21011
g68
(lp21012
I80
assS'xlstmg'
p21013
(dp21014
g72
(lp21015
I2792
assS'techreport'
p21016
(dp21017
g26
(lp21018
I3397
assS'wuertz'
p21019
(dp21020
g174
(lp21021
I2407
assS'captur'
p21022
(dp21023
g26
(lp21024
sg438
(lp21025
I90
asg74
(lp21026
sg329
(lp21027
sg63
(lp21028
sg4
(lp21029
sg277
(lp21030
sg245
(lp21031
sg262
(lp21032
sg126
(lp21033
sg114
(lp21034
sg535
(lp21035
sg102
(lp21036
sg306
(lp21037
sg85
(lp21038
sg132
(lp21039
sg303
(lp21040
sg350
(lp21041
sg138
(lp21042
sg354
(lp21043
ssS'lthe'
p21044
(dp21045
g76
(lp21046
I900
assS'intoler'
p21047
(dp21048
g34
(lp21049
I650
assS'ledley'
p21050
(dp21051
g91
(lp21052
I351
assS'guarante'
p21053
(dp21054
g230
(lp21055
sg287
(lp21056
sg440
(lp21057
sg121
(lp21058
sg8
(lp21059
sg344
(lp21060
sg145
(lp21061
sg38
(lp21062
sg74
(lp21063
sg535
(lp21064
sg42
(lp21065
I873
asg306
(lp21066
sg87
(lp21067
sg89
(lp21068
sg91
(lp21069
sg128
(lp21070
sg130
(lp21071
sg104
(lp21072
sg163
(lp21073
sg138
(lp21074
sg46
(lp21075
ssS'arimoto'
p21076
(dp21077
g42
(lp21078
I13
assS'comon'
p21079
(dp21080
g163
(lp21081
I55
assS'oneself'
p21082
(dp21083
g40
(lp21084
I567
assS'milql'
p21085
(dp21086
g440
(lp21087
I425
assS'retinocentr'
p21088
(dp21089
g303
(lp21090
I625
assS'feder'
p21091
(dp21092
g118
(lp21093
I11
assS'avoid'
p21094
(dp21095
g283
(lp21096
sg70
(lp21097
sg26
(lp21098
sg163
(lp21099
sg30
(lp21100
sg74
(lp21101
sg76
(lp21102
sg293
(lp21103
sg295
(lp21104
sg183
(lp21105
sg83
(lp21106
sg42
(lp21107
I2135
asg87
(lp21108
sg91
(lp21109
sg48
(lp21110
sg429
(lp21111
sg114
(lp21112
sg329
(lp21113
sg22
(lp21114
sg6
(lp21115
sg8
(lp21116
sg36
(lp21117
sg124
(lp21118
sg126
(lp21119
sg10
(lp21120
sg128
(lp21121
sg130
(lp21122
ssS'isupportl'
p21123
(dp21124
g89
(lp21125
I1909
assS'summand'
p21126
(dp21127
g8
(lp21128
I1157
assS'duplic'
p21129
(dp21130
g484
(lp21131
I539
assS'swindal'
p21132
(dp21133
g48
(lp21134
I172
assS'tioulpul'
p21135
(dp21136
g76
(lp21137
I1021
assS'imaginari'
p21138
(dp21139
g22
(lp21140
I480
assS'dissert'
p21141
(dp21142
g438
(lp21143
I2400
asg440
(lp21144
sg332
(lp21145
sg99
(lp21146
sg183
(lp21147
ssS'aoicir'
p21148
(dp21149
g135
(lp21150
I605
assS'stage'
p21151
(dp21152
g70
(lp21153
sg256
(lp21154
sg76
(lp21155
sg344
(lp21156
sg59
(lp21157
sg38
(lp21158
sg303
(lp21159
sg42
(lp21160
I749
asg306
(lp21161
sg87
(lp21162
sg46
(lp21163
sg20
(lp21164
sg99
(lp21165
sg149
(lp21166
sg118
(lp21167
sg102
(lp21168
sg63
(lp21169
sg174
(lp21170
sg216
(lp21171
sg438
(lp21172
sg332
(lp21173
sg22
(lp21174
sg34
(lp21175
sg460
(lp21176
sg135
(lp21177
sg138
(lp21178
ssS'iwan'
p21179
(dp21180
g78
(lp21181
I23
assS'viora'
p21182
(dp21183
g4
(lp21184
I1534
assS'rudnicki'
p21185
(dp21186
g332
(lp21187
I2656
assS'krn'
p21188
(dp21189
g68
(lp21190
I2806
assS'peak'
p21191
(dp21192
g216
(lp21193
sg174
(lp21194
sg318
(lp21195
sg145
(lp21196
sg256
(lp21197
sg6
(lp21198
sg329
(lp21199
sg116
(lp21200
sg83
(lp21201
sg10
(lp21202
sg303
(lp21203
sg76
(lp21204
sg102
(lp21205
sg91
(lp21206
sg12
(lp21207
sg221
(lp21208
sg138
(lp21209
I460
asg277
(lp21210
sg350
(lp21211
ssS'metr'
p21212
(dp21213
g36
(lp21214
I3278
assS'orthog'
p21215
(dp21216
g48
(lp21217
I2302
assS'assess'
p21218
(dp21219
g4
(lp21220
sg6
(lp21221
sg344
(lp21222
sg295
(lp21223
sg183
(lp21224
sg124
(lp21225
sg102
(lp21226
sg89
(lp21227
sg91
(lp21228
sg132
(lp21229
sg221
(lp21230
sg138
(lp21231
I302
asg277
(lp21232
ssS'polak'
p21233
(dp21234
g34
(lp21235
I2121
assS'sparcstat'
p21236
(dp21237
g10
(lp21238
I2313
assS'rectifi'
p21239
(dp21240
g30
(lp21241
sg174
(lp21242
I681
assS'mere'
p21243
(dp21244
g287
(lp21245
sg460
(lp21246
sg72
(lp21247
sg102
(lp21248
sg44
(lp21249
sg12
(lp21250
sg223
(lp21251
sg354
(lp21252
I2833
assS'merg'
p21253
(dp21254
g30
(lp21255
sg20
(lp21256
I2440
asg87
(lp21257
ssS'ermedi'
p21258
(dp21259
g174
(lp21260
I2392
assS'specul'
p21261
(dp21262
g128
(lp21263
I2457
assS'depr'
p21264
(dp21265
g91
(lp21266
I2128
assS'jzk'
p21267
(dp21268
g102
(lp21269
I1196
assS'purkinj'
p21270
(dp21271
g99
(lp21272
I2620
asg350
(lp21273
ssS'yrn'
p21274
(dp21275
g72
(lp21276
I2529
assS'assumpt'
p21277
(dp21278
g70
(lp21279
sg78
(lp21280
sg281
(lp21281
sg181
(lp21282
sg26
(lp21283
sg287
(lp21284
sg74
(lp21285
sg176
(lp21286
sg145
(lp21287
sg262
(lp21288
sg295
(lp21289
sg183
(lp21290
sg484
(lp21291
sg85
(lp21292
sg306
(lp21293
sg87
(lp21294
sg89
(lp21295
sg91
(lp21296
sg99
(lp21297
sg313
(lp21298
sg223
(lp21299
sg350
(lp21300
sg116
(lp21301
sg293
(lp21302
sg460
(lp21303
sg106
(lp21304
sg110
(lp21305
sg63
(lp21306
sg52
(lp21307
sg22
(lp21308
sg230
(lp21309
sg438
(lp21310
I373
asg440
(lp21311
sg318
(lp21312
sg4
(lp21313
sg6
(lp21314
sg384
(lp21315
sg68
(lp21316
sg341
(lp21317
sg535
(lp21318
sg130
(lp21319
sg138
(lp21320
sg140
(lp21321
ssS'snyder'
p21322
(dp21323
g18
(lp21324
I2680
asg262
(lp21325
sg350
(lp21326
ssS'dragana'
p21327
(dp21328
g32
(lp21329
I3099
assS'cumput'
p21330
(dp21331
g181
(lp21332
I1923
assS'zeitouni'
p21333
(dp21334
g438
(lp21335
I2495
assS'bd'
p21336
(dp21337
g230
(lp21338
I1651
assS'adaptitj'
p21339
(dp21340
g46
(lp21341
I3682
assS'function'
p21342
(dp21343
g80
(lp21344
sg293
(lp21345
sg344
(lp21346
sg78
(lp21347
sg59
(lp21348
sg484
(lp21349
sg38
(lp21350
sg83
(lp21351
sg85
(lp21352
sg303
(lp21353
sg116
(lp21354
sg118
(lp21355
sg34
(lp21356
sg36
(lp21357
sg460
(lp21358
sg68
(lp21359
sg72
(lp21360
sg281
(lp21361
sg10
(lp21362
sg40
(lp21363
sg283
(lp21364
sg70
(lp21365
sg26
(lp21366
sg277
(lp21367
sg163
(lp21368
sg89
(lp21369
sg91
(lp21370
sg12
(lp21371
sg94
(lp21372
sg96
(lp21373
sg48
(lp21374
sg99
(lp21375
sg313
(lp21376
sg44
(lp21377
sg149
(lp21378
sg429
(lp21379
sg102
(lp21380
sg104
(lp21381
sg106
(lp21382
sg108
(lp21383
sg110
(lp21384
sg63
(lp21385
sg52
(lp21386
sg114
(lp21387
sg128
(lp21388
sg130
(lp21389
sg132
(lp21390
sg14
(lp21391
sg16
(lp21392
sg135
(lp21393
sg50
(lp21394
sg138
(lp21395
sg140
(lp21396
sg354
(lp21397
sg306
(lp21398
sg87
(lp21399
sg245
(lp21400
sg46
(lp21401
sg20
(lp21402
sg18
(lp21403
sg221
(lp21404
sg535
(lp21405
sg223
(lp21406
sg350
(lp21407
sg438
(lp21408
sg440
(lp21409
sg332
(lp21410
sg121
(lp21411
sg4
(lp21412
sg8
(lp21413
sg126
(lp21414
sg341
(lp21415
sg30
(lp21416
sg287
(lp21417
sg74
(lp21418
sg176
(lp21419
sg145
(lp21420
sg256
(lp21421
sg76
(lp21422
sg262
(lp21423
sg295
(lp21424
sg183
(lp21425
sg42
(lp21426
I3345
asg230
(lp21427
sg329
(lp21428
sg32
(lp21429
sg318
(lp21430
sg178
(lp21431
sg22
(lp21432
sg181
(lp21433
sg235
(lp21434
sg384
(lp21435
sg124
(lp21436
ssS'controlierh'
p21437
(dp21438
g230
(lp21439
I2929
assS'tsoi'
p21440
(dp21441
g121
(lp21442
sg128
(lp21443
I2755
assS'dissatisfact'
p21444
(dp21445
g83
(lp21446
I1081
assS'ofir'
p21447
(dp21448
g287
(lp21449
I2743
assS'determi'
p21450
(dp21451
g132
(lp21452
I3326
assS'imagawa'
p21453
(dp21454
g178
(lp21455
I2077
assS'grate'
p21456
(dp21457
g440
(lp21458
sg176
(lp21459
sg70
(lp21460
sg126
(lp21461
sg341
(lp21462
sg130
(lp21463
sg132
(lp21464
I3421
asg18
(lp21465
ssS'count'
p21466
(dp21467
g178
(lp21468
sg277
(lp21469
sg6
(lp21470
I59
asg163
(lp21471
sg341
(lp21472
sg429
(lp21473
sg20
(lp21474
ssS'packard'
p21475
(dp21476
g10
(lp21477
I914
assS'parthasarathi'
p21478
(dp21479
g230
(lp21480
sg128
(lp21481
I261
assS'smooth'
p21482
(dp21483
g68
(lp21484
sg70
(lp21485
sg277
(lp21486
sg30
(lp21487
sg287
(lp21488
sg176
(lp21489
sg295
(lp21490
sg183
(lp21491
sg85
(lp21492
sg87
(lp21493
sg89
(lp21494
sg245
(lp21495
sg46
(lp21496
sg99
(lp21497
sg313
(lp21498
sg149
(lp21499
sg429
(lp21500
sg102
(lp21501
sg108
(lp21502
sg63
(lp21503
sg116
(lp21504
sg174
(lp21505
sg440
(lp21506
sg318
(lp21507
sg121
(lp21508
sg181
(lp21509
sg8
(lp21510
sg36
(lp21511
sg124
(lp21512
sg72
(lp21513
sg281
(lp21514
sg535
(lp21515
sg132
(lp21516
sg138
(lp21517
sg140
(lp21518
I1141
assS'olshen'
p21519
(dp21520
g183
(lp21521
I6619
assS'evidenc'
p21522
(dp21523
g96
(lp21524
sg99
(lp21525
I2045
asg80
(lp21526
ssS'haykin'
p21527
(dp21528
g341
(lp21529
I2893
assS'otherwis'
p21530
(dp21531
g26
(lp21532
sg287
(lp21533
sg74
(lp21534
sg76
(lp21535
sg293
(lp21536
sg344
(lp21537
sg78
(lp21538
sg59
(lp21539
sg38
(lp21540
sg42
(lp21541
I1499
asg89
(lp21542
sg91
(lp21543
sg94
(lp21544
sg44
(lp21545
sg116
(lp21546
sg102
(lp21547
sg108
(lp21548
sg52
(lp21549
sg22
(lp21550
sg230
(lp21551
sg121
(lp21552
sg4
(lp21553
sg8
(lp21554
sg34
(lp21555
sg68
(lp21556
sg341
(lp21557
sg132
(lp21558
ssS'problem'
p21559
(dp21560
g68
(lp21561
sg70
(lp21562
sg78
(lp21563
sg277
(lp21564
sg163
(lp21565
sg72
(lp21566
sg281
(lp21567
sg36
(lp21568
sg40
(lp21569
sg26
(lp21570
sg287
(lp21571
sg74
(lp21572
sg176
(lp21573
sg76
(lp21574
sg118
(lp21575
sg295
(lp21576
sg183
(lp21577
sg59
(lp21578
sg484
(lp21579
sg38
(lp21580
sg83
(lp21581
sg85
(lp21582
sg63
(lp21583
sg42
(lp21584
I344
asg306
(lp21585
sg87
(lp21586
sg89
(lp21587
sg91
(lp21588
sg46
(lp21589
sg96
(lp21590
sg99
(lp21591
sg313
(lp21592
sg44
(lp21593
sg329
(lp21594
sg293
(lp21595
sg178
(lp21596
sg429
(lp21597
sg318
(lp21598
sg102
(lp21599
sg104
(lp21600
sg108
(lp21601
sg110
(lp21602
sg20
(lp21603
sg52
(lp21604
sg230
(lp21605
sg438
(lp21606
sg440
(lp21607
sg332
(lp21608
sg121
(lp21609
sg4
(lp21610
sg181
(lp21611
sg8
(lp21612
sg34
(lp21613
sg221
(lp21614
sg460
(lp21615
sg124
(lp21616
sg126
(lp21617
sg341
(lp21618
sg10
(lp21619
sg535
(lp21620
sg344
(lp21621
sg223
(lp21622
sg128
(lp21623
sg130
(lp21624
sg132
(lp21625
sg14
(lp21626
sg16
(lp21627
sg135
(lp21628
sg50
(lp21629
sg138
(lp21630
sg140
(lp21631
sg354
(lp21632
ssS'bigram'
p21633
(dp21634
g87
(lp21635
I2001
assS'int'
p21636
(dp21637
g174
(lp21638
sg440
(lp21639
sg178
(lp21640
sg22
(lp21641
sg181
(lp21642
sg130
(lp21643
sg63
(lp21644
sg42
(lp21645
I3457
asg94
(lp21646
sg14
(lp21647
sg20
(lp21648
sg313
(lp21649
ssS'inv'
p21650
(dp21651
g20
(lp21652
I1227
asg63
(lp21653
ssS'inr'
p21654
(dp21655
g121
(lp21656
I1967
assS'iverson'
p21657
(dp21658
g70
(lp21659
I2666
assS'novemb'
p21660
(dp21661
g230
(lp21662
sg135
(lp21663
I2509
asg223
(lp21664
ssS'replica'
p21665
(dp21666
g384
(lp21667
I235
assS'inh'
p21668
(dp21669
g70
(lp21670
I1050
assS'westheim'
p21671
(dp21672
g12
(lp21673
I2877
asg32
(lp21674
ssS'ink'
p21675
(dp21676
g138
(lp21677
I253
assS'ine'
p21678
(dp21679
g102
(lp21680
I3086
asg145
(lp21681
ssS'inf'
p21682
(dp21683
g36
(lp21684
I22
assS'ing'
p21685
(dp21686
g116
(lp21687
sg318
(lp21688
sg121
(lp21689
I2504
asg22
(lp21690
ssS'ina'
p21691
(dp21692
g429
(lp21693
sg332
(lp21694
I2710
asg176
(lp21695
ssS'inc'
p21696
(dp21697
g230
(lp21698
sg438
(lp21699
I35
asg26
(lp21700
sg174
(lp21701
sg78
(lp21702
sg163
(lp21703
sg91
(lp21704
sg46
(lp21705
sg96
(lp21706
sg108
(lp21707
ssS'wxt'
p21708
(dp21709
g341
(lp21710
I1342
assS'nonetheless'
p21711
(dp21712
g102
(lp21713
sg34
(lp21714
sg89
(lp21715
I1388
asg277
(lp21716
sg181
(lp21717
ssS'yomdin'
p21718
(dp21719
g32
(lp21720
I3103
assS'lookup'
p21721
(dp21722
g89
(lp21723
I52
asg83
(lp21724
sg10
(lp21725
ssS'varieti'
p21726
(dp21727
g283
(lp21728
sg76
(lp21729
sg295
(lp21730
sg78
(lp21731
sg80
(lp21732
sg85
(lp21733
sg306
(lp21734
sg89
(lp21735
sg46
(lp21736
sg20
(lp21737
sg48
(lp21738
sg99
(lp21739
sg535
(lp21740
sg223
(lp21741
sg350
(lp21742
sg110
(lp21743
sg63
(lp21744
sg318
(lp21745
sg121
(lp21746
sg4
(lp21747
sg281
(lp21748
sg183
(lp21749
sg14
(lp21750
sg16
(lp21751
I813
assS'francisco'
p21752
(dp21753
g30
(lp21754
sg20
(lp21755
sg149
(lp21756
I3095
assS'rex'
p21757
(dp21758
g329
(lp21759
I951
asg460
(lp21760
sg176
(lp21761
ssS'zhoa'
p21762
(dp21763
g87
(lp21764
I414
assS'neurbiolog'
p21765
(dp21766
g70
(lp21767
I2629
assS'vein'
p21768
(dp21769
g87
(lp21770
I948
asg181
(lp21771
ssS'niveau'
p21772
(dp21773
g34
(lp21774
I1009
assS'endgam'
p21775
(dp21776
g132
(lp21777
I283
assS'syllabl'
p21778
(dp21779
g116
(lp21780
sg99
(lp21781
I2792
assS'unnikrishnan'
p21782
(dp21783
g128
(lp21784
I2912
asg163
(lp21785
ssS'competl'
p21786
(dp21787
g48
(lp21788
I251
assS'semilinear'
p21789
(dp21790
g283
(lp21791
I1459
assS'entropi'
p21792
(dp21793
g26
(lp21794
sg329
(lp21795
sg440
(lp21796
sg318
(lp21797
sg178
(lp21798
sg277
(lp21799
sg76
(lp21800
sg163
(lp21801
sg36
(lp21802
sg72
(lp21803
sg341
(lp21804
sg91
(lp21805
sg130
(lp21806
sg102
(lp21807
sg96
(lp21808
sg50
(lp21809
sg354
(lp21810
I3172
assS'profici'
p21811
(dp21812
g94
(lp21813
sg99
(lp21814
I3038
assS'khalil'
p21815
(dp21816
g535
(lp21817
I2190
assS'rev'
p21818
(dp21819
g178
(lp21820
sg262
(lp21821
sg36
(lp21822
sg384
(lp21823
sg38
(lp21824
sg48
(lp21825
I2504
asg350
(lp21826
ssS'lwhere'
p21827
(dp21828
g121
(lp21829
I614
assS'inpu'
p21830
(dp21831
g20
(lp21832
I1554
asg76
(lp21833
ssS'vapnik'
p21834
(dp21835
g287
(lp21836
I436
asg85
(lp21837
sg183
(lp21838
ssS'const'
p21839
(dp21840
g48
(lp21841
I760
asg176
(lp21842
ssS'ventral'
p21843
(dp21844
g303
(lp21845
I274
assS'ondahon'
p21846
(dp21847
g72
(lp21848
I2695
assS'albuquerqu'
p21849
(dp21850
g46
(lp21851
I30
assS'deviat'
p21852
(dp21853
g163
(lp21854
sg74
(lp21855
sg145
(lp21856
sg76
(lp21857
sg78
(lp21858
sg59
(lp21859
sg38
(lp21860
sg85
(lp21861
sg91
(lp21862
sg245
(lp21863
sg94
(lp21864
sg221
(lp21865
sg429
(lp21866
sg46
(lp21867
sg178
(lp21868
sg52
(lp21869
sg22
(lp21870
sg121
(lp21871
sg4
(lp21872
sg6
(lp21873
sg99
(lp21874
sg384
(lp21875
sg124
(lp21876
sg130
(lp21877
sg135
(lp21878
sg354
(lp21879
I2363
assS'vhlibabr'
p21880
(dp21881
g350
(lp21882
I1767
assS'slowest'
p21883
(dp21884
g135
(lp21885
I1703
asg38
(lp21886
ssS'spec'
p21887
(dp21888
g48
(lp21889
I2006
assS'avrim'
p21890
(dp21891
g145
(lp21892
I3023
assS'distors'
p21893
(dp21894
g96
(lp21895
I1462
assS'simmon'
p21896
(dp21897
g277
(lp21898
I3195
assS'distort'
p21899
(dp21900
g22
(lp21901
sg181
(lp21902
sg295
(lp21903
sg183
(lp21904
sg44
(lp21905
sg102
(lp21906
sg135
(lp21907
sg138
(lp21908
I320
asg223
(lp21909
sg114
(lp21910
ssS'shoot'
p21911
(dp21912
g59
(lp21913
I1053
assS'jacob'
p21914
(dp21915
g30
(lp21916
sg329
(lp21917
sg59
(lp21918
sg295
(lp21919
sg183
(lp21920
sg460
(lp21921
sg34
(lp21922
sg87
(lp21923
sg138
(lp21924
I2968
assS'knierim'
p21925
(dp21926
g80
(lp21927
I1966
assS'eztens'
p21928
(dp21929
g484
(lp21930
I2610
assS'onnectionist'
p21931
(dp21932
g89
(lp21933
I2776
assS'diuj'
p21934
(dp21935
g18
(lp21936
I1164
assS'ithabonr'
p21937
(dp21938
g59
(lp21939
I2519
assS'affirm'
p21940
(dp21941
g287
(lp21942
I923
assS'cma'
p21943
(dp21944
g145
(lp21945
I2677
assS'enquir'
p21946
(dp21947
g52
(lp21948
I173
assS'hornik'
p21949
(dp21950
g68
(lp21951
sg354
(lp21952
I1991
assS'rprop'
p21953
(dp21954
g87
(lp21955
I1377
assS'cmu'
p21956
(dp21957
g277
(lp21958
sg126
(lp21959
sg42
(lp21960
I3469
asg306
(lp21961
sg89
(lp21962
sg132
(lp21963
sg94
(lp21964
sg313
(lp21965
sg223
(lp21966
ssS'apertur'
p21967
(dp21968
g245
(lp21969
I1646
assS'widcn'
p21970
(dp21971
g354
(lp21972
I1560
assS'alx'
p21973
(dp21974
g32
(lp21975
I840
assS'speci'
p21976
(dp21977
g116
(lp21978
sg48
(lp21979
I506
assS'phonet'
p21980
(dp21981
g174
(lp21982
sg96
(lp21983
I113
asg87
(lp21984
sg440
(lp21985
sg52
(lp21986
ssS'horn'
p21987
(dp21988
g128
(lp21989
I10
assS'consequ'
p21990
(dp21991
g74
(lp21992
sg85
(lp21993
sg303
(lp21994
sg42
(lp21995
I425
asg306
(lp21996
sg245
(lp21997
sg114
(lp21998
sg221
(lp21999
sg535
(lp22000
sg149
(lp22001
sg102
(lp22002
sg106
(lp22003
sg4
(lp22004
sg216
(lp22005
sg118
(lp22006
sg440
(lp22007
sg48
(lp22008
sg22
(lp22009
sg6
(lp22010
sg8
(lp22011
sg384
(lp22012
sg235
(lp22013
sg281
(lp22014
sg132
(lp22015
sg460
(lp22016
ssS'viir'
p22017
(dp22018
g350
(lp22019
I310
assS'phonem'
p22020
(dp22021
g174
(lp22022
sg440
(lp22023
sg121
(lp22024
sg181
(lp22025
sg74
(lp22026
sg10
(lp22027
sg94
(lp22028
sg96
(lp22029
sg108
(lp22030
I2600
assS'spect'
p22031
(dp22032
g48
(lp22033
I1270
assS'supervis'
p22034
(dp22035
g230
(lp22036
sg329
(lp22037
sg178
(lp22038
sg76
(lp22039
sg295
(lp22040
sg183
(lp22041
sg59
(lp22042
sg72
(lp22043
sg341
(lp22044
sg30
(lp22045
sg91
(lp22046
sg96
(lp22047
sg313
(lp22048
I2162
asg223
(lp22049
ssS'alu'
p22050
(dp22051
g283
(lp22052
I1630
assS'kwta'
p22053
(dp22054
g8
(lp22055
I1128
assS'near'
p22056
(dp22057
g70
(lp22058
sg277
(lp22059
sg30
(lp22060
sg176
(lp22061
sg262
(lp22062
sg344
(lp22063
sg78
(lp22064
sg85
(lp22065
sg42
(lp22066
I658
asg89
(lp22067
sg18
(lp22068
sg44
(lp22069
sg329
(lp22070
sg429
(lp22071
sg63
(lp22072
sg174
(lp22073
sg48
(lp22074
sg6
(lp22075
sg34
(lp22076
sg384
(lp22077
sg124
(lp22078
sg126
(lp22079
sg281
(lp22080
sg14
(lp22081
sg16
(lp22082
sg50
(lp22083
sg354
(lp22084
ssS'tohoku'
p22085
(dp22086
g20
(lp22087
I29
assS'poddar'
p22088
(dp22089
g128
(lp22090
I2908
assS'bipolar'
p22091
(dp22092
g245
(lp22093
sg106
(lp22094
I1056
asg256
(lp22095
ssS'onjlruct'
p22096
(dp22097
g22
(lp22098
I697
assS'churcher'
p22099
(dp22100
g14
(lp22101
I2685
assS'plutowski'
p22102
(dp22103
g313
(lp22104
sg354
(lp22105
I298
assS'topolog'
p22106
(dp22107
g30
(lp22108
sg32
(lp22109
sg460
(lp22110
sg306
(lp22111
sg140
(lp22112
I1091
asg14
(lp22113
sg16
(lp22114
sg52
(lp22115
sg149
(lp22116
ssS'told'
p22117
(dp22118
g30
(lp22119
I2412
asg59
(lp22120
ssS'gimbal'
p22121
(dp22122
g32
(lp22123
I1328
assS'culham'
p22124
(dp22125
g14
(lp22126
sg16
(lp22127
I54
assS'euraj'
p22128
(dp22129
g116
(lp22130
I1821
assS'sld'
p22131
(dp22132
g295
(lp22133
I2133
asg183
(lp22134
ssS'locomot'
p22135
(dp22136
g350
(lp22137
I2916
assS'pursuit'
p22138
(dp22139
g245
(lp22140
sg104
(lp22141
I3079
assS'eural'
p22142
(dp22143
g287
(lp22144
I3634
assS'apasci'
p22145
(dp22146
g96
(lp22147
I128
assS'pleasur'
p22148
(dp22149
g63
(lp22150
sg130
(lp22151
I3055
assS'pouget'
p22152
(dp22153
g303
(lp22154
I12
assS'juang'
p22155
(dp22156
g440
(lp22157
I2701
asg460
(lp22158
sg76
(lp22159
ssS'correlogram'
p22160
(dp22161
g6
(lp22162
I111
assS'ecamax'
p22163
(dp22164
g318
(lp22165
I1988
assS'akt'
p22166
(dp22167
g68
(lp22168
I2737
assS'akk'
p22169
(dp22170
g235
(lp22171
I1522
assS'smoke'
p22172
(dp22173
g42
(lp22174
I2293
assS'akl'
p22175
(dp22176
g235
(lp22177
I1517
assS'ainsworth'
p22178
(dp22179
g174
(lp22180
I163
assS'shub'
p22181
(dp22182
g287
(lp22183
I1064
assS'studi'
p22184
(dp22185
g26
(lp22186
sg277
(lp22187
sg72
(lp22188
sg181
(lp22189
sg287
(lp22190
sg74
(lp22191
sg262
(lp22192
sg344
(lp22193
sg183
(lp22194
sg59
(lp22195
sg484
(lp22196
sg85
(lp22197
sg303
(lp22198
sg306
(lp22199
sg91
(lp22200
sg12
(lp22201
sg96
(lp22202
sg48
(lp22203
sg99
(lp22204
sg535
(lp22205
sg223
(lp22206
sg149
(lp22207
sg118
(lp22208
sg116
(lp22209
sg174
(lp22210
sg32
(lp22211
sg350
(lp22212
sg318
(lp22213
sg102
(lp22214
sg106
(lp22215
sg110
(lp22216
sg63
(lp22217
sg52
(lp22218
sg114
(lp22219
sg216
(lp22220
sg438
(lp22221
I658
asg440
(lp22222
sg18
(lp22223
sg4
(lp22224
sg6
(lp22225
sg8
(lp22226
sg34
(lp22227
sg36
(lp22228
sg384
(lp22229
sg235
(lp22230
sg126
(lp22231
sg10
(lp22232
sg40
(lp22233
sg130
(lp22234
sg132
(lp22235
sg135
(lp22236
sg50
(lp22237
sg138
(lp22238
ssS'akg'
p22239
(dp22240
g235
(lp22241
I1573
assS'cohn'
p22242
(dp22243
g91
(lp22244
sg313
(lp22245
sg354
(lp22246
I306
assS'stimuluscent'
p22247
(dp22248
g303
(lp22249
I333
assS'iear'
p22250
(dp22251
g295
(lp22252
I1218
asg183
(lp22253
ssS'brs'
p22254
(dp22255
g341
(lp22256
I1076
assS'mitchel'
p22257
(dp22258
g132
(lp22259
I3454
asg484
(lp22260
sg277
(lp22261
sg223
(lp22262
ssS'franz'
p22263
(dp22264
g384
(lp22265
I1539
assS'qzj'
p22266
(dp22267
g121
(lp22268
I482
assS'cslu'
p22269
(dp22270
g440
(lp22271
I2079
assS'bre'
p22272
(dp22273
g221
(lp22274
I322
assS'contrarili'
p22275
(dp22276
g42
(lp22277
I865
assS'bra'
p22278
(dp22279
g116
(lp22280
I2579
assS'tefter'
p22281
(dp22282
g22
(lp22283
I2555
assS'plot'
p22284
(dp22285
g283
(lp22286
sg145
(lp22287
sg256
(lp22288
sg293
(lp22289
sg85
(lp22290
sg245
(lp22291
sg46
(lp22292
sg20
(lp22293
sg48
(lp22294
sg313
(lp22295
sg116
(lp22296
sg118
(lp22297
sg102
(lp22298
sg108
(lp22299
sg114
(lp22300
sg230
(lp22301
sg329
(lp22302
sg18
(lp22303
sg22
(lp22304
sg6
(lp22305
sg235
(lp22306
sg36
(lp22307
sg124
(lp22308
sg281
(lp22309
sg14
(lp22310
sg16
(lp22311
sg50
(lp22312
sg140
(lp22313
I2414
assS'sarpeshkar'
p22314
(dp22315
g256
(lp22316
I2121
assS'lexograph'
p22317
(dp22318
g128
(lp22319
I1277
assS'laughfon'
p22320
(dp22321
g384
(lp22322
I1923
assS'computat'
p22323
(dp22324
g72
(lp22325
I3424
assS'godfrey'
p22326
(dp22327
g22
(lp22328
I2375
assS'intracavitari'
p22329
(dp22330
g135
(lp22331
I2496
assS'reced'
p22332
(dp22333
g83
(lp22334
I2145
assS'dunde'
p22335
(dp22336
g14
(lp22337
I2708
assS'nletter'
p22338
(dp22339
g76
(lp22340
I1670
assS'recen'
p22341
(dp22342
g42
(lp22343
I1605
assS'award'
p22344
(dp22345
g40
(lp22346
I2402
assS'recep'
p22347
(dp22348
g295
(lp22349
I1018
asg183
(lp22350
ssS'mather'
p22351
(dp22352
g216
(lp22353
I615
assS'maxiefh'
p22354
(dp22355
g78
(lp22356
I2298
assS'irvin'
p22357
(dp22358
g344
(lp22359
sg91
(lp22360
I2187
assS'springer'
p22361
(dp22362
g230
(lp22363
sg174
(lp22364
sg178
(lp22365
sg256
(lp22366
sg262
(lp22367
sg287
(lp22368
sg295
(lp22369
sg183
(lp22370
sg59
(lp22371
sg124
(lp22372
sg116
(lp22373
sg85
(lp22374
sg138
(lp22375
sg429
(lp22376
sg14
(lp22377
sg16
(lp22378
sg20
(lp22379
sg221
(lp22380
sg106
(lp22381
I2745
asg52
(lp22382
sg354
(lp22383
ssS'word'
p22384
(dp22385
g30
(lp22386
sg287
(lp22387
sg76
(lp22388
sg344
(lp22389
sg183
(lp22390
sg42
(lp22391
I386
asg87
(lp22392
sg94
(lp22393
sg96
(lp22394
sg99
(lp22395
sg440
(lp22396
sg52
(lp22397
sg114
(lp22398
sg230
(lp22399
sg118
(lp22400
sg32
(lp22401
sg318
(lp22402
sg121
(lp22403
sg181
(lp22404
sg341
(lp22405
sg128
(lp22406
sg130
(lp22407
ssS'err'
p22408
(dp22409
g295
(lp22410
I2096
asg183
(lp22411
ssS'restor'
p22412
(dp22413
g245
(lp22414
I2028
assS'edgeworth'
p22415
(dp22416
g163
(lp22417
I608
assS'work'
p22418
(dp22419
g68
(lp22420
sg78
(lp22421
sg277
(lp22422
sg163
(lp22423
sg116
(lp22424
sg281
(lp22425
sg283
(lp22426
sg460
(lp22427
sg181
(lp22428
sg26
(lp22429
sg30
(lp22430
sg287
(lp22431
sg74
(lp22432
sg176
(lp22433
sg145
(lp22434
sg256
(lp22435
sg80
(lp22436
sg118
(lp22437
sg295
(lp22438
sg183
(lp22439
sg59
(lp22440
sg38
(lp22441
sg83
(lp22442
sg85
(lp22443
sg63
(lp22444
sg42
(lp22445
I567
asg306
(lp22446
sg87
(lp22447
sg89
(lp22448
sg91
(lp22449
sg12
(lp22450
sg94
(lp22451
sg96
(lp22452
sg18
(lp22453
sg99
(lp22454
sg313
(lp22455
sg44
(lp22456
sg350
(lp22457
sg230
(lp22458
sg174
(lp22459
sg293
(lp22460
sg32
(lp22461
sg178
(lp22462
sg245
(lp22463
sg429
(lp22464
sg318
(lp22465
sg46
(lp22466
sg104
(lp22467
sg108
(lp22468
sg110
(lp22469
sg20
(lp22470
sg52
(lp22471
sg114
(lp22472
sg216
(lp22473
sg438
(lp22474
sg440
(lp22475
sg332
(lp22476
sg121
(lp22477
sg4
(lp22478
sg6
(lp22479
sg8
(lp22480
sg34
(lp22481
sg384
(lp22482
sg124
(lp22483
sg72
(lp22484
sg341
(lp22485
sg10
(lp22486
sg40
(lp22487
sg344
(lp22488
sg128
(lp22489
sg130
(lp22490
sg132
(lp22491
sg14
(lp22492
sg16
(lp22493
sg135
(lp22494
sg138
(lp22495
sg354
(lp22496
ssS'cant'
p22497
(dp22498
g10
(lp22499
I2837
assS'mammalian'
p22500
(dp22501
g12
(lp22502
sg174
(lp22503
sg176
(lp22504
sg52
(lp22505
sg149
(lp22506
I2836
assS'ere'
p22507
(dp22508
g283
(lp22509
I430
assS'erf'
p22510
(dp22511
g38
(lp22512
sg262
(lp22513
I290
assS'wors'
p22514
(dp22515
g30
(lp22516
sg484
(lp22517
sg121
(lp22518
sg4
(lp22519
sg235
(lp22520
sg183
(lp22521
sg68
(lp22522
sg85
(lp22523
sg110
(lp22524
sg128
(lp22525
sg36
(lp22526
sg132
(lp22527
I3449
asg96
(lp22528
sg99
(lp22529
ssS'era'
p22530
(dp22531
g344
(lp22532
I3185
assS'erb'
p22533
(dp22534
g332
(lp22535
I816
assS'elbow'
p22536
(dp22537
g99
(lp22538
I305
assS'ern'
p22539
(dp22540
g48
(lp22541
I749
assS'reston'
p22542
(dp22543
g96
(lp22544
I2806
assS'tni'
p22545
(dp22546
g176
(lp22547
I1273
assS'oyli'
p22548
(dp22549
g32
(lp22550
I1093
assS'pierr'
p22551
(dp22552
g68
(lp22553
I9
assS'tuckwel'
p22554
(dp22555
g262
(lp22556
I906
assS'jerrum'
p22557
(dp22558
g287
(lp22559
I784
assS'ither'
p22560
(dp22561
g341
(lp22562
I2492
assS'india'
p22563
(dp22564
g329
(lp22565
I3000
assS'rbjk'
p22566
(dp22567
g46
(lp22568
I2510
assS'indic'
p22569
(dp22570
g283
(lp22571
sg70
(lp22572
sg26
(lp22573
sg277
(lp22574
sg85
(lp22575
sg40
(lp22576
sg74
(lp22577
sg176
(lp22578
sg145
(lp22579
sg256
(lp22580
sg76
(lp22581
sg293
(lp22582
sg295
(lp22583
sg183
(lp22584
sg59
(lp22585
sg38
(lp22586
sg114
(lp22587
sg63
(lp22588
sg87
(lp22589
sg89
(lp22590
sg91
(lp22591
sg245
(lp22592
sg94
(lp22593
sg20
(lp22594
sg48
(lp22595
sg99
(lp22596
sg313
(lp22597
sg223
(lp22598
sg149
(lp22599
sg116
(lp22600
sg118
(lp22601
sg429
(lp22602
sg332
(lp22603
sg46
(lp22604
sg104
(lp22605
sg108
(lp22606
sg110
(lp22607
sg96
(lp22608
sg22
(lp22609
sg216
(lp22610
sg329
(lp22611
sg18
(lp22612
sg121
(lp22613
sg181
(lp22614
sg6
(lp22615
sg8
(lp22616
sg221
(lp22617
sg384
(lp22618
sg124
(lp22619
sg535
(lp22620
sg344
(lp22621
sg318
(lp22622
sg128
(lp22623
sg78
(lp22624
sg14
(lp22625
sg350
(lp22626
sg50
(lp22627
sg138
(lp22628
sg303
(lp22629
sg354
(lp22630
I2452
assS'aaaa'
p22631
(dp22632
g110
(lp22633
I1354
assS'schapir'
p22634
(dp22635
g344
(lp22636
sg183
(lp22637
I4063
assS'repon'
p22638
(dp22639
g110
(lp22640
I3206
asg281
(lp22641
ssS'aaai'
p22642
(dp22643
g293
(lp22644
I3067
assS'masquerad'
p22645
(dp22646
g114
(lp22647
I1817
assS'indiv'
p22648
(dp22649
g484
(lp22650
I1804
assS'yang'
p22651
(dp22652
g36
(lp22653
I43
asg72
(lp22654
ssS'gigant'
p22655
(dp22656
g59
(lp22657
I3408
assS'neuroscien'
p22658
(dp22659
g116
(lp22660
I2566
assS'btxydx'
p22661
(dp22662
g295
(lp22663
I2454
asg183
(lp22664
ssS'ordinari'
p22665
(dp22666
g42
(lp22667
I250
asg46
(lp22668
sg70
(lp22669
sg344
(lp22670
ssS'song'
p22671
(dp22672
g116
(lp22673
I34
assS'recoveri'
p22674
(dp22675
g216
(lp22676
I1051
asg118
(lp22677
sg303
(lp22678
ssS'soni'
p22679
(dp22680
g181
(lp22681
I1184
assS'geostatist'
p22682
(dp22683
g124
(lp22684
I1249
assS'verifi'
p22685
(dp22686
g74
(lp22687
sg70
(lp22688
sg295
(lp22689
sg183
(lp22690
sg85
(lp22691
sg40
(lp22692
sg89
(lp22693
sg94
(lp22694
sg20
(lp22695
sg350
(lp22696
sg138
(lp22697
I1576
asg114
(lp22698
ssS'lal'
p22699
(dp22700
g145
(lp22701
I1028
assS'highord'
p22702
(dp22703
g344
(lp22704
I3324
assS'recogn'
p22705
(dp22706
g104
(lp22707
sg30
(lp22708
sg76
(lp22709
sg293
(lp22710
sg295
(lp22711
sg183
(lp22712
sg59
(lp22713
sg85
(lp22714
sg42
(lp22715
I1612
asg89
(lp22716
sg96
(lp22717
sg221
(lp22718
sg223
(lp22719
sg149
(lp22720
sg429
(lp22721
sg178
(lp22722
sg110
(lp22723
sg63
(lp22724
sg116
(lp22725
sg121
(lp22726
sg181
(lp22727
sg124
(lp22728
sg344
(lp22729
sg132
(lp22730
sg138
(lp22731
ssS'lai'
p22732
(dp22733
g384
(lp22734
I538
assS'chines'
p22735
(dp22736
g42
(lp22737
I1637
asg72
(lp22738
ssS'after'
p22739
(dp22740
g329
(lp22741
sg70
(lp22742
sg78
(lp22743
sg277
(lp22744
sg116
(lp22745
sg26
(lp22746
sg287
(lp22747
sg74
(lp22748
sg176
(lp22749
sg145
(lp22750
sg76
(lp22751
sg262
(lp22752
sg295
(lp22753
sg183
(lp22754
sg59
(lp22755
sg80
(lp22756
sg38
(lp22757
sg83
(lp22758
sg63
(lp22759
sg42
(lp22760
I775
asg89
(lp22761
sg91
(lp22762
sg12
(lp22763
sg94
(lp22764
sg96
(lp22765
sg18
(lp22766
sg99
(lp22767
sg313
(lp22768
sg44
(lp22769
sg149
(lp22770
sg230
(lp22771
sg174
(lp22772
sg293
(lp22773
sg68
(lp22774
sg104
(lp22775
sg106
(lp22776
sg108
(lp22777
sg110
(lp22778
sg20
(lp22779
sg22
(lp22780
sg216
(lp22781
sg438
(lp22782
sg318
(lp22783
sg178
(lp22784
sg4
(lp22785
sg181
(lp22786
sg34
(lp22787
sg124
(lp22788
sg72
(lp22789
sg341
(lp22790
sg10
(lp22791
sg535
(lp22792
sg344
(lp22793
sg223
(lp22794
sg130
(lp22795
sg132
(lp22796
sg14
(lp22797
sg350
(lp22798
sg138
(lp22799
sg140
(lp22800
sg354
(lp22801
ssS'lab'
p22802
(dp22803
g287
(lp22804
sg277
(lp22805
sg293
(lp22806
sg99
(lp22807
sg72
(lp22808
sg341
(lp22809
sg36
(lp22810
sg135
(lp22811
I1440
asg221
(lp22812
sg63
(lp22813
sg114
(lp22814
ssS'tsontagghilbert'
p22815
(dp22816
g287
(lp22817
I313
assS'lay'
p22818
(dp22819
g63
(lp22820
I1073
assS'adij'
p22821
(dp22822
g130
(lp22823
I2549
assS'law'
p22824
(dp22825
g32
(lp22826
sg350
(lp22827
sg535
(lp22828
sg85
(lp22829
sg8
(lp22830
I2057
assS'arch'
p22831
(dp22832
g70
(lp22833
sg128
(lp22834
I50
assS'kaltenmei'
p22835
(dp22836
g96
(lp22837
I2810
assS'demonstr'
p22838
(dp22839
g283
(lp22840
sg78
(lp22841
sg163
(lp22842
sg36
(lp22843
sg176
(lp22844
sg145
(lp22845
sg80
(lp22846
sg295
(lp22847
sg183
(lp22848
sg59
(lp22849
sg83
(lp22850
sg303
(lp22851
sg42
(lp22852
I94
asg89
(lp22853
sg12
(lp22854
sg94
(lp22855
sg20
(lp22856
sg114
(lp22857
sg99
(lp22858
sg313
(lp22859
sg44
(lp22860
sg149
(lp22861
sg116
(lp22862
sg118
(lp22863
sg32
(lp22864
sg318
(lp22865
sg106
(lp22866
sg110
(lp22867
sg63
(lp22868
sg52
(lp22869
sg22
(lp22870
sg216
(lp22871
sg174
(lp22872
sg440
(lp22873
sg332
(lp22874
sg4
(lp22875
sg235
(lp22876
sg221
(lp22877
sg341
(lp22878
sg10
(lp22879
sg223
(lp22880
sg130
(lp22881
sg14
(lp22882
sg16
(lp22883
sg135
(lp22884
sg50
(lp22885
sg138
(lp22886
sg354
(lp22887
ssS'las'
p22888
(dp22889
g78
(lp22890
I3138
assS'domin'
p22891
(dp22892
g438
(lp22893
I53
asg262
(lp22894
sg6
(lp22895
sg8
(lp22896
sg34
(lp22897
sg235
(lp22898
sg38
(lp22899
sg10
(lp22900
sg223
(lp22901
sg130
(lp22902
sg12
(lp22903
sg106
(lp22904
sg48
(lp22905
sg99
(lp22906
sg44
(lp22907
sg149
(lp22908
ssS'hebert'
p22909
(dp22910
g94
(lp22911
I3474
assS'fak'
p22912
(dp22913
g59
(lp22914
I20
assS'opaqu'
p22915
(dp22916
g176
(lp22917
I1902
assS'schneider'
p22918
(dp22919
g26
(lp22920
I3273
assS'gtop'
p22921
(dp22922
g36
(lp22923
I2724
assS'vermeer'
p22924
(dp22925
g18
(lp22926
I28
assS'green'
p22927
(dp22928
g80
(lp22929
I2522
assS'hasti'
p22930
(dp22931
g295
(lp22932
sg183
(lp22933
sg221
(lp22934
sg44
(lp22935
I384
assS'worst'
p22936
(dp22937
g116
(lp22938
sg126
(lp22939
sg89
(lp22940
sg85
(lp22941
sg44
(lp22942
I1727
assS'order'
p22943
(dp22944
g124
(lp22945
sg70
(lp22946
sg78
(lp22947
sg277
(lp22948
sg163
(lp22949
sg72
(lp22950
sg68
(lp22951
sg281
(lp22952
sg283
(lp22953
sg85
(lp22954
sg181
(lp22955
sg303
(lp22956
sg26
(lp22957
sg287
(lp22958
sg74
(lp22959
sg176
(lp22960
sg145
(lp22961
sg80
(lp22962
sg262
(lp22963
sg295
(lp22964
sg183
(lp22965
sg484
(lp22966
sg38
(lp22967
sg83
(lp22968
sg114
(lp22969
sg63
(lp22970
sg42
(lp22971
I3150
asg87
(lp22972
sg89
(lp22973
sg91
(lp22974
sg12
(lp22975
sg94
(lp22976
sg96
(lp22977
sg18
(lp22978
sg99
(lp22979
sg535
(lp22980
sg44
(lp22981
sg149
(lp22982
sg118
(lp22983
sg230
(lp22984
sg329
(lp22985
sg293
(lp22986
sg116
(lp22987
sg245
(lp22988
sg429
(lp22989
sg318
(lp22990
sg46
(lp22991
sg102
(lp22992
sg104
(lp22993
sg108
(lp22994
sg110
(lp22995
sg20
(lp22996
sg52
(lp22997
sg22
(lp22998
sg216
(lp22999
sg438
(lp23000
sg32
(lp23001
sg332
(lp23002
sg121
(lp23003
sg4
(lp23004
sg6
(lp23005
sg8
(lp23006
sg34
(lp23007
sg36
(lp23008
sg384
(lp23009
sg235
(lp23010
sg126
(lp23011
sg341
(lp23012
sg40
(lp23013
sg344
(lp23014
sg223
(lp23015
sg128
(lp23016
sg130
(lp23017
sg132
(lp23018
sg14
(lp23019
sg16
(lp23020
sg135
(lp23021
sg50
(lp23022
sg138
(lp23023
sg354
(lp23024
ssS'algorihm'
p23025
(dp23026
g145
(lp23027
I3198
assS'diagnos'
p23028
(dp23029
g484
(lp23030
sg91
(lp23031
I2488
asg277
(lp23032
ssS'consent'
p23033
(dp23034
g114
(lp23035
I185
assS'offici'
p23036
(dp23037
g87
(lp23038
sg138
(lp23039
I2028
assS'typewrit'
p23040
(dp23041
g52
(lp23042
I2607
assS'pascal'
p23043
(dp23044
g287
(lp23045
I7
assS'sive'
p23046
(dp23047
g20
(lp23048
I294
assS'incid'
p23049
(dp23050
g256
(lp23051
I928
assS'japan'
p23052
(dp23053
g440
(lp23054
sg36
(lp23055
sg68
(lp23056
sg42
(lp23057
I18
asg20
(lp23058
sg18
(lp23059
ssS'flexibl'
p23060
(dp23061
g74
(lp23062
sg318
(lp23063
sg34
(lp23064
sg10
(lp23065
sg63
(lp23066
sg429
(lp23067
sg96
(lp23068
sg14
(lp23069
sg16
(lp23070
I1857
asg20
(lp23071
sg52
(lp23072
ssS'mayor'
p23073
(dp23074
g68
(lp23075
sg138
(lp23076
I2353
asg85
(lp23077
ssS'offir'
p23078
(dp23079
g332
(lp23080
I1137
assS'offix'
p23081
(dp23082
g68
(lp23083
I553
assS'denver'
p23084
(dp23085
g102
(lp23086
sg318
(lp23087
sg50
(lp23088
I1642
assS'offic'
p23089
(dp23090
g438
(lp23091
I2368
asg440
(lp23092
sg181
(lp23093
sg6
(lp23094
sg118
(lp23095
sg78
(lp23096
sg293
(lp23097
sg83
(lp23098
sg40
(lp23099
sg63
(lp23100
sg12
(lp23101
sg94
(lp23102
sg106
(lp23103
sg138
(lp23104
sg114
(lp23105
ssS'shear'
p23106
(dp23107
g138
(lp23108
I742
asg76
(lp23109
sg63
(lp23110
ssS'versus'
p23111
(dp23112
g121
(lp23113
sg256
(lp23114
sg262
(lp23115
sg384
(lp23116
sg145
(lp23117
sg10
(lp23118
sg40
(lp23119
sg102
(lp23120
sg89
(lp23121
sg46
(lp23122
sg132
(lp23123
sg14
(lp23124
sg16
(lp23125
I1420
asg460
(lp23126
sg245
(lp23127
ssS'then'
p23128
(dp23129
g80
(lp23130
sg293
(lp23131
sg344
(lp23132
sg78
(lp23133
sg59
(lp23134
sg484
(lp23135
sg83
(lp23136
sg85
(lp23137
sg438
(lp23138
sg118
(lp23139
sg34
(lp23140
sg36
(lp23141
sg460
(lp23142
sg68
(lp23143
sg72
(lp23144
sg281
(lp23145
sg40
(lp23146
sg283
(lp23147
sg26
(lp23148
sg277
(lp23149
sg163
(lp23150
sg89
(lp23151
sg91
(lp23152
sg12
(lp23153
sg94
(lp23154
sg96
(lp23155
sg99
(lp23156
sg313
(lp23157
sg44
(lp23158
sg149
(lp23159
sg429
(lp23160
sg102
(lp23161
sg104
(lp23162
sg106
(lp23163
sg108
(lp23164
sg110
(lp23165
sg63
(lp23166
sg52
(lp23167
sg114
(lp23168
sg128
(lp23169
sg132
(lp23170
sg14
(lp23171
sg16
(lp23172
sg135
(lp23173
sg50
(lp23174
sg138
(lp23175
sg140
(lp23176
sg354
(lp23177
sg306
(lp23178
sg87
(lp23179
sg245
(lp23180
sg46
(lp23181
sg20
(lp23182
sg18
(lp23183
sg221
(lp23184
sg535
(lp23185
sg223
(lp23186
sg350
(lp23187
sg216
(lp23188
sg174
(lp23189
sg440
(lp23190
sg332
(lp23191
sg4
(lp23192
sg6
(lp23193
sg8
(lp23194
sg126
(lp23195
sg341
(lp23196
sg30
(lp23197
sg287
(lp23198
sg74
(lp23199
sg176
(lp23200
sg145
(lp23201
sg256
(lp23202
sg76
(lp23203
sg262
(lp23204
sg295
(lp23205
sg183
(lp23206
sg42
(lp23207
I209
asg230
(lp23208
sg329
(lp23209
sg32
(lp23210
sg318
(lp23211
sg178
(lp23212
sg22
(lp23213
sg181
(lp23214
sg235
(lp23215
sg384
(lp23216
sg124
(lp23217
ssS'them'
p23218
(dp23219
g283
(lp23220
sg70
(lp23221
sg26
(lp23222
sg277
(lp23223
sg163
(lp23224
sg72
(lp23225
sg40
(lp23226
sg30
(lp23227
sg287
(lp23228
sg176
(lp23229
sg80
(lp23230
sg262
(lp23231
sg295
(lp23232
sg183
(lp23233
sg59
(lp23234
sg484
(lp23235
sg83
(lp23236
sg85
(lp23237
sg91
(lp23238
sg94
(lp23239
sg221
(lp23240
sg535
(lp23241
sg223
(lp23242
sg293
(lp23243
sg102
(lp23244
sg104
(lp23245
sg108
(lp23246
sg63
(lp23247
sg52
(lp23248
sg181
(lp23249
sg8
(lp23250
sg34
(lp23251
sg99
(lp23252
sg460
(lp23253
sg235
(lp23254
sg126
(lp23255
sg313
(lp23256
sg44
(lp23257
sg128
(lp23258
sg50
(lp23259
sg138
(lp23260
sg140
(lp23261
sg354
(lp23262
I1386
assS'fragment'
p23263
(dp23264
g130
(lp23265
I2409
assS'dirk'
p23266
(dp23267
g221
(lp23268
I13
assS'safe'
p23269
(dp23270
g277
(lp23271
sg80
(lp23272
sg59
(lp23273
sg306
(lp23274
sg89
(lp23275
sg132
(lp23276
sg135
(lp23277
I226
asg384
(lp23278
ssS'break'
p23279
(dp23280
g174
(lp23281
sg277
(lp23282
sg38
(lp23283
sg12
(lp23284
sg48
(lp23285
sg99
(lp23286
I1838
asg63
(lp23287
ssS'band'
p23288
(dp23289
g116
(lp23290
sg174
(lp23291
sg176
(lp23292
sg22
(lp23293
sg293
(lp23294
sg68
(lp23295
sg281
(lp23296
sg283
(lp23297
sg14
(lp23298
sg16
(lp23299
I2652
asg48
(lp23300
ssS'they'
p23301
(dp23302
g329
(lp23303
sg70
(lp23304
sg78
(lp23305
sg277
(lp23306
sg116
(lp23307
sg68
(lp23308
sg80
(lp23309
sg293
(lp23310
sg460
(lp23311
sg40
(lp23312
sg26
(lp23313
sg30
(lp23314
sg287
(lp23315
sg74
(lp23316
sg176
(lp23317
sg145
(lp23318
sg256
(lp23319
sg76
(lp23320
sg118
(lp23321
sg295
(lp23322
sg183
(lp23323
sg59
(lp23324
sg484
(lp23325
sg38
(lp23326
sg83
(lp23327
sg85
(lp23328
sg303
(lp23329
sg87
(lp23330
sg89
(lp23331
sg91
(lp23332
sg12
(lp23333
sg94
(lp23334
sg20
(lp23335
sg48
(lp23336
sg99
(lp23337
sg313
(lp23338
sg44
(lp23339
sg149
(lp23340
sg230
(lp23341
sg174
(lp23342
sg18
(lp23343
sg32
(lp23344
sg350
(lp23345
sg429
(lp23346
sg318
(lp23347
sg46
(lp23348
sg102
(lp23349
sg104
(lp23350
sg106
(lp23351
sg110
(lp23352
sg63
(lp23353
sg52
(lp23354
sg114
(lp23355
sg216
(lp23356
sg438
(lp23357
I1519
asg440
(lp23358
sg332
(lp23359
sg178
(lp23360
sg181
(lp23361
sg6
(lp23362
sg235
(lp23363
sg34
(lp23364
sg36
(lp23365
sg384
(lp23366
sg124
(lp23367
sg72
(lp23368
sg341
(lp23369
sg10
(lp23370
sg535
(lp23371
sg344
(lp23372
sg223
(lp23373
sg128
(lp23374
sg130
(lp23375
sg132
(lp23376
sg135
(lp23377
sg50
(lp23378
sg138
(lp23379
sg140
(lp23380
sg354
(lp23381
ssS'caffein'
p23382
(dp23383
g283
(lp23384
I697
assS'lifelong'
p23385
(dp23386
g223
(lp23387
I40
assS'bank'
p23388
(dp23389
g116
(lp23390
sg174
(lp23391
sg283
(lp23392
sg26
(lp23393
sg460
(lp23394
sg68
(lp23395
sg10
(lp23396
sg96
(lp23397
I1748
asg114
(lp23398
ssS'bread'
p23399
(dp23400
g20
(lp23401
I1119
assS'tendenc'
p23402
(dp23403
g42
(lp23404
I2631
asg332
(lp23405
sg4
(lp23406
ssS'eccv'
p23407
(dp23408
g59
(lp23409
I3167
assS'l'
p23410
(dp23411
g80
(lp23412
sg293
(lp23413
sg344
(lp23414
sg78
(lp23415
sg59
(lp23416
sg484
(lp23417
sg38
(lp23418
sg83
(lp23419
sg85
(lp23420
sg303
(lp23421
sg438
(lp23422
sg116
(lp23423
sg118
(lp23424
sg34
(lp23425
sg36
(lp23426
sg460
(lp23427
sg68
(lp23428
sg72
(lp23429
sg281
(lp23430
sg10
(lp23431
sg40
(lp23432
sg283
(lp23433
sg70
(lp23434
sg26
(lp23435
sg277
(lp23436
sg163
(lp23437
sg89
(lp23438
sg91
(lp23439
sg12
(lp23440
sg94
(lp23441
sg96
(lp23442
sg48
(lp23443
sg99
(lp23444
sg313
(lp23445
sg44
(lp23446
sg149
(lp23447
sg429
(lp23448
sg102
(lp23449
sg104
(lp23450
sg106
(lp23451
sg108
(lp23452
sg110
(lp23453
sg63
(lp23454
sg52
(lp23455
sg114
(lp23456
sg128
(lp23457
sg130
(lp23458
sg132
(lp23459
sg14
(lp23460
sg16
(lp23461
sg135
(lp23462
sg50
(lp23463
sg138
(lp23464
sg140
(lp23465
sg354
(lp23466
sg306
(lp23467
sg87
(lp23468
sg245
(lp23469
sg46
(lp23470
sg20
(lp23471
sg18
(lp23472
sg221
(lp23473
sg535
(lp23474
sg223
(lp23475
sg350
(lp23476
sg174
(lp23477
sg440
(lp23478
sg332
(lp23479
sg121
(lp23480
sg4
(lp23481
sg6
(lp23482
sg8
(lp23483
sg126
(lp23484
sg341
(lp23485
sg30
(lp23486
sg287
(lp23487
sg74
(lp23488
sg176
(lp23489
sg145
(lp23490
sg256
(lp23491
sg76
(lp23492
sg262
(lp23493
sg295
(lp23494
sg183
(lp23495
sg42
(lp23496
I896
asg230
(lp23497
sg329
(lp23498
sg32
(lp23499
sg318
(lp23500
sg178
(lp23501
sg22
(lp23502
sg181
(lp23503
sg235
(lp23504
sg384
(lp23505
sg124
(lp23506
ssS'logt'
p23507
(dp23508
g36
(lp23509
I1650
assS'logp'
p23510
(dp23511
g329
(lp23512
sg74
(lp23513
sg72
(lp23514
sg91
(lp23515
I1055
asg36
(lp23516
ssS'epsiod'
p23517
(dp23518
g30
(lp23519
I1516
assS'logz'
p23520
(dp23521
g50
(lp23522
I1268
assS'loge'
p23523
(dp23524
g124
(lp23525
I2477
asg85
(lp23526
ssS'logn'
p23527
(dp23528
g221
(lp23529
I1032
assS'logl'
p23530
(dp23531
g256
(lp23532
I972
assS'networl'
p23533
(dp23534
g121
(lp23535
I1203
assS'network'
p23536
(dp23537
g293
(lp23538
sg344
(lp23539
sg78
(lp23540
sg59
(lp23541
sg484
(lp23542
sg38
(lp23543
sg83
(lp23544
sg85
(lp23545
sg303
(lp23546
sg438
(lp23547
sg116
(lp23548
sg118
(lp23549
sg34
(lp23550
sg36
(lp23551
sg460
(lp23552
sg68
(lp23553
sg72
(lp23554
sg281
(lp23555
sg10
(lp23556
sg40
(lp23557
sg283
(lp23558
sg70
(lp23559
sg26
(lp23560
sg277
(lp23561
sg163
(lp23562
sg89
(lp23563
sg91
(lp23564
sg12
(lp23565
sg94
(lp23566
sg96
(lp23567
sg99
(lp23568
sg313
(lp23569
sg44
(lp23570
sg149
(lp23571
sg429
(lp23572
sg102
(lp23573
sg104
(lp23574
sg106
(lp23575
sg108
(lp23576
sg110
(lp23577
sg63
(lp23578
sg52
(lp23579
sg114
(lp23580
sg128
(lp23581
sg130
(lp23582
sg132
(lp23583
sg14
(lp23584
sg16
(lp23585
sg135
(lp23586
sg50
(lp23587
sg138
(lp23588
sg140
(lp23589
sg354
(lp23590
sg306
(lp23591
sg87
(lp23592
sg245
(lp23593
sg46
(lp23594
sg20
(lp23595
sg18
(lp23596
sg221
(lp23597
sg535
(lp23598
sg223
(lp23599
sg350
(lp23600
sg216
(lp23601
sg174
(lp23602
sg440
(lp23603
sg332
(lp23604
sg121
(lp23605
sg4
(lp23606
sg8
(lp23607
sg126
(lp23608
sg341
(lp23609
sg30
(lp23610
sg287
(lp23611
sg74
(lp23612
sg176
(lp23613
sg256
(lp23614
sg76
(lp23615
sg262
(lp23616
sg295
(lp23617
sg183
(lp23618
sg42
(lp23619
I73
asg230
(lp23620
sg329
(lp23621
sg32
(lp23622
sg318
(lp23623
sg178
(lp23624
sg22
(lp23625
sg181
(lp23626
sg235
(lp23627
sg384
(lp23628
sg124
(lp23629
ssS'morpholog'
p23630
(dp23631
g135
(lp23632
I2
assS'fellowship'
p23633
(dp23634
g287
(lp23635
sg256
(lp23636
sg277
(lp23637
sg36
(lp23638
sg40
(lp23639
sg89
(lp23640
I2337
asg350
(lp23641
ssS'ltv'
p23642
(dp23643
g110
(lp23644
I537
assS'grandmast'
p23645
(dp23646
g132
(lp23647
I244
assS'forth'
p23648
(dp23649
g20
(lp23650
I261
asg145
(lp23651
ssS'hogan'
p23652
(dp23653
g99
(lp23654
I247
assS'preceed'
p23655
(dp23656
g74
(lp23657
I1878
assS'barrier'
p23658
(dp23659
g256
(lp23660
sg8
(lp23661
I1923
assS'terzopoulo'
p23662
(dp23663
g429
(lp23664
sg8
(lp23665
I1973
assS'standard'
p23666
(dp23667
g78
(lp23668
sg277
(lp23669
sg163
(lp23670
sg36
(lp23671
sg287
(lp23672
sg74
(lp23673
sg145
(lp23674
sg76
(lp23675
sg295
(lp23676
sg183
(lp23677
sg484
(lp23678
sg63
(lp23679
sg306
(lp23680
sg87
(lp23681
sg91
(lp23682
sg46
(lp23683
sg96
(lp23684
sg221
(lp23685
sg313
(lp23686
sg223
(lp23687
sg32
(lp23688
sg94
(lp23689
sg178
(lp23690
sg106
(lp23691
I1069
asg20
(lp23692
sg22
(lp23693
sg329
(lp23694
sg440
(lp23695
sg121
(lp23696
sg4
(lp23697
sg6
(lp23698
sg99
(lp23699
sg460
(lp23700
sg124
(lp23701
sg10
(lp23702
sg535
(lp23703
sg44
(lp23704
sg130
(lp23705
sg14
(lp23706
sg16
(lp23707
sg135
(lp23708
sg138
(lp23709
sg354
(lp23710
ssS'nth'
p23711
(dp23712
g102
(lp23713
I564
asg36
(lp23714
sg74
(lp23715
sg83
(lp23716
ssS'techno'
p23717
(dp23718
g52
(lp23719
I2636
assS'nmda'
p23720
(dp23721
g106
(lp23722
I1212
asg70
(lp23723
ssS'admitt'
p23724
(dp23725
g42
(lp23726
I2796
asg99
(lp23727
ssS'bmrd'
p23728
(dp23729
g132
(lp23730
I1373
assS'fring'
p23731
(dp23732
g22
(lp23733
I1709
assS'angl'
p23734
(dp23735
g30
(lp23736
sg116
(lp23737
sg32
(lp23738
sg283
(lp23739
sg80
(lp23740
sg181
(lp23741
sg293
(lp23742
sg36
(lp23743
sg59
(lp23744
sg163
(lp23745
sg114
(lp23746
sg42
(lp23747
I2233
asg429
(lp23748
sg12
(lp23749
sg14
(lp23750
sg16
(lp23751
sg48
(lp23752
sg44
(lp23753
sg350
(lp23754
ssS'shrubberi'
p23755
(dp23756
g181
(lp23757
I83
assS'regress'
p23758
(dp23759
g30
(lp23760
sg484
(lp23761
sg68
(lp23762
sg295
(lp23763
sg183
(lp23764
sg124
(lp23765
sg126
(lp23766
sg281
(lp23767
sg303
(lp23768
sg34
(lp23769
sg223
(lp23770
sg89
(lp23771
sg91
(lp23772
sg38
(lp23773
sg96
(lp23774
sg221
(lp23775
sg313
(lp23776
sg140
(lp23777
sg354
(lp23778
I1219
assS'mvj'
p23779
(dp23780
g130
(lp23781
I2740
assS'subtl'
p23782
(dp23783
g318
(lp23784
I2628
assS'dimens'
p23785
(dp23786
g281
(lp23787
sg30
(lp23788
sg287
(lp23789
sg74
(lp23790
sg262
(lp23791
sg295
(lp23792
sg183
(lp23793
sg85
(lp23794
sg303
(lp23795
sg42
(lp23796
I530
asg306
(lp23797
sg91
(lp23798
sg46
(lp23799
sg96
(lp23800
sg44
(lp23801
sg350
(lp23802
sg108
(lp23803
sg110
(lp23804
sg230
(lp23805
sg118
(lp23806
sg32
(lp23807
sg121
(lp23808
sg4
(lp23809
sg181
(lp23810
sg8
(lp23811
sg34
(lp23812
sg36
(lp23813
sg460
(lp23814
sg124
(lp23815
sg72
(lp23816
sg341
(lp23817
sg130
(lp23818
sg138
(lp23819
ssS'render'
p23820
(dp23821
g293
(lp23822
sg256
(lp23823
sg130
(lp23824
I1890
assS'independ'
p23825
(dp23826
g68
(lp23827
sg70
(lp23828
sg78
(lp23829
sg163
(lp23830
sg72
(lp23831
sg80
(lp23832
sg26
(lp23833
sg30
(lp23834
sg74
(lp23835
sg176
(lp23836
sg256
(lp23837
sg76
(lp23838
sg262
(lp23839
sg295
(lp23840
sg183
(lp23841
sg484
(lp23842
sg38
(lp23843
sg83
(lp23844
sg85
(lp23845
sg303
(lp23846
sg87
(lp23847
sg89
(lp23848
sg91
(lp23849
sg12
(lp23850
sg46
(lp23851
sg96
(lp23852
sg48
(lp23853
sg44
(lp23854
sg118
(lp23855
sg460
(lp23856
sg102
(lp23857
sg106
(lp23858
sg108
(lp23859
sg110
(lp23860
sg52
(lp23861
sg22
(lp23862
sg216
(lp23863
sg438
(lp23864
I368
asg178
(lp23865
sg181
(lp23866
sg6
(lp23867
sg235
(lp23868
sg36
(lp23869
sg384
(lp23870
sg124
(lp23871
sg126
(lp23872
sg281
(lp23873
sg10
(lp23874
sg40
(lp23875
sg223
(lp23876
sg128
(lp23877
sg130
(lp23878
sg14
(lp23879
sg16
(lp23880
sg135
(lp23881
sg138
(lp23882
sg140
(lp23883
sg354
(lp23884
ssS'thick'
p23885
(dp23886
g80
(lp23887
sg283
(lp23888
sg6
(lp23889
sg76
(lp23890
sg484
(lp23891
sg14
(lp23892
sg106
(lp23893
I1029
asg63
(lp23894
ssS'cartesian'
p23895
(dp23896
g42
(lp23897
I141
asg32
(lp23898
sg48
(lp23899
sg59
(lp23900
sg303
(lp23901
ssS'nomin'
p23902
(dp23903
g6
(lp23904
sg344
(lp23905
sg460
(lp23906
sg91
(lp23907
sg135
(lp23908
I2083
asg384
(lp23909
ssS'airi'
p23910
(dp23911
g135
(lp23912
I603
assS'teo'
p23913
(dp23914
g22
(lp23915
I372
assS'equilibria'
p23916
(dp23917
g14
(lp23918
sg16
(lp23919
I876
asg535
(lp23920
sg46
(lp23921
ssS'wooldridg'
p23922
(dp23923
g354
(lp23924
I2031
assS'john'
p23925
(dp23926
g176
(lp23927
sg22
(lp23928
sg429
(lp23929
sg59
(lp23930
sg281
(lp23931
sg10
(lp23932
sg83
(lp23933
sg102
(lp23934
sg306
(lp23935
sg132
(lp23936
sg106
(lp23937
I28
asg18
(lp23938
sg99
(lp23939
sg63
(lp23940
sg114
(lp23941
ssS'coplanar'
p23942
(dp23943
g118
(lp23944
I104
assS'rpeak'
p23945
(dp23946
g6
(lp23947
I642
assS'olcillatl'
p23948
(dp23949
g20
(lp23950
I1245
assS'albert'
p23951
(dp23952
g106
(lp23953
I297
assS'inexact'
p23954
(dp23955
g429
(lp23956
sg283
(lp23957
sg8
(lp23958
I2524
assS'happili'
p23959
(dp23960
g104
(lp23961
I2725
assS'toronto'
p23962
(dp23963
g30
(lp23964
sg74
(lp23965
sg124
(lp23966
sg126
(lp23967
sg221
(lp23968
sg138
(lp23969
sg354
(lp23970
I3091
assS'target'
p23971
(dp23972
g68
(lp23973
sg70
(lp23974
sg26
(lp23975
sg277
(lp23976
sg283
(lp23977
sg74
(lp23978
sg145
(lp23979
sg76
(lp23980
sg293
(lp23981
sg295
(lp23982
sg183
(lp23983
sg59
(lp23984
sg38
(lp23985
sg83
(lp23986
sg85
(lp23987
sg303
(lp23988
sg306
(lp23989
sg89
(lp23990
sg245
(lp23991
sg46
(lp23992
sg96
(lp23993
sg99
(lp23994
sg223
(lp23995
sg350
(lp23996
sg32
(lp23997
sg94
(lp23998
sg52
(lp23999
sg114
(lp24000
sg116
(lp24001
sg329
(lp24002
sg440
(lp24003
sg135
(lp24004
sg121
(lp24005
sg4
(lp24006
sg235
(lp24007
sg124
(lp24008
sg344
(lp24009
sg128
(lp24010
sg132
(lp24011
sg14
(lp24012
sg149
(lp24013
sg138
(lp24014
sg140
(lp24015
sg354
(lp24016
I2223
assS'provid'
p24017
(dp24018
g68
(lp24019
sg70
(lp24020
sg26
(lp24021
sg277
(lp24022
sg163
(lp24023
sg36
(lp24024
sg181
(lp24025
sg303
(lp24026
sg30
(lp24027
sg287
(lp24028
sg74
(lp24029
sg145
(lp24030
sg256
(lp24031
sg76
(lp24032
sg262
(lp24033
sg78
(lp24034
sg59
(lp24035
sg484
(lp24036
sg38
(lp24037
sg83
(lp24038
sg85
(lp24039
sg63
(lp24040
sg306
(lp24041
sg87
(lp24042
sg91
(lp24043
sg245
(lp24044
sg46
(lp24045
sg96
(lp24046
sg48
(lp24047
sg99
(lp24048
sg313
(lp24049
sg223
(lp24050
sg149
(lp24051
sg118
(lp24052
sg174
(lp24053
sg293
(lp24054
sg32
(lp24055
sg350
(lp24056
sg429
(lp24057
sg102
(lp24058
sg104
(lp24059
sg110
(lp24060
sg178
(lp24061
sg52
(lp24062
sg22
(lp24063
sg116
(lp24064
sg438
(lp24065
I30
asg440
(lp24066
sg318
(lp24067
sg121
(lp24068
sg4
(lp24069
sg6
(lp24070
sg235
(lp24071
sg34
(lp24072
sg221
(lp24073
sg384
(lp24074
sg124
(lp24075
sg72
(lp24076
sg281
(lp24077
sg10
(lp24078
sg535
(lp24079
sg128
(lp24080
sg130
(lp24081
sg132
(lp24082
sg14
(lp24083
sg16
(lp24084
sg135
(lp24085
sg50
(lp24086
sg138
(lp24087
sg140
(lp24088
sg354
(lp24089
ssS'bonhoeff'
p24090
(dp24091
g48
(lp24092
I2182
assS'distal'
p24093
(dp24094
g230
(lp24095
I3320
asg181
(lp24096
ssS'rate'
p24097
(dp24098
g68
(lp24099
sg70
(lp24100
sg277
(lp24101
sg281
(lp24102
sg283
(lp24103
sg181
(lp24104
sg83
(lp24105
sg176
(lp24106
sg76
(lp24107
sg262
(lp24108
sg295
(lp24109
sg183
(lp24110
sg59
(lp24111
sg484
(lp24112
sg38
(lp24113
sg149
(lp24114
sg85
(lp24115
sg63
(lp24116
sg42
(lp24117
I1997
asg87
(lp24118
sg91
(lp24119
sg20
(lp24120
sg350
(lp24121
sg116
(lp24122
sg329
(lp24123
sg293
(lp24124
sg318
(lp24125
sg102
(lp24126
sg104
(lp24127
sg96
(lp24128
sg22
(lp24129
sg216
(lp24130
sg174
(lp24131
sg440
(lp24132
sg332
(lp24133
sg121
(lp24134
sg4
(lp24135
sg6
(lp24136
sg34
(lp24137
sg36
(lp24138
sg384
(lp24139
sg124
(lp24140
sg126
(lp24141
sg341
(lp24142
sg10
(lp24143
sg128
(lp24144
sg78
(lp24145
sg132
(lp24146
sg135
(lp24147
sg138
(lp24148
ssS'kth'
p24149
(dp24150
g318
(lp24151
sg104
(lp24152
sg108
(lp24153
I1139
asg235
(lp24154
sg46
(lp24155
ssS'jepson'
p24156
(dp24157
g138
(lp24158
I3238
assS'njural'
p24159
(dp24160
g174
(lp24161
I1858
assS'minut'
p24162
(dp24163
g181
(lp24164
sg6
(lp24165
sg295
(lp24166
sg183
(lp24167
sg124
(lp24168
sg126
(lp24169
sg83
(lp24170
sg52
(lp24171
sg78
(lp24172
sg14
(lp24173
sg16
(lp24174
sg135
(lp24175
I314
asg99
(lp24176
sg44
(lp24177
ssS'minus'
p24178
(dp24179
g118
(lp24180
sg32
(lp24181
sg176
(lp24182
sg121
(lp24183
I2036
asg6
(lp24184
sg262
(lp24185
ssS'provis'
p24186
(dp24187
g216
(lp24188
I331
assS'ollunct'
p24189
(dp24190
g344
(lp24191
I1809
assS'pessimist'
p24192
(dp24193
g183
(lp24194
I5699
assS'lsidori'
p24195
(dp24196
g230
(lp24197
I568
assS'manner'
p24198
(dp24199
g30
(lp24200
sg438
(lp24201
I95
asg32
(lp24202
sg176
(lp24203
sg121
(lp24204
sg4
(lp24205
sg76
(lp24206
sg329
(lp24207
sg124
(lp24208
sg126
(lp24209
sg281
(lp24210
sg10
(lp24211
sg429
(lp24212
sg87
(lp24213
sg46
(lp24214
sg102
(lp24215
sg94
(lp24216
sg106
(lp24217
sg20
(lp24218
sg313
(lp24219
sg350
(lp24220
ssS'strength'
p24221
(dp24222
g216
(lp24223
sg438
(lp24224
I457
asg118
(lp24225
sg332
(lp24226
sg145
(lp24227
sg4
(lp24228
sg329
(lp24229
sg295
(lp24230
sg183
(lp24231
sg38
(lp24232
sg63
(lp24233
sg12
(lp24234
sg30
(lp24235
sg132
(lp24236
sg303
(lp24237
sg106
(lp24238
sg108
(lp24239
sg138
(lp24240
sg149
(lp24241
ssS'luxuri'
p24242
(dp24243
g85
(lp24244
I3133
assS'magnus'
p24245
(dp24246
g91
(lp24247
I13
assS'latter'
p24248
(dp24249
g174
(lp24250
sg32
(lp24251
sg178
(lp24252
sg262
(lp24253
sg80
(lp24254
sg8
(lp24255
sg295
(lp24256
sg183
(lp24257
sg384
(lp24258
sg235
(lp24259
sg281
(lp24260
sg293
(lp24261
sg40
(lp24262
sg42
(lp24263
I76
asg34
(lp24264
sg132
(lp24265
sg104
(lp24266
sg303
(lp24267
sg221
(lp24268
sg83
(lp24269
sg223
(lp24270
ssS'superimpos'
p24271
(dp24272
g42
(lp24273
I2482
asg106
(lp24274
sg50
(lp24275
ssS'casual'
p24276
(dp24277
g114
(lp24278
I606
assS'umiac'
p24279
(dp24280
g128
(lp24281
I183
assS'transmit'
p24282
(dp24283
g216
(lp24284
sg438
(lp24285
I218
asg72
(lp24286
ssS'nlog'
p24287
(dp24288
g145
(lp24289
I3194
assS'neurobiolog'
p24290
(dp24291
g116
(lp24292
sg318
(lp24293
sg6
(lp24294
sg8
(lp24295
sg262
(lp24296
sg303
(lp24297
sg350
(lp24298
sg91
(lp24299
sg12
(lp24300
sg106
(lp24301
I311
asg18
(lp24302
sg50
(lp24303
sg149
(lp24304
ssS'mirumum'
p24305
(dp24306
g72
(lp24307
I2926
assS'ypm'
p24308
(dp24309
g72
(lp24310
I2239
assS'anatomi'
p24311
(dp24312
g99
(lp24313
I3210
assS'germani'
p24314
(dp24315
g216
(lp24316
sg34
(lp24317
sg36
(lp24318
sg130
(lp24319
sg132
(lp24320
sg221
(lp24321
sg535
(lp24322
sg223
(lp24323
sg354
(lp24324
I23
assS'bruck'
p24325
(dp24326
g344
(lp24327
sg40
(lp24328
I9
assS'bruce'
p24329
(dp24330
g99
(lp24331
I2793
asg350
(lp24332
ssS'dbound'
p24333
(dp24334
g85
(lp24335
I1852
assS'lexic'
p24336
(dp24337
g96
(lp24338
I2183
assS'phase'
p24339
(dp24340
g68
(lp24341
sg116
(lp24342
sg74
(lp24343
sg78
(lp24344
sg38
(lp24345
sg85
(lp24346
sg91
(lp24347
sg245
(lp24348
sg46
(lp24349
sg18
(lp24350
sg350
(lp24351
sg230
(lp24352
sg94
(lp24353
sg102
(lp24354
sg106
(lp24355
I638
asg110
(lp24356
sg63
(lp24357
sg52
(lp24358
sg216
(lp24359
sg329
(lp24360
sg22
(lp24361
sg8
(lp24362
sg34
(lp24363
sg124
(lp24364
sg126
(lp24365
sg341
(lp24366
sg135
(lp24367
sg50
(lp24368
sg354
(lp24369
ssS'fxn'
p24370
(dp24371
g26
(lp24372
I1475
assS'fitter'
p24373
(dp24374
g89
(lp24375
I737
assS'bracket'
p24376
(dp24377
g32
(lp24378
sg221
(lp24379
sg350
(lp24380
I805
assS'radford'
p24381
(dp24382
g124
(lp24383
sg126
(lp24384
I2894
assS'maxima'
p24385
(dp24386
g329
(lp24387
sg74
(lp24388
sg460
(lp24389
sg102
(lp24390
I2507
asg46
(lp24391
sg221
(lp24392
ssS'notion'
p24393
(dp24394
g287
(lp24395
sg32
(lp24396
sg181
(lp24397
sg118
(lp24398
sg34
(lp24399
sg78
(lp24400
sg341
(lp24401
I1583
asg85
(lp24402
sg344
(lp24403
sg223
(lp24404
ssS'tnnni'
p24405
(dp24406
g116
(lp24407
I1797
assS'biocytin'
p24408
(dp24409
g149
(lp24410
I2918
assS'opposit'
p24411
(dp24412
g216
(lp24413
sg118
(lp24414
sg4
(lp24415
sg80
(lp24416
sg36
(lp24417
sg245
(lp24418
sg89
(lp24419
sg26
(lp24420
sg12
(lp24421
sg106
(lp24422
I591
asg99
(lp24423
sg149
(lp24424
ssS'bimbo'
p24425
(dp24426
g76
(lp24427
I2241
assS'mostafa'
p24428
(dp24429
g110
(lp24430
sg277
(lp24431
sg223
(lp24432
I3080
assS'overload'
p24433
(dp24434
g78
(lp24435
I645
assS'identifi'
p24436
(dp24437
g283
(lp24438
sg70
(lp24439
sg26
(lp24440
sg277
(lp24441
sg303
(lp24442
sg30
(lp24443
sg74
(lp24444
sg293
(lp24445
sg295
(lp24446
sg183
(lp24447
sg59
(lp24448
sg38
(lp24449
sg85
(lp24450
sg63
(lp24451
sg42
(lp24452
I1810
asg306
(lp24453
sg91
(lp24454
sg94
(lp24455
sg96
(lp24456
sg313
(lp24457
sg104
(lp24458
sg106
(lp24459
sg110
(lp24460
sg20
(lp24461
sg116
(lp24462
sg32
(lp24463
sg121
(lp24464
sg181
(lp24465
sg8
(lp24466
sg34
(lp24467
sg281
(lp24468
sg344
(lp24469
sg128
(lp24470
sg78
(lp24471
sg50
(lp24472
sg138
(lp24473
ssS'involv'
p24474
(dp24475
g26
(lp24476
sg460
(lp24477
sg287
(lp24478
sg74
(lp24479
sg176
(lp24480
sg295
(lp24481
sg183
(lp24482
sg38
(lp24483
sg85
(lp24484
sg303
(lp24485
sg306
(lp24486
sg91
(lp24487
sg12
(lp24488
sg94
(lp24489
sg114
(lp24490
sg99
(lp24491
sg223
(lp24492
sg350
(lp24493
sg32
(lp24494
sg429
(lp24495
sg332
(lp24496
sg46
(lp24497
sg106
(lp24498
I85
asg110
(lp24499
sg63
(lp24500
sg52
(lp24501
sg22
(lp24502
sg116
(lp24503
sg118
(lp24504
sg440
(lp24505
sg318
(lp24506
sg4
(lp24507
sg181
(lp24508
sg8
(lp24509
sg384
(lp24510
sg124
(lp24511
sg126
(lp24512
sg281
(lp24513
sg40
(lp24514
sg344
(lp24515
sg128
(lp24516
sg78
(lp24517
sg14
(lp24518
sg16
(lp24519
sg138
(lp24520
ssS'tadep'
p24521
(dp24522
g132
(lp24523
I307
assS'latent'
p24524
(dp24525
g130
(lp24526
I3172
assS'oonfer'
p24527
(dp24528
g114
(lp24529
I2302
assS'latenc'
p24530
(dp24531
g4
(lp24532
I651
asg76
(lp24533
ssS'wzh'
p24534
(dp24535
g287
(lp24536
I2235
assS'predecessor'
p24537
(dp24538
g287
(lp24539
I1654
assS'do'
p24540
(dp24541
g329
(lp24542
sg70
(lp24543
sg26
(lp24544
sg277
(lp24545
sg281
(lp24546
sg283
(lp24547
sg287
(lp24548
sg74
(lp24549
sg176
(lp24550
sg145
(lp24551
sg80
(lp24552
sg76
(lp24553
sg118
(lp24554
sg295
(lp24555
sg183
(lp24556
sg59
(lp24557
sg484
(lp24558
sg38
(lp24559
sg83
(lp24560
sg85
(lp24561
sg303
(lp24562
sg306
(lp24563
sg89
(lp24564
sg91
(lp24565
sg12
(lp24566
sg94
(lp24567
sg20
(lp24568
sg18
(lp24569
sg68
(lp24570
sg313
(lp24571
sg44
(lp24572
sg149
(lp24573
sg174
(lp24574
sg293
(lp24575
sg429
(lp24576
sg318
(lp24577
sg46
(lp24578
sg104
(lp24579
sg110
(lp24580
sg116
(lp24581
sg438
(lp24582
I1513
asg332
(lp24583
sg178
(lp24584
sg181
(lp24585
sg6
(lp24586
sg34
(lp24587
sg36
(lp24588
sg124
(lp24589
sg126
(lp24590
sg341
(lp24591
sg344
(lp24592
sg223
(lp24593
sg128
(lp24594
sg130
(lp24595
sg132
(lp24596
sg14
(lp24597
sg135
(lp24598
sg50
(lp24599
sg138
(lp24600
sg140
(lp24601
ssS'dl'
p24602
(dp24603
g438
(lp24604
I1292
asg318
(lp24605
sg102
(lp24606
sg91
(lp24607
sg12
(lp24608
sg63
(lp24609
ssS'dm'
p24610
(dp24611
g102
(lp24612
sg384
(lp24613
sg99
(lp24614
I3471
asg85
(lp24615
ssS'dj'
p24616
(dp24617
g440
(lp24618
sg176
(lp24619
sg295
(lp24620
sg183
(lp24621
sg384
(lp24622
sg341
(lp24623
sg12
(lp24624
I2763
assS'dk'
p24625
(dp24626
g484
(lp24627
sg178
(lp24628
sg26
(lp24629
sg130
(lp24630
I1164
assS'dh'
p24631
(dp24632
g12
(lp24633
sg287
(lp24634
sg108
(lp24635
I654
assS'di'
p24636
(dp24637
g230
(lp24638
sg438
(lp24639
I1373
asg344
(lp24640
sg484
(lp24641
sg83
(lp24642
sg85
(lp24643
sg130
(lp24644
sg96
(lp24645
sg18
(lp24646
sg44
(lp24647
sg350
(lp24648
ssS'df'
p24649
(dp24650
g18
(lp24651
I602
assS'dg'
p24652
(dp24653
g74
(lp24654
I2421
assS'dd'
p24655
(dp24656
g14
(lp24657
I2711
asg384
(lp24658
sg52
(lp24659
ssS'de'
p24660
(dp24661
g30
(lp24662
sg287
(lp24663
sg295
(lp24664
sg183
(lp24665
sg59
(lp24666
sg12
(lp24667
sg94
(lp24668
sg96
(lp24669
sg48
(lp24670
sg221
(lp24671
sg116
(lp24672
sg32
(lp24673
sg106
(lp24674
I1252
asg216
(lp24675
sg118
(lp24676
sg440
(lp24677
sg121
(lp24678
sg22
(lp24679
sg8
(lp24680
sg34
(lp24681
sg36
(lp24682
sg40
(lp24683
sg128
(lp24684
sg130
(lp24685
sg132
(lp24686
sg354
(lp24687
ssS'db'
p24688
(dp24689
g14
(lp24690
sg16
(lp24691
I58
asg484
(lp24692
sg22
(lp24693
sg78
(lp24694
ssS'dc'
p24695
(dp24696
g12
(lp24697
I2809
asg20
(lp24698
sg178
(lp24699
sg22
(lp24700
sg256
(lp24701
ssS'da'
p24702
(dp24703
g174
(lp24704
sg4
(lp24705
sg130
(lp24706
I2392
assS'watson'
p24707
(dp24708
g102
(lp24709
sg89
(lp24710
I2755
asg76
(lp24711
sg303
(lp24712
ssS'dz'
p24713
(dp24714
g384
(lp24715
sg138
(lp24716
I371
asg341
(lp24717
ssS'dx'
p24718
(dp24719
g174
(lp24720
sg176
(lp24721
sg329
(lp24722
sg34
(lp24723
sg281
(lp24724
sg130
(lp24725
I558
asg313
(lp24726
sg223
(lp24727
ssS'dy'
p24728
(dp24729
g318
(lp24730
sg163
(lp24731
sg384
(lp24732
sg72
(lp24733
sg176
(lp24734
sg130
(lp24735
sg18
(lp24736
sg354
(lp24737
I854
assS'dv'
p24738
(dp24739
g484
(lp24740
sg318
(lp24741
sg262
(lp24742
sg68
(lp24743
sg8
(lp24744
I1030
assS'dw'
p24745
(dp24746
g42
(lp24747
I1665
asg12
(lp24748
sg535
(lp24749
sg354
(lp24750
ssS'dt'
p24751
(dp24752
g216
(lp24753
sg174
(lp24754
sg118
(lp24755
sg332
(lp24756
sg4
(lp24757
sg262
(lp24758
sg36
(lp24759
sg384
(lp24760
sg68
(lp24761
sg318
(lp24762
sg12
(lp24763
sg104
(lp24764
sg44
(lp24765
I1168
assS'recalcul'
p24766
(dp24767
g138
(lp24768
I1081
assS'dr'
p24769
(dp24770
g283
(lp24771
sg83
(lp24772
sg130
(lp24773
sg135
(lp24774
I2424
asg44
(lp24775
sg350
(lp24776
ssS'ds'
p24777
(dp24778
g12
(lp24779
I2637
asg80
(lp24780
ssS'dp'
p24781
(dp24782
g89
(lp24783
I157
asg83
(lp24784
ssS'ltchhh'
p24785
(dp24786
g178
(lp24787
I1422
assS'pearson'
p24788
(dp24789
g176
(lp24790
sg6
(lp24791
I404
assS'emb'
p24792
(dp24793
g130
(lp24794
I871
assS'ravenswood'
p24795
(dp24796
g87
(lp24797
I243
assS'inpullay'
p24798
(dp24799
g116
(lp24800
I662
assS'emj'
p24801
(dp24802
g8
(lp24803
I40
assS'emu'
p24804
(dp24805
g80
(lp24806
sg6
(lp24807
I994
assS'anza'
p24808
(dp24809
g42
(lp24810
I608
assS'cutofffrequ'
p24811
(dp24812
g22
(lp24813
I794
assS'gaug'
p24814
(dp24815
g104
(lp24816
I371
asg78
(lp24817
ssS'septemb'
p24818
(dp24819
g183
(lp24820
sg96
(lp24821
sg135
(lp24822
I2478
asg89
(lp24823
sg22
(lp24824
ssS'schreiber'
p24825
(dp24826
g4
(lp24827
I32
assS'interllig'
p24828
(dp24829
g20
(lp24830
I2543
assS'dillard'
p24831
(dp24832
g32
(lp24833
I3137
assS'lowpass'
p24834
(dp24835
g48
(lp24836
I1267
asg22
(lp24837
ssS'christian'
p24838
(dp24839
g78
(lp24840
I14
assS'optimum'
p24841
(dp24842
g283
(lp24843
sg121
(lp24844
sg8
(lp24845
sg36
(lp24846
sg281
(lp24847
sg44
(lp24848
sg108
(lp24849
sg52
(lp24850
sg354
(lp24851
I2669
assS'lihi'
p24852
(dp24853
g313
(lp24854
I1401
assS'awar'
p24855
(dp24856
g104
(lp24857
I1863
asg83
(lp24858
ssS'coenen'
p24859
(dp24860
g350
(lp24861
I19
assS'away'
p24862
(dp24863
g230
(lp24864
sg174
(lp24865
sg176
(lp24866
sg295
(lp24867
sg183
(lp24868
sg384
(lp24869
sg68
(lp24870
sg72
(lp24871
sg34
(lp24872
sg30
(lp24873
sg89
(lp24874
sg91
(lp24875
sg18
(lp24876
sg138
(lp24877
I704
assS'lippman'
p24878
(dp24879
g178
(lp24880
I2480
assS'viwi'
p24881
(dp24882
g216
(lp24883
I1081
assS'lobit'
p24884
(dp24885
g135
(lp24886
I2536
assS'vhlibalu'
p24887
(dp24888
g350
(lp24889
I1881
assS'volper'
p24890
(dp24891
g110
(lp24892
I742
assS'drawn'
p24893
(dp24894
g277
(lp24895
sg30
(lp24896
sg256
(lp24897
sg262
(lp24898
sg295
(lp24899
sg183
(lp24900
sg484
(lp24901
sg38
(lp24902
sg85
(lp24903
sg12
(lp24904
sg313
(lp24905
sg223
(lp24906
sg102
(lp24907
sg52
(lp24908
sg318
(lp24909
sg181
(lp24910
sg235
(lp24911
sg384
(lp24912
sg281
(lp24913
sg344
(lp24914
sg138
(lp24915
sg140
(lp24916
I435
assS'yamashita'
p24917
(dp24918
g20
(lp24919
I21
assS'wg'
p24920
(dp24921
g36
(lp24922
I2253
assS'wf'
p24923
(dp24924
g183
(lp24925
I5340
asg36
(lp24926
ssS'accord'
p24927
(dp24928
g70
(lp24929
sg277
(lp24930
sg163
(lp24931
sg30
(lp24932
sg287
(lp24933
sg74
(lp24934
sg145
(lp24935
sg76
(lp24936
sg293
(lp24937
sg295
(lp24938
sg183
(lp24939
sg59
(lp24940
sg38
(lp24941
sg85
(lp24942
sg303
(lp24943
sg42
(lp24944
I1832
asg87
(lp24945
sg12
(lp24946
sg20
(lp24947
sg48
(lp24948
sg221
(lp24949
sg313
(lp24950
sg223
(lp24951
sg149
(lp24952
sg116
(lp24953
sg174
(lp24954
sg32
(lp24955
sg429
(lp24956
sg114
(lp24957
sg230
(lp24958
sg438
(lp24959
sg440
(lp24960
sg135
(lp24961
sg121
(lp24962
sg22
(lp24963
sg34
(lp24964
sg384
(lp24965
sg124
(lp24966
sg126
(lp24967
sg281
(lp24968
sg40
(lp24969
sg44
(lp24970
sg130
(lp24971
sg132
(lp24972
sg350
(lp24973
sg50
(lp24974
sg138
(lp24975
sg354
(lp24976
ssS'wd'
p24977
(dp24978
g74
(lp24979
sg124
(lp24980
sg341
(lp24981
I483
assS'wc'
p24982
(dp24983
g384
(lp24984
I25
asg176
(lp24985
ssS'wb'
p24986
(dp24987
g354
(lp24988
I1342
assS'wa'
p24989
(dp24990
g178
(lp24991
sg140
(lp24992
I486
assS'wo'
p24993
(dp24994
g74
(lp24995
sg34
(lp24996
sg36
(lp24997
sg341
(lp24998
sg102
(lp24999
sg52
(lp25000
sg354
(lp25001
I2499
assS'wn'
p25002
(dp25003
g42
(lp25004
I1660
asg287
(lp25005
sg40
(lp25006
sg341
(lp25007
sg36
(lp25008
ssS'wl'
p25009
(dp25010
g329
(lp25011
sg318
(lp25012
sg121
(lp25013
sg163
(lp25014
sg344
(lp25015
sg183
(lp25016
sg40
(lp25017
sg341
(lp25018
sg281
(lp25019
sg283
(lp25020
sg108
(lp25021
sg221
(lp25022
sg354
(lp25023
I1915
assS'wk'
p25024
(dp25025
g74
(lp25026
sg22
(lp25027
sg235
(lp25028
sg384
(lp25029
sg91
(lp25030
I1423
asg221
(lp25031
sg460
(lp25032
ssS'wj'
p25033
(dp25034
g460
(lp25035
sg96
(lp25036
sg138
(lp25037
I2176
asg91
(lp25038
ssS'wi'
p25039
(dp25040
g216
(lp25041
sg329
(lp25042
sg145
(lp25043
sg230
(lp25044
sg181
(lp25045
sg8
(lp25046
sg34
(lp25047
sg221
(lp25048
sg384
(lp25049
sg124
(lp25050
sg341
(lp25051
sg344
(lp25052
sg40
(lp25053
sg36
(lp25054
sg287
(lp25055
sg183
(lp25056
sg78
(lp25057
sg104
(lp25058
sg14
(lp25059
sg50
(lp25060
sg354
(lp25061
I1341
assS'wh'
p25062
(dp25063
g6
(lp25064
I2343
asg40
(lp25065
ssS'ww'
p25066
(dp25067
g76
(lp25068
I1014
assS'mcenerney'
p25069
(dp25070
g14
(lp25071
sg16
(lp25072
I2614
assS'wu'
p25073
(dp25074
g174
(lp25075
sg440
(lp25076
sg108
(lp25077
I13
assS'wt'
p25078
(dp25079
g230
(lp25080
sg36
(lp25081
sg34
(lp25082
sg341
(lp25083
sg6
(lp25084
I2306
assS'preprocessor'
p25085
(dp25086
g63
(lp25087
I315
assS'huang'
p25088
(dp25089
g106
(lp25090
I2669
asg96
(lp25091
ssS'wp'
p25092
(dp25093
g354
(lp25094
I1559
assS'wz'
p25095
(dp25096
g287
(lp25097
I2071
assS'wy'
p25098
(dp25099
g318
(lp25100
I1131
assS'ham'
p25101
(dp25102
g96
(lp25103
I1721
asg283
(lp25104
sg110
(lp25105
ssS'ilf'
p25106
(dp25107
g68
(lp25108
I2269
assS'ilg'
p25109
(dp25110
g223
(lp25111
I1215
asg40
(lp25112
ssS'cos'
p25113
(dp25114
g30
(lp25115
sg118
(lp25116
sg32
(lp25117
sg181
(lp25118
sg116
(lp25119
sg384
(lp25120
sg124
(lp25121
sg85
(lp25122
sg14
(lp25123
sg16
(lp25124
I906
asg44
(lp25125
ssS'easi'
p25126
(dp25127
g440
(lp25128
sg332
(lp25129
sg277
(lp25130
sg235
(lp25131
sg344
(lp25132
sg183
(lp25133
sg460
(lp25134
sg341
(lp25135
sg74
(lp25136
sg42
(lp25137
I483
asg306
(lp25138
sg89
(lp25139
sg91
(lp25140
sg46
(lp25141
sg132
(lp25142
sg94
(lp25143
sg40
(lp25144
sg140
(lp25145
ssS'iln'
p25146
(dp25147
g108
(lp25148
I855
assS'ilo'
p25149
(dp25150
g438
(lp25151
I933
assS'howev'
p25152
(dp25153
g329
(lp25154
sg78
(lp25155
sg277
(lp25156
sg72
(lp25157
sg68
(lp25158
sg80
(lp25159
sg281
(lp25160
sg283
(lp25161
sg40
(lp25162
sg26
(lp25163
sg30
(lp25164
sg287
(lp25165
sg74
(lp25166
sg176
(lp25167
sg145
(lp25168
sg256
(lp25169
sg76
(lp25170
sg293
(lp25171
sg295
(lp25172
sg183
(lp25173
sg59
(lp25174
sg484
(lp25175
sg85
(lp25176
sg303
(lp25177
sg42
(lp25178
I189
asg306
(lp25179
sg87
(lp25180
sg89
(lp25181
sg91
(lp25182
sg245
(lp25183
sg94
(lp25184
sg20
(lp25185
sg48
(lp25186
sg99
(lp25187
sg313
(lp25188
sg44
(lp25189
sg149
(lp25190
sg230
(lp25191
sg174
(lp25192
sg18
(lp25193
sg32
(lp25194
sg350
(lp25195
sg429
(lp25196
sg318
(lp25197
sg46
(lp25198
sg102
(lp25199
sg104
(lp25200
sg106
(lp25201
sg108
(lp25202
sg110
(lp25203
sg52
(lp25204
sg22
(lp25205
sg216
(lp25206
sg438
(lp25207
sg440
(lp25208
sg332
(lp25209
sg121
(lp25210
sg4
(lp25211
sg6
(lp25212
sg235
(lp25213
sg34
(lp25214
sg221
(lp25215
sg384
(lp25216
sg124
(lp25217
sg126
(lp25218
sg341
(lp25219
sg10
(lp25220
sg535
(lp25221
sg344
(lp25222
sg223
(lp25223
sg128
(lp25224
sg36
(lp25225
sg132
(lp25226
sg14
(lp25227
sg16
(lp25228
sg135
(lp25229
sg50
(lp25230
sg138
(lp25231
sg140
(lp25232
sg354
(lp25233
ssS'coz'
p25234
(dp25235
g256
(lp25236
I987
assS'ili'
p25237
(dp25238
g329
(lp25239
sg460
(lp25240
sg6
(lp25241
I1846
assS'ilv'
p25242
(dp25243
g262
(lp25244
I1249
assS'ilw'
p25245
(dp25246
g245
(lp25247
I1327
asg74
(lp25248
ssS'ilt'
p25249
(dp25250
g262
(lp25251
I1230
assS'cod'
p25252
(dp25253
g72
(lp25254
I979
assS'futil'
p25255
(dp25256
g44
(lp25257
I573
assS'coi'
p25258
(dp25259
g106
(lp25260
I1993
assS'coh'
p25261
(dp25262
g332
(lp25263
I1626
assS'ilz'
p25264
(dp25265
g138
(lp25266
I501
assS'schafer'
p25267
(dp25268
g132
(lp25269
I266
assS'ilx'
p25270
(dp25271
g460
(lp25272
sg223
(lp25273
I1040
assS'con'
p25274
(dp25275
g108
(lp25276
I1872
assS'eyesight'
p25277
(dp25278
g42
(lp25279
I35
assS'widen'
p25280
(dp25281
g42
(lp25282
I3317
asg354
(lp25283
ssS'tong'
p25284
(dp25285
g281
(lp25286
I12
assS'toni'
p25287
(dp25288
g87
(lp25289
sg138
(lp25290
I2289
asg76
(lp25291
ssS'royal'
p25292
(dp25293
g174
(lp25294
sg440
(lp25295
sg283
(lp25296
sg460
(lp25297
sg72
(lp25298
sg91
(lp25299
sg221
(lp25300
sg313
(lp25301
sg354
(lp25302
I2994
assS'trunk'
p25303
(dp25304
g303
(lp25305
I517
assS'codon'
p25306
(dp25307
g344
(lp25308
I3193
assS'unrealiz'
p25309
(dp25310
g38
(lp25311
I1293
assS'permut'
p25312
(dp25313
g38
(lp25314
I1247
assS'guid'
p25315
(dp25316
g116
(lp25317
sg4
(lp25318
sg181
(lp25319
sg293
(lp25320
sg295
(lp25321
sg183
(lp25322
sg59
(lp25323
sg72
(lp25324
sg429
(lp25325
sg306
(lp25326
sg132
(lp25327
I1771
asg313
(lp25328
sg44
(lp25329
sg22
(lp25330
ssS'speak'
p25331
(dp25332
g287
(lp25333
sg74
(lp25334
sg68
(lp25335
sg42
(lp25336
I631
asg132
(lp25337
sg46
(lp25338
sg63
(lp25339
ssS'degener'
p25340
(dp25341
g36
(lp25342
I668
asg293
(lp25343
ssS'lmanesthet'
p25344
(dp25345
g174
(lp25346
I2585
assS'convolut'
p25347
(dp25348
g216
(lp25349
sg174
(lp25350
sg178
(lp25351
sg118
(lp25352
sg59
(lp25353
sg262
(lp25354
sg429
(lp25355
sg63
(lp25356
sg44
(lp25357
I1891
assS'lxcx'
p25358
(dp25359
g313
(lp25360
I1501
assS'girosi'
p25361
(dp25362
g36
(lp25363
sg96
(lp25364
I539
asg124
(lp25365
sg293
(lp25366
ssS'baum'
p25367
(dp25368
g287
(lp25369
sg440
(lp25370
I2025
asg110
(lp25371
sg341
(lp25372
ssS'deskew'
p25373
(dp25374
g63
(lp25375
I624
assS'kwabena'
p25376
(dp25377
g256
(lp25378
I12
assS'physician'
p25379
(dp25380
g344
(lp25381
I2779
assS'inhibit'
p25382
(dp25383
g216
(lp25384
sg438
(lp25385
I929
asg118
(lp25386
sg332
(lp25387
sg178
(lp25388
sg4
(lp25389
sg6
(lp25390
sg262
(lp25391
sg50
(lp25392
sg256
(lp25393
sg70
(lp25394
sg303
(lp25395
sg176
(lp25396
sg104
(lp25397
sg12
(lp25398
sg14
(lp25399
sg106
(lp25400
sg99
(lp25401
sg535
(lp25402
sg149
(lp25403
ssS'ident'
p25404
(dp25405
g68
(lp25406
sg26
(lp25407
sg277
(lp25408
sg181
(lp25409
sg30
(lp25410
sg287
(lp25411
sg76
(lp25412
sg293
(lp25413
sg295
(lp25414
sg183
(lp25415
sg80
(lp25416
sg38
(lp25417
sg83
(lp25418
sg114
(lp25419
sg303
(lp25420
sg306
(lp25421
sg18
(lp25422
sg221
(lp25423
sg350
(lp25424
sg102
(lp25425
sg106
(lp25426
I826
asg108
(lp25427
sg110
(lp25428
sg63
(lp25429
sg52
(lp25430
sg4
(lp25431
sg329
(lp25432
sg32
(lp25433
sg318
(lp25434
sg22
(lp25435
sg6
(lp25436
sg235
(lp25437
sg384
(lp25438
sg124
(lp25439
sg126
(lp25440
sg341
(lp25441
sg40
(lp25442
sg344
(lp25443
sg50
(lp25444
sg138
(lp25445
sg140
(lp25446
ssS'gnu'
p25447
(dp25448
g132
(lp25449
I368
asg94
(lp25450
sg10
(lp25451
ssS'aiw'
p25452
(dp25453
g34
(lp25454
I1558
assS'properti'
p25455
(dp25456
g70
(lp25457
sg26
(lp25458
sg281
(lp25459
sg287
(lp25460
sg74
(lp25461
sg145
(lp25462
sg256
(lp25463
sg80
(lp25464
sg295
(lp25465
sg183
(lp25466
sg59
(lp25467
sg85
(lp25468
sg303
(lp25469
sg42
(lp25470
I719
asg306
(lp25471
sg91
(lp25472
sg12
(lp25473
sg46
(lp25474
sg96
(lp25475
sg48
(lp25476
sg99
(lp25477
sg535
(lp25478
sg44
(lp25479
sg149
(lp25480
sg116
(lp25481
sg329
(lp25482
sg18
(lp25483
sg32
(lp25484
sg245
(lp25485
sg429
(lp25486
sg102
(lp25487
sg104
(lp25488
sg106
(lp25489
sg108
(lp25490
sg110
(lp25491
sg230
(lp25492
sg438
(lp25493
sg440
(lp25494
sg332
(lp25495
sg4
(lp25496
sg181
(lp25497
sg8
(lp25498
sg34
(lp25499
sg384
(lp25500
sg68
(lp25501
sg341
(lp25502
sg40
(lp25503
sg223
(lp25504
sg128
(lp25505
sg130
(lp25506
sg14
(lp25507
sg16
(lp25508
sg460
(lp25509
sg354
(lp25510
ssS'air'
p25511
(dp25512
g116
(lp25513
sg287
(lp25514
sg283
(lp25515
sg6
(lp25516
I2235
asg118
(lp25517
sg341
(lp25518
sg281
(lp25519
sg83
(lp25520
ssS'aim'
p25521
(dp25522
g174
(lp25523
sg283
(lp25524
sg145
(lp25525
sg26
(lp25526
sg535
(lp25527
sg48
(lp25528
sg138
(lp25529
sg52
(lp25530
sg354
(lp25531
I963
assS'ail'
p25532
(dp25533
g245
(lp25534
I2043
asg36
(lp25535
sg76
(lp25536
ssS'aio'
p25537
(dp25538
g245
(lp25539
I2870
asg221
(lp25540
ssS'pairwis'
p25541
(dp25542
g344
(lp25543
sg126
(lp25544
sg130
(lp25545
I35
assS'aij'
p25546
(dp25547
g174
(lp25548
I1842
asg460
(lp25549
sg40
(lp25550
ssS'aid'
p25551
(dp25552
g94
(lp25553
I3190
asg20
(lp25554
sg59
(lp25555
sg4
(lp25556
sg277
(lp25557
ssS'vagu'
p25558
(dp25559
g216
(lp25560
I699
assS'subcort'
p25561
(dp25562
g176
(lp25563
I809
assS'cons'
p25564
(dp25565
g48
(lp25566
I1295
assS'cont'
p25567
(dp25568
g108
(lp25569
I1149
asg99
(lp25570
ssS'memoryless'
p25571
(dp25572
g18
(lp25573
I2511
assS'hauptstra'
p25574
(dp25575
g68
(lp25576
I30
assS'bartogc'
p25577
(dp25578
g83
(lp25579
I32
assS'cone'
p25580
(dp25581
g245
(lp25582
I1307
asg256
(lp25583
ssS'conf'
p25584
(dp25585
g318
(lp25586
sg178
(lp25587
sg181
(lp25588
sg94
(lp25589
sg72
(lp25590
sg10
(lp25591
sg63
(lp25592
sg42
(lp25593
I3477
asg344
(lp25594
sg104
(lp25595
sg14
(lp25596
sg313
(lp25597
sg52
(lp25598
ssS'hebrew'
p25599
(dp25600
g6
(lp25601
sg130
(lp25602
I3156
assS'gamlin'
p25603
(dp25604
g350
(lp25605
I1222
assS'keipm'
p25606
(dp25607
g72
(lp25608
I2850
assS'btransit'
p25609
(dp25610
g18
(lp25611
I1926
assS'conn'
p25612
(dp25613
g8
(lp25614
I1392
assS'descent'
p25615
(dp25616
g283
(lp25617
sg26
(lp25618
sg287
(lp25619
sg295
(lp25620
sg183
(lp25621
sg38
(lp25622
sg306
(lp25623
sg91
(lp25624
sg235
(lp25625
sg46
(lp25626
sg96
(lp25627
sg99
(lp25628
sg44
(lp25629
sg110
(lp25630
sg329
(lp25631
sg121
(lp25632
sg8
(lp25633
sg34
(lp25634
sg36
(lp25635
sg68
(lp25636
sg341
(lp25637
sg128
(lp25638
sg140
(lp25639
I1952
assS'postmultipl'
p25640
(dp25641
g22
(lp25642
I1117
assS'perform'
p25643
(dp25644
g329
(lp25645
sg70
(lp25646
sg78
(lp25647
sg277
(lp25648
sg72
(lp25649
sg283
(lp25650
sg85
(lp25651
sg36
(lp25652
sg181
(lp25653
sg303
(lp25654
sg26
(lp25655
sg30
(lp25656
sg287
(lp25657
sg74
(lp25658
sg145
(lp25659
sg256
(lp25660
sg76
(lp25661
sg118
(lp25662
sg295
(lp25663
sg183
(lp25664
sg59
(lp25665
sg484
(lp25666
sg38
(lp25667
sg83
(lp25668
sg114
(lp25669
sg124
(lp25670
sg42
(lp25671
I193
asg87
(lp25672
sg89
(lp25673
sg91
(lp25674
sg245
(lp25675
sg94
(lp25676
sg96
(lp25677
sg18
(lp25678
sg99
(lp25679
sg313
(lp25680
sg44
(lp25681
sg350
(lp25682
sg174
(lp25683
sg293
(lp25684
sg178
(lp25685
sg429
(lp25686
sg104
(lp25687
sg108
(lp25688
sg110
(lp25689
sg20
(lp25690
sg52
(lp25691
sg22
(lp25692
sg116
(lp25693
sg438
(lp25694
sg32
(lp25695
sg332
(lp25696
sg121
(lp25697
sg4
(lp25698
sg6
(lp25699
sg8
(lp25700
sg34
(lp25701
sg221
(lp25702
sg460
(lp25703
sg235
(lp25704
sg126
(lp25705
sg281
(lp25706
sg10
(lp25707
sg40
(lp25708
sg344
(lp25709
sg63
(lp25710
sg223
(lp25711
sg128
(lp25712
sg130
(lp25713
sg132
(lp25714
sg14
(lp25715
sg16
(lp25716
sg135
(lp25717
sg50
(lp25718
sg138
(lp25719
sg140
(lp25720
sg354
(lp25721
ssS'amount'
p25722
(dp25723
g68
(lp25724
sg26
(lp25725
sg283
(lp25726
sg287
(lp25727
sg74
(lp25728
sg145
(lp25729
sg80
(lp25730
sg295
(lp25731
sg183
(lp25732
sg83
(lp25733
sg85
(lp25734
sg303
(lp25735
sg94
(lp25736
sg96
(lp25737
sg18
(lp25738
sg99
(lp25739
sg223
(lp25740
sg178
(lp25741
sg63
(lp25742
sg114
(lp25743
sg216
(lp25744
sg438
(lp25745
I406
asg318
(lp25746
sg121
(lp25747
sg22
(lp25748
sg6
(lp25749
sg235
(lp25750
sg34
(lp25751
sg124
(lp25752
sg126
(lp25753
sg130
(lp25754
sg132
(lp25755
sg50
(lp25756
sg138
(lp25757
ssS'ijri'
p25758
(dp25759
g18
(lp25760
I1113
assS'lecun'
p25761
(dp25762
g132
(lp25763
I3759
asg94
(lp25764
sg36
(lp25765
sg223
(lp25766
sg183
(lp25767
ssS'descend'
p25768
(dp25769
g42
(lp25770
I1494
assS'jerri'
p25771
(dp25772
g10
(lp25773
I2621
assS'wheel'
p25774
(dp25775
g256
(lp25776
I1571
assS'mii'
p25777
(dp25778
g138
(lp25779
I404
assS'nih'
p25780
(dp25781
g70
(lp25782
I12
assS'nii'
p25783
(dp25784
g306
(lp25785
I2180
assS'nil'
p25786
(dp25787
g102
(lp25788
sg114
(lp25789
sg130
(lp25790
I2438
assS'rail'
p25791
(dp25792
g14
(lp25793
I3635
assS'rain'
p25794
(dp25795
g89
(lp25796
I2426
assS'hand'
p25797
(dp25798
g283
(lp25799
sg70
(lp25800
sg36
(lp25801
sg176
(lp25802
sg293
(lp25803
sg183
(lp25804
sg59
(lp25805
sg484
(lp25806
sg303
(lp25807
sg42
(lp25808
I365
asg306
(lp25809
sg87
(lp25810
sg89
(lp25811
sg91
(lp25812
sg245
(lp25813
sg96
(lp25814
sg18
(lp25815
sg221
(lp25816
sg350
(lp25817
sg32
(lp25818
sg104
(lp25819
sg63
(lp25820
sg52
(lp25821
sg114
(lp25822
sg216
(lp25823
sg329
(lp25824
sg440
(lp25825
sg332
(lp25826
sg178
(lp25827
sg99
(lp25828
sg384
(lp25829
sg124
(lp25830
sg10
(lp25831
sg130
(lp25832
sg132
(lp25833
sg135
(lp25834
sg50
(lp25835
sg138
(lp25836
sg140
(lp25837
ssS'fuse'
p25838
(dp25839
g332
(lp25840
I1914
assS'nis'
p25841
(dp25842
g78
(lp25843
I30
assS'nip'
p25844
(dp25845
g176
(lp25846
sg235
(lp25847
sg36
(lp25848
sg126
(lp25849
sg72
(lp25850
sg87
(lp25851
sg89
(lp25852
sg128
(lp25853
sg135
(lp25854
sg110
(lp25855
sg140
(lp25856
sg354
(lp25857
I2955
assS'nit'
p25858
(dp25859
g68
(lp25860
I2099
assS'mil'
p25861
(dp25862
g102
(lp25863
sg130
(lp25864
I2295
assS'recursi'
p25865
(dp25866
g42
(lp25867
I728
assS'vermont'
p25868
(dp25869
g281
(lp25870
I22
assS'kept'
p25871
(dp25872
g30
(lp25873
sg116
(lp25874
sg70
(lp25875
sg4
(lp25876
sg36
(lp25877
sg126
(lp25878
sg102
(lp25879
sg350
(lp25880
sg140
(lp25881
I1661
asg114
(lp25882
ssS'undesir'
p25883
(dp25884
g110
(lp25885
I402
assS'scenario'
p25886
(dp25887
g230
(lp25888
sg38
(lp25889
sg89
(lp25890
I1594
asg223
(lp25891
sg235
(lp25892
ssS'hypothes'
p25893
(dp25894
g440
(lp25895
sg4
(lp25896
sg6
(lp25897
sg344
(lp25898
sg183
(lp25899
sg80
(lp25900
sg85
(lp25901
sg303
(lp25902
sg18
(lp25903
sg138
(lp25904
I1376
asg223
(lp25905
sg26
(lp25906
ssS'hypothet'
p25907
(dp25908
g102
(lp25909
I1590
asg350
(lp25910
ssS'contact'
p25911
(dp25912
g14
(lp25913
sg16
(lp25914
I2024
asg176
(lp25915
sg460
(lp25916
ssS'wherebi'
p25917
(dp25918
g118
(lp25919
I897
assS'thg'
p25920
(dp25921
g34
(lp25922
I1357
assS'the'
p25923
(dp25924
g80
(lp25925
sg293
(lp25926
sg344
(lp25927
sg78
(lp25928
sg59
(lp25929
sg484
(lp25930
sg38
(lp25931
sg83
(lp25932
sg85
(lp25933
sg303
(lp25934
sg438
(lp25935
sg116
(lp25936
sg118
(lp25937
sg34
(lp25938
sg36
(lp25939
sg460
(lp25940
sg68
(lp25941
sg72
(lp25942
sg281
(lp25943
sg10
(lp25944
sg40
(lp25945
sg283
(lp25946
sg70
(lp25947
sg26
(lp25948
sg277
(lp25949
sg163
(lp25950
sg89
(lp25951
sg91
(lp25952
sg12
(lp25953
sg94
(lp25954
sg96
(lp25955
sg48
(lp25956
sg99
(lp25957
sg313
(lp25958
sg44
(lp25959
sg149
(lp25960
sg429
(lp25961
sg102
(lp25962
sg104
(lp25963
sg106
(lp25964
sg108
(lp25965
sg110
(lp25966
sg63
(lp25967
sg52
(lp25968
sg114
(lp25969
sg128
(lp25970
sg130
(lp25971
sg132
(lp25972
sg14
(lp25973
sg16
(lp25974
sg135
(lp25975
sg50
(lp25976
sg138
(lp25977
sg140
(lp25978
sg354
(lp25979
sg306
(lp25980
sg87
(lp25981
sg245
(lp25982
sg46
(lp25983
sg20
(lp25984
sg18
(lp25985
sg221
(lp25986
sg535
(lp25987
sg223
(lp25988
sg350
(lp25989
sg216
(lp25990
sg174
(lp25991
sg440
(lp25992
sg332
(lp25993
sg121
(lp25994
sg4
(lp25995
sg6
(lp25996
sg8
(lp25997
sg126
(lp25998
sg341
(lp25999
sg30
(lp26000
sg287
(lp26001
sg74
(lp26002
sg176
(lp26003
sg145
(lp26004
sg256
(lp26005
sg76
(lp26006
sg262
(lp26007
sg295
(lp26008
sg183
(lp26009
sg42
(lp26010
I37
asg230
(lp26011
sg329
(lp26012
sg32
(lp26013
sg318
(lp26014
sg178
(lp26015
sg22
(lp26016
sg181
(lp26017
sg235
(lp26018
sg384
(lp26019
sg124
(lp26020
ssS'thb'
p26021
(dp26022
g130
(lp26023
I1531
assS'boyan'
p26024
(dp26025
g132
(lp26026
I3537
asg306
(lp26027
sg89
(lp26028
ssS'medicin'
p26029
(dp26030
g174
(lp26031
sg277
(lp26032
sg344
(lp26033
sg281
(lp26034
sg91
(lp26035
sg106
(lp26036
I301
asg149
(lp26037
ssS'stringent'
p26038
(dp26039
g230
(lp26040
sg48
(lp26041
I1889
assS'bismus'
p26042
(dp26043
g48
(lp26044
I2270
assS'abiaxa'
p26045
(dp26046
g8
(lp26047
I920
assS'newton'
p26048
(dp26049
g230
(lp26050
sg295
(lp26051
sg183
(lp26052
sg34
(lp26053
sg8
(lp26054
I1337
assS'elman'
p26055
(dp26056
g116
(lp26057
sg80
(lp26058
sg36
(lp26059
sg83
(lp26060
sg128
(lp26061
sg132
(lp26062
I3869
assS'kalo'
p26063
(dp26064
g354
(lp26065
I1477
assS'farther'
p26066
(dp26067
g293
(lp26068
sg262
(lp26069
sg89
(lp26070
sg149
(lp26071
I1737
assS'curnrn'
p26072
(dp26073
g350
(lp26074
I1193
assS'jijrg'
p26075
(dp26076
g354
(lp26077
I1411
assS'jose'
p26078
(dp26079
g78
(lp26080
sg40
(lp26081
I2419
assS'zook'
p26082
(dp26083
g176
(lp26084
I2598
assS'boot'
p26085
(dp26086
g484
(lp26087
I1164
assS'jvmjv'
p26088
(dp26089
g130
(lp26090
I2367
assS'weiss'
p26091
(dp26092
g74
(lp26093
I3012
assS'spread'
p26094
(dp26095
g174
(lp26096
sg74
(lp26097
sg332
(lp26098
sg76
(lp26099
sg8
(lp26100
sg118
(lp26101
sg176
(lp26102
sg91
(lp26103
sg303
(lp26104
sg149
(lp26105
I1011
assS'board'
p26106
(dp26107
g121
(lp26108
sg26
(lp26109
sg181
(lp26110
sg59
(lp26111
sg10
(lp26112
sg132
(lp26113
sg14
(lp26114
I4074
asg20
(lp26115
sg223
(lp26116
ssS'fedorov'
p26117
(dp26118
g313
(lp26119
sg354
(lp26120
I3151
assS'lunless'
p26121
(dp26122
g313
(lp26123
I653
assS'cttjijxmn'
p26124
(dp26125
g149
(lp26126
I1083
assS'plasma'
p26127
(dp26128
g14
(lp26129
sg16
(lp26130
I7
assS'midrang'
p26131
(dp26132
g116
(lp26133
I1497
assS'rnn'
p26134
(dp26135
g87
(lp26136
sg128
(lp26137
I124
assS'mayb'
p26138
(dp26139
g26
(lp26140
sg149
(lp26141
I592
assS'premis'
p26142
(dp26143
g34
(lp26144
I1650
assS'rne'
p26145
(dp26146
g18
(lp26147
I859
assS'rnd'
p26148
(dp26149
g128
(lp26150
I1515
assS'fusion'
p26151
(dp26152
g14
(lp26153
sg16
(lp26154
I61
asg277
(lp26155
ssS'archetyp'
p26156
(dp26157
g128
(lp26158
I176
assS'rnr'
p26159
(dp26160
g332
(lp26161
I1095
assS'lcl'
p26162
(dp26163
g68
(lp26164
I2831
assS'capa'
p26165
(dp26166
g174
(lp26167
I2404
assS'lch'
p26168
(dp26169
g384
(lp26170
I592
assS'riddl'
p26171
(dp26172
g74
(lp26173
I1709
assS'stroke'
p26174
(dp26175
g178
(lp26176
I668
asg63
(lp26177
sg76
(lp26178
sg114
(lp26179
ssS'graviti'
p26180
(dp26181
g303
(lp26182
sg350
(lp26183
I902
assS'coolen'
p26184
(dp26185
g384
(lp26186
I15
assS'gandhi'
p26187
(dp26188
g83
(lp26189
I2773
assS'qxt'
p26190
(dp26191
g230
(lp26192
I1055
assS'amorph'
p26193
(dp26194
g14
(lp26195
I2675
asg68
(lp26196
ssS'isotropi'
p26197
(dp26198
g48
(lp26199
I988
assS'mapclus'
p26200
(dp26201
g74
(lp26202
I712
assS'oxid'
p26203
(dp26204
g256
(lp26205
I990
assS'dimitri'
p26206
(dp26207
g8
(lp26208
I9
assS'manual'
p26209
(dp26210
g76
(lp26211
sg78
(lp26212
sg59
(lp26213
sg10
(lp26214
sg91
(lp26215
sg94
(lp26216
I548
assS'percentag'
p26217
(dp26218
g30
(lp26219
sg329
(lp26220
sg121
(lp26221
sg118
(lp26222
sg34
(lp26223
sg36
(lp26224
sg83
(lp26225
sg42
(lp26226
I2650
asg128
(lp26227
sg94
(lp26228
sg48
(lp26229
sg221
(lp26230
sg63
(lp26231
sg44
(lp26232
ssS'born'
p26233
(dp26234
g135
(lp26235
I488
asg85
(lp26236
ssS'flatten'
p26237
(dp26238
g329
(lp26239
I1075
assS'rnxm'
p26240
(dp26241
g306
(lp26242
I1316
assS'dupatch'
p26243
(dp26244
g83
(lp26245
I2904
assS'midplan'
p26246
(dp26247
g14
(lp26248
sg16
(lp26249
I957
assS'yki'
p26250
(dp26251
g121
(lp26252
I376
assS'ofneurosci'
p26253
(dp26254
g80
(lp26255
I2484
assS'overcom'
p26256
(dp26257
g332
(lp26258
sg4
(lp26259
sg8
(lp26260
sg163
(lp26261
sg306
(lp26262
sg87
(lp26263
sg132
(lp26264
I3407
assS'peen'
p26265
(dp26266
g104
(lp26267
I383
assS'plp'
p26268
(dp26269
g440
(lp26270
sg121
(lp26271
I1223
assS'pose'
p26272
(dp26273
g230
(lp26274
sg332
(lp26275
sg277
(lp26276
sg181
(lp26277
sg293
(lp26278
sg83
(lp26279
sg138
(lp26280
I1577
assS'confer'
p26281
(dp26282
g124
(lp26283
sg277
(lp26284
sg30
(lp26285
sg176
(lp26286
sg80
(lp26287
sg78
(lp26288
sg12
(lp26289
sg223
(lp26290
sg149
(lp26291
sg130
(lp26292
sg108
(lp26293
sg110
(lp26294
sg230
(lp26295
sg440
(lp26296
sg318
(lp26297
sg121
(lp26298
sg4
(lp26299
sg68
(lp26300
sg44
(lp26301
sg183
(lp26302
sg132
(lp26303
sg14
(lp26304
sg50
(lp26305
sg140
(lp26306
I252
assS'repositori'
p26307
(dp26308
g484
(lp26309
sg91
(lp26310
I2802
assS'ditori'
p26311
(dp26312
g174
(lp26313
I2754
assS'peer'
p26314
(dp26315
g295
(lp26316
sg183
(lp26317
sg50
(lp26318
I231
assS'epfl'
p26319
(dp26320
g174
(lp26321
I2544
assS'post'
p26322
(dp26323
g176
(lp26324
sg6
(lp26325
sg384
(lp26326
sg63
(lp26327
sg12
(lp26328
sg14
(lp26329
sg106
(lp26330
I736
asg99
(lp26331
sg16
(lp26332
sg52
(lp26333
sg114
(lp26334
ssS'exapl'
p26335
(dp26336
g178
(lp26337
I1629
assS'oe'
p26338
(dp26339
g181
(lp26340
sg350
(lp26341
I513
assS'obd'
p26342
(dp26343
g344
(lp26344
I2599
assS'literatur'
p26345
(dp26346
g30
(lp26347
sg32
(lp26348
sg68
(lp26349
sg178
(lp26350
sg4
(lp26351
sg76
(lp26352
sg344
(lp26353
sg124
(lp26354
sg72
(lp26355
sg341
(lp26356
sg85
(lp26357
sg74
(lp26358
sg306
(lp26359
sg40
(lp26360
sg102
(lp26361
sg108
(lp26362
I388
asg99
(lp26363
sg350
(lp26364
ssS'horizon'
p26365
(dp26366
g132
(lp26367
I2796
asg306
(lp26368
sg83
(lp26369
sg293
(lp26370
ssS'multuerv'
p26371
(dp26372
g83
(lp26373
I2895
assS'xu'
p26374
(dp26375
g72
(lp26376
sg281
(lp26377
I13
assS'dilemma'
p26378
(dp26379
g484
(lp26380
sg140
(lp26381
I3146
assS'llms'
p26382
(dp26383
g59
(lp26384
I1247
assS'float'
p26385
(dp26386
g14
(lp26387
I2876
asg20
(lp26388
sg10
(lp26389
ssS'profession'
p26390
(dp26391
g94
(lp26392
I240
asg114
(lp26393
ssS'bound'
p26394
(dp26395
g70
(lp26396
sg26
(lp26397
sg163
(lp26398
sg281
(lp26399
sg287
(lp26400
sg145
(lp26401
sg344
(lp26402
sg85
(lp26403
sg303
(lp26404
sg306
(lp26405
sg89
(lp26406
sg12
(lp26407
sg46
(lp26408
sg18
(lp26409
sg221
(lp26410
sg535
(lp26411
sg116
(lp26412
sg102
(lp26413
sg110
(lp26414
sg52
(lp26415
sg230
(lp26416
sg34
(lp26417
sg460
(lp26418
sg68
(lp26419
sg341
(lp26420
sg40
(lp26421
sg130
(lp26422
sg132
(lp26423
sg50
(lp26424
sg140
(lp26425
I2482
assS'hasselmo'
p26426
(dp26427
g124
(lp26428
sg262
(lp26429
sg350
(lp26430
I2941
assS'lewi'
p26431
(dp26432
g99
(lp26433
I2885
asg83
(lp26434
ssS'opportun'
p26435
(dp26436
g4
(lp26437
sg277
(lp26438
sg429
(lp26439
sg12
(lp26440
sg20
(lp26441
sg99
(lp26442
I1119
asg313
(lp26443
sg223
(lp26444
ssS'wan'
p26445
(dp26446
g230
(lp26447
sg121
(lp26448
I984
asg80
(lp26449
ssS'rahul'
p26450
(dp26451
g256
(lp26452
I2120
assS'disori'
p26453
(dp26454
g80
(lp26455
I233
assS'gherriti'
p26456
(dp26457
g132
(lp26458
I284
assS'wae'
p26459
(dp26460
g140
(lp26461
I1897
assS'nwnber'
p26462
(dp26463
g110
(lp26464
I2383
assS'hogon'
p26465
(dp26466
g48
(lp26467
I1157
assS'fizt'
p26468
(dp26469
g68
(lp26470
I2480
assS'way'
p26471
(dp26472
g70
(lp26473
sg26
(lp26474
sg277
(lp26475
sg163
(lp26476
sg460
(lp26477
sg30
(lp26478
sg74
(lp26479
sg176
(lp26480
sg76
(lp26481
sg293
(lp26482
sg295
(lp26483
sg183
(lp26484
sg59
(lp26485
sg484
(lp26486
sg38
(lp26487
sg83
(lp26488
sg85
(lp26489
sg303
(lp26490
sg42
(lp26491
I947
asg306
(lp26492
sg87
(lp26493
sg89
(lp26494
sg91
(lp26495
sg12
(lp26496
sg94
(lp26497
sg48
(lp26498
sg99
(lp26499
sg313
(lp26500
sg44
(lp26501
sg350
(lp26502
sg116
(lp26503
sg174
(lp26504
sg32
(lp26505
sg429
(lp26506
sg318
(lp26507
sg46
(lp26508
sg102
(lp26509
sg104
(lp26510
sg108
(lp26511
sg110
(lp26512
sg63
(lp26513
sg52
(lp26514
sg22
(lp26515
sg230
(lp26516
sg438
(lp26517
sg440
(lp26518
sg332
(lp26519
sg4
(lp26520
sg181
(lp26521
sg235
(lp26522
sg221
(lp26523
sg384
(lp26524
sg68
(lp26525
sg126
(lp26526
sg341
(lp26527
sg535
(lp26528
sg223
(lp26529
sg128
(lp26530
sg78
(lp26531
sg132
(lp26532
sg14
(lp26533
sg16
(lp26534
sg135
(lp26535
sg138
(lp26536
sg140
(lp26537
sg354
(lp26538
ssS'wav'
p26539
(dp26540
g22
(lp26541
I708
assS'wat'
p26542
(dp26543
g341
(lp26544
I485
assS'was'
p26545
(dp26546
g80
(lp26547
sg293
(lp26548
sg344
(lp26549
sg78
(lp26550
sg59
(lp26551
sg484
(lp26552
sg38
(lp26553
sg83
(lp26554
sg85
(lp26555
sg303
(lp26556
sg438
(lp26557
sg116
(lp26558
sg118
(lp26559
sg34
(lp26560
sg36
(lp26561
sg460
(lp26562
sg68
(lp26563
sg72
(lp26564
sg281
(lp26565
sg10
(lp26566
sg40
(lp26567
sg283
(lp26568
sg70
(lp26569
sg26
(lp26570
sg277
(lp26571
sg163
(lp26572
sg89
(lp26573
sg91
(lp26574
sg12
(lp26575
sg94
(lp26576
sg96
(lp26577
sg48
(lp26578
sg99
(lp26579
sg313
(lp26580
sg44
(lp26581
sg149
(lp26582
sg429
(lp26583
sg102
(lp26584
sg104
(lp26585
sg106
(lp26586
sg108
(lp26587
sg110
(lp26588
sg63
(lp26589
sg52
(lp26590
sg114
(lp26591
sg128
(lp26592
sg130
(lp26593
sg132
(lp26594
sg14
(lp26595
sg16
(lp26596
sg135
(lp26597
sg50
(lp26598
sg138
(lp26599
sg140
(lp26600
sg354
(lp26601
sg306
(lp26602
sg87
(lp26603
sg245
(lp26604
sg46
(lp26605
sg20
(lp26606
sg18
(lp26607
sg221
(lp26608
sg223
(lp26609
sg350
(lp26610
sg216
(lp26611
sg174
(lp26612
sg440
(lp26613
sg332
(lp26614
sg121
(lp26615
sg4
(lp26616
sg6
(lp26617
sg8
(lp26618
sg126
(lp26619
sg30
(lp26620
sg287
(lp26621
sg74
(lp26622
sg176
(lp26623
sg145
(lp26624
sg256
(lp26625
sg76
(lp26626
sg262
(lp26627
sg295
(lp26628
sg183
(lp26629
sg42
(lp26630
I2266
asg230
(lp26631
sg329
(lp26632
sg32
(lp26633
sg318
(lp26634
sg22
(lp26635
sg181
(lp26636
sg235
(lp26637
sg384
(lp26638
sg124
(lp26639
ssS'lowest'
p26640
(dp26641
g283
(lp26642
sg76
(lp26643
sg8
(lp26644
sg124
(lp26645
sg87
(lp26646
sg110
(lp26647
sg140
(lp26648
I2127
assS'hyperpolaris'
p26649
(dp26650
g256
(lp26651
I364
assS'kroneck'
p26652
(dp26653
g108
(lp26654
I542
asg235
(lp26655
ssS'somehow'
p26656
(dp26657
g70
(lp26658
I186
assS'feinstein'
p26659
(dp26660
g438
(lp26661
I2481
assS'cedex'
p26662
(dp26663
g287
(lp26664
I22
assS'lautrup'
p26665
(dp26666
g26
(lp26667
I3194
assS'true'
p26668
(dp26669
g124
(lp26670
sg277
(lp26671
sg30
(lp26672
sg74
(lp26673
sg145
(lp26674
sg76
(lp26675
sg293
(lp26676
sg344
(lp26677
sg85
(lp26678
sg89
(lp26679
sg46
(lp26680
sg48
(lp26681
sg221
(lp26682
sg52
(lp26683
sg114
(lp26684
sg318
(lp26685
sg121
(lp26686
sg80
(lp26687
sg235
(lp26688
sg36
(lp26689
sg460
(lp26690
sg68
(lp26691
sg72
(lp26692
sg281
(lp26693
sg128
(lp26694
sg130
(lp26695
sg135
(lp26696
sg140
(lp26697
sg354
(lp26698
I660
assS'reset'
p26699
(dp26700
g174
(lp26701
sg80
(lp26702
sg262
(lp26703
sg34
(lp26704
sg42
(lp26705
I1463
asg104
(lp26706
sg20
(lp26707
ssS'retin'
p26708
(dp26709
g118
(lp26710
sg70
(lp26711
sg256
(lp26712
sg80
(lp26713
sg303
(lp26714
sg245
(lp26715
sg149
(lp26716
I550
assS'coset'
p26717
(dp26718
g32
(lp26719
I2195
assS'braver'
p26720
(dp26721
g4
(lp26722
I10
assS'emin'
p26723
(dp26724
g34
(lp26725
I1335
asg85
(lp26726
ssS'inquir'
p26727
(dp26728
g42
(lp26729
I2495
asg130
(lp26730
ssS'retic'
p26731
(dp26732
g63
(lp26733
I162
assS'maximum'
p26734
(dp26735
g283
(lp26736
sg26
(lp26737
sg72
(lp26738
sg40
(lp26739
sg74
(lp26740
sg176
(lp26741
sg76
(lp26742
sg484
(lp26743
sg83
(lp26744
sg306
(lp26745
sg87
(lp26746
sg89
(lp26747
sg91
(lp26748
sg245
(lp26749
sg94
(lp26750
sg20
(lp26751
sg221
(lp26752
sg313
(lp26753
sg329
(lp26754
sg12
(lp26755
sg318
(lp26756
sg102
(lp26757
sg96
(lp26758
sg52
(lp26759
sg116
(lp26760
sg438
(lp26761
I680
asg440
(lp26762
sg332
(lp26763
sg178
(lp26764
sg36
(lp26765
sg460
(lp26766
sg124
(lp26767
sg126
(lp26768
sg281
(lp26769
sg10
(lp26770
sg535
(lp26771
sg130
(lp26772
sg14
(lp26773
sg135
(lp26774
sg138
(lp26775
sg354
(lp26776
ssS'anel'
p26777
(dp26778
g108
(lp26779
I216
assS'crystal'
p26780
(dp26781
g283
(lp26782
I723
assS'inaccur'
p26783
(dp26784
g132
(lp26785
I1629
asg145
(lp26786
sg460
(lp26787
sg89
(lp26788
sg76
(lp26789
ssS'anew'
p26790
(dp26791
g74
(lp26792
I62
assS'absenc'
p26793
(dp26794
g216
(lp26795
sg74
(lp26796
sg318
(lp26797
sg4
(lp26798
sg6
(lp26799
sg78
(lp26800
sg460
(lp26801
sg484
(lp26802
sg429
(lp26803
sg89
(lp26804
sg91
(lp26805
sg104
(lp26806
sg108
(lp26807
I1414
assS'emit'
p26808
(dp26809
g174
(lp26810
I1266
asg36
(lp26811
sg332
(lp26812
sg262
(lp26813
ssS'hillclimb'
p26814
(dp26815
g313
(lp26816
sg354
(lp26817
I2653
assS'repartit'
p26818
(dp26819
g30
(lp26820
I2527
assS'abstract'
p26821
(dp26822
g80
(lp26823
sg293
(lp26824
sg344
(lp26825
sg78
(lp26826
sg59
(lp26827
sg484
(lp26828
sg38
(lp26829
sg83
(lp26830
sg303
(lp26831
sg438
(lp26832
sg116
(lp26833
sg118
(lp26834
sg34
(lp26835
sg36
(lp26836
sg460
(lp26837
sg68
(lp26838
sg72
(lp26839
sg281
(lp26840
sg10
(lp26841
sg40
(lp26842
sg283
(lp26843
sg70
(lp26844
sg26
(lp26845
sg277
(lp26846
sg163
(lp26847
sg89
(lp26848
sg91
(lp26849
sg12
(lp26850
sg94
(lp26851
sg96
(lp26852
sg48
(lp26853
sg99
(lp26854
sg313
(lp26855
sg44
(lp26856
sg149
(lp26857
sg429
(lp26858
sg102
(lp26859
sg104
(lp26860
sg106
(lp26861
sg108
(lp26862
sg110
(lp26863
sg63
(lp26864
sg52
(lp26865
sg114
(lp26866
sg128
(lp26867
sg130
(lp26868
sg132
(lp26869
sg14
(lp26870
sg16
(lp26871
sg135
(lp26872
sg50
(lp26873
sg138
(lp26874
sg140
(lp26875
sg354
(lp26876
sg306
(lp26877
sg87
(lp26878
sg245
(lp26879
sg46
(lp26880
sg20
(lp26881
sg18
(lp26882
sg221
(lp26883
sg535
(lp26884
sg223
(lp26885
sg350
(lp26886
sg216
(lp26887
sg174
(lp26888
sg440
(lp26889
sg332
(lp26890
sg121
(lp26891
sg4
(lp26892
sg6
(lp26893
sg8
(lp26894
sg126
(lp26895
sg341
(lp26896
sg30
(lp26897
sg287
(lp26898
sg74
(lp26899
sg176
(lp26900
sg145
(lp26901
sg256
(lp26902
sg76
(lp26903
sg262
(lp26904
sg295
(lp26905
sg183
(lp26906
sg42
(lp26907
I19
asg230
(lp26908
sg329
(lp26909
sg32
(lp26910
sg318
(lp26911
sg178
(lp26912
sg22
(lp26913
sg181
(lp26914
sg235
(lp26915
sg384
(lp26916
sg124
(lp26917
ssS'reexamin'
p26918
(dp26919
g350
(lp26920
I3099
assS'flaw'
p26921
(dp26922
g52
(lp26923
I872
assS'harvil'
p26924
(dp26925
g59
(lp26926
I3445
assS'fuster'
p26927
(dp26928
g4
(lp26929
I351
assS'minheh'
p26930
(dp26931
g85
(lp26932
I1341
assS'face'
p26933
(dp26934
g176
(lp26935
sg70
(lp26936
sg181
(lp26937
sg293
(lp26938
sg460
(lp26939
sg68
(lp26940
sg126
(lp26941
sg83
(lp26942
sg303
(lp26943
sg42
(lp26944
I2338
asg12
(lp26945
sg87
(lp26946
sg89
(lp26947
sg132
(lp26948
sg99
(lp26949
sg138
(lp26950
sg223
(lp26951
ssS'ltll'
p26952
(dp26953
g440
(lp26954
I505
assS'interf'
p26955
(dp26956
g99
(lp26957
sg50
(lp26958
I1023
assS'tesi'
p26959
(dp26960
g341
(lp26961
I2874
assS'ljtjif'
p26962
(dp26963
g104
(lp26964
I1599
assS'circulatori'
p26965
(dp26966
g221
(lp26967
I1907
assS'dunwiddi'
p26968
(dp26969
g106
(lp26970
I2803
assS'doya'
p26971
(dp26972
g116
(lp26973
sg18
(lp26974
I13
assS'jone'
p26975
(dp26976
g59
(lp26977
sg124
(lp26978
I1295
assS'test'
p26979
(dp26980
g124
(lp26981
sg70
(lp26982
sg78
(lp26983
sg277
(lp26984
sg72
(lp26985
sg80
(lp26986
sg283
(lp26987
sg85
(lp26988
sg303
(lp26989
sg30
(lp26990
sg145
(lp26991
sg256
(lp26992
sg76
(lp26993
sg262
(lp26994
sg295
(lp26995
sg183
(lp26996
sg59
(lp26997
sg484
(lp26998
sg83
(lp26999
sg114
(lp27000
sg63
(lp27001
sg87
(lp27002
sg89
(lp27003
sg91
(lp27004
sg12
(lp27005
sg94
(lp27006
sg96
(lp27007
sg48
(lp27008
sg99
(lp27009
sg313
(lp27010
sg44
(lp27011
sg350
(lp27012
sg230
(lp27013
sg245
(lp27014
sg68
(lp27015
sg46
(lp27016
sg178
(lp27017
sg106
(lp27018
sg110
(lp27019
sg20
(lp27020
sg52
(lp27021
sg22
(lp27022
sg216
(lp27023
sg438
(lp27024
I1888
asg440
(lp27025
sg18
(lp27026
sg121
(lp27027
sg4
(lp27028
sg181
(lp27029
sg8
(lp27030
sg34
(lp27031
sg221
(lp27032
sg460
(lp27033
sg235
(lp27034
sg126
(lp27035
sg281
(lp27036
sg344
(lp27037
sg223
(lp27038
sg128
(lp27039
sg36
(lp27040
sg132
(lp27041
sg14
(lp27042
sg16
(lp27043
sg135
(lp27044
sg138
(lp27045
sg140
(lp27046
sg354
(lp27047
ssS'kullback'
p27048
(dp27049
g329
(lp27050
sg36
(lp27051
sg72
(lp27052
sg130
(lp27053
I1016
assS'realiti'
p27054
(dp27055
g59
(lp27056
sg32
(lp27057
sg318
(lp27058
I2393
asg70
(lp27059
sg384
(lp27060
ssS'mwlw'
p27061
(dp27062
g306
(lp27063
I1928
assS'insensit'
p27064
(dp27065
g174
(lp27066
sg85
(lp27067
sg181
(lp27068
sg44
(lp27069
I2732
asg235
(lp27070
ssS'hiii'
p27071
(dp27072
g6
(lp27073
I1787
assS'abscissa'
p27074
(dp27075
g163
(lp27076
I1247
assS'bedk'
p27077
(dp27078
g10
(lp27079
I38
assS'prepot'
p27080
(dp27081
g4
(lp27082
I1126
assS'concept'
p27083
(dp27084
g287
(lp27085
sg74
(lp27086
sg332
(lp27087
sg70
(lp27088
sg163
(lp27089
sg344
(lp27090
sg176
(lp27091
sg42
(lp27092
I221
asg318
(lp27093
sg91
(lp27094
sg46
(lp27095
sg20
(lp27096
sg535
(lp27097
sg223
(lp27098
sg354
(lp27099
ssS'erfolgsorientiert'
p27100
(dp27101
g132
(lp27102
I3707
assS'global'
p27103
(dp27104
g68
(lp27105
sg26
(lp27106
sg30
(lp27107
sg295
(lp27108
sg183
(lp27109
sg83
(lp27110
sg42
(lp27111
I1628
asg89
(lp27112
sg245
(lp27113
sg46
(lp27114
sg20
(lp27115
sg48
(lp27116
sg535
(lp27117
sg44
(lp27118
sg329
(lp27119
sg32
(lp27120
sg102
(lp27121
sg96
(lp27122
sg52
(lp27123
sg216
(lp27124
sg438
(lp27125
sg440
(lp27126
sg332
(lp27127
sg121
(lp27128
sg181
(lp27129
sg8
(lp27130
sg34
(lp27131
sg235
(lp27132
sg341
(lp27133
sg50
(lp27134
sg138
(lp27135
sg354
(lp27136
ssS'datum'
p27137
(dp27138
g104
(lp27139
sg130
(lp27140
I286
assS'afosr'
p27141
(dp27142
g287
(lp27143
sg118
(lp27144
sg68
(lp27145
sg429
(lp27146
sg8
(lp27147
I2377
assS'trigram'
p27148
(dp27149
g87
(lp27150
I2047
assS'supplement'
p27151
(dp27152
g216
(lp27153
I2356
asg30
(lp27154
ssS'combinin'
p27155
(dp27156
g138
(lp27157
I3378
assS'computationallyeffici'
p27158
(dp27159
g8
(lp27160
I317
assS'technometr'
p27161
(dp27162
g354
(lp27163
I2976
assS'middl'
p27164
(dp27165
g26
(lp27166
sg174
(lp27167
sg74
(lp27168
sg70
(lp27169
sg4
(lp27170
sg76
(lp27171
sg118
(lp27172
sg256
(lp27173
sg484
(lp27174
sg89
(lp27175
sg132
(lp27176
sg94
(lp27177
sg106
(lp27178
I629
asg44
(lp27179
sg22
(lp27180
ssS'grape'
p27181
(dp27182
g181
(lp27183
I517
assS'transduc'
p27184
(dp27185
g99
(lp27186
I319
assS'graph'
p27187
(dp27188
g287
(lp27189
sg59
(lp27190
sg121
(lp27191
sg4
(lp27192
sg262
(lp27193
sg183
(lp27194
sg384
(lp27195
sg145
(lp27196
sg293
(lp27197
sg429
(lp27198
sg78
(lp27199
sg14
(lp27200
sg16
(lp27201
I2296
asg18
(lp27202
sg110
(lp27203
sg460
(lp27204
sg44
(lp27205
ssS'flash'
p27206
(dp27207
g118
(lp27208
sg32
(lp27209
sg350
(lp27210
I2458
assS'rightmost'
p27211
(dp27212
g132
(lp27213
I1316
asg118
(lp27214
sg303
(lp27215
ssS'tracker'
p27216
(dp27217
g59
(lp27218
I3353
asg293
(lp27219
ssS'rhythm'
p27220
(dp27221
g135
(lp27222
I168
assS'psom'
p27223
(dp27224
g59
(lp27225
I1366
assS'krste'
p27226
(dp27227
g10
(lp27228
I18
assS'brown'
p27229
(dp27230
g438
(lp27231
I28
asg332
(lp27232
sg178
(lp27233
sg174
(lp27234
sg484
(lp27235
sg106
(lp27236
sg52
(lp27237
ssS'lvaldi'
p27238
(dp27239
g99
(lp27240
I19
assS'kitten'
p27241
(dp27242
g438
(lp27243
I1154
asg106
(lp27244
sg149
(lp27245
ssS'octob'
p27246
(dp27247
g34
(lp27248
sg429
(lp27249
sg108
(lp27250
I2613
assS'tegrnenti'
p27251
(dp27252
g350
(lp27253
I1198
assS'jushfi'
p27254
(dp27255
g72
(lp27256
I3491
assS'frorn'
p27257
(dp27258
g350
(lp27259
I882
assS'cunningham'
p27260
(dp27261
g74
(lp27262
I3256
assS'valuabl'
p27263
(dp27264
g36
(lp27265
sg59
(lp27266
sg341
(lp27267
sg535
(lp27268
sg50
(lp27269
sg63
(lp27270
sg140
(lp27271
I3076
assS'bring'
p27272
(dp27273
g440
(lp27274
sg176
(lp27275
sg277
(lp27276
sg38
(lp27277
sg32
(lp27278
sg102
(lp27279
I1396
assS'gun'
p27280
(dp27281
g295
(lp27282
I57
asg183
(lp27283
ssS'ffts'
p27284
(dp27285
g78
(lp27286
I2289
assS'p'
p27287
(dp27288
g80
(lp27289
sg293
(lp27290
sg344
(lp27291
sg78
(lp27292
sg59
(lp27293
sg484
(lp27294
sg38
(lp27295
sg83
(lp27296
sg85
(lp27297
sg303
(lp27298
sg438
(lp27299
I2392
asg116
(lp27300
sg118
(lp27301
sg34
(lp27302
sg36
(lp27303
sg460
(lp27304
sg68
(lp27305
sg72
(lp27306
sg281
(lp27307
sg40
(lp27308
sg283
(lp27309
sg70
(lp27310
sg26
(lp27311
sg277
(lp27312
sg163
(lp27313
sg89
(lp27314
sg91
(lp27315
sg12
(lp27316
sg96
(lp27317
sg48
(lp27318
sg99
(lp27319
sg313
(lp27320
sg44
(lp27321
sg149
(lp27322
sg429
(lp27323
sg102
(lp27324
sg104
(lp27325
sg106
(lp27326
sg108
(lp27327
sg63
(lp27328
sg52
(lp27329
sg114
(lp27330
sg128
(lp27331
sg130
(lp27332
sg132
(lp27333
sg14
(lp27334
sg16
(lp27335
sg135
(lp27336
sg50
(lp27337
sg138
(lp27338
sg140
(lp27339
sg354
(lp27340
sg306
(lp27341
sg87
(lp27342
sg245
(lp27343
sg46
(lp27344
sg18
(lp27345
sg221
(lp27346
sg223
(lp27347
sg350
(lp27348
sg174
(lp27349
sg440
(lp27350
sg332
(lp27351
sg4
(lp27352
sg6
(lp27353
sg126
(lp27354
sg341
(lp27355
sg30
(lp27356
sg287
(lp27357
sg74
(lp27358
sg176
(lp27359
sg145
(lp27360
sg256
(lp27361
sg76
(lp27362
sg262
(lp27363
sg295
(lp27364
sg183
(lp27365
sg230
(lp27366
sg329
(lp27367
sg32
(lp27368
sg318
(lp27369
sg178
(lp27370
sg181
(lp27371
sg235
(lp27372
sg384
(lp27373
sg124
(lp27374
ssS'offchip'
p27375
(dp27376
g256
(lp27377
I1497
assS'anisotropi'
p27378
(dp27379
g96
(lp27380
I1176
asg48
(lp27381
ssS'xtfx'
p27382
(dp27383
g295
(lp27384
I2452
asg183
(lp27385
ssS'upper'
p27386
(dp27387
g70
(lp27388
sg26
(lp27389
sg163
(lp27390
sg287
(lp27391
sg145
(lp27392
sg295
(lp27393
sg183
(lp27394
sg484
(lp27395
sg42
(lp27396
I2113
asg89
(lp27397
sg12
(lp27398
sg18
(lp27399
sg99
(lp27400
sg535
(lp27401
sg44
(lp27402
sg104
(lp27403
sg106
(lp27404
sg110
(lp27405
sg63
(lp27406
sg114
(lp27407
sg32
(lp27408
sg6
(lp27409
sg34
(lp27410
sg384
(lp27411
sg281
(lp27412
sg313
(lp27413
sg130
(lp27414
sg50
(lp27415
sg460
(lp27416
sg354
(lp27417
ssS'gallinari'
p27418
(dp27419
g63
(lp27420
I2993
assS'coso'
p27421
(dp27422
g32
(lp27423
I2545
assS'cosi'
p27424
(dp27425
g174
(lp27426
I166
assS'cost'
p27427
(dp27428
g283
(lp27429
sg78
(lp27430
sg277
(lp27431
sg26
(lp27432
sg145
(lp27433
sg293
(lp27434
sg295
(lp27435
sg183
(lp27436
sg59
(lp27437
sg83
(lp27438
sg85
(lp27439
sg42
(lp27440
I2177
asg306
(lp27441
sg89
(lp27442
sg91
(lp27443
sg20
(lp27444
sg223
(lp27445
sg429
(lp27446
sg102
(lp27447
sg63
(lp27448
sg52
(lp27449
sg114
(lp27450
sg230
(lp27451
sg329
(lp27452
sg318
(lp27453
sg8
(lp27454
sg72
(lp27455
sg281
(lp27456
sg10
(lp27457
sg130
(lp27458
sg132
(lp27459
sg14
(lp27460
sg135
(lp27461
sg354
(lp27462
ssS'aijaj'
p27463
(dp27464
g40
(lp27465
I2123
assS'kedar'
p27466
(dp27467
g132
(lp27468
I3602
assS'comparis'
p27469
(dp27470
g138
(lp27471
I2311
asg4
(lp27472
ssS'poppleston'
p27473
(dp27474
g429
(lp27475
I349
assS'appear'
p27476
(dp27477
g26
(lp27478
sg85
(lp27479
sg30
(lp27480
sg287
(lp27481
sg176
(lp27482
sg145
(lp27483
sg76
(lp27484
sg344
(lp27485
sg183
(lp27486
sg59
(lp27487
sg80
(lp27488
sg83
(lp27489
sg114
(lp27490
sg303
(lp27491
sg42
(lp27492
I690
asg306
(lp27493
sg89
(lp27494
sg12
(lp27495
sg94
(lp27496
sg20
(lp27497
sg48
(lp27498
sg44
(lp27499
sg350
(lp27500
sg230
(lp27501
sg118
(lp27502
sg18
(lp27503
sg245
(lp27504
sg429
(lp27505
sg46
(lp27506
sg102
(lp27507
sg104
(lp27508
sg106
(lp27509
sg108
(lp27510
sg110
(lp27511
sg63
(lp27512
sg22
(lp27513
sg216
(lp27514
sg174
(lp27515
sg32
(lp27516
sg332
(lp27517
sg121
(lp27518
sg4
(lp27519
sg6
(lp27520
sg8
(lp27521
sg34
(lp27522
sg36
(lp27523
sg460
(lp27524
sg72
(lp27525
sg281
(lp27526
sg10
(lp27527
sg223
(lp27528
sg128
(lp27529
sg132
(lp27530
sg14
(lp27531
sg16
(lp27532
sg135
(lp27533
sg354
(lp27534
ssS'hochberg'
p27535
(dp27536
g87
(lp27537
I18
asg76
(lp27538
ssS'uniform'
p27539
(dp27540
g68
(lp27541
sg277
(lp27542
sg163
(lp27543
sg72
(lp27544
sg30
(lp27545
sg287
(lp27546
sg74
(lp27547
sg176
(lp27548
sg145
(lp27549
sg76
(lp27550
sg295
(lp27551
sg183
(lp27552
sg484
(lp27553
sg38
(lp27554
sg85
(lp27555
sg303
(lp27556
sg306
(lp27557
sg149
(lp27558
sg118
(lp27559
sg429
(lp27560
sg102
(lp27561
sg108
(lp27562
sg52
(lp27563
sg329
(lp27564
sg318
(lp27565
sg22
(lp27566
sg235
(lp27567
sg34
(lp27568
sg36
(lp27569
sg460
(lp27570
sg124
(lp27571
sg126
(lp27572
sg281
(lp27573
sg128
(lp27574
sg138
(lp27575
sg140
(lp27576
sg354
(lp27577
I1961
assS'derrida'
p27578
(dp27579
g384
(lp27580
I203
assS'lnaij'
p27581
(dp27582
g460
(lp27583
I1609
assS'ancient'
p27584
(dp27585
g72
(lp27586
I796
assS'indiffer'
p27587
(dp27588
g20
(lp27589
I2273
assS'appeal'
p27590
(dp27591
g74
(lp27592
sg183
(lp27593
sg484
(lp27594
sg85
(lp27595
sg87
(lp27596
sg221
(lp27597
I152
assS'nakamura'
p27598
(dp27599
g96
(lp27600
I383
assS'genez'
p27601
(dp27602
g110
(lp27603
I1642
assS'defici'
p27604
(dp27605
g295
(lp27606
I2976
asg183
(lp27607
sg281
(lp27608
ssS'abrupt'
p27609
(dp27610
g68
(lp27611
I601
assS'gener'
p27612
(dp27613
g38
(lp27614
I2763
assS'gawn'
p27615
(dp27616
g6
(lp27617
I2190
assS'genet'
p27618
(dp27619
g34
(lp27620
sg130
(lp27621
I116
assS'ebner'
p27622
(dp27623
g438
(lp27624
I2455
assS'radiatum'
p27625
(dp27626
g106
(lp27627
I1013
assS'satisfi'
p27628
(dp27629
g230
(lp27630
sg438
(lp27631
I880
asg74
(lp27632
sg18
(lp27633
sg287
(lp27634
sg429
(lp27635
sg68
(lp27636
sg38
(lp27637
sg341
(lp27638
sg85
(lp27639
sg281
(lp27640
sg102
(lp27641
sg306
(lp27642
sg40
(lp27643
sg130
(lp27644
sg12
(lp27645
sg104
(lp27646
sg96
(lp27647
sg108
(lp27648
sg46
(lp27649
ssS'weigend'
p27650
(dp27651
g132
(lp27652
I3872
asg36
(lp27653
sg83
(lp27654
sg80
(lp27655
ssS'teacher'
p27656
(dp27657
g30
(lp27658
sg116
(lp27659
sg230
(lp27660
sg36
(lp27661
sg38
(lp27662
sg83
(lp27663
sg42
(lp27664
I3086
asg104
(lp27665
ssS'idynam'
p27666
(dp27667
g34
(lp27668
I2634
assS'eke'
p27669
(dp27670
g130
(lp27671
I1623
assS'trial'
p27672
(dp27673
g277
(lp27674
sg181
(lp27675
sg74
(lp27676
sg145
(lp27677
sg80
(lp27678
sg262
(lp27679
sg85
(lp27680
sg42
(lp27681
I2009
asg94
(lp27682
sg293
(lp27683
sg110
(lp27684
sg116
(lp27685
sg329
(lp27686
sg318
(lp27687
sg4
(lp27688
sg6
(lp27689
sg34
(lp27690
sg36
(lp27691
sg68
(lp27692
sg281
(lp27693
sg128
(lp27694
sg138
(lp27695
sg354
(lp27696
ssS'behav'
p27697
(dp27698
g178
(lp27699
sg6
(lp27700
I1703
asg85
(lp27701
sg303
(lp27702
sg18
(lp27703
sg110
(lp27704
sg63
(lp27705
ssS'poisson'
p27706
(dp27707
g70
(lp27708
sg83
(lp27709
sg6
(lp27710
I615
asg262
(lp27711
ssS'ekv'
p27712
(dp27713
g130
(lp27714
I1032
assS'tape'
p27715
(dp27716
g78
(lp27717
I2155
assS'macintosh'
p27718
(dp27719
g94
(lp27720
I579
assS'fbs'
p27721
(dp27722
g91
(lp27723
I2091
assS'regardless'
p27724
(dp27725
g4
(lp27726
sg44
(lp27727
sg46
(lp27728
sg18
(lp27729
sg223
(lp27730
sg354
(lp27731
I321
assS'extra'
p27732
(dp27733
g329
(lp27734
sg332
(lp27735
sg277
(lp27736
sg76
(lp27737
sg295
(lp27738
sg183
(lp27739
sg10
(lp27740
sg34
(lp27741
sg128
(lp27742
sg135
(lp27743
I1387
asg52
(lp27744
ssS'rosenfeld'
p27745
(dp27746
g281
(lp27747
I2441
assS'icnn'
p27748
(dp27749
g72
(lp27750
I3522
assS'mobil'
p27751
(dp27752
g42
(lp27753
I90
asg223
(lp27754
ssS'market'
p27755
(dp27756
g245
(lp27757
I38
asg277
(lp27758
ssS'angelo'
p27759
(dp27760
g78
(lp27761
I12
assS'prolong'
p27762
(dp27763
g216
(lp27764
sg106
(lp27765
I2196
asg70
(lp27766
ssS'prove'
p27767
(dp27768
g440
(lp27769
sg176
(lp27770
sg145
(lp27771
sg34
(lp27772
sg221
(lp27773
sg341
(lp27774
sg40
(lp27775
sg36
(lp27776
sg344
(lp27777
sg306
(lp27778
sg89
(lp27779
sg104
(lp27780
sg245
(lp27781
sg14
(lp27782
sg16
(lp27783
sg135
(lp27784
sg50
(lp27785
sg96
(lp27786
sg354
(lp27787
I2059
assS'konnektionistisch'
p27788
(dp27789
g34
(lp27790
I2915
assS'pickard'
p27791
(dp27792
g135
(lp27793
I18
assS'live'
p27794
(dp27795
g18
(lp27796
I1517
asg277
(lp27797
ssS'regel'
p27798
(dp27799
g96
(lp27800
I2812
assS'jaj'
p27801
(dp27802
g384
(lp27803
I570
assS'bigg'
p27804
(dp27805
g287
(lp27806
I414
assS'lgtl'
p27807
(dp27808
g34
(lp27809
I1250
assS'finit'
p27810
(dp27811
g124
(lp27812
sg281
(lp27813
sg104
(lp27814
sg287
(lp27815
sg344
(lp27816
sg38
(lp27817
sg83
(lp27818
sg85
(lp27819
sg42
(lp27820
I108
asg306
(lp27821
sg89
(lp27822
sg245
(lp27823
sg46
(lp27824
sg350
(lp27825
sg68
(lp27826
sg178
(lp27827
sg108
(lp27828
sg318
(lp27829
sg121
(lp27830
sg8
(lp27831
sg34
(lp27832
sg384
(lp27833
sg235
(lp27834
sg341
(lp27835
sg128
(lp27836
sg130
(lp27837
sg354
(lp27838
ssS'clue'
p27839
(dp27840
g91
(lp27841
I294
assS'inaijmai'
p27842
(dp27843
g429
(lp27844
I1012
assS'logarithm'
p27845
(dp27846
g26
(lp27847
sg174
(lp27848
sg22
(lp27849
sg181
(lp27850
sg329
(lp27851
sg235
(lp27852
sg91
(lp27853
sg245
(lp27854
sg108
(lp27855
sg163
(lp27856
sg138
(lp27857
I1591
asg256
(lp27858
ssS'fragnier'
p27859
(dp27860
g174
(lp27861
I470
assS'graphic'
p27862
(dp27863
g121
(lp27864
sg293
(lp27865
sg138
(lp27866
I3423
asg429
(lp27867
sg104
(lp27868
sg221
(lp27869
sg63
(lp27870
ssS'ibm'
p27871
(dp27872
g10
(lp27873
sg40
(lp27874
sg42
(lp27875
I3412
asg102
(lp27876
sg89
(lp27877
sg132
(lp27878
sg63
(lp27879
sg114
(lp27880
ssS'car'
p27881
(dp27882
g277
(lp27883
sg89
(lp27884
I824
asg83
(lp27885
sg223
(lp27886
ssS'prepar'
p27887
(dp27888
g178
(lp27889
sg76
(lp27890
sg235
(lp27891
sg42
(lp27892
I1584
asg106
(lp27893
sg99
(lp27894
sg350
(lp27895
ssS'cap'
p27896
(dp27897
g332
(lp27898
I2266
assS'cat'
p27899
(dp27900
g438
(lp27901
I959
asg440
(lp27902
sg174
(lp27903
sg12
(lp27904
sg106
(lp27905
sg63
(lp27906
sg149
(lp27907
ssS'can'
p27908
(dp27909
g80
(lp27910
sg293
(lp27911
sg344
(lp27912
sg78
(lp27913
sg59
(lp27914
sg484
(lp27915
sg38
(lp27916
sg83
(lp27917
sg85
(lp27918
sg303
(lp27919
sg438
(lp27920
sg116
(lp27921
sg118
(lp27922
sg34
(lp27923
sg36
(lp27924
sg460
(lp27925
sg68
(lp27926
sg72
(lp27927
sg281
(lp27928
sg10
(lp27929
sg40
(lp27930
sg283
(lp27931
sg70
(lp27932
sg26
(lp27933
sg277
(lp27934
sg163
(lp27935
sg89
(lp27936
sg91
(lp27937
sg12
(lp27938
sg94
(lp27939
sg96
(lp27940
sg48
(lp27941
sg99
(lp27942
sg313
(lp27943
sg44
(lp27944
sg149
(lp27945
sg429
(lp27946
sg102
(lp27947
sg104
(lp27948
sg106
(lp27949
sg108
(lp27950
sg110
(lp27951
sg63
(lp27952
sg52
(lp27953
sg114
(lp27954
sg128
(lp27955
sg130
(lp27956
sg132
(lp27957
sg14
(lp27958
sg16
(lp27959
sg135
(lp27960
sg50
(lp27961
sg138
(lp27962
sg140
(lp27963
sg354
(lp27964
sg306
(lp27965
sg87
(lp27966
sg245
(lp27967
sg46
(lp27968
sg20
(lp27969
sg18
(lp27970
sg221
(lp27971
sg535
(lp27972
sg223
(lp27973
sg350
(lp27974
sg216
(lp27975
sg174
(lp27976
sg440
(lp27977
sg332
(lp27978
sg121
(lp27979
sg4
(lp27980
sg6
(lp27981
sg8
(lp27982
sg126
(lp27983
sg341
(lp27984
sg30
(lp27985
sg287
(lp27986
sg74
(lp27987
sg176
(lp27988
sg145
(lp27989
sg256
(lp27990
sg76
(lp27991
sg262
(lp27992
sg295
(lp27993
sg183
(lp27994
sg42
(lp27995
I40
asg230
(lp27996
sg329
(lp27997
sg32
(lp27998
sg318
(lp27999
sg178
(lp28000
sg22
(lp28001
sg181
(lp28002
sg235
(lp28003
sg384
(lp28004
sg124
(lp28005
ssS'cam'
p28006
(dp28007
g87
(lp28008
I35
asg76
(lp28009
ssS'cal'
p28010
(dp28011
g106
(lp28012
I191
asg40
(lp28013
ssS'elfv'
p28014
(dp28015
g281
(lp28016
I2124
assS'cad'
p28017
(dp28018
g438
(lp28019
I2475
assS'heart'
p28020
(dp28021
g94
(lp28022
sg135
(lp28023
I309
asg91
(lp28024
ssS'underestim'
p28025
(dp28026
g89
(lp28027
I1022
assS'practition'
p28028
(dp28029
g306
(lp28030
sg313
(lp28031
I920
assS'amaldi'
p28032
(dp28033
g40
(lp28034
I1050
assS'chip'
p28035
(dp28036
g283
(lp28037
sg22
(lp28038
sg256
(lp28039
sg59
(lp28040
sg10
(lp28041
sg63
(lp28042
sg245
(lp28043
sg14
(lp28044
sg16
(lp28045
sg135
(lp28046
I105
asg20
(lp28047
sg114
(lp28048
ssS'dxp'
p28049
(dp28050
g140
(lp28051
I697
assS'topic'
p28052
(dp28053
g42
(lp28054
I554
asg104
(lp28055
sg295
(lp28056
sg63
(lp28057
sg183
(lp28058
ssS'jypist'
p28059
(dp28060
g94
(lp28061
I119
assS'abort'
p28062
(dp28063
g145
(lp28064
I2127
assS'dxi'
p28065
(dp28066
g130
(lp28067
I565
assS'sofrki'
p28068
(dp28069
g70
(lp28070
I1045
assS'occur'
p28071
(dp28072
g68
(lp28073
sg70
(lp28074
sg26
(lp28075
sg181
(lp28076
sg176
(lp28077
sg145
(lp28078
sg80
(lp28079
sg76
(lp28080
sg295
(lp28081
sg183
(lp28082
sg59
(lp28083
sg484
(lp28084
sg38
(lp28085
sg83
(lp28086
sg85
(lp28087
sg87
(lp28088
sg89
(lp28089
sg91
(lp28090
sg94
(lp28091
sg20
(lp28092
sg18
(lp28093
sg221
(lp28094
sg149
(lp28095
sg116
(lp28096
sg174
(lp28097
sg429
(lp28098
sg102
(lp28099
sg104
(lp28100
sg106
(lp28101
sg63
(lp28102
sg52
(lp28103
sg230
(lp28104
sg438
(lp28105
I1606
asg332
(lp28106
sg4
(lp28107
sg6
(lp28108
sg8
(lp28109
sg34
(lp28110
sg36
(lp28111
sg460
(lp28112
sg235
(lp28113
sg341
(lp28114
sg344
(lp28115
sg78
(lp28116
sg14
(lp28117
sg135
(lp28118
sg50
(lp28119
sg138
(lp28120
ssS'multipl'
p28121
(dp28122
g283
(lp28123
sg70
(lp28124
sg163
(lp28125
sg83
(lp28126
sg74
(lp28127
sg80
(lp28128
sg293
(lp28129
sg295
(lp28130
sg183
(lp28131
sg59
(lp28132
sg149
(lp28133
sg303
(lp28134
sg87
(lp28135
sg89
(lp28136
sg20
(lp28137
sg18
(lp28138
sg223
(lp28139
sg350
(lp28140
sg429
(lp28141
sg102
(lp28142
sg52
(lp28143
sg216
(lp28144
sg32
(lp28145
sg318
(lp28146
sg121
(lp28147
sg22
(lp28148
sg181
(lp28149
sg460
(lp28150
sg68
(lp28151
sg126
(lp28152
sg341
(lp28153
sg10
(lp28154
sg40
(lp28155
sg14
(lp28156
sg16
(lp28157
sg135
(lp28158
sg50
(lp28159
sg138
(lp28160
I2955
assS'satisfact'
p28161
(dp28162
g42
(lp28163
I2604
assS'write'
p28164
(dp28165
g30
(lp28166
sg438
(lp28167
sg287
(lp28168
sg68
(lp28169
sg114
(lp28170
sg63
(lp28171
sg42
(lp28172
I852
asg10
(lp28173
sg94
(lp28174
sg14
(lp28175
sg135
(lp28176
sg99
(lp28177
sg313
(lp28178
sg350
(lp28179
ssS'richardc'
p28180
(dp28181
g135
(lp28182
I35
assS'ventil'
p28183
(dp28184
g78
(lp28185
I2816
assS'flank'
p28186
(dp28187
g102
(lp28188
sg50
(lp28189
I899
assS'criterion'
p28190
(dp28191
g277
(lp28192
sg163
(lp28193
sg183
(lp28194
sg42
(lp28195
I2183
asg87
(lp28196
sg245
(lp28197
sg94
(lp28198
sg535
(lp28199
sg44
(lp28200
sg429
(lp28201
sg102
(lp28202
sg178
(lp28203
sg110
(lp28204
sg329
(lp28205
sg440
(lp28206
sg121
(lp28207
sg8
(lp28208
sg34
(lp28209
sg72
(lp28210
sg313
(lp28211
sg130
(lp28212
sg132
(lp28213
sg354
(lp28214
ssS'syst'
p28215
(dp28216
g287
(lp28217
sg59
(lp28218
sg283
(lp28219
sg99
(lp28220
I3170
asg350
(lp28221
ssS'birdsong'
p28222
(dp28223
g116
(lp28224
I5
assS'cardiovert'
p28225
(dp28226
g135
(lp28227
I72
assS'product'
p28228
(dp28229
g216
(lp28230
sg230
(lp28231
sg32
(lp28232
sg145
(lp28233
sg80
(lp28234
sg287
(lp28235
sg116
(lp28236
sg68
(lp28237
sg10
(lp28238
sg42
(lp28239
I142
asg70
(lp28240
sg52
(lp28241
sg128
(lp28242
sg78
(lp28243
sg245
(lp28244
sg46
(lp28245
sg350
(lp28246
sg221
(lp28247
sg138
(lp28248
sg44
(lp28249
sg149
(lp28250
ssS'mccann'
p28251
(dp28252
g118
(lp28253
I473
assS'confernc'
p28254
(dp28255
g85
(lp28256
I4087
assS'southern'
p28257
(dp28258
g245
(lp28259
I19
asg181
(lp28260
ssS'aoj'
p28261
(dp28262
g176
(lp28263
I969
assS'prerequisit'
p28264
(dp28265
g59
(lp28266
I915
assS'drastic'
p28267
(dp28268
g132
(lp28269
I2956
asg306
(lp28270
sg283
(lp28271
sg145
(lp28272
ssS'flourish'
p28273
(dp28274
g42
(lp28275
I584
assS'explicit'
p28276
(dp28277
g26
(lp28278
sg163
(lp28279
sg30
(lp28280
sg262
(lp28281
sg295
(lp28282
sg183
(lp28283
sg38
(lp28284
sg83
(lp28285
sg303
(lp28286
sg89
(lp28287
sg46
(lp28288
sg18
(lp28289
sg313
(lp28290
sg223
(lp28291
sg149
(lp28292
sg329
(lp28293
sg293
(lp28294
sg429
(lp28295
sg102
(lp28296
sg104
(lp28297
sg108
(lp28298
sg438
(lp28299
I1296
asg32
(lp28300
sg178
(lp28301
sg181
(lp28302
sg34
(lp28303
sg384
(lp28304
sg68
(lp28305
sg281
(lp28306
sg344
(lp28307
sg130
(lp28308
sg460
(lp28309
sg354
(lp28310
ssS'bunkyo'
p28311
(dp28312
g36
(lp28313
I28
assS'ghl'
p28314
(dp28315
g245
(lp28316
I997
assS'rnotion'
p28317
(dp28318
g350
(lp28319
I856
assS'zlwj'
p28320
(dp28321
g91
(lp28322
I838
assS'michel'
p28323
(dp28324
g293
(lp28325
I3228
assS'stlte'
p28326
(dp28327
g440
(lp28328
I686
assS'gllq'
p28329
(dp28330
g99
(lp28331
I1030
assS'approx'
p28332
(dp28333
g126
(lp28334
I1341
assS'usc'
p28335
(dp28336
g181
(lp28337
I30
assS'brain'
p28338
(dp28339
g329
(lp28340
sg70
(lp28341
sg74
(lp28342
sg176
(lp28343
sg80
(lp28344
sg344
(lp28345
sg303
(lp28346
sg12
(lp28347
sg99
(lp28348
sg313
(lp28349
sg350
(lp28350
sg438
(lp28351
I2447
asg429
(lp28352
sg106
(lp28353
sg52
(lp28354
sg116
(lp28355
sg118
(lp28356
sg32
(lp28357
sg332
(lp28358
sg178
(lp28359
sg4
(lp28360
sg181
(lp28361
sg36
(lp28362
sg460
(lp28363
sg40
(lp28364
ssS'physiol'
p28365
(dp28366
g12
(lp28367
sg106
(lp28368
I2587
assS'cold'
p28369
(dp28370
g149
(lp28371
I3052
assS'still'
p28372
(dp28373
g78
(lp28374
sg26
(lp28375
sg287
(lp28376
sg74
(lp28377
sg145
(lp28378
sg76
(lp28379
sg295
(lp28380
sg183
(lp28381
sg59
(lp28382
sg484
(lp28383
sg83
(lp28384
sg85
(lp28385
sg87
(lp28386
sg89
(lp28387
sg94
(lp28388
sg20
(lp28389
sg48
(lp28390
sg221
(lp28391
sg350
(lp28392
sg32
(lp28393
sg52
(lp28394
sg329
(lp28395
sg440
(lp28396
sg4
(lp28397
sg8
(lp28398
sg384
(lp28399
sg235
(lp28400
sg281
(lp28401
sg10
(lp28402
sg344
(lp28403
sg130
(lp28404
sg132
(lp28405
sg14
(lp28406
sg135
(lp28407
sg140
(lp28408
sg354
(lp28409
I990
assS'ieee'
p28410
(dp28411
g124
(lp28412
sg26
(lp28413
sg277
(lp28414
sg281
(lp28415
sg283
(lp28416
sg40
(lp28417
sg76
(lp28418
sg293
(lp28419
sg344
(lp28420
sg78
(lp28421
sg85
(lp28422
sg63
(lp28423
sg42
(lp28424
I3397
asg87
(lp28425
sg245
(lp28426
sg20
(lp28427
sg535
(lp28428
sg44
(lp28429
sg149
(lp28430
sg178
(lp28431
sg429
(lp28432
sg68
(lp28433
sg104
(lp28434
sg108
(lp28435
sg110
(lp28436
sg96
(lp28437
sg114
(lp28438
sg230
(lp28439
sg174
(lp28440
sg440
(lp28441
sg318
(lp28442
sg121
(lp28443
sg22
(lp28444
sg181
(lp28445
sg8
(lp28446
sg36
(lp28447
sg460
(lp28448
sg235
(lp28449
sg72
(lp28450
sg341
(lp28451
sg10
(lp28452
sg313
(lp28453
sg128
(lp28454
sg130
(lp28455
sg14
(lp28456
sg50
(lp28457
sg140
(lp28458
sg354
(lp28459
ssS'dynam'
p28460
(dp28461
g68
(lp28462
sg70
(lp28463
sg26
(lp28464
sg176
(lp28465
sg76
(lp28466
sg262
(lp28467
sg295
(lp28468
sg183
(lp28469
sg38
(lp28470
sg83
(lp28471
sg114
(lp28472
sg306
(lp28473
sg89
(lp28474
sg460
(lp28475
sg12
(lp28476
sg46
(lp28477
sg96
(lp28478
sg18
(lp28479
sg99
(lp28480
sg313
(lp28481
sg149
(lp28482
sg230
(lp28483
sg118
(lp28484
sg245
(lp28485
sg429
(lp28486
sg102
(lp28487
sg104
(lp28488
sg108
(lp28489
sg22
(lp28490
sg216
(lp28491
sg174
(lp28492
sg440
(lp28493
sg332
(lp28494
sg121
(lp28495
sg4
(lp28496
sg181
(lp28497
sg8
(lp28498
sg34
(lp28499
sg384
(lp28500
sg124
(lp28501
sg126
(lp28502
sg535
(lp28503
sg128
(lp28504
sg130
(lp28505
sg132
(lp28506
sg14
(lp28507
sg350
(lp28508
sg50
(lp28509
sg138
(lp28510
I155
assS'conjunct'
p28511
(dp28512
g118
(lp28513
sg178
(lp28514
sg181
(lp28515
sg344
(lp28516
sg306
(lp28517
I582
asg110
(lp28518
sg63
(lp28519
ssS'enno'
p28520
(dp28521
g59
(lp28522
I28
assS'precondit'
p28523
(dp28524
g8
(lp28525
I1715
assS'infonn'
p28526
(dp28527
g295
(lp28528
sg183
(lp28529
sg130
(lp28530
sg102
(lp28531
sg50
(lp28532
I587
asg350
(lp28533
ssS'window'
p28534
(dp28535
g116
(lp28536
sg174
(lp28537
sg440
(lp28538
sg332
(lp28539
sg121
(lp28540
sg26
(lp28541
sg6
(lp28542
sg72
(lp28543
sg126
(lp28544
sg63
(lp28545
sg42
(lp28546
I1676
asg318
(lp28547
sg128
(lp28548
sg94
(lp28549
sg96
(lp28550
sg138
(lp28551
sg52
(lp28552
ssS'wixi'
p28553
(dp28554
g40
(lp28555
I447
assS'mediocr'
p28556
(dp28557
g63
(lp28558
I2386
assS'critesgc'
p28559
(dp28560
g83
(lp28561
I18
assS'non'
p28562
(dp28563
g124
(lp28564
sg70
(lp28565
sg78
(lp28566
sg163
(lp28567
sg72
(lp28568
sg283
(lp28569
sg36
(lp28570
sg40
(lp28571
sg26
(lp28572
sg30
(lp28573
sg287
(lp28574
sg74
(lp28575
sg176
(lp28576
sg145
(lp28577
sg76
(lp28578
sg118
(lp28579
sg295
(lp28580
sg183
(lp28581
sg59
(lp28582
sg484
(lp28583
sg38
(lp28584
sg114
(lp28585
sg87
(lp28586
sg94
(lp28587
sg20
(lp28588
sg18
(lp28589
sg99
(lp28590
sg313
(lp28591
sg44
(lp28592
sg116
(lp28593
sg329
(lp28594
sg293
(lp28595
sg429
(lp28596
sg68
(lp28597
sg46
(lp28598
sg102
(lp28599
sg178
(lp28600
sg110
(lp28601
sg22
(lp28602
sg216
(lp28603
sg438
(lp28604
I754
asg32
(lp28605
sg318
(lp28606
sg121
(lp28607
sg4
(lp28608
sg181
(lp28609
sg8
(lp28610
sg34
(lp28611
sg221
(lp28612
sg384
(lp28613
sg235
(lp28614
sg126
(lp28615
sg341
(lp28616
sg10
(lp28617
sg535
(lp28618
sg344
(lp28619
sg128
(lp28620
sg130
(lp28621
sg132
(lp28622
sg14
(lp28623
sg16
(lp28624
sg135
(lp28625
sg50
(lp28626
sg460
(lp28627
sg140
(lp28628
sg354
(lp28629
ssS'evok'
p28630
(dp28631
g116
(lp28632
sg106
(lp28633
I649
asg176
(lp28634
ssS'recal'
p28635
(dp28636
g30
(lp28637
sg32
(lp28638
sg318
(lp28639
sg145
(lp28640
sg163
(lp28641
sg344
(lp28642
sg384
(lp28643
sg341
(lp28644
sg85
(lp28645
sg281
(lp28646
sg42
(lp28647
I1945
asg102
(lp28648
sg104
(lp28649
sg12
(lp28650
sg94
(lp28651
sg99
(lp28652
sg46
(lp28653
sg44
(lp28654
ssS'halt'
p28655
(dp28656
g277
(lp28657
I858
assS'barron'
p28658
(dp28659
g96
(lp28660
I779
asg85
(lp28661
ssS'tji'
p28662
(dp28663
g104
(lp28664
sg96
(lp28665
I1072
assS'nishimori'
p28666
(dp28667
g384
(lp28668
I636
assS'misclassifi'
p28669
(dp28670
g183
(lp28671
sg145
(lp28672
I363
asg63
(lp28673
ssS'tjo'
p28674
(dp28675
g38
(lp28676
I3162
assS'trng'
p28677
(dp28678
g72
(lp28679
I984
assS'tjs'
p28680
(dp28681
g106
(lp28682
I2502
assS'tjt'
p28683
(dp28684
g235
(lp28685
I721
assS'half'
p28686
(dp28687
g70
(lp28688
sg76
(lp28689
sg78
(lp28690
sg484
(lp28691
sg303
(lp28692
sg42
(lp28693
I50
asg94
(lp28694
sg99
(lp28695
sg44
(lp28696
sg46
(lp28697
sg110
(lp28698
sg52
(lp28699
sg116
(lp28700
sg329
(lp28701
sg318
(lp28702
sg178
(lp28703
sg6
(lp28704
sg181
(lp28705
sg126
(lp28706
sg130
(lp28707
sg132
(lp28708
sg140
(lp28709
ssS'not'
p28710
(dp28711
g80
(lp28712
sg293
(lp28713
sg344
(lp28714
sg78
(lp28715
sg59
(lp28716
sg484
(lp28717
sg38
(lp28718
sg83
(lp28719
sg85
(lp28720
sg303
(lp28721
sg438
(lp28722
sg116
(lp28723
sg118
(lp28724
sg34
(lp28725
sg36
(lp28726
sg460
(lp28727
sg68
(lp28728
sg72
(lp28729
sg281
(lp28730
sg10
(lp28731
sg40
(lp28732
sg283
(lp28733
sg70
(lp28734
sg26
(lp28735
sg277
(lp28736
sg163
(lp28737
sg89
(lp28738
sg91
(lp28739
sg12
(lp28740
sg94
(lp28741
sg48
(lp28742
sg99
(lp28743
sg313
(lp28744
sg44
(lp28745
sg149
(lp28746
sg429
(lp28747
sg102
(lp28748
sg104
(lp28749
sg106
(lp28750
sg108
(lp28751
sg110
(lp28752
sg63
(lp28753
sg52
(lp28754
sg114
(lp28755
sg128
(lp28756
sg130
(lp28757
sg132
(lp28758
sg14
(lp28759
sg16
(lp28760
sg135
(lp28761
sg50
(lp28762
sg138
(lp28763
sg140
(lp28764
sg354
(lp28765
sg306
(lp28766
sg87
(lp28767
sg245
(lp28768
sg46
(lp28769
sg20
(lp28770
sg18
(lp28771
sg221
(lp28772
sg223
(lp28773
sg350
(lp28774
sg216
(lp28775
sg174
(lp28776
sg440
(lp28777
sg332
(lp28778
sg121
(lp28779
sg4
(lp28780
sg6
(lp28781
sg8
(lp28782
sg126
(lp28783
sg341
(lp28784
sg30
(lp28785
sg287
(lp28786
sg74
(lp28787
sg176
(lp28788
sg145
(lp28789
sg256
(lp28790
sg76
(lp28791
sg262
(lp28792
sg295
(lp28793
sg183
(lp28794
sg42
(lp28795
I961
asg230
(lp28796
sg329
(lp28797
sg32
(lp28798
sg318
(lp28799
sg178
(lp28800
sg22
(lp28801
sg181
(lp28802
sg235
(lp28803
sg384
(lp28804
sg124
(lp28805
ssS'tjx'
p28806
(dp28807
g96
(lp28808
I1071
assS'nov'
p28809
(dp28810
g42
(lp28811
I1661
asg102
(lp28812
sg72
(lp28813
ssS'now'
p28814
(dp28815
g68
(lp28816
sg26
(lp28817
sg163
(lp28818
sg287
(lp28819
sg74
(lp28820
sg176
(lp28821
sg76
(lp28822
sg262
(lp28823
sg295
(lp28824
sg183
(lp28825
sg59
(lp28826
sg85
(lp28827
sg303
(lp28828
sg42
(lp28829
I462
asg87
(lp28830
sg89
(lp28831
sg91
(lp28832
sg46
(lp28833
sg48
(lp28834
sg99
(lp28835
sg535
(lp28836
sg223
(lp28837
sg329
(lp28838
sg429
(lp28839
sg102
(lp28840
sg104
(lp28841
sg63
(lp28842
sg52
(lp28843
sg438
(lp28844
sg32
(lp28845
sg318
(lp28846
sg4
(lp28847
sg235
(lp28848
sg221
(lp28849
sg384
(lp28850
sg124
(lp28851
sg341
(lp28852
sg344
(lp28853
sg36
(lp28854
sg50
(lp28855
sg460
(lp28856
ssS'discuss'
p28857
(dp28858
g329
(lp28859
sg70
(lp28860
sg277
(lp28861
sg163
(lp28862
sg72
(lp28863
sg293
(lp28864
sg281
(lp28865
sg460
(lp28866
sg181
(lp28867
sg303
(lp28868
sg30
(lp28869
sg350
(lp28870
sg74
(lp28871
sg176
(lp28872
sg145
(lp28873
sg76
(lp28874
sg262
(lp28875
sg295
(lp28876
sg183
(lp28877
sg59
(lp28878
sg80
(lp28879
sg38
(lp28880
sg83
(lp28881
sg85
(lp28882
sg124
(lp28883
sg42
(lp28884
I53
asg306
(lp28885
sg87
(lp28886
sg89
(lp28887
sg68
(lp28888
sg12
(lp28889
sg46
(lp28890
sg96
(lp28891
sg48
(lp28892
sg221
(lp28893
sg313
(lp28894
sg223
(lp28895
sg149
(lp28896
sg230
(lp28897
sg174
(lp28898
sg18
(lp28899
sg116
(lp28900
sg32
(lp28901
sg245
(lp28902
sg318
(lp28903
sg102
(lp28904
sg104
(lp28905
sg106
(lp28906
sg110
(lp28907
sg178
(lp28908
sg52
(lp28909
sg216
(lp28910
sg438
(lp28911
sg440
(lp28912
sg332
(lp28913
sg121
(lp28914
sg4
(lp28915
sg6
(lp28916
sg8
(lp28917
sg36
(lp28918
sg384
(lp28919
sg235
(lp28920
sg126
(lp28921
sg341
(lp28922
sg10
(lp28923
sg118
(lp28924
sg287
(lp28925
sg63
(lp28926
sg130
(lp28927
sg132
(lp28928
sg14
(lp28929
sg16
(lp28930
sg135
(lp28931
sg50
(lp28932
sg138
(lp28933
sg140
(lp28934
sg354
(lp28935
ssS'nor'
p28936
(dp28937
g318
(lp28938
sg4
(lp28939
sg344
(lp28940
sg36
(lp28941
sg281
(lp28942
sg303
(lp28943
sg78
(lp28944
sg14
(lp28945
I4553
asg63
(lp28946
ssS'introduct'
p28947
(dp28948
g293
(lp28949
sg78
(lp28950
sg59
(lp28951
sg484
(lp28952
sg83
(lp28953
sg85
(lp28954
sg303
(lp28955
sg116
(lp28956
sg118
(lp28957
sg34
(lp28958
sg36
(lp28959
sg460
(lp28960
sg68
(lp28961
sg72
(lp28962
sg281
(lp28963
sg10
(lp28964
sg40
(lp28965
sg283
(lp28966
sg70
(lp28967
sg26
(lp28968
sg163
(lp28969
sg89
(lp28970
sg91
(lp28971
sg12
(lp28972
sg94
(lp28973
sg96
(lp28974
sg99
(lp28975
sg44
(lp28976
sg149
(lp28977
sg429
(lp28978
sg102
(lp28979
sg104
(lp28980
sg106
(lp28981
sg108
(lp28982
sg110
(lp28983
sg52
(lp28984
sg114
(lp28985
sg128
(lp28986
sg130
(lp28987
sg132
(lp28988
sg14
(lp28989
sg16
(lp28990
sg135
(lp28991
sg50
(lp28992
sg140
(lp28993
sg354
(lp28994
sg306
(lp28995
sg87
(lp28996
sg245
(lp28997
sg46
(lp28998
sg20
(lp28999
sg18
(lp29000
sg221
(lp29001
sg535
(lp29002
sg223
(lp29003
sg350
(lp29004
sg216
(lp29005
sg440
(lp29006
sg332
(lp29007
sg121
(lp29008
sg4
(lp29009
sg6
(lp29010
sg8
(lp29011
sg126
(lp29012
sg341
(lp29013
sg30
(lp29014
sg287
(lp29015
sg74
(lp29016
sg176
(lp29017
sg145
(lp29018
sg256
(lp29019
sg76
(lp29020
sg262
(lp29021
sg295
(lp29022
sg183
(lp29023
sg42
(lp29024
I95
asg230
(lp29025
sg329
(lp29026
sg32
(lp29027
sg318
(lp29028
sg178
(lp29029
sg22
(lp29030
sg181
(lp29031
sg235
(lp29032
sg384
(lp29033
sg124
(lp29034
ssS'devijv'
p29035
(dp29036
g281
(lp29037
I924
assS'bregler'
p29038
(dp29039
g30
(lp29040
I549
assS'drop'
p29041
(dp29042
g116
(lp29043
sg235
(lp29044
sg384
(lp29045
sg87
(lp29046
sg128
(lp29047
sg14
(lp29048
sg313
(lp29049
sg140
(lp29050
sg354
(lp29051
I2411
assS'kern'
p29052
(dp29053
g40
(lp29054
sg313
(lp29055
I1518
asg22
(lp29056
ssS'magazin'
p29057
(dp29058
g121
(lp29059
sg460
(lp29060
sg99
(lp29061
I2295
asg313
(lp29062
sg76
(lp29063
ssS'zohari'
p29064
(dp29065
g6
(lp29066
I26
assS'h'
p29067
(dp29068
g283
(lp29069
sg70
(lp29070
sg26
(lp29071
sg277
(lp29072
sg163
(lp29073
sg72
(lp29074
sg303
(lp29075
sg281
(lp29076
sg85
(lp29077
sg460
(lp29078
sg36
(lp29079
sg181
(lp29080
sg40
(lp29081
sg80
(lp29082
sg287
(lp29083
sg176
(lp29084
sg145
(lp29085
sg256
(lp29086
sg76
(lp29087
sg262
(lp29088
sg295
(lp29089
sg183
(lp29090
sg59
(lp29091
sg484
(lp29092
sg38
(lp29093
sg83
(lp29094
sg114
(lp29095
sg63
(lp29096
sg42
(lp29097
I1672
asg87
(lp29098
sg91
(lp29099
sg245
(lp29100
sg46
(lp29101
sg96
(lp29102
sg48
(lp29103
sg99
(lp29104
sg313
(lp29105
sg223
(lp29106
sg350
(lp29107
sg118
(lp29108
sg230
(lp29109
sg329
(lp29110
sg18
(lp29111
sg116
(lp29112
sg32
(lp29113
sg178
(lp29114
sg429
(lp29115
sg318
(lp29116
sg104
(lp29117
sg106
(lp29118
sg108
(lp29119
sg20
(lp29120
sg22
(lp29121
sg216
(lp29122
sg174
(lp29123
sg440
(lp29124
sg332
(lp29125
sg121
(lp29126
sg4
(lp29127
sg6
(lp29128
sg235
(lp29129
sg34
(lp29130
sg221
(lp29131
sg384
(lp29132
sg124
(lp29133
sg126
(lp29134
sg341
(lp29135
sg535
(lp29136
sg344
(lp29137
sg128
(lp29138
sg130
(lp29139
sg132
(lp29140
sg14
(lp29141
sg16
(lp29142
sg135
(lp29143
sg50
(lp29144
sg138
(lp29145
sg140
(lp29146
sg354
(lp29147
ssS'januari'
p29148
(dp29149
g135
(lp29150
I2617
asg68
(lp29151
sg223
(lp29152
ssS'em'
p29153
(dp29154
g30
(lp29155
sg183
(lp29156
sg440
(lp29157
sg332
(lp29158
sg295
(lp29159
sg221
(lp29160
sg460
(lp29161
sg72
(lp29162
sg74
(lp29163
sg176
(lp29164
sg138
(lp29165
I1046
asg87
(lp29166
sg91
(lp29167
sg130
(lp29168
sg59
(lp29169
sg104
(lp29170
sg99
(lp29171
sg313
(lp29172
sg38
(lp29173
ssS'el'
p29174
(dp29175
g230
(lp29176
sg46
(lp29177
I2478
asg32
(lp29178
sg68
(lp29179
sg72
(lp29180
ssS'domain'
p29181
(dp29182
g283
(lp29183
sg277
(lp29184
sg287
(lp29185
sg74
(lp29186
sg293
(lp29187
sg295
(lp29188
sg183
(lp29189
sg83
(lp29190
sg306
(lp29191
sg89
(lp29192
sg91
(lp29193
sg245
(lp29194
sg18
(lp29195
sg313
(lp29196
sg223
(lp29197
sg118
(lp29198
sg329
(lp29199
sg32
(lp29200
sg22
(lp29201
sg181
(lp29202
sg8
(lp29203
sg34
(lp29204
sg460
(lp29205
sg126
(lp29206
sg341
(lp29207
sg344
(lp29208
sg44
(lp29209
sg128
(lp29210
sg132
(lp29211
sg135
(lp29212
I545
assS'en'
p29213
(dp29214
g230
(lp29215
sg287
(lp29216
sg295
(lp29217
sg183
(lp29218
sg68
(lp29219
sg38
(lp29220
sg48
(lp29221
I269
asg114
(lp29222
ssS'ei'
p29223
(dp29224
g230
(lp29225
sg74
(lp29226
sg26
(lp29227
sg384
(lp29228
sg72
(lp29229
sg108
(lp29230
I1928
asg221
(lp29231
sg350
(lp29232
ssS'eh'
p29233
(dp29234
g14
(lp29235
I2697
assS'ek'
p29236
(dp29237
g46
(lp29238
sg18
(lp29239
I1710
assS'ej'
p29240
(dp29241
g30
(lp29242
sg74
(lp29243
sg130
(lp29244
I984
assS'ee'
p29245
(dp29246
g245
(lp29247
I410
asg32
(lp29248
sg429
(lp29249
ssS'replac'
p29250
(dp29251
g78
(lp29252
sg287
(lp29253
sg293
(lp29254
sg183
(lp29255
sg484
(lp29256
sg85
(lp29257
sg306
(lp29258
sg89
(lp29259
sg245
(lp29260
sg221
(lp29261
sg313
(lp29262
sg350
(lp29263
sg102
(lp29264
sg438
(lp29265
I527
asg440
(lp29266
sg121
(lp29267
sg22
(lp29268
sg181
(lp29269
sg460
(lp29270
sg72
(lp29271
sg281
(lp29272
sg128
(lp29273
sg130
(lp29274
sg14
(lp29275
sg354
(lp29276
ssS'eg'
p29277
(dp29278
g30
(lp29279
I1555
asg38
(lp29280
sg85
(lp29281
ssS'ef'
p29282
(dp29283
g96
(lp29284
I794
asg10
(lp29285
ssS'ea'
p29286
(dp29287
g287
(lp29288
sg318
(lp29289
sg145
(lp29290
sg262
(lp29291
sg72
(lp29292
sg140
(lp29293
I1747
assS'gaussian'
p29294
(dp29295
g163
(lp29296
sg72
(lp29297
sg30
(lp29298
sg74
(lp29299
sg80
(lp29300
sg262
(lp29301
sg295
(lp29302
sg183
(lp29303
sg484
(lp29304
sg38
(lp29305
sg303
(lp29306
sg91
(lp29307
sg12
(lp29308
sg96
(lp29309
sg18
(lp29310
sg221
(lp29311
sg313
(lp29312
sg44
(lp29313
sg149
(lp29314
sg116
(lp29315
sg329
(lp29316
sg460
(lp29317
sg318
(lp29318
sg102
(lp29319
sg108
(lp29320
sg22
(lp29321
sg230
(lp29322
sg174
(lp29323
sg118
(lp29324
sg332
(lp29325
sg4
(lp29326
sg6
(lp29327
sg235
(lp29328
sg99
(lp29329
sg384
(lp29330
sg124
(lp29331
sg126
(lp29332
sg130
(lp29333
sg138
(lp29334
sg354
(lp29335
I1228
assS'ec'
p29336
(dp29337
g230
(lp29338
sg36
(lp29339
sg85
(lp29340
sg306
(lp29341
sg130
(lp29342
I2396
asg110
(lp29343
ssS'eb'
p29344
(dp29345
g32
(lp29346
sg318
(lp29347
I1011
assS'bradley'
p29348
(dp29349
g256
(lp29350
I2123
assS'ey'
p29351
(dp29352
g145
(lp29353
I2434
asg85
(lp29354
ssS'ex'
p29355
(dp29356
g30
(lp29357
sg116
(lp29358
sg32
(lp29359
sg145
(lp29360
sg235
(lp29361
sg344
(lp29362
sg221
(lp29363
sg124
(lp29364
sg126
(lp29365
sg74
(lp29366
sg303
(lp29367
sg38
(lp29368
sg12
(lp29369
sg354
(lp29370
I1263
asg50
(lp29371
sg223
(lp29372
sg149
(lp29373
ssS'year'
p29374
(dp29375
g230
(lp29376
sg145
(lp29377
sg277
(lp29378
sg78
(lp29379
sg114
(lp29380
sg44
(lp29381
sg128
(lp29382
sg94
(lp29383
sg135
(lp29384
I415
asg223
(lp29385
sg91
(lp29386
ssS'eu'
p29387
(dp29388
g116
(lp29389
sg104
(lp29390
I2369
asg38
(lp29391
ssS'et'
p29392
(dp29393
g329
(lp29394
sg26
(lp29395
sg277
(lp29396
sg163
(lp29397
sg72
(lp29398
sg80
(lp29399
sg283
(lp29400
sg181
(lp29401
sg303
(lp29402
sg30
(lp29403
sg287
(lp29404
sg74
(lp29405
sg176
(lp29406
sg256
(lp29407
sg76
(lp29408
sg118
(lp29409
sg460
(lp29410
sg484
(lp29411
sg38
(lp29412
sg83
(lp29413
sg85
(lp29414
sg124
(lp29415
sg42
(lp29416
I3374
asg87
(lp29417
sg89
(lp29418
sg91
(lp29419
sg245
(lp29420
sg94
(lp29421
sg96
(lp29422
sg48
(lp29423
sg99
(lp29424
sg313
(lp29425
sg44
(lp29426
sg149
(lp29427
sg174
(lp29428
sg32
(lp29429
sg350
(lp29430
sg429
(lp29431
sg318
(lp29432
sg178
(lp29433
sg106
(lp29434
sg108
(lp29435
sg110
(lp29436
sg63
(lp29437
sg52
(lp29438
sg438
(lp29439
sg440
(lp29440
sg18
(lp29441
sg121
(lp29442
sg4
(lp29443
sg6
(lp29444
sg8
(lp29445
sg36
(lp29446
sg384
(lp29447
sg235
(lp29448
sg126
(lp29449
sg344
(lp29450
sg128
(lp29451
sg130
(lp29452
sg14
(lp29453
sg16
(lp29454
sg135
(lp29455
sg50
(lp29456
sg138
(lp29457
sg354
(lp29458
ssS'happen'
p29459
(dp29460
g277
(lp29461
sg8
(lp29462
sg295
(lp29463
sg183
(lp29464
sg460
(lp29465
sg306
(lp29466
sg89
(lp29467
sg140
(lp29468
I1693
asg63
(lp29469
sg223
(lp29470
ssS'ev'
p29471
(dp29472
g344
(lp29473
sg130
(lp29474
I2149
assS'eq'
p29475
(dp29476
g230
(lp29477
sg68
(lp29478
sg8
(lp29479
sg36
(lp29480
sg460
(lp29481
sg262
(lp29482
sg126
(lp29483
sg72
(lp29484
sg163
(lp29485
sg130
(lp29486
sg132
(lp29487
I742
asg293
(lp29488
sg99
(lp29489
sg38
(lp29490
ssS'ep'
p29491
(dp29492
g38
(lp29493
sg76
(lp29494
I2663
assS'es'
p29495
(dp29496
g80
(lp29497
sg72
(lp29498
sg341
(lp29499
sg42
(lp29500
I1229
asg48
(lp29501
sg221
(lp29502
ssS'er'
p29503
(dp29504
g230
(lp29505
sg22
(lp29506
sg76
(lp29507
sg295
(lp29508
sg183
(lp29509
sg245
(lp29510
I597
assS'subnet'
p29511
(dp29512
g295
(lp29513
sg104
(lp29514
I45
asg183
(lp29515
ssS'shown'
p29516
(dp29517
g124
(lp29518
sg70
(lp29519
sg78
(lp29520
sg277
(lp29521
sg163
(lp29522
sg72
(lp29523
sg303
(lp29524
sg283
(lp29525
sg85
(lp29526
sg460
(lp29527
sg36
(lp29528
sg181
(lp29529
sg40
(lp29530
sg26
(lp29531
sg30
(lp29532
sg350
(lp29533
sg145
(lp29534
sg256
(lp29535
sg76
(lp29536
sg118
(lp29537
sg295
(lp29538
sg183
(lp29539
sg59
(lp29540
sg80
(lp29541
sg38
(lp29542
sg114
(lp29543
sg63
(lp29544
sg42
(lp29545
I3284
asg87
(lp29546
sg89
(lp29547
sg91
(lp29548
sg12
(lp29549
sg94
(lp29550
sg20
(lp29551
sg18
(lp29552
sg99
(lp29553
sg313
(lp29554
sg44
(lp29555
sg149
(lp29556
sg230
(lp29557
sg329
(lp29558
sg293
(lp29559
sg116
(lp29560
sg32
(lp29561
sg245
(lp29562
sg429
(lp29563
sg68
(lp29564
sg46
(lp29565
sg102
(lp29566
sg104
(lp29567
sg106
(lp29568
sg108
(lp29569
sg110
(lp29570
sg178
(lp29571
sg52
(lp29572
sg22
(lp29573
sg216
(lp29574
sg438
(lp29575
sg440
(lp29576
sg332
(lp29577
sg121
(lp29578
sg4
(lp29579
sg6
(lp29580
sg8
(lp29581
sg34
(lp29582
sg221
(lp29583
sg384
(lp29584
sg235
(lp29585
sg126
(lp29586
sg281
(lp29587
sg10
(lp29588
sg535
(lp29589
sg344
(lp29590
sg223
(lp29591
sg128
(lp29592
sg130
(lp29593
sg14
(lp29594
sg16
(lp29595
sg135
(lp29596
sg138
(lp29597
sg140
(lp29598
sg354
(lp29599
ssS'accomplish'
p29600
(dp29601
g116
(lp29602
sg118
(lp29603
sg332
(lp29604
sg70
(lp29605
sg8
(lp29606
sg295
(lp29607
sg183
(lp29608
sg163
(lp29609
sg42
(lp29610
I1800
asg20
(lp29611
sg99
(lp29612
ssS'exang'
p29613
(dp29614
g91
(lp29615
I2094
assS'space'
p29616
(dp29617
g124
(lp29618
sg70
(lp29619
sg277
(lp29620
sg163
(lp29621
sg72
(lp29622
sg68
(lp29623
sg293
(lp29624
sg85
(lp29625
sg460
(lp29626
sg30
(lp29627
sg287
(lp29628
sg74
(lp29629
sg145
(lp29630
sg256
(lp29631
sg80
(lp29632
sg262
(lp29633
sg295
(lp29634
sg183
(lp29635
sg59
(lp29636
sg484
(lp29637
sg38
(lp29638
sg83
(lp29639
sg114
(lp29640
sg303
(lp29641
sg42
(lp29642
I927
asg306
(lp29643
sg87
(lp29644
sg89
(lp29645
sg91
(lp29646
sg12
(lp29647
sg46
(lp29648
sg96
(lp29649
sg48
(lp29650
sg99
(lp29651
sg313
(lp29652
sg44
(lp29653
sg149
(lp29654
sg118
(lp29655
sg230
(lp29656
sg329
(lp29657
sg18
(lp29658
sg116
(lp29659
sg32
(lp29660
sg245
(lp29661
sg318
(lp29662
sg102
(lp29663
sg104
(lp29664
sg108
(lp29665
sg110
(lp29666
sg63
(lp29667
sg52
(lp29668
sg22
(lp29669
sg216
(lp29670
sg438
(lp29671
sg440
(lp29672
sg332
(lp29673
sg178
(lp29674
sg4
(lp29675
sg181
(lp29676
sg8
(lp29677
sg221
(lp29678
sg384
(lp29679
sg235
(lp29680
sg126
(lp29681
sg341
(lp29682
sg10
(lp29683
sg40
(lp29684
sg344
(lp29685
sg223
(lp29686
sg128
(lp29687
sg130
(lp29688
sg50
(lp29689
sg138
(lp29690
sg140
(lp29691
sg354
(lp29692
ssS'barnum'
p29693
(dp29694
g63
(lp29695
I3003
assS'pathfind'
p29696
(dp29697
g91
(lp29698
I477
assS'disinhibit'
p29699
(dp29700
g216
(lp29701
sg4
(lp29702
I473
assS'irong'
p29703
(dp29704
g106
(lp29705
I569
assS'rational'
p29706
(dp29707
g183
(lp29708
I5607
assS'overlook'
p29709
(dp29710
g34
(lp29711
I640
asg293
(lp29712
ssS'magnocellular'
p29713
(dp29714
g116
(lp29715
I375
assS'rld'
p29716
(dp29717
g83
(lp29718
I2249
assS'lms'
p29719
(dp29720
g63
(lp29721
I2441
assS'argu'
p29722
(dp29723
g118
(lp29724
sg74
(lp29725
sg332
(lp29726
sg4
(lp29727
sg6
(lp29728
sg8
(lp29729
sg36
(lp29730
sg124
(lp29731
sg72
(lp29732
sg85
(lp29733
sg318
(lp29734
sg128
(lp29735
I917
asg18
(lp29736
ssS'cart'
p29737
(dp29738
g183
(lp29739
I5433
assS'lmi'
p29740
(dp29741
g72
(lp29742
I2702
assS'kearn'
p29743
(dp29744
g145
(lp29745
I3030
asg85
(lp29746
ssS'carl'
p29747
(dp29748
g132
(lp29749
I3524
asg124
(lp29750
sg126
(lp29751
ssS'spemannstrafi'
p29752
(dp29753
g216
(lp29754
I19
assS'rlt'
p29755
(dp29756
g6
(lp29757
I497
assS'pentland'
p29758
(dp29759
g59
(lp29760
I3125
asg293
(lp29761
ssS'rlp'
p29762
(dp29763
g83
(lp29764
I2247
assS'card'
p29765
(dp29766
g283
(lp29767
sg4
(lp29768
sg80
(lp29769
sg78
(lp29770
sg10
(lp29771
sg14
(lp29772
sg16
(lp29773
sg135
(lp29774
I1441
asg114
(lp29775
ssS'care'
p29776
(dp29777
g277
(lp29778
sg34
(lp29779
sg59
(lp29780
sg10
(lp29781
sg12
(lp29782
sg132
(lp29783
sg14
(lp29784
sg16
(lp29785
I851
asg63
(lp29786
sg44
(lp29787
sg245
(lp29788
ssS'oflong'
p29789
(dp29790
g106
(lp29791
I2813
asg99
(lp29792
ssS'ojpm'
p29793
(dp29794
g329
(lp29795
I1249
assS'zeiger'
p29796
(dp29797
g104
(lp29798
I862
assS'annaswami'
p29799
(dp29800
g230
(lp29801
sg46
(lp29802
I3680
assS'forecast'
p29803
(dp29804
g295
(lp29805
sg183
(lp29806
sg70
(lp29807
sg235
(lp29808
I3036
assS'british'
p29809
(dp29810
g216
(lp29811
I2351
asg87
(lp29812
ssS'mella'
p29813
(dp29814
g460
(lp29815
I326
assS'fukunaga'
p29816
(dp29817
g281
(lp29818
I925
assS'lla'
p29819
(dp29820
g221
(lp29821
I961
assS'eeprom'
p29822
(dp29823
g14
(lp29824
I2875
assS'lambda'
p29825
(dp29826
g132
(lp29827
I3730
assS'xj'
p29828
(dp29829
g438
(lp29830
I1926
asg440
(lp29831
sg287
(lp29832
sg78
(lp29833
sg38
(lp29834
sg429
(lp29835
sg128
(lp29836
sg130
(lp29837
sg46
(lp29838
sg535
(lp29839
ssS'xk'
p29840
(dp29841
g30
(lp29842
sg36
(lp29843
sg38
(lp29844
sg42
(lp29845
I1662
asg130
(lp29846
sg18
(lp29847
sg221
(lp29848
sg535
(lp29849
sg223
(lp29850
ssS'xi'
p29851
(dp29852
g281
(lp29853
sg40
(lp29854
sg287
(lp29855
sg145
(lp29856
sg76
(lp29857
sg78
(lp29858
sg38
(lp29859
sg42
(lp29860
I1317
asg306
(lp29861
sg89
(lp29862
sg46
(lp29863
sg96
(lp29864
sg535
(lp29865
sg223
(lp29866
sg149
(lp29867
sg116
(lp29868
sg429
(lp29869
sg104
(lp29870
sg230
(lp29871
sg438
(lp29872
sg332
(lp29873
sg34
(lp29874
sg36
(lp29875
sg72
(lp29876
sg341
(lp29877
sg313
(lp29878
sg128
(lp29879
sg130
(lp29880
sg354
(lp29881
ssS'xn'
p29882
(dp29883
g440
(lp29884
sg281
(lp29885
sg341
(lp29886
sg89
(lp29887
sg46
(lp29888
sg354
(lp29889
I557
assS'xo'
p29890
(dp29891
g46
(lp29892
sg126
(lp29893
sg341
(lp29894
I1982
asg52
(lp29895
sg287
(lp29896
ssS'xl'
p29897
(dp29898
g277
(lp29899
sg163
(lp29900
sg145
(lp29901
sg80
(lp29902
sg262
(lp29903
sg78
(lp29904
sg484
(lp29905
sg38
(lp29906
sg42
(lp29907
I780
asg89
(lp29908
sg46
(lp29909
sg32
(lp29910
sg230
(lp29911
sg440
(lp29912
sg332
(lp29913
sg76
(lp29914
sg36
(lp29915
sg124
(lp29916
sg72
(lp29917
sg281
(lp29918
sg40
(lp29919
sg128
(lp29920
sg354
(lp29921
ssS'xm'
p29922
(dp29923
g104
(lp29924
I1235
asg281
(lp29925
ssS'xb'
p29926
(dp29927
g318
(lp29928
I1607
assS'rint'
p29929
(dp29930
g72
(lp29931
I3497
assS'xa'
p29932
(dp29933
g318
(lp29934
sg145
(lp29935
sg8
(lp29936
I957
assS'xf'
p29937
(dp29938
g38
(lp29939
I842
assS'cpus'
p29940
(dp29941
g10
(lp29942
I1788
assS'xd'
p29943
(dp29944
g145
(lp29945
sg277
(lp29946
sg76
(lp29947
sg124
(lp29948
sg341
(lp29949
sg313
(lp29950
I1358
asg46
(lp29951
sg535
(lp29952
sg149
(lp29953
ssS'blink'
p29954
(dp29955
g6
(lp29956
I1977
assS'xz'
p29957
(dp29958
g145
(lp29959
I714
assS'xx'
p29960
(dp29961
g295
(lp29962
I1687
asg183
(lp29963
sg70
(lp29964
ssS'xy'
p29965
(dp29966
g313
(lp29967
I1000
asg6
(lp29968
sg63
(lp29969
ssS'xr'
p29970
(dp29971
g76
(lp29972
I1686
assS'ring'
p29973
(dp29974
g132
(lp29975
sg20
(lp29976
sg68
(lp29977
sg178
(lp29978
sg354
(lp29979
I2916
assS'xv'
p29980
(dp29981
g130
(lp29982
I1308
assS'angular'
p29983
(dp29984
g42
(lp29985
I2021
asg32
(lp29986
sg48
(lp29987
sg350
(lp29988
ssS'xt'
p29989
(dp29990
g230
(lp29991
sg76
(lp29992
sg235
(lp29993
sg36
(lp29994
sg341
(lp29995
sg42
(lp29996
I794
asg130
(lp29997
sg46
(lp29998
sg354
(lp29999
ssS'size'
p30000
(dp30001
g124
(lp30002
sg78
(lp30003
sg277
(lp30004
sg163
(lp30005
sg68
(lp30006
sg281
(lp30007
sg283
(lp30008
sg85
(lp30009
sg181
(lp30010
sg26
(lp30011
sg287
(lp30012
sg74
(lp30013
sg176
(lp30014
sg145
(lp30015
sg256
(lp30016
sg295
(lp30017
sg183
(lp30018
sg484
(lp30019
sg38
(lp30020
sg83
(lp30021
sg114
(lp30022
sg42
(lp30023
I1360
asg306
(lp30024
sg91
(lp30025
sg245
(lp30026
sg20
(lp30027
sg18
(lp30028
sg221
(lp30029
sg223
(lp30030
sg149
(lp30031
sg318
(lp30032
sg104
(lp30033
sg110
(lp30034
sg63
(lp30035
sg22
(lp30036
sg216
(lp30037
sg438
(lp30038
sg332
(lp30039
sg178
(lp30040
sg4
(lp30041
sg6
(lp30042
sg8
(lp30043
sg34
(lp30044
sg36
(lp30045
sg460
(lp30046
sg235
(lp30047
sg126
(lp30048
sg341
(lp30049
sg10
(lp30050
sg40
(lp30051
sg344
(lp30052
sg128
(lp30053
sg130
(lp30054
sg50
(lp30055
sg140
(lp30056
sg354
(lp30057
ssS'wilczek'
p30058
(dp30059
g341
(lp30060
I2844
assS'siipola'
p30061
(dp30062
g99
(lp30063
I2864
assS'silent'
p30064
(dp30065
g174
(lp30066
sg106
(lp30067
I2157
assS'tibshirani'
p30068
(dp30069
g295
(lp30070
sg183
(lp30071
sg484
(lp30072
sg221
(lp30073
sg138
(lp30074
I3246
assS'yil'
p30075
(dp30076
g72
(lp30077
I2854
assS'caught'
p30078
(dp30079
g126
(lp30080
I1531
assS'yin'
p30081
(dp30082
g42
(lp30083
I1510
asg341
(lp30084
ssS'yii'
p30085
(dp30086
g130
(lp30087
I1306
assS'yij'
p30088
(dp30089
g118
(lp30090
sg384
(lp30091
sg318
(lp30092
sg140
(lp30093
I420
assS'silenc'
p30094
(dp30095
g116
(lp30096
sg96
(lp30097
I1907
assS'cumul'
p30098
(dp30099
g384
(lp30100
sg114
(lp30101
sg163
(lp30102
sg4
(lp30103
I1345
asg8
(lp30104
ssS'yix'
p30105
(dp30106
g354
(lp30107
I1886
assS'friend'
p30108
(dp30109
g22
(lp30110
I2364
asg63
(lp30111
ssS'baldi'
p30112
(dp30113
g68
(lp30114
I10
assS'tennin'
p30115
(dp30116
g110
(lp30117
I574
assS'especi'
p30118
(dp30119
g230
(lp30120
sg30
(lp30121
sg318
(lp30122
sg4
(lp30123
sg262
(lp30124
sg34
(lp30125
sg59
(lp30126
sg484
(lp30127
sg126
(lp30128
sg281
(lp30129
sg44
(lp30130
sg42
(lp30131
I1122
asg344
(lp30132
sg91
(lp30133
sg223
(lp30134
ssS'pittsburgh'
p30135
(dp30136
g4
(lp30137
sg80
(lp30138
sg344
(lp30139
sg484
(lp30140
sg277
(lp30141
sg89
(lp30142
sg91
(lp30143
sg132
(lp30144
I3499
asg94
(lp30145
sg221
(lp30146
sg313
(lp30147
sg223
(lp30148
sg149
(lp30149
ssS'quar'
p30150
(dp30151
g83
(lp30152
I1094
assS'pnl'
p30153
(dp30154
g235
(lp30155
I3092
assS'quak'
p30156
(dp30157
g181
(lp30158
I29
assS'thai'
p30159
(dp30160
g91
(lp30161
I2098
assS'than'
p30162
(dp30163
g80
(lp30164
sg293
(lp30165
sg344
(lp30166
sg78
(lp30167
sg59
(lp30168
sg484
(lp30169
sg38
(lp30170
sg83
(lp30171
sg85
(lp30172
sg303
(lp30173
sg438
(lp30174
sg116
(lp30175
sg118
(lp30176
sg34
(lp30177
sg36
(lp30178
sg460
(lp30179
sg68
(lp30180
sg281
(lp30181
sg10
(lp30182
sg40
(lp30183
sg283
(lp30184
sg70
(lp30185
sg26
(lp30186
sg277
(lp30187
sg163
(lp30188
sg89
(lp30189
sg91
(lp30190
sg94
(lp30191
sg96
(lp30192
sg48
(lp30193
sg99
(lp30194
sg44
(lp30195
sg429
(lp30196
sg102
(lp30197
sg104
(lp30198
sg106
(lp30199
sg108
(lp30200
sg110
(lp30201
sg63
(lp30202
sg52
(lp30203
sg114
(lp30204
sg128
(lp30205
sg130
(lp30206
sg132
(lp30207
sg14
(lp30208
sg16
(lp30209
sg135
(lp30210
sg50
(lp30211
sg138
(lp30212
sg140
(lp30213
sg354
(lp30214
sg306
(lp30215
sg87
(lp30216
sg245
(lp30217
sg46
(lp30218
sg20
(lp30219
sg18
(lp30220
sg221
(lp30221
sg535
(lp30222
sg223
(lp30223
sg350
(lp30224
sg216
(lp30225
sg174
(lp30226
sg440
(lp30227
sg332
(lp30228
sg121
(lp30229
sg4
(lp30230
sg6
(lp30231
sg126
(lp30232
sg30
(lp30233
sg287
(lp30234
sg74
(lp30235
sg176
(lp30236
sg145
(lp30237
sg256
(lp30238
sg76
(lp30239
sg262
(lp30240
sg295
(lp30241
sg183
(lp30242
sg42
(lp30243
I2251
asg329
(lp30244
sg32
(lp30245
sg318
(lp30246
sg178
(lp30247
sg22
(lp30248
sg181
(lp30249
sg235
(lp30250
sg124
(lp30251
ssS'recruit'
p30252
(dp30253
g99
(lp30254
I2247
assS'hemineglect'
p30255
(dp30256
g303
(lp30257
I10
assS'fork'
p30258
(dp30259
g132
(lp30260
I855
assS'delic'
p30261
(dp30262
g42
(lp30263
I2683
assS'hikaridai'
p30264
(dp30265
g295
(lp30266
sg183
(lp30267
sg18
(lp30268
I40
assS'angel'
p30269
(dp30270
g181
(lp30271
I26
asg303
(lp30272
ssS'ubennay'
p30273
(dp30274
g48
(lp30275
I297
assS'delin'
p30276
(dp30277
g50
(lp30278
I901
assS'slab'
p30279
(dp30280
g104
(lp30281
sg48
(lp30282
I1081
assS'windsor'
p30283
(dp30284
g14
(lp30285
sg16
(lp30286
I51
assS'unsampl'
p30287
(dp30288
g484
(lp30289
I628
assS'engin'
p30290
(dp30291
g283
(lp30292
sg104
(lp30293
sg176
(lp30294
sg76
(lp30295
sg59
(lp30296
sg87
(lp30297
sg91
(lp30298
sg245
(lp30299
sg46
(lp30300
sg20
(lp30301
sg178
(lp30302
sg63
(lp30303
sg114
(lp30304
sg230
(lp30305
sg329
(lp30306
sg121
(lp30307
sg22
(lp30308
sg181
(lp30309
sg8
(lp30310
sg36
(lp30311
sg460
(lp30312
sg281
(lp30313
sg10
(lp30314
sg40
(lp30315
sg132
(lp30316
sg14
(lp30317
sg135
(lp30318
sg50
(lp30319
I32
assS'waibel'
p30320
(dp30321
g94
(lp30322
sg108
(lp30323
sg121
(lp30324
sg128
(lp30325
I2849
assS'lumpi'
p30326
(dp30327
g181
(lp30328
I2151
assS'wcb'
p30329
(dp30330
g354
(lp30331
I1447
assS'particl'
p30332
(dp30333
g438
(lp30334
I1761
asg283
(lp30335
ssS'similiar'
p30336
(dp30337
g128
(lp30338
I1840
assS'begin'
p30339
(dp30340
g277
(lp30341
sg74
(lp30342
sg176
(lp30343
sg76
(lp30344
sg262
(lp30345
sg344
(lp30346
sg59
(lp30347
sg38
(lp30348
sg85
(lp30349
sg20
(lp30350
sg221
(lp30351
sg44
(lp30352
sg149
(lp30353
sg329
(lp30354
sg440
(lp30355
sg318
(lp30356
sg99
(lp30357
sg68
(lp30358
sg281
(lp30359
sg128
(lp30360
sg132
(lp30361
sg50
(lp30362
sg354
(lp30363
I2763
assS'sprach'
p30364
(dp30365
g440
(lp30366
I2500
assS'price'
p30367
(dp30368
g384
(lp30369
sg126
(lp30370
sg8
(lp30371
I534
assS'silhouett'
p30372
(dp30373
g293
(lp30374
I635
assS'america'
p30375
(dp30376
g174
(lp30377
I2727
asg32
(lp30378
sg118
(lp30379
ssS'synthesi'
p30380
(dp30381
g68
(lp30382
sg138
(lp30383
I3293
asg293
(lp30384
ssS'renal'
p30385
(dp30386
g87
(lp30387
I169
assS'largish'
p30388
(dp30389
g74
(lp30390
I1989
assS'biasesl'
p30391
(dp30392
g295
(lp30393
I2675
asg183
(lp30394
ssS'aerosol'
p30395
(dp30396
g283
(lp30397
I1937
assS'mri'
p30398
(dp30399
g318
(lp30400
sg99
(lp30401
I3268
assS'gazzaniga'
p30402
(dp30403
g18
(lp30404
I2625
asg176
(lp30405
ssS'cerebellurn'
p30406
(dp30407
g350
(lp30408
I1211
assS'abraham'
p30409
(dp30410
g106
(lp30411
I2665
asg163
(lp30412
ssS'steadi'
p30413
(dp30414
g230
(lp30415
sg176
(lp30416
sg256
(lp30417
sg6
(lp30418
I1335
asg262
(lp30419
sg245
(lp30420
ssS'german'
p30421
(dp30422
g132
(lp30423
sg354
(lp30424
I12
assS'concurr'
p30425
(dp30426
g174
(lp30427
I2326
assS'handwrit'
p30428
(dp30429
g76
(lp30430
sg138
(lp30431
I3489
asg44
(lp30432
sg114
(lp30433
ssS'thefit'
p30434
(dp30435
g85
(lp30436
I2402
assS'fifth'
p30437
(dp30438
g30
(lp30439
sg116
(lp30440
sg318
(lp30441
sg484
(lp30442
sg429
(lp30443
sg140
(lp30444
I3186
assS'ground'
p30445
(dp30446
g118
(lp30447
sg20
(lp30448
sg293
(lp30449
sg313
(lp30450
I811
asg74
(lp30451
ssS'onli'
p30452
(dp30453
g80
(lp30454
sg293
(lp30455
sg344
(lp30456
sg78
(lp30457
sg59
(lp30458
sg484
(lp30459
sg38
(lp30460
sg83
(lp30461
sg85
(lp30462
sg303
(lp30463
sg116
(lp30464
sg118
(lp30465
sg34
(lp30466
sg36
(lp30467
sg460
(lp30468
sg68
(lp30469
sg72
(lp30470
sg10
(lp30471
sg40
(lp30472
sg283
(lp30473
sg70
(lp30474
sg26
(lp30475
sg277
(lp30476
sg163
(lp30477
sg89
(lp30478
sg91
(lp30479
sg12
(lp30480
sg94
(lp30481
sg96
(lp30482
sg48
(lp30483
sg99
(lp30484
sg313
(lp30485
sg44
(lp30486
sg149
(lp30487
sg429
(lp30488
sg102
(lp30489
sg104
(lp30490
sg106
(lp30491
sg108
(lp30492
sg110
(lp30493
sg63
(lp30494
sg52
(lp30495
sg114
(lp30496
sg128
(lp30497
sg130
(lp30498
sg132
(lp30499
sg14
(lp30500
sg16
(lp30501
sg135
(lp30502
sg50
(lp30503
sg138
(lp30504
sg140
(lp30505
sg354
(lp30506
sg306
(lp30507
sg87
(lp30508
sg245
(lp30509
sg46
(lp30510
sg20
(lp30511
sg18
(lp30512
sg221
(lp30513
sg535
(lp30514
sg223
(lp30515
sg350
(lp30516
sg216
(lp30517
sg438
(lp30518
sg440
(lp30519
sg121
(lp30520
sg4
(lp30521
sg6
(lp30522
sg126
(lp30523
sg341
(lp30524
sg30
(lp30525
sg287
(lp30526
sg74
(lp30527
sg176
(lp30528
sg145
(lp30529
sg256
(lp30530
sg76
(lp30531
sg262
(lp30532
sg295
(lp30533
sg183
(lp30534
sg42
(lp30535
I962
asg329
(lp30536
sg32
(lp30537
sg178
(lp30538
sg22
(lp30539
sg181
(lp30540
sg235
(lp30541
sg384
(lp30542
sg124
(lp30543
ssS'ratio'
p30544
(dp30545
g216
(lp30546
sg438
(lp30547
sg118
(lp30548
sg22
(lp30549
sg181
(lp30550
sg8
(lp30551
sg256
(lp30552
sg85
(lp30553
sg42
(lp30554
I3158
asg350
(lp30555
sg36
(lp30556
sg183
(lp30557
sg94
(lp30558
sg96
(lp30559
sg135
(lp30560
sg63
(lp30561
sg354
(lp30562
ssS'jezzard'
p30563
(dp30564
g99
(lp30565
I3113
assS'proportion'
p30566
(dp30567
g63
(lp30568
I1346
assS'smallvik'
p30569
(dp30570
g130
(lp30571
I2915
assS'daag'
p30572
(dp30573
g438
(lp30574
I2382
assS'threefold'
p30575
(dp30576
g329
(lp30577
I2103
assS'thompson'
p30578
(dp30579
g12
(lp30580
I2764
asg287
(lp30581
ssS'cannon'
p30582
(dp30583
g350
(lp30584
I1010
assS'clki'
p30585
(dp30586
g22
(lp30587
I1160
assS'samuel'
p30588
(dp30589
g132
(lp30590
I195
assS'unrealist'
p30591
(dp30592
g460
(lp30593
sg68
(lp30594
I2615
assS'cannot'
p30595
(dp30596
g70
(lp30597
sg26
(lp30598
sg281
(lp30599
sg30
(lp30600
sg287
(lp30601
sg145
(lp30602
sg256
(lp30603
sg118
(lp30604
sg295
(lp30605
sg183
(lp30606
sg38
(lp30607
sg83
(lp30608
sg303
(lp30609
sg42
(lp30610
I2781
asg89
(lp30611
sg94
(lp30612
sg48
(lp30613
sg313
(lp30614
sg223
(lp30615
sg230
(lp30616
sg174
(lp30617
sg293
(lp30618
sg104
(lp30619
sg216
(lp30620
sg438
(lp30621
sg18
(lp30622
sg22
(lp30623
sg34
(lp30624
sg460
(lp30625
sg68
(lp30626
sg341
(lp30627
sg40
(lp30628
sg344
(lp30629
sg44
(lp30630
sg14
(lp30631
sg135
(lp30632
sg140
(lp30633
sg354
(lp30634
ssS'truli'
p30635
(dp30636
g14
(lp30637
I4512
asg318
(lp30638
sg76
(lp30639
ssS'seldom'
p30640
(dp30641
g140
(lp30642
I1692
assS'wkctest'
p30643
(dp30644
g235
(lp30645
I843
assS'lloo'
p30646
(dp30647
g306
(lp30648
I1026
assS'thalach'
p30649
(dp30650
g91
(lp30651
I2093
assS'aza'
p30652
(dp30653
g20
(lp30654
I31
assS'sonar'
p30655
(dp30656
g114
(lp30657
I288
assS'telecommun'
p30658
(dp30659
g121
(lp30660
I2616
assS'husband'
p30661
(dp30662
g72
(lp30663
I2557
assS'salomon'
p30664
(dp30665
g34
(lp30666
I896
assS'sejnowskit'
p30667
(dp30668
g350
(lp30669
I14
assS'burst'
p30670
(dp30671
g174
(lp30672
sg106
(lp30673
I239
asg6
(lp30674
ssS'unfil'
p30675
(dp30676
g245
(lp30677
I770
assS'pcmp'
p30678
(dp30679
g256
(lp30680
I25
assS'anali'
p30681
(dp30682
g318
(lp30683
I2873
assS'angina'
p30684
(dp30685
g91
(lp30686
I2126
assS'sport'
p30687
(dp30688
g20
(lp30689
I2474
assS'concern'
p30690
(dp30691
g438
(lp30692
I2181
asg32
(lp30693
sg176
(lp30694
sg287
(lp30695
sg344
(lp30696
sg384
(lp30697
sg124
(lp30698
sg126
(lp30699
sg429
(lp30700
sg118
(lp30701
sg52
(lp30702
sg306
(lp30703
sg89
(lp30704
sg223
(lp30705
sg96
(lp30706
sg106
(lp30707
sg313
(lp30708
sg44
(lp30709
sg114
(lp30710
ssS'ufrj'
p30711
(dp30712
g118
(lp30713
I24
assS'blanchard'
p30714
(dp30715
g59
(lp30716
I3440
assS'jtl'
p30717
(dp30718
g174
(lp30719
I1825
assS'ytl'
p30720
(dp30721
g104
(lp30722
I1238
assS'jti'
p30723
(dp30724
g18
(lp30725
I979
assS'jth'
p30726
(dp30727
g438
(lp30728
I1951
asg104
(lp30729
sg46
(lp30730
sg108
(lp30731
sg535
(lp30732
sg114
(lp30733
ssS'between'
p30734
(dp30735
g80
(lp30736
sg293
(lp30737
sg78
(lp30738
sg59
(lp30739
sg484
(lp30740
sg38
(lp30741
sg83
(lp30742
sg85
(lp30743
sg303
(lp30744
sg438
(lp30745
sg116
(lp30746
sg118
(lp30747
sg34
(lp30748
sg36
(lp30749
sg460
(lp30750
sg68
(lp30751
sg281
(lp30752
sg10
(lp30753
sg40
(lp30754
sg283
(lp30755
sg70
(lp30756
sg26
(lp30757
sg277
(lp30758
sg163
(lp30759
sg91
(lp30760
sg12
(lp30761
sg94
(lp30762
sg96
(lp30763
sg48
(lp30764
sg99
(lp30765
sg313
(lp30766
sg44
(lp30767
sg149
(lp30768
sg429
(lp30769
sg102
(lp30770
sg104
(lp30771
sg106
(lp30772
sg108
(lp30773
sg110
(lp30774
sg63
(lp30775
sg52
(lp30776
sg130
(lp30777
sg132
(lp30778
sg14
(lp30779
sg16
(lp30780
sg50
(lp30781
sg138
(lp30782
sg140
(lp30783
sg354
(lp30784
sg306
(lp30785
sg87
(lp30786
sg245
(lp30787
sg46
(lp30788
sg18
(lp30789
sg221
(lp30790
sg535
(lp30791
sg223
(lp30792
sg350
(lp30793
sg216
(lp30794
sg174
(lp30795
sg440
(lp30796
sg332
(lp30797
sg121
(lp30798
sg4
(lp30799
sg6
(lp30800
sg8
(lp30801
sg126
(lp30802
sg341
(lp30803
sg30
(lp30804
sg287
(lp30805
sg74
(lp30806
sg176
(lp30807
sg145
(lp30808
sg256
(lp30809
sg76
(lp30810
sg262
(lp30811
sg42
(lp30812
I1780
asg329
(lp30813
sg32
(lp30814
sg318
(lp30815
sg178
(lp30816
sg22
(lp30817
sg181
(lp30818
sg235
(lp30819
sg384
(lp30820
sg124
(lp30821
ssS'import'
p30822
(dp30823
g124
(lp30824
sg70
(lp30825
sg26
(lp30826
sg277
(lp30827
sg163
(lp30828
sg283
(lp30829
sg303
(lp30830
sg287
(lp30831
sg74
(lp30832
sg176
(lp30833
sg145
(lp30834
sg118
(lp30835
sg295
(lp30836
sg183
(lp30837
sg59
(lp30838
sg484
(lp30839
sg85
(lp30840
sg63
(lp30841
sg306
(lp30842
sg89
(lp30843
sg91
(lp30844
sg12
(lp30845
sg96
(lp30846
sg48
(lp30847
sg221
(lp30848
sg44
(lp30849
sg350
(lp30850
sg329
(lp30851
sg293
(lp30852
sg318
(lp30853
sg18
(lp30854
sg106
(lp30855
sg110
(lp30856
sg20
(lp30857
sg52
(lp30858
sg114
(lp30859
sg216
(lp30860
sg438
(lp30861
I2054
asg32
(lp30862
sg332
(lp30863
sg121
(lp30864
sg4
(lp30865
sg6
(lp30866
sg8
(lp30867
sg460
(lp30868
sg235
(lp30869
sg281
(lp30870
sg10
(lp30871
sg40
(lp30872
sg344
(lp30873
sg223
(lp30874
sg128
(lp30875
sg130
(lp30876
sg132
(lp30877
sg14
(lp30878
sg16
(lp30879
sg135
(lp30880
sg138
(lp30881
sg140
(lp30882
sg354
(lp30883
ssS'inflex'
p30884
(dp30885
g318
(lp30886
I298
assS'sompolinski'
p30887
(dp30888
g36
(lp30889
sg85
(lp30890
sg140
(lp30891
I3178
assS'measm'
p30892
(dp30893
g106
(lp30894
I939
assS'injormatik'
p30895
(dp30896
g221
(lp30897
I25
assS'kogur'
p30898
(dp30899
g114
(lp30900
I2440
assS'pertain'
p30901
(dp30902
g230
(lp30903
I411
asg181
(lp30904
ssS'parmanto'
p30905
(dp30906
g484
(lp30907
I8
assS'nearbi'
p30908
(dp30909
g216
(lp30910
sg438
(lp30911
I1778
asg318
(lp30912
sg70
(lp30913
sg34
(lp30914
sg124
(lp30915
sg102
(lp30916
sg12
(lp30917
sg149
(lp30918
ssS'mont'
p30919
(dp30920
g30
(lp30921
sg74
(lp30922
sg124
(lp30923
sg126
(lp30924
sg313
(lp30925
sg354
(lp30926
I80
assS'inconsist'
p30927
(dp30928
g262
(lp30929
sg91
(lp30930
sg130
(lp30931
I2908
assS'fracto'
p30932
(dp30933
g48
(lp30934
I2001
assS'comer'
p30935
(dp30936
g80
(lp30937
sg149
(lp30938
I765
assS'overview'
p30939
(dp30940
g318
(lp30941
sg26
(lp30942
sg293
(lp30943
sg68
(lp30944
sg306
(lp30945
sg87
(lp30946
sg104
(lp30947
sg96
(lp30948
sg138
(lp30949
I612
asg44
(lp30950
sg350
(lp30951
ssS'camcord'
p30952
(dp30953
g181
(lp30954
I1187
assS'qwerti'
p30955
(dp30956
g94
(lp30957
I452
assS'baltimor'
p30958
(dp30959
g106
(lp30960
I31
asg59
(lp30961
sg99
(lp30962
sg22
(lp30963
ssS'exploit'
p30964
(dp30965
g329
(lp30966
sg59
(lp30967
sg8
(lp30968
sg94
(lp30969
sg460
(lp30970
sg126
(lp30971
sg52
(lp30972
sg34
(lp30973
sg89
(lp30974
sg223
(lp30975
sg130
(lp30976
sg132
(lp30977
sg14
(lp30978
sg16
(lp30979
sg313
(lp30980
sg96
(lp30981
sg354
(lp30982
I1806
assS'amino'
p30983
(dp30984
g106
(lp30985
I1528
asg26
(lp30986
ssS'submanifold'
p30987
(dp30988
g32
(lp30989
I2800
asg72
(lp30990
ssS'wien'
p30991
(dp30992
g68
(lp30993
I28
assS'invert'
p30994
(dp30995
g174
(lp30996
sg22
(lp30997
sg138
(lp30998
I2406
asg96
(lp30999
sg20
(lp31000
sg223
(lp31001
ssS'asanuma'
p31002
(dp31003
g303
(lp31004
I2769
assS'invers'
p31005
(dp31006
g124
(lp31007
sg283
(lp31008
sg176
(lp31009
sg295
(lp31010
sg183
(lp31011
sg59
(lp31012
sg38
(lp31013
sg245
(lp31014
sg20
(lp31015
sg99
(lp31016
sg313
(lp31017
sg223
(lp31018
sg429
(lp31019
sg108
(lp31020
sg96
(lp31021
sg52
(lp31022
sg329
(lp31023
sg332
(lp31024
sg22
(lp31025
sg8
(lp31026
sg36
(lp31027
sg68
(lp31028
sg72
(lp31029
sg344
(lp31030
sg130
(lp31031
sg50
(lp31032
I1159
assS'springerverlag'
p31033
(dp31034
g22
(lp31035
I2450
assS'odel'
p31036
(dp31037
g87
(lp31038
I1756
assS'buhmann'
p31039
(dp31040
g130
(lp31041
I9
assS'bertrand'
p31042
(dp31043
g10
(lp31044
I2634
assS'avian'
p31045
(dp31046
g116
(lp31047
I1603
assS'emphas'
p31048
(dp31049
g32
(lp31050
sg6
(lp31051
sg181
(lp31052
sg124
(lp31053
sg281
(lp31054
sg85
(lp31055
sg303
(lp31056
sg12
(lp31057
I281
asg63
(lp31058
ssS'bienenstock'
p31059
(dp31060
g438
(lp31061
I62
asg318
(lp31062
sg484
(lp31063
sg429
(lp31064
sg106
(lp31065
sg140
(lp31066
ssS'textbook'
p31067
(dp31068
g287
(lp31069
I409
assS'aviat'
p31070
(dp31071
g281
(lp31072
I2410
assS'countless'
p31073
(dp31074
g114
(lp31075
I2038
assS'thesi'
p31076
(dp31077
g344
(lp31078
sg329
(lp31079
sg318
(lp31080
sg283
(lp31081
sg76
(lp31082
sg6
(lp31083
sg262
(lp31084
sg34
(lp31085
sg460
(lp31086
sg124
(lp31087
sg126
(lp31088
sg83
(lp31089
sg12
(lp31090
sg484
(lp31091
sg89
(lp31092
sg91
(lp31093
sg132
(lp31094
sg135
(lp31095
sg221
(lp31096
sg138
(lp31097
I3510
asg245
(lp31098
ssS'eiw'
p31099
(dp31100
g130
(lp31101
I2079
assS'these'
p31102
(dp31103
g329
(lp31104
sg70
(lp31105
sg78
(lp31106
sg277
(lp31107
sg163
(lp31108
sg116
(lp31109
sg68
(lp31110
sg80
(lp31111
sg281
(lp31112
sg283
(lp31113
sg85
(lp31114
sg460
(lp31115
sg26
(lp31116
sg30
(lp31117
sg287
(lp31118
sg74
(lp31119
sg176
(lp31120
sg256
(lp31121
sg76
(lp31122
sg262
(lp31123
sg295
(lp31124
sg183
(lp31125
sg59
(lp31126
sg484
(lp31127
sg38
(lp31128
sg83
(lp31129
sg114
(lp31130
sg303
(lp31131
sg306
(lp31132
sg87
(lp31133
sg89
(lp31134
sg91
(lp31135
sg12
(lp31136
sg94
(lp31137
sg96
(lp31138
sg18
(lp31139
sg99
(lp31140
sg313
(lp31141
sg44
(lp31142
sg149
(lp31143
sg118
(lp31144
sg230
(lp31145
sg174
(lp31146
sg293
(lp31147
sg32
(lp31148
sg350
(lp31149
sg429
(lp31150
sg318
(lp31151
sg46
(lp31152
sg102
(lp31153
sg178
(lp31154
sg106
(lp31155
sg108
(lp31156
sg110
(lp31157
sg63
(lp31158
sg52
(lp31159
sg22
(lp31160
sg216
(lp31161
sg438
(lp31162
I440
asg440
(lp31163
sg332
(lp31164
sg121
(lp31165
sg4
(lp31166
sg235
(lp31167
sg34
(lp31168
sg221
(lp31169
sg384
(lp31170
sg124
(lp31171
sg126
(lp31172
sg341
(lp31173
sg10
(lp31174
sg535
(lp31175
sg344
(lp31176
sg223
(lp31177
sg128
(lp31178
sg130
(lp31179
sg132
(lp31180
sg14
(lp31181
sg16
(lp31182
sg135
(lp31183
sg50
(lp31184
sg138
(lp31185
sg140
(lp31186
sg354
(lp31187
ssS'pram'
p31188
(dp31189
g283
(lp31190
I355
assS'trick'
p31191
(dp31192
g44
(lp31193
I2187
assS'explo'
p31194
(dp31195
g20
(lp31196
I281
assS'aeconatruotlon'
p31197
(dp31198
g72
(lp31199
I988
assS'eij'
p31200
(dp31201
g118
(lp31202
sg460
(lp31203
sg149
(lp31204
I937
assS'ramp'
p31205
(dp31206
g14
(lp31207
I4281
asg20
(lp31208
ssS'metric'
p31209
(dp31210
g32
(lp31211
sg78
(lp31212
sg181
(lp31213
sg295
(lp31214
sg183
(lp31215
sg281
(lp31216
sg80
(lp31217
sg341
(lp31218
sg74
(lp31219
sg429
(lp31220
sg44
(lp31221
sg130
(lp31222
I143
asg94
(lp31223
sg96
(lp31224
sg277
(lp31225
sg110
(lp31226
sg223
(lp31227
ssS'henc'
p31228
(dp31229
g163
(lp31230
sg287
(lp31231
sg176
(lp31232
sg256
(lp31233
sg80
(lp31234
sg484
(lp31235
sg303
(lp31236
sg42
(lp31237
I864
asg306
(lp31238
sg87
(lp31239
sg245
(lp31240
sg46
(lp31241
sg18
(lp31242
sg223
(lp31243
sg230
(lp31244
sg104
(lp31245
sg110
(lp31246
sg52
(lp31247
sg216
(lp31248
sg32
(lp31249
sg318
(lp31250
sg121
(lp31251
sg235
(lp31252
sg34
(lp31253
sg36
(lp31254
sg68
(lp31255
sg341
(lp31256
sg40
(lp31257
sg132
(lp31258
sg14
(lp31259
sg135
(lp31260
ssS'redlich'
p31261
(dp31262
g12
(lp31263
I2918
asg72
(lp31264
ssS'haploscop'
p31265
(dp31266
g118
(lp31267
I331
assS'yj'
p31268
(dp31269
g230
(lp31270
sg262
(lp31271
sg87
(lp31272
sg104
(lp31273
sg96
(lp31274
sg50
(lp31275
I1092
assS'eras'
p31276
(dp31277
g295
(lp31278
sg14
(lp31279
I3238
asg183
(lp31280
ssS'stipend'
p31281
(dp31282
g176
(lp31283
I2457
assS'uncod'
p31284
(dp31285
g72
(lp31286
I369
assS'uxi'
p31287
(dp31288
g313
(lp31289
I1400
assS'develop'
p31290
(dp31291
g283
(lp31292
sg26
(lp31293
sg277
(lp31294
sg116
(lp31295
sg303
(lp31296
sg74
(lp31297
sg145
(lp31298
sg80
(lp31299
sg76
(lp31300
sg293
(lp31301
sg344
(lp31302
sg78
(lp31303
sg59
(lp31304
sg484
(lp31305
sg63
(lp31306
sg42
(lp31307
I569
asg306
(lp31308
sg87
(lp31309
sg91
(lp31310
sg12
(lp31311
sg94
(lp31312
sg96
(lp31313
sg48
(lp31314
sg99
(lp31315
sg223
(lp31316
sg149
(lp31317
sg230
(lp31318
sg118
(lp31319
sg429
(lp31320
sg318
(lp31321
sg102
(lp31322
sg104
(lp31323
sg106
(lp31324
sg110
(lp31325
sg20
(lp31326
sg52
(lp31327
sg22
(lp31328
sg216
(lp31329
sg438
(lp31330
sg440
(lp31331
sg332
(lp31332
sg4
(lp31333
sg181
(lp31334
sg72
(lp31335
sg281
(lp31336
sg10
(lp31337
sg40
(lp31338
sg132
(lp31339
sg14
(lp31340
sg16
(lp31341
sg135
(lp31342
sg138
(lp31343
sg140
(lp31344
sg354
(lp31345
ssS'yo'
p31346
(dp31347
g230
(lp31348
I1965
asg68
(lp31349
sg72
(lp31350
ssS'medin'
p31351
(dp31352
g110
(lp31353
I1230
assS'media'
p31354
(dp31355
g293
(lp31356
I17
assS'medic'
p31357
(dp31358
g74
(lp31359
sg318
(lp31360
sg277
(lp31361
sg6
(lp31362
sg110
(lp31363
sg303
(lp31364
sg91
(lp31365
I6
asg221
(lp31366
sg350
(lp31367
ssS'food'
p31368
(dp31369
g18
(lp31370
I128
asg80
(lp31371
ssS'epoch'
p31372
(dp31373
g121
(lp31374
sg277
(lp31375
sg295
(lp31376
sg183
(lp31377
sg126
(lp31378
sg85
(lp31379
sg34
(lp31380
sg89
(lp31381
sg128
(lp31382
sg178
(lp31383
sg96
(lp31384
sg138
(lp31385
I2111
assS'altmin'
p31386
(dp31387
g72
(lp31388
I1297
assS'document'
p31389
(dp31390
g30
(lp31391
sg26
(lp31392
sg76
(lp31393
sg484
(lp31394
sg85
(lp31395
sg42
(lp31396
I1670
asg114
(lp31397
ssS'finish'
p31398
(dp31399
g94
(lp31400
I1481
assS'closest'
p31401
(dp31402
g30
(lp31403
sg283
(lp31404
sg121
(lp31405
sg277
(lp31406
sg36
(lp31407
sg281
(lp31408
sg83
(lp31409
sg42
(lp31410
I1881
asg110
(lp31411
sg44
(lp31412
sg350
(lp31413
ssS'characlcrprobablllll'
p31414
(dp31415
g76
(lp31416
I1009
assS'someon'
p31417
(dp31418
g178
(lp31419
I176
asg83
(lp31420
ssS'rcurr'
p31421
(dp31422
g354
(lp31423
I1825
assS'condilion'
p31424
(dp31425
g4
(lp31426
I1559
assS'broadclass'
p31427
(dp31428
g87
(lp31429
I371
assS'ccs'
p31430
(dp31431
g89
(lp31432
I2920
assS'ccr'
p31433
(dp31434
g344
(lp31435
sg163
(lp31436
sg40
(lp31437
I2403
assS'ccd'
p31438
(dp31439
g283
(lp31440
sg114
(lp31441
I785
assS'ccg'
p31442
(dp31443
g6
(lp31444
I340
assS'dupli'
p31445
(dp31446
g72
(lp31447
I2423
assS'alpaydin'
p31448
(dp31449
g178
(lp31450
I23
assS'mulluay'
p31451
(dp31452
g128
(lp31453
I1585
assS'tjle'
p31454
(dp31455
g87
(lp31456
I1587
assS'neon'
p31457
(dp31458
g118
(lp31459
I2501
assS'bitmap'
p31460
(dp31461
g178
(lp31462
I367
assS'touch'
p31463
(dp31464
g176
(lp31465
I199
asg63
(lp31466
ssS'melton'
p31467
(dp31468
g99
(lp31469
I2794
assS'ergod'
p31470
(dp31471
g96
(lp31472
I1917
assS'offailur'
p31473
(dp31474
g44
(lp31475
I1330
assS'death'
p31476
(dp31477
g277
(lp31478
I576
assS'yu'
p31479
(dp31480
g230
(lp31481
I9
assS'specialpurpos'
p31482
(dp31483
g70
(lp31484
I1158
assS'desktop'
p31485
(dp31486
g52
(lp31487
I1927
assS'identif'
p31488
(dp31489
g230
(lp31490
sg438
(lp31491
sg108
(lp31492
sg181
(lp31493
sg293
(lp31494
sg295
(lp31495
sg183
(lp31496
sg85
(lp31497
sg303
(lp31498
sg42
(lp31499
I2600
asg283
(lp31500
sg128
(lp31501
sg46
(lp31502
sg104
(lp31503
sg135
(lp31504
sg110
(lp31505
sg114
(lp31506
ssS'treatment'
p31507
(dp31508
g32
(lp31509
sg124
(lp31510
sg277
(lp31511
sg484
(lp31512
sg138
(lp31513
I626
asg149
(lp31514
ssS'versa'
p31515
(dp31516
g429
(lp31517
sg332
(lp31518
sg70
(lp31519
sg176
(lp31520
sg140
(lp31521
I1638
assS'cb'
p31522
(dp31523
g87
(lp31524
I26
assS'momentum'
p31525
(dp31526
g34
(lp31527
sg108
(lp31528
I1830
asg126
(lp31529
sg114
(lp31530
sg124
(lp31531
ssS'earli'
p31532
(dp31533
g329
(lp31534
sg277
(lp31535
sg262
(lp31536
sg78
(lp31537
sg484
(lp31538
sg303
(lp31539
sg91
(lp31540
sg12
(lp31541
sg48
(lp31542
sg149
(lp31543
sg118
(lp31544
sg63
(lp31545
sg52
(lp31546
sg174
(lp31547
sg116
(lp31548
sg438
(lp31549
I1002
asg332
(lp31550
sg8
(lp31551
sg36
(lp31552
sg124
(lp31553
sg132
(lp31554
sg14
(lp31555
sg16
(lp31556
ssS'bistabl'
p31557
(dp31558
g18
(lp31559
I2384
assS'psi'
p31560
(dp31561
g12
(lp31562
I2748
assS'nielsen'
p31563
(dp31564
g36
(lp31565
I216
assS'rasta'
p31566
(dp31567
g440
(lp31568
sg121
(lp31569
I1213
assS'read'
p31570
(dp31571
g70
(lp31572
sg22
(lp31573
sg76
(lp31574
sg163
(lp31575
sg42
(lp31576
I1912
asg14
(lp31577
sg106
(lp31578
sg99
(lp31579
sg52
(lp31580
sg114
(lp31581
ssS'omnisci'
p31582
(dp31583
g83
(lp31584
I1464
assS'ame'
p31585
(dp31586
g110
(lp31587
I3049
asg293
(lp31588
ssS'leapfrog'
p31589
(dp31590
g124
(lp31591
sg126
(lp31592
I1144
assS'oftwo'
p31593
(dp31594
g96
(lp31595
I1325
assS'deplet'
p31596
(dp31597
g135
(lp31598
I344
asg256
(lp31599
ssS'amp'
p31600
(dp31601
g135
(lp31602
I999
assS'psp'
p31603
(dp31604
g384
(lp31605
I1328
assS'reap'
p31606
(dp31607
g89
(lp31608
I125
assS'pst'
p31609
(dp31610
g174
(lp31611
I2425
assS'parzen'
p31612
(dp31613
g318
(lp31614
I715
asg72
(lp31615
sg281
(lp31616
sg63
(lp31617
ssS'underyl'
p31618
(dp31619
g80
(lp31620
I495
assS'lhami'
p31621
(dp31622
g72
(lp31623
I2701
assS'muroga'
p31624
(dp31625
g40
(lp31626
I464
assS'linial'
p31627
(dp31628
g145
(lp31629
I3130
assS'benefit'
p31630
(dp31631
g70
(lp31632
sg145
(lp31633
sg4
(lp31634
sg277
(lp31635
sg8
(lp31636
sg295
(lp31637
sg183
(lp31638
sg59
(lp31639
sg235
(lp31640
sg89
(lp31641
sg91
(lp31642
sg128
(lp31643
sg245
(lp31644
sg94
(lp31645
sg114
(lp31646
sg221
(lp31647
sg63
(lp31648
sg140
(lp31649
sg354
(lp31650
I168
assS'cx'
p31651
(dp31652
g230
(lp31653
I2339
asg429
(lp31654
ssS't'
p31655
(dp31656
g80
(lp31657
sg293
(lp31658
sg344
(lp31659
sg78
(lp31660
sg59
(lp31661
sg484
(lp31662
sg38
(lp31663
sg83
(lp31664
sg85
(lp31665
sg303
(lp31666
sg438
(lp31667
sg116
(lp31668
sg118
(lp31669
sg34
(lp31670
sg36
(lp31671
sg460
(lp31672
sg68
(lp31673
sg72
(lp31674
sg281
(lp31675
sg40
(lp31676
sg283
(lp31677
sg70
(lp31678
sg26
(lp31679
sg277
(lp31680
sg163
(lp31681
sg89
(lp31682
sg91
(lp31683
sg12
(lp31684
sg94
(lp31685
sg96
(lp31686
sg48
(lp31687
sg99
(lp31688
sg313
(lp31689
sg44
(lp31690
sg149
(lp31691
sg429
(lp31692
sg102
(lp31693
sg104
(lp31694
sg106
(lp31695
sg108
(lp31696
sg110
(lp31697
sg63
(lp31698
sg52
(lp31699
sg114
(lp31700
sg128
(lp31701
sg130
(lp31702
sg132
(lp31703
sg14
(lp31704
sg16
(lp31705
sg50
(lp31706
sg138
(lp31707
sg140
(lp31708
sg354
(lp31709
sg306
(lp31710
sg87
(lp31711
sg245
(lp31712
sg46
(lp31713
sg20
(lp31714
sg18
(lp31715
sg221
(lp31716
sg535
(lp31717
sg223
(lp31718
sg350
(lp31719
sg216
(lp31720
sg174
(lp31721
sg440
(lp31722
sg332
(lp31723
sg121
(lp31724
sg4
(lp31725
sg6
(lp31726
sg8
(lp31727
sg126
(lp31728
sg341
(lp31729
sg30
(lp31730
sg287
(lp31731
sg74
(lp31732
sg176
(lp31733
sg145
(lp31734
sg256
(lp31735
sg76
(lp31736
sg262
(lp31737
sg295
(lp31738
sg183
(lp31739
sg42
(lp31740
I1008
asg230
(lp31741
sg329
(lp31742
sg32
(lp31743
sg318
(lp31744
sg178
(lp31745
sg22
(lp31746
sg181
(lp31747
sg235
(lp31748
sg384
(lp31749
sg124
(lp31750
ssS'cascad'
p31751
(dp31752
g59
(lp31753
sg121
(lp31754
sg22
(lp31755
sg128
(lp31756
I375
assS'output'
p31757
(dp31758
g329
(lp31759
sg70
(lp31760
sg78
(lp31761
sg277
(lp31762
sg163
(lp31763
sg303
(lp31764
sg283
(lp31765
sg85
(lp31766
sg40
(lp31767
sg26
(lp31768
sg287
(lp31769
sg176
(lp31770
sg145
(lp31771
sg256
(lp31772
sg76
(lp31773
sg262
(lp31774
sg295
(lp31775
sg183
(lp31776
sg59
(lp31777
sg484
(lp31778
sg38
(lp31779
sg83
(lp31780
sg114
(lp31781
sg63
(lp31782
sg42
(lp31783
I46
asg87
(lp31784
sg91
(lp31785
sg12
(lp31786
sg94
(lp31787
sg96
(lp31788
sg18
(lp31789
sg99
(lp31790
sg313
(lp31791
sg223
(lp31792
sg350
(lp31793
sg118
(lp31794
sg116
(lp31795
sg174
(lp31796
sg178
(lp31797
sg245
(lp31798
sg68
(lp31799
sg46
(lp31800
sg102
(lp31801
sg104
(lp31802
sg108
(lp31803
sg110
(lp31804
sg20
(lp31805
sg52
(lp31806
sg22
(lp31807
sg230
(lp31808
sg438
(lp31809
sg440
(lp31810
sg332
(lp31811
sg121
(lp31812
sg4
(lp31813
sg181
(lp31814
sg235
(lp31815
sg34
(lp31816
sg221
(lp31817
sg460
(lp31818
sg124
(lp31819
sg126
(lp31820
sg341
(lp31821
sg10
(lp31822
sg535
(lp31823
sg344
(lp31824
sg128
(lp31825
sg36
(lp31826
sg132
(lp31827
sg14
(lp31828
sg16
(lp31829
sg135
(lp31830
sg50
(lp31831
sg138
(lp31832
sg140
(lp31833
sg354
(lp31834
ssS'downward'
p31835
(dp31836
g42
(lp31837
I2340
asg44
(lp31838
ssS'verbal'
p31839
(dp31840
g74
(lp31841
sg99
(lp31842
I2770
asg181
(lp31843
ssS'rsisooo'
p31844
(dp31845
g10
(lp31846
I2719
assS'qdik'
p31847
(dp31848
g38
(lp31849
I2264
assS'depolar'
p31850
(dp31851
g106
(lp31852
I143
assS'nucleic'
p31853
(dp31854
g344
(lp31855
I3619
assS'suppl'
p31856
(dp31857
g283
(lp31858
I1939
assS'cyde'
p31859
(dp31860
g63
(lp31861
I1304
assS'nonzero'
p31862
(dp31863
g287
(lp31864
sg235
(lp31865
sg344
(lp31866
sg68
(lp31867
sg85
(lp31868
sg102
(lp31869
I1330
assS'wokingham'
p31870
(dp31871
g50
(lp31872
I1616
assS'kaiser'
p31873
(dp31874
g22
(lp31875
I2362
assS'lippmann'
p31876
(dp31877
g132
(lp31878
sg223
(lp31879
sg138
(lp31880
I3355
asg44
(lp31881
ssS'tlioa'
p31882
(dp31883
g132
(lp31884
I1275
assS'cw'
p31885
(dp31886
g124
(lp31887
I3020
assS'characteris'
p31888
(dp31889
g135
(lp31890
I306
assS'ethod'
p31891
(dp31892
g8
(lp31893
I1331
assS'rfispond'
p31894
(dp31895
g52
(lp31896
I1514
assS'deficit'
p31897
(dp31898
g135
(lp31899
I812
asg4
(lp31900
sg303
(lp31901
ssS'ig'
p31902
(dp31903
g287
(lp31904
I2871
assS'torsion'
p31905
(dp31906
g32
(lp31907
sg350
(lp31908
I2853
assS'welsch'
p31909
(dp31910
g295
(lp31911
I1790
asg183
(lp31912
ssS'vg'
p31913
(dp31914
g256
(lp31915
I883
assS'tdl'
p31916
(dp31917
g128
(lp31918
I314
assS'sixti'
p31919
(dp31920
g128
(lp31921
I1104
assS'sixth'
p31922
(dp31923
g132
(lp31924
I3826
asg76
(lp31925
sg163
(lp31926
ssS'ifc'
p31927
(dp31928
g281
(lp31929
I849
assS'aetiv'
p31930
(dp31931
g63
(lp31932
I1207
assS'hanusa'
p31933
(dp31934
g277
(lp31935
I3145
assS'ptkfos'
p31936
(dp31937
g74
(lp31938
I2431
assS'comparison'
p31939
(dp31940
g277
(lp31941
sg30
(lp31942
sg80
(lp31943
sg295
(lp31944
sg183
(lp31945
sg59
(lp31946
sg85
(lp31947
sg63
(lp31948
sg87
(lp31949
sg91
(lp31950
sg12
(lp31951
sg96
(lp31952
sg18
(lp31953
sg221
(lp31954
sg223
(lp31955
sg350
(lp31956
sg460
(lp31957
sg104
(lp31958
sg20
(lp31959
sg52
(lp31960
sg230
(lp31961
sg329
(lp31962
sg440
(lp31963
sg332
(lp31964
sg121
(lp31965
sg4
(lp31966
sg6
(lp31967
sg8
(lp31968
sg34
(lp31969
sg384
(lp31970
sg72
(lp31971
sg341
(lp31972
sg10
(lp31973
sg48
(lp31974
sg44
(lp31975
sg128
(lp31976
sg130
(lp31977
sg132
(lp31978
sg14
(lp31979
sg16
(lp31980
sg138
(lp31981
I2178
assS'central'
p31982
(dp31983
g26
(lp31984
sg287
(lp31985
sg74
(lp31986
sg76
(lp31987
sg59
(lp31988
sg484
(lp31989
sg85
(lp31990
sg245
(lp31991
sg221
(lp31992
sg44
(lp31993
sg350
(lp31994
sg102
(lp31995
sg114
(lp31996
sg216
(lp31997
sg118
(lp31998
sg32
(lp31999
sg4
(lp32000
sg6
(lp32001
sg68
(lp32002
sg130
(lp32003
sg132
(lp32004
sg140
(lp32005
I2802
assS'ozlz'
p32006
(dp32007
g32
(lp32008
I1094
assS'srj'
p32009
(dp32010
g74
(lp32011
I1459
assS'choi'
p32012
(dp32013
g20
(lp32014
I2687
assS'bonisson'
p32015
(dp32016
g354
(lp32017
I3106
assS'njtlrl'
p32018
(dp32019
g174
(lp32020
I1812
assS'kawano'
p32021
(dp32022
g99
(lp32023
I3411
assS'degre'
p32024
(dp32025
g80
(lp32026
sg76
(lp32027
sg262
(lp32028
sg484
(lp32029
sg38
(lp32030
sg83
(lp32031
sg85
(lp32032
sg303
(lp32033
sg48
(lp32034
sg12
(lp32035
sg18
(lp32036
sg221
(lp32037
sg149
(lp32038
sg116
(lp32039
sg429
(lp32040
sg318
(lp32041
sg63
(lp32042
sg216
(lp32043
sg32
(lp32044
sg332
(lp32045
sg121
(lp32046
sg6
(lp32047
sg181
(lp32048
sg99
(lp32049
I1730
asg384
(lp32050
sg68
(lp32051
sg281
(lp32052
sg10
(lp32053
sg40
(lp32054
sg460
(lp32055
ssS'ockerbloom'
p32056
(dp32057
g429
(lp32058
I2357
assS'wold'
p32059
(dp32060
g341
(lp32061
I589
assS'chow'
p32062
(dp32063
g138
(lp32064
I3301
assS'backup'
p32065
(dp32066
g89
(lp32067
I386
assS'processor'
p32068
(dp32069
g174
(lp32070
I2626
asg332
(lp32071
sg22
(lp32072
sg126
(lp32073
sg83
(lp32074
sg10
(lp32075
sg283
(lp32076
sg114
(lp32077
ssS'outlook'
p32078
(dp32079
g484
(lp32080
I1546
assS'elementari'
p32081
(dp32082
g42
(lp32083
I450
assS'darrel'
p32084
(dp32085
g293
(lp32086
I9
assS'resampl'
p32087
(dp32088
g183
(lp32089
sg484
(lp32090
sg221
(lp32091
I307
assS'resampi'
p32092
(dp32093
g221
(lp32094
I2483
assS'neurophi'
p32095
(dp32096
g174
(lp32097
I2432
assS'loc'
p32098
(dp32099
g59
(lp32100
I1194
assS'lof'
p32101
(dp32102
g32
(lp32103
I1647
assS'log'
p32104
(dp32105
g26
(lp32106
sg72
(lp32107
sg281
(lp32108
sg287
(lp32109
sg145
(lp32110
sg256
(lp32111
sg76
(lp32112
sg344
(lp32113
sg183
(lp32114
sg85
(lp32115
sg87
(lp32116
sg91
(lp32117
sg96
(lp32118
sg221
(lp32119
sg329
(lp32120
sg32
(lp32121
sg174
(lp32122
sg440
(lp32123
sg318
(lp32124
sg181
(lp32125
sg36
(lp32126
sg460
(lp32127
sg124
(lp32128
sg126
(lp32129
sg341
(lp32130
sg40
(lp32131
sg130
(lp32132
sg135
(lp32133
sg50
(lp32134
sg138
(lp32135
I533
assS'lod'
p32136
(dp32137
g20
(lp32138
I1070
assS'area'
p32139
(dp32140
g176
(lp32141
sg145
(lp32142
sg344
(lp32143
sg59
(lp32144
sg83
(lp32145
sg303
(lp32146
sg42
(lp32147
I1850
asg89
(lp32148
sg245
(lp32149
sg48
(lp32150
sg99
(lp32151
sg149
(lp32152
sg116
(lp32153
sg429
(lp32154
sg318
(lp32155
sg106
(lp32156
sg110
(lp32157
sg52
(lp32158
sg4
(lp32159
sg216
(lp32160
sg174
(lp32161
sg18
(lp32162
sg22
(lp32163
sg6
(lp32164
sg8
(lp32165
sg36
(lp32166
sg341
(lp32167
sg10
(lp32168
sg40
(lp32169
sg14
(lp32170
sg135
(lp32171
sg138
(lp32172
sg140
(lp32173
sg354
(lp32174
ssS'aren'
p32175
(dp32176
g30
(lp32177
I2350
assS'lon'
p32178
(dp32179
g106
(lp32180
I1246
assS'loo'
p32181
(dp32182
g42
(lp32183
I1671
asg332
(lp32184
sg38
(lp32185
sg10
(lp32186
ssS'hertz'
p32187
(dp32188
g235
(lp32189
sg26
(lp32190
sg130
(lp32191
I213
assS'lom'
p32192
(dp32193
g174
(lp32194
I1792
assS'lor'
p32195
(dp32196
g12
(lp32197
I1126
asg104
(lp32198
sg110
(lp32199
sg344
(lp32200
ssS'start'
p32201
(dp32202
g124
(lp32203
sg26
(lp32204
sg287
(lp32205
sg74
(lp32206
sg76
(lp32207
sg262
(lp32208
sg183
(lp32209
sg42
(lp32210
I1787
asg306
(lp32211
sg89
(lp32212
sg91
(lp32213
sg46
(lp32214
sg221
(lp32215
sg44
(lp32216
sg149
(lp32217
sg174
(lp32218
sg429
(lp32219
sg68
(lp32220
sg102
(lp32221
sg110
(lp32222
sg63
(lp32223
sg52
(lp32224
sg329
(lp32225
sg440
(lp32226
sg332
(lp32227
sg8
(lp32228
sg36
(lp32229
sg460
(lp32230
sg235
(lp32231
sg72
(lp32232
sg40
(lp32233
sg128
(lp32234
sg78
(lp32235
sg135
(lp32236
sg138
(lp32237
sg140
(lp32238
sg354
(lp32239
ssS'low'
p32240
(dp32241
g283
(lp32242
sg78
(lp32243
sg277
(lp32244
sg163
(lp32245
sg74
(lp32246
sg145
(lp32247
sg256
(lp32248
sg80
(lp32249
sg262
(lp32250
sg295
(lp32251
sg183
(lp32252
sg59
(lp32253
sg484
(lp32254
sg83
(lp32255
sg42
(lp32256
I2350
asg89
(lp32257
sg245
(lp32258
sg20
(lp32259
sg114
(lp32260
sg221
(lp32261
sg313
(lp32262
sg44
(lp32263
sg149
(lp32264
sg293
(lp32265
sg429
(lp32266
sg318
(lp32267
sg178
(lp32268
sg106
(lp32269
sg63
(lp32270
sg52
(lp32271
sg22
(lp32272
sg116
(lp32273
sg329
(lp32274
sg440
(lp32275
sg332
(lp32276
sg121
(lp32277
sg4
(lp32278
sg181
(lp32279
sg8
(lp32280
sg99
(lp32281
sg124
(lp32282
sg126
(lp32283
sg10
(lp32284
sg130
(lp32285
sg14
(lp32286
sg135
(lp32287
sg50
(lp32288
sg140
(lp32289
sg354
(lp32290
ssS'lot'
p32291
(dp32292
g216
(lp32293
sg174
(lp32294
sg26
(lp32295
sg140
(lp32296
I286
assS'lox'
p32297
(dp32298
g178
(lp32299
sg89
(lp32300
I911
assS'loy'
p32301
(dp32302
g89
(lp32303
I912
assS'fsms'
p32304
(dp32305
g128
(lp32306
I1254
assS'arguin'
p32307
(dp32308
g303
(lp32309
I854
assS'electr'
p32310
(dp32311
g230
(lp32312
sg329
(lp32313
sg283
(lp32314
sg121
(lp32315
sg22
(lp32316
sg8
(lp32317
sg78
(lp32318
sg460
(lp32319
sg281
(lp32320
sg10
(lp32321
sg46
(lp32322
sg245
(lp32323
sg14
(lp32324
sg16
(lp32325
sg135
(lp32326
I27
asg20
(lp32327
sg114
(lp32328
ssS'oscin'
p32329
(dp32330
g116
(lp32331
I155
assS'oscil'
p32332
(dp32333
g174
(lp32334
sg332
(lp32335
sg22
(lp32336
sg78
(lp32337
sg384
(lp32338
sg68
(lp32339
sg102
(lp32340
I2562
asg20
(lp32341
sg18
(lp32342
sg535
(lp32343
sg350
(lp32344
ssS'posterior'
p32345
(dp32346
g30
(lp32347
sg329
(lp32348
sg440
(lp32349
sg76
(lp32350
sg116
(lp32351
sg460
(lp32352
sg124
(lp32353
sg126
(lp32354
sg281
(lp32355
sg87
(lp32356
sg130
(lp32357
sg354
(lp32358
I563
asg221
(lp32359
sg138
(lp32360
sg350
(lp32361
ssS'satur'
p32362
(dp32363
g121
(lp32364
sg256
(lp32365
sg181
(lp32366
sg36
(lp32367
sg384
(lp32368
sg341
(lp32369
sg10
(lp32370
sg52
(lp32371
sg14
(lp32372
sg50
(lp32373
I736
asg223
(lp32374
ssS'lymph'
p32375
(dp32376
g91
(lp32377
I480
assS'argmin'
p32378
(dp32379
g460
(lp32380
sg89
(lp32381
I571
assS'stanford'
p32382
(dp32383
g40
(lp32384
sg110
(lp32385
sg63
(lp32386
sg114
(lp32387
I18
assS'default'
p32388
(dp32389
g287
(lp32390
sg121
(lp32391
sg295
(lp32392
sg183
(lp32393
sg126
(lp32394
sg138
(lp32395
I2116
assS'bucket'
p32396
(dp32397
g135
(lp32398
I110
assS'hiri'
p32399
(dp32400
g216
(lp32401
I463
assS'autocomplet'
p32402
(dp32403
g94
(lp32404
I561
assS'stochast'
p32405
(dp32406
g163
(lp32407
sg30
(lp32408
sg76
(lp32409
sg262
(lp32410
sg83
(lp32411
sg306
(lp32412
sg89
(lp32413
sg18
(lp32414
sg329
(lp32415
sg293
(lp32416
sg318
(lp32417
sg178
(lp32418
sg216
(lp32419
sg438
(lp32420
I756
asg440
(lp32421
sg332
(lp32422
sg121
(lp32423
sg4
(lp32424
sg235
(lp32425
sg36
(lp32426
sg384
(lp32427
sg124
(lp32428
sg130
(lp32429
sg50
(lp32430
ssS'scanner'
p32431
(dp32432
g42
(lp32433
I1691
assS'llumber'
p32434
(dp32435
g108
(lp32436
I1589
assS'lcas'
p32437
(dp32438
g14
(lp32439
I4772
assS'nerv'
p32440
(dp32441
g12
(lp32442
I803
asg174
(lp32443
sg332
(lp32444
sg116
(lp32445
sg176
(lp32446
ssS'arabi'
p32447
(dp32448
g74
(lp32449
I33
assS'decreas'
p32450
(dp32451
g78
(lp32452
sg116
(lp32453
sg26
(lp32454
sg74
(lp32455
sg176
(lp32456
sg145
(lp32457
sg183
(lp32458
sg484
(lp32459
sg38
(lp32460
sg85
(lp32461
sg303
(lp32462
sg42
(lp32463
I173
asg91
(lp32464
sg94
(lp32465
sg96
(lp32466
sg99
(lp32467
sg313
(lp32468
sg44
(lp32469
sg350
(lp32470
sg230
(lp32471
sg329
(lp32472
sg102
(lp32473
sg178
(lp32474
sg106
(lp32475
sg20
(lp32476
sg52
(lp32477
sg114
(lp32478
sg216
(lp32479
sg174
(lp32480
sg440
(lp32481
sg318
(lp32482
sg121
(lp32483
sg4
(lp32484
sg181
(lp32485
sg235
(lp32486
sg34
(lp32487
sg36
(lp32488
sg460
(lp32489
sg130
(lp32490
sg14
(lp32491
sg149
(lp32492
sg138
(lp32493
sg140
(lp32494
sg354
(lp32495
ssS'reprogramm'
p32496
(dp32497
g14
(lp32498
I2802
assS'hovland'
p32499
(dp32500
g99
(lp32501
I2799
assS'demba'
p32502
(dp32503
g281
(lp32504
I318
assS'cooperat'
p32505
(dp32506
g59
(lp32507
I498
assS'mathematica'
p32508
(dp32509
g256
(lp32510
I1588
assS'achromat'
p32511
(dp32512
g118
(lp32513
I228
assS'ific'
p32514
(dp32515
g318
(lp32516
I2870
assS'caltech'
p32517
(dp32518
g216
(lp32519
sg178
(lp32520
sg6
(lp32521
I47
asg181
(lp32522
sg68
(lp32523
sg256
(lp32524
ssS'lgc'
p32525
(dp32526
g283
(lp32527
I1629
assS'you'
p32528
(dp32529
g140
(lp32530
I412
assS'christof'
p32531
(dp32532
g181
(lp32533
sg6
(lp32534
I41
assS'yop'
p32535
(dp32536
g128
(lp32537
I1552
assS'poor'
p32538
(dp32539
g30
(lp32540
sg121
(lp32541
sg76
(lp32542
sg8
(lp32543
sg183
(lp32544
sg68
(lp32545
sg126
(lp32546
sg83
(lp32547
sg306
(lp32548
sg89
(lp32549
sg44
(lp32550
sg128
(lp32551
sg132
(lp32552
sg108
(lp32553
sg99
(lp32554
sg313
(lp32555
sg140
(lp32556
I1918
assS'polar'
p32557
(dp32558
g245
(lp32559
I2022
asg32
(lp32560
sg256
(lp32561
sg52
(lp32562
ssS'phys'
p32563
(dp32564
g174
(lp32565
sg32
(lp32566
sg262
(lp32567
sg36
(lp32568
sg384
(lp32569
sg235
(lp32570
sg38
(lp32571
sg48
(lp32572
I2503
assS'drift'
p32573
(dp32574
g34
(lp32575
sg135
(lp32576
I88
asg80
(lp32577
sg6
(lp32578
ssS'multidimension'
p32579
(dp32580
g287
(lp32581
sg96
(lp32582
sg74
(lp32583
sg181
(lp32584
sg130
(lp32585
I1
assS'polynomi'
p32586
(dp32587
g287
(lp32588
sg318
(lp32589
sg145
(lp32590
sg163
(lp32591
sg344
(lp32592
sg68
(lp32593
sg85
(lp32594
sg40
(lp32595
sg429
(lp32596
sg89
(lp32597
I241
asg46
(lp32598
ssS'peal'
p32599
(dp32600
g83
(lp32601
I2835
assS'mammon'
p32602
(dp32603
g140
(lp32604
I3122
asg235
(lp32605
ssS'queensland'
p32606
(dp32607
g121
(lp32608
I31
assS'nonmonoton'
p32609
(dp32610
g50
(lp32611
I1531
assS'pool'
p32612
(dp32613
g116
(lp32614
sg74
(lp32615
sg277
(lp32616
sg6
(lp32617
sg281
(lp32618
sg138
(lp32619
I2164
asg130
(lp32620
sg535
(lp32621
sg114
(lp32622
ssS'reduc'
p32623
(dp32624
g124
(lp32625
sg78
(lp32626
sg277
(lp32627
sg163
(lp32628
sg72
(lp32629
sg283
(lp32630
sg303
(lp32631
sg26
(lp32632
sg74
(lp32633
sg80
(lp32634
sg76
(lp32635
sg118
(lp32636
sg295
(lp32637
sg183
(lp32638
sg59
(lp32639
sg484
(lp32640
sg38
(lp32641
sg85
(lp32642
sg63
(lp32643
sg42
(lp32644
I788
asg87
(lp32645
sg91
(lp32646
sg245
(lp32647
sg94
(lp32648
sg96
(lp32649
sg99
(lp32650
sg535
(lp32651
sg44
(lp32652
sg116
(lp32653
sg174
(lp32654
sg460
(lp32655
sg12
(lp32656
sg429
(lp32657
sg68
(lp32658
sg46
(lp32659
sg102
(lp32660
sg106
(lp32661
sg110
(lp32662
sg20
(lp32663
sg52
(lp32664
sg216
(lp32665
sg438
(lp32666
sg440
(lp32667
sg318
(lp32668
sg178
(lp32669
sg4
(lp32670
sg8
(lp32671
sg221
(lp32672
sg384
(lp32673
sg235
(lp32674
sg126
(lp32675
sg281
(lp32676
sg10
(lp32677
sg130
(lp32678
sg135
(lp32679
sg138
(lp32680
sg354
(lp32681
ssS'assert'
p32682
(dp32683
g74
(lp32684
I1646
assS'munro'
p32685
(dp32686
g438
(lp32687
I2391
asg106
(lp32688
sg318
(lp32689
sg484
(lp32690
ssS'gisbergen'
p32691
(dp32692
g32
(lp32693
I418
assS'affer'
p32694
(dp32695
g438
(lp32696
I189
asg176
(lp32697
sg70
(lp32698
sg12
(lp32699
sg106
(lp32700
sg149
(lp32701
sg350
(lp32702
ssS'inevit'
p32703
(dp32704
g70
(lp32705
sg91
(lp32706
I499
assS'cvc'
p32707
(dp32708
g484
(lp32709
I790
assS'precondition'
p32710
(dp32711
g8
(lp32712
I1763
assS'skeleton'
p32713
(dp32714
g63
(lp32715
I884
assS'messi'
p32716
(dp32717
g384
(lp32718
I1851
assS'rooer'
p32719
(dp32720
g34
(lp32721
I1338
assS'month'
p32722
(dp32723
g14
(lp32724
I4045
assS'mighel'
p32725
(dp32726
g114
(lp32727
I11
assS'correl'
p32728
(dp32729
g283
(lp32730
sg70
(lp32731
sg26
(lp32732
sg163
(lp32733
sg74
(lp32734
sg176
(lp32735
sg80
(lp32736
sg293
(lp32737
sg295
(lp32738
sg183
(lp32739
sg484
(lp32740
sg38
(lp32741
sg149
(lp32742
sg42
(lp32743
I907
asg245
(lp32744
sg48
(lp32745
sg99
(lp32746
sg313
(lp32747
sg350
(lp32748
sg12
(lp32749
sg102
(lp32750
sg106
(lp32751
sg110
(lp32752
sg6
(lp32753
sg235
(lp32754
sg384
(lp32755
sg124
(lp32756
sg344
(lp32757
sg128
(lp32758
sg130
(lp32759
sg132
(lp32760
sg135
(lp32761
sg50
(lp32762
sg140
(lp32763
ssS'cardiologist'
p32764
(dp32765
g135
(lp32766
I1554
assS'figlu'
p32767
(dp32768
g110
(lp32769
I2370
assS'trestbp'
p32770
(dp32771
g91
(lp32772
I2089
assS'correc'
p32773
(dp32774
g4
(lp32775
I1568
assS'griffin'
p32776
(dp32777
g163
(lp32778
I2252
assS'articl'
p32779
(dp32780
g20
(lp32781
I598
assS'wmn'
p32782
(dp32783
g149
(lp32784
I1084
assS'apport'
p32785
(dp32786
g102
(lp32787
I1450
assS'slawni'
p32788
(dp32789
g341
(lp32790
I2823
assS'mpg'
p32791
(dp32792
g126
(lp32793
I2498
assS'seventh'
p32794
(dp32795
g110
(lp32796
sg85
(lp32797
sg223
(lp32798
I2280
assS'augustin'
p32799
(dp32800
g354
(lp32801
I22
assS'rhesus'
p32802
(dp32803
g350
(lp32804
I2802
assS'mpj'
p32805
(dp32806
g429
(lp32807
I2087
assS'pulsestream'
p32808
(dp32809
g14
(lp32810
I2669
assS'verb'
p32811
(dp32812
g94
(lp32813
I1181
assS'mechan'
p32814
(dp32815
g70
(lp32816
sg163
(lp32817
sg116
(lp32818
sg181
(lp32819
sg74
(lp32820
sg176
(lp32821
sg145
(lp32822
sg80
(lp32823
sg293
(lp32824
sg78
(lp32825
sg38
(lp32826
sg85
(lp32827
sg303
(lp32828
sg48
(lp32829
sg245
(lp32830
sg94
(lp32831
sg20
(lp32832
sg18
(lp32833
sg99
(lp32834
sg535
(lp32835
sg223
(lp32836
sg350
(lp32837
sg230
(lp32838
sg118
(lp32839
sg12
(lp32840
sg106
(lp32841
I155
asg96
(lp32842
sg216
(lp32843
sg174
(lp32844
sg332
(lp32845
sg4
(lp32846
sg6
(lp32847
sg8
(lp32848
sg384
(lp32849
sg130
(lp32850
sg132
(lp32851
sg14
(lp32852
sg149
(lp32853
sg50
(lp32854
ssS'parra'
p32855
(dp32856
g163
(lp32857
I7
assS'veri'
p32858
(dp32859
g329
(lp32860
sg70
(lp32861
sg78
(lp32862
sg277
(lp32863
sg68
(lp32864
sg283
(lp32865
sg85
(lp32866
sg36
(lp32867
sg26
(lp32868
sg30
(lp32869
sg350
(lp32870
sg74
(lp32871
sg176
(lp32872
sg145
(lp32873
sg256
(lp32874
sg76
(lp32875
sg262
(lp32876
sg295
(lp32877
sg183
(lp32878
sg59
(lp32879
sg484
(lp32880
sg38
(lp32881
sg83
(lp32882
sg114
(lp32883
sg303
(lp32884
sg42
(lp32885
I2209
asg89
(lp32886
sg91
(lp32887
sg12
(lp32888
sg94
(lp32889
sg20
(lp32890
sg48
(lp32891
sg99
(lp32892
sg313
(lp32893
sg44
(lp32894
sg149
(lp32895
sg174
(lp32896
sg245
(lp32897
sg318
(lp32898
sg46
(lp32899
sg104
(lp32900
sg110
(lp32901
sg63
(lp32902
sg22
(lp32903
sg216
(lp32904
sg438
(lp32905
sg440
(lp32906
sg332
(lp32907
sg121
(lp32908
sg4
(lp32909
sg181
(lp32910
sg344
(lp32911
sg34
(lp32912
sg221
(lp32913
sg384
(lp32914
sg124
(lp32915
sg126
(lp32916
sg281
(lp32917
sg40
(lp32918
sg287
(lp32919
sg130
(lp32920
sg14
(lp32921
sg16
(lp32922
sg135
(lp32923
sg50
(lp32924
sg138
(lp32925
sg140
(lp32926
ssS'maximis'
p32927
(dp32928
g318
(lp32929
sg50
(lp32930
I1628
assS'emul'
p32931
(dp32932
g46
(lp32933
I246
asg10
(lp32934
ssS'cosin'
p32935
(dp32936
g116
(lp32937
sg22
(lp32938
I1371
assS'rneasur'
p32939
(dp32940
g350
(lp32941
I1155
assS'anal'
p32942
(dp32943
g42
(lp32944
I3436
asg235
(lp32945
ssS'nordrhein'
p32946
(dp32947
g130
(lp32948
I3104
assS'integrateand'
p32949
(dp32950
g174
(lp32951
I89
assS'stagnat'
p32952
(dp32953
g59
(lp32954
I2205
assS'shadlen'
p32955
(dp32956
g6
(lp32957
I2204
asg262
(lp32958
ssS'souli'
p32959
(dp32960
g178
(lp32961
I2513
assS'sinusoid'
p32962
(dp32963
g78
(lp32964
sg313
(lp32965
sg128
(lp32966
I2236
asg22
(lp32967
ssS'paass'
p32968
(dp32969
g313
(lp32970
sg354
(lp32971
I9
assS'eduifac'
p32972
(dp32973
g295
(lp32974
I23
asg183
(lp32975
ssS'preper'
p32976
(dp32977
g341
(lp32978
I2792
assS'hamper'
p32979
(dp32980
g14
(lp32981
I2781
asg74
(lp32982
ssS'bilinear'
p32983
(dp32984
g32
(lp32985
sg128
(lp32986
I235
assS'nanj'
p32987
(dp32988
g72
(lp32989
I3577
assS'nano'
p32990
(dp32991
g135
(lp32992
I998
assS'nanb'
p32993
(dp32994
g318
(lp32995
I1310
assS'learner'
p32996
(dp32997
g30
(lp32998
sg329
(lp32999
sg277
(lp33000
sg344
(lp33001
sg183
(lp33002
sg140
(lp33003
I2284
asg132
(lp33004
sg110
(lp33005
sg313
(lp33006
sg223
(lp33007
ssS'nand'
p33008
(dp33009
g42
(lp33010
I1484
asg287
(lp33011
sg68
(lp33012
sg344
(lp33013
sg8
(lp33014
ssS'parafovea'
p33015
(dp33016
g48
(lp33017
I390
asg178
(lp33018
ssS'consecut'
p33019
(dp33020
g32
(lp33021
sg76
(lp33022
sg344
(lp33023
sg78
(lp33024
sg74
(lp33025
sg306
(lp33026
sg20
(lp33027
I160
asg110
(lp33028
ssS'essenc'
p33029
(dp33030
g341
(lp33031
I1456
assS'jaakkola'
p33032
(dp33033
g293
(lp33034
I3184
assS'vet'
p33035
(dp33036
g262
(lp33037
I185
assS'iwil'
p33038
(dp33039
g40
(lp33040
I1630
assS'modular'
p33041
(dp33042
g59
(lp33043
sg68
(lp33044
sg303
(lp33045
sg87
(lp33046
sg89
(lp33047
sg14
(lp33048
sg16
(lp33049
I1849
assS'lamina'
p33050
(dp33051
g256
(lp33052
I420
assS'excess'
p33053
(dp33054
g6
(lp33055
sg78
(lp33056
sg38
(lp33057
sg42
(lp33058
I2074
asg132
(lp33059
sg221
(lp33060
ssS'stili'
p33061
(dp33062
g85
(lp33063
I1452
assS'strong'
p33064
(dp33065
g70
(lp33066
sg181
(lp33067
sg30
(lp33068
sg80
(lp33069
sg344
(lp33070
sg183
(lp33071
sg38
(lp33072
sg83
(lp33073
sg85
(lp33074
sg303
(lp33075
sg42
(lp33076
I1156
asg91
(lp33077
sg20
(lp33078
sg99
(lp33079
sg313
(lp33080
sg149
(lp33081
sg118
(lp33082
sg106
(lp33083
sg52
(lp33084
sg216
(lp33085
sg174
(lp33086
sg318
(lp33087
sg4
(lp33088
sg6
(lp33089
sg235
(lp33090
sg34
(lp33091
sg36
(lp33092
sg281
(lp33093
sg14
(lp33094
sg16
(lp33095
sg50
(lp33096
sg138
(lp33097
sg140
(lp33098
ssS'modifi'
p33099
(dp33100
g68
(lp33101
sg262
(lp33102
sg344
(lp33103
sg38
(lp33104
sg85
(lp33105
sg306
(lp33106
sg87
(lp33107
sg48
(lp33108
sg535
(lp33109
sg149
(lp33110
sg102
(lp33111
sg104
(lp33112
sg108
(lp33113
sg116
(lp33114
sg438
(lp33115
I819
asg332
(lp33116
sg4
(lp33117
sg8
(lp33118
sg34
(lp33119
sg36
(lp33120
sg124
(lp33121
sg126
(lp33122
sg10
(lp33123
sg130
(lp33124
sg135
(lp33125
sg354
(lp33126
ssS'scanpath'
p33127
(dp33128
g178
(lp33129
I238
assS'arena'
p33130
(dp33131
g80
(lp33132
I105
assS'arend'
p33133
(dp33134
g118
(lp33135
I474
assS'ahead'
p33136
(dp33137
g230
(lp33138
sg32
(lp33139
sg277
(lp33140
sg59
(lp33141
sg303
(lp33142
sg91
(lp33143
I1850
asg104
(lp33144
sg350
(lp33145
ssS'lomo'
p33146
(dp33147
g106
(lp33148
I116
assS'mirank'
p33149
(dp33150
g8
(lp33151
I727
assS'contralater'
p33152
(dp33153
g303
(lp33154
I137
assS'ppatch'
p33155
(dp33156
g30
(lp33157
I1331
assS'reent'
p33158
(dp33159
g59
(lp33160
I650
assS'euratom'
p33161
(dp33162
g14
(lp33163
sg16
(lp33164
I59
assS'jre'
p33165
(dp33166
g277
(lp33167
I1998
assS'probiem'
p33168
(dp33169
g78
(lp33170
I406
assS'uyfi'
p33171
(dp33172
g262
(lp33173
I1654
assS'luse'
p33174
(dp33175
g318
(lp33176
I1191
assS'put'
p33177
(dp33178
g34
(lp33179
sg59
(lp33180
sg484
(lp33181
sg72
(lp33182
sg535
(lp33183
sg42
(lp33184
I1459
asg44
(lp33185
sg104
(lp33186
sg40
(lp33187
sg138
(lp33188
sg140
(lp33189
sg350
(lp33190
ssS'wlogw'
p33191
(dp33192
g287
(lp33193
I601
assS'famili'
p33194
(dp33195
g30
(lp33196
sg74
(lp33197
sg72
(lp33198
sg130
(lp33199
sg132
(lp33200
I513
asg221
(lp33201
sg223
(lp33202
ssS'auxilliari'
p33203
(dp33204
g460
(lp33205
I2862
assS'perceptron'
p33206
(dp33207
g440
(lp33208
sg108
(lp33209
sg121
(lp33210
sg76
(lp33211
sg235
(lp33212
sg344
(lp33213
sg460
(lp33214
sg163
(lp33215
sg341
(lp33216
sg85
(lp33217
sg87
(lp33218
sg91
(lp33219
sg128
(lp33220
sg94
(lp33221
sg14
(lp33222
sg16
(lp33223
sg135
(lp33224
sg96
(lp33225
sg52
(lp33226
sg354
(lp33227
I1242
assS'mahowald'
p33228
(dp33229
g174
(lp33230
I2618
asg20
(lp33231
sg256
(lp33232
ssS'oppportun'
p33233
(dp33234
g74
(lp33235
I2741
assS'aggress'
p33236
(dp33237
g89
(lp33238
I2140
asg277
(lp33239
ssS'bartheld'
p33240
(dp33241
g78
(lp33242
I856
assS'injuri'
p33243
(dp33244
g94
(lp33245
sg135
(lp33246
I339
assS'spectrogram'
p33247
(dp33248
g116
(lp33249
I2085
assS'premotor'
p33250
(dp33251
g303
(lp33252
I280
assS'formul'
p33253
(dp33254
g287
(lp33255
sg440
(lp33256
sg318
(lp33257
sg26
(lp33258
sg8
(lp33259
sg293
(lp33260
sg384
(lp33261
sg124
(lp33262
sg38
(lp33263
sg74
(lp33264
sg429
(lp33265
sg91
(lp33266
I665
asg221
(lp33267
sg63
(lp33268
sg223
(lp33269
ssS'anaesthet'
p33270
(dp33271
g106
(lp33272
I2578
assS'taken'
p33273
(dp33274
g30
(lp33275
sg76
(lp33276
sg262
(lp33277
sg295
(lp33278
sg183
(lp33279
sg484
(lp33280
sg38
(lp33281
sg83
(lp33282
sg303
(lp33283
sg306
(lp33284
sg87
(lp33285
sg245
(lp33286
sg94
(lp33287
sg48
(lp33288
sg99
(lp33289
sg223
(lp33290
sg149
(lp33291
sg118
(lp33292
sg102
(lp33293
sg18
(lp33294
sg110
(lp33295
sg63
(lp33296
sg52
(lp33297
sg4
(lp33298
sg116
(lp33299
sg174
(lp33300
sg318
(lp33301
sg178
(lp33302
sg22
(lp33303
sg8
(lp33304
sg34
(lp33305
sg36
(lp33306
sg235
(lp33307
sg10
(lp33308
sg44
(lp33309
sg78
(lp33310
sg132
(lp33311
sg14
(lp33312
sg16
(lp33313
sg354
(lp33314
I665
assS'takeo'
p33315
(dp33316
g20
(lp33317
I20
assS'brunswick'
p33318
(dp33319
g287
(lp33320
I34
assS'ofor'
p33321
(dp33322
g341
(lp33323
I2684
assS'omput'
p33324
(dp33325
g116
(lp33326
I31
assS'linden'
p33327
(dp33328
g30
(lp33329
sg313
(lp33330
sg354
(lp33331
I2914
assS'site'
p33332
(dp33333
g438
(lp33334
I674
asg283
(lp33335
sg295
(lp33336
sg183
(lp33337
sg78
(lp33338
sg106
(lp33339
sg350
(lp33340
ssS'postsubiculum'
p33341
(dp33342
g80
(lp33343
I152
assS'vp'
p33344
(dp33345
g135
(lp33346
I1143
asg10
(lp33347
ssS'vq'
p33348
(dp33349
g72
(lp33350
I534
assS'edinborough'
p33351
(dp33352
g429
(lp33353
I2399
assS'histori'
p33354
(dp33355
g32
(lp33356
sg121
(lp33357
sg277
(lp33358
sg293
(lp33359
sg91
(lp33360
I170
asg70
(lp33361
sg18
(lp33362
sg535
(lp33363
ssS'nine'
p33364
(dp33365
g94
(lp33366
I1321
asg484
(lp33367
sg83
(lp33368
sg293
(lp33369
ssS'ning'
p33370
(dp33371
g223
(lp33372
I1889
assS'overtrain'
p33373
(dp33374
g183
(lp33375
sg277
(lp33376
sg128
(lp33377
I1908
asg36
(lp33378
ssS'typograph'
p33379
(dp33380
g42
(lp33381
I1613
assS'sharad'
p33382
(dp33383
g108
(lp33384
I9
assS'toroid'
p33385
(dp33386
g14
(lp33387
sg16
(lp33388
I295
asg70
(lp33389
ssS'templat'
p33390
(dp33391
g116
(lp33392
sg135
(lp33393
I419
asg283
(lp33394
sg63
(lp33395
sg293
(lp33396
ssS'postsynapt'
p33397
(dp33398
g106
(lp33399
I142
asg70
(lp33400
ssS'unreli'
p33401
(dp33402
g163
(lp33403
sg354
(lp33404
I1313
assS'incom'
p33405
(dp33406
g174
(lp33407
sg332
(lp33408
sg256
(lp33409
sg135
(lp33410
I709
asg38
(lp33411
sg18
(lp33412
ssS'phrase'
p33413
(dp33414
g116
(lp33415
sg96
(lp33416
I713
assS'oytim'
p33417
(dp33418
g221
(lp33419
I2174
assS'watanab'
p33420
(dp33421
g63
(lp33422
I168
assS'arnpbmk'
p33423
(dp33424
g230
(lp33425
I1053
assS'anoth'
p33426
(dp33427
g68
(lp33428
sg70
(lp33429
sg26
(lp33430
sg277
(lp33431
sg460
(lp33432
sg30
(lp33433
sg74
(lp33434
sg76
(lp33435
sg293
(lp33436
sg295
(lp33437
sg183
(lp33438
sg59
(lp33439
sg484
(lp33440
sg83
(lp33441
sg42
(lp33442
I115
asg87
(lp33443
sg89
(lp33444
sg91
(lp33445
sg12
(lp33446
sg94
(lp33447
sg96
(lp33448
sg48
(lp33449
sg99
(lp33450
sg44
(lp33451
sg230
(lp33452
sg32
(lp33453
sg429
(lp33454
sg318
(lp33455
sg102
(lp33456
sg104
(lp33457
sg106
(lp33458
sg110
(lp33459
sg52
(lp33460
sg114
(lp33461
sg216
(lp33462
sg118
(lp33463
sg440
(lp33464
sg18
(lp33465
sg178
(lp33466
sg22
(lp33467
sg6
(lp33468
sg8
(lp33469
sg34
(lp33470
sg384
(lp33471
sg235
(lp33472
sg72
(lp33473
sg341
(lp33474
sg344
(lp33475
sg223
(lp33476
sg128
(lp33477
sg78
(lp33478
sg132
(lp33479
sg14
(lp33480
sg138
(lp33481
sg140
(lp33482
ssS'reject'
p33483
(dp33484
g124
(lp33485
sg126
(lp33486
sg138
(lp33487
sg42
(lp33488
I3137
asg63
(lp33489
sg114
(lp33490
ssS'kung'
p33491
(dp33492
g440
(lp33493
I2733
assS'fp'
p33494
(dp33495
g303
(lp33496
I316
assS'fr'
p33497
(dp33498
g230
(lp33499
sg174
(lp33500
sg287
(lp33501
sg34
(lp33502
sg89
(lp33503
sg12
(lp33504
sg535
(lp33505
sg354
(lp33506
I1907
assS'fs'
p33507
(dp33508
g22
(lp33509
I2010
assS'ft'
p33510
(dp33511
g145
(lp33512
sg76
(lp33513
sg235
(lp33514
I859
assS'threat'
p33515
(dp33516
g132
(lp33517
I1880
assS'rude'
p33518
(dp33519
g221
(lp33520
I792
assS'fw'
p33521
(dp33522
g12
(lp33523
I2865
asg287
(lp33524
sg20
(lp33525
ssS'fx'
p33526
(dp33527
g96
(lp33528
I722
assS'gase'
p33529
(dp33530
g277
(lp33531
I503
assS'wahba'
p33532
(dp33533
g124
(lp33534
I1095
assS'aepr'
p33535
(dp33536
g72
(lp33537
I986
assS'fa'
p33538
(dp33539
g145
(lp33540
sg26
(lp33541
sg295
(lp33542
sg183
(lp33543
sg42
(lp33544
I755
asg20
(lp33545
sg99
(lp33546
sg140
(lp33547
sg114
(lp33548
ssS'fc'
p33549
(dp33550
g332
(lp33551
I908
assS'embrac'
p33552
(dp33553
g535
(lp33554
I693
assS'fe'
p33555
(dp33556
g174
(lp33557
sg48
(lp33558
I602
assS'ff'
p33559
(dp33560
g429
(lp33561
sg535
(lp33562
I1250
assS'fg'
p33563
(dp33564
g38
(lp33565
sg178
(lp33566
sg535
(lp33567
I1173
assS'fh'
p33568
(dp33569
g230
(lp33570
I1684
assS'fi'
p33571
(dp33572
g216
(lp33573
sg174
(lp33574
sg181
(lp33575
sg50
(lp33576
sg68
(lp33577
sg281
(lp33578
sg40
(lp33579
sg42
(lp33580
I790
asg46
(lp33581
sg104
(lp33582
sg108
(lp33583
sg99
(lp33584
sg313
(lp33585
sg350
(lp33586
ssS'fj'
p33587
(dp33588
g230
(lp33589
sg332
(lp33590
sg235
(lp33591
sg293
(lp33592
sg38
(lp33593
sg40
(lp33594
sg42
(lp33595
I1871
asg429
(lp33596
sg535
(lp33597
sg350
(lp33598
ssS'fk'
p33599
(dp33600
g174
(lp33601
I17
asg384
(lp33602
sg235
(lp33603
ssS'fl'
p33604
(dp33605
g42
(lp33606
I812
asg230
(lp33607
sg20
(lp33608
sg256
(lp33609
ssS'fn'
p33610
(dp33611
g68
(lp33612
sg223
(lp33613
I1560
assS'fo'
p33614
(dp33615
g230
(lp33616
sg235
(lp33617
sg295
(lp33618
sg183
(lp33619
sg87
(lp33620
sg130
(lp33621
I1110
asg99
(lp33622
ssS'a'
p33623
(dp33624
g80
(lp33625
sg293
(lp33626
sg344
(lp33627
sg78
(lp33628
sg59
(lp33629
sg484
(lp33630
sg38
(lp33631
sg83
(lp33632
sg85
(lp33633
sg303
(lp33634
sg438
(lp33635
sg116
(lp33636
sg118
(lp33637
sg34
(lp33638
sg36
(lp33639
sg460
(lp33640
sg68
(lp33641
sg72
(lp33642
sg281
(lp33643
sg10
(lp33644
sg40
(lp33645
sg283
(lp33646
sg70
(lp33647
sg26
(lp33648
sg277
(lp33649
sg163
(lp33650
sg89
(lp33651
sg91
(lp33652
sg12
(lp33653
sg94
(lp33654
sg96
(lp33655
sg48
(lp33656
sg99
(lp33657
sg313
(lp33658
sg44
(lp33659
sg149
(lp33660
sg429
(lp33661
sg102
(lp33662
sg104
(lp33663
sg106
(lp33664
sg108
(lp33665
sg110
(lp33666
sg63
(lp33667
sg52
(lp33668
sg114
(lp33669
sg128
(lp33670
sg130
(lp33671
sg132
(lp33672
sg14
(lp33673
sg16
(lp33674
sg135
(lp33675
sg50
(lp33676
sg138
(lp33677
sg140
(lp33678
sg354
(lp33679
sg306
(lp33680
sg87
(lp33681
sg245
(lp33682
sg46
(lp33683
sg20
(lp33684
sg18
(lp33685
sg221
(lp33686
sg535
(lp33687
sg223
(lp33688
sg350
(lp33689
sg216
(lp33690
sg174
(lp33691
sg440
(lp33692
sg332
(lp33693
sg121
(lp33694
sg4
(lp33695
sg6
(lp33696
sg8
(lp33697
sg126
(lp33698
sg341
(lp33699
sg30
(lp33700
sg287
(lp33701
sg74
(lp33702
sg176
(lp33703
sg145
(lp33704
sg256
(lp33705
sg76
(lp33706
sg262
(lp33707
sg295
(lp33708
sg183
(lp33709
sg42
(lp33710
I68
asg230
(lp33711
sg329
(lp33712
sg32
(lp33713
sg318
(lp33714
sg178
(lp33715
sg22
(lp33716
sg181
(lp33717
sg235
(lp33718
sg384
(lp33719
sg124
(lp33720
ssS'stabil'
p33721
(dp33722
g230
(lp33723
sg438
(lp33724
I915
asg318
(lp33725
sg121
(lp33726
sg4
(lp33727
sg80
(lp33728
sg34
(lp33729
sg221
(lp33730
sg68
(lp33731
sg306
(lp33732
sg176
(lp33733
sg128
(lp33734
sg26
(lp33735
sg245
(lp33736
sg14
(lp33737
sg16
(lp33738
sg18
(lp33739
sg50
(lp33740
sg535
(lp33741
sg350
(lp33742
ssS'undergo'
p33743
(dp33744
g68
(lp33745
sg10
(lp33746
sg181
(lp33747
I1947
assS'spectra'
p33748
(dp33749
g332
(lp33750
sg78
(lp33751
sg96
(lp33752
I1324
asg48
(lp33753
ssS'qtiut'
p33754
(dp33755
g87
(lp33756
I1087
assS'egm'
p33757
(dp33758
g332
(lp33759
I2263
assS'kosaka'
p33760
(dp33761
g20
(lp33762
I2538
assS'help'
p33763
(dp33764
g70
(lp33765
sg277
(lp33766
sg83
(lp33767
sg74
(lp33768
sg256
(lp33769
sg80
(lp33770
sg262
(lp33771
sg149
(lp33772
sg85
(lp33773
sg306
(lp33774
sg91
(lp33775
sg12
(lp33776
sg94
(lp33777
sg18
(lp33778
sg313
(lp33779
sg223
(lp33780
sg350
(lp33781
sg118
(lp33782
sg116
(lp33783
sg438
(lp33784
I2352
asg440
(lp33785
sg178
(lp33786
sg6
(lp33787
sg181
(lp33788
sg34
(lp33789
sg36
(lp33790
sg124
(lp33791
sg126
(lp33792
sg40
(lp33793
sg44
(lp33794
sg128
(lp33795
sg132
(lp33796
sg135
(lp33797
sg138
(lp33798
ssS'cybern'
p33799
(dp33800
g48
(lp33801
I2163
asg176
(lp33802
ssS'fft'
p33803
(dp33804
g116
(lp33805
sg78
(lp33806
sg22
(lp33807
I1667
assS'sooo'
p33808
(dp33809
g59
(lp33810
I2517
assS'soon'
p33811
(dp33812
g145
(lp33813
sg8
(lp33814
I204
assS'held'
p33815
(dp33816
g30
(lp33817
sg287
(lp33818
sg235
(lp33819
sg85
(lp33820
sg429
(lp33821
sg128
(lp33822
sg104
(lp33823
sg108
(lp33824
sg140
(lp33825
I1414
assS'thermal'
p33826
(dp33827
g14
(lp33828
sg16
(lp33829
I626
asg256
(lp33830
sg78
(lp33831
ssS'uvm'
p33832
(dp33833
g281
(lp33834
I319
assS'hierarchi'
p33835
(dp33836
g74
(lp33837
I2733
asg68
(lp33838
sg429
(lp33839
ssS'fff'
p33840
(dp33841
g78
(lp33842
I1396
assS'paramet'
p33843
(dp33844
g329
(lp33845
sg70
(lp33846
sg26
(lp33847
sg277
(lp33848
sg163
(lp33849
sg68
(lp33850
sg293
(lp33851
sg283
(lp33852
sg460
(lp33853
sg40
(lp33854
sg30
(lp33855
sg287
(lp33856
sg74
(lp33857
sg145
(lp33858
sg76
(lp33859
sg262
(lp33860
sg295
(lp33861
sg183
(lp33862
sg80
(lp33863
sg38
(lp33864
sg83
(lp33865
sg85
(lp33866
sg124
(lp33867
sg42
(lp33868
I247
asg306
(lp33869
sg87
(lp33870
sg91
(lp33871
sg12
(lp33872
sg46
(lp33873
sg96
(lp33874
sg48
(lp33875
sg221
(lp33876
sg313
(lp33877
sg44
(lp33878
sg149
(lp33879
sg116
(lp33880
sg174
(lp33881
sg18
(lp33882
sg32
(lp33883
sg429
(lp33884
sg318
(lp33885
sg102
(lp33886
sg104
(lp33887
sg108
(lp33888
sg110
(lp33889
sg178
(lp33890
sg22
(lp33891
sg230
(lp33892
sg438
(lp33893
sg440
(lp33894
sg332
(lp33895
sg121
(lp33896
sg4
(lp33897
sg181
(lp33898
sg8
(lp33899
sg34
(lp33900
sg36
(lp33901
sg384
(lp33902
sg235
(lp33903
sg126
(lp33904
sg10
(lp33905
sg535
(lp33906
sg344
(lp33907
sg130
(lp33908
sg132
(lp33909
sg14
(lp33910
sg16
(lp33911
sg350
(lp33912
sg138
(lp33913
sg354
(lp33914
ssS'dvorak'
p33915
(dp33916
g94
(lp33917
I382
assS'evltest'
p33918
(dp33919
g87
(lp33920
I2070
assS'finer'
p33921
(dp33922
g8
(lp33923
I684
assS'yi'
p33924
(dp33925
g283
(lp33926
sg163
(lp33927
sg30
(lp33928
sg145
(lp33929
sg38
(lp33930
sg42
(lp33931
I1318
asg87
(lp33932
sg96
(lp33933
sg18
(lp33934
sg535
(lp33935
sg223
(lp33936
sg149
(lp33937
sg104
(lp33938
sg230
(lp33939
sg118
(lp33940
sg121
(lp33941
sg36
(lp33942
sg124
(lp33943
sg72
(lp33944
sg341
(lp33945
sg313
(lp33946
sg50
(lp33947
sg354
(lp33948
ssS'yk'
p33949
(dp33950
g460
(lp33951
sg14
(lp33952
sg16
(lp33953
I974
asg121
(lp33954
sg22
(lp33955
ssS'ced'
p33956
(dp33957
g4
(lp33958
I2854
assS'ym'
p33959
(dp33960
g38
(lp33961
sg85
(lp33962
I3573
assS'yl'
p33963
(dp33964
g230
(lp33965
sg30
(lp33966
sg318
(lp33967
sg80
(lp33968
sg287
(lp33969
sg124
(lp33970
sg341
(lp33971
sg42
(lp33972
I781
asg68
(lp33973
sg96
(lp33974
sg163
(lp33975
ssS'cea'
p33976
(dp33977
g245
(lp33978
I728
assS'yn'
p33979
(dp33980
g230
(lp33981
sg287
(lp33982
sg68
(lp33983
sg38
(lp33984
sg341
(lp33985
sg85
(lp33986
sg96
(lp33987
sg354
(lp33988
I558
assS'ya'
p33989
(dp33990
g102
(lp33991
sg14
(lp33992
I4313
asg318
(lp33993
sg174
(lp33994
sg130
(lp33995
ssS'sentenc'
p33996
(dp33997
g94
(lp33998
I1244
asg96
(lp33999
sg440
(lp34000
ssS'yb'
p34001
(dp34002
g318
(lp34003
sg121
(lp34004
I649
assS'ye'
p34005
(dp34006
g245
(lp34007
I1358
assS'yd'
p34008
(dp34009
g163
(lp34010
sg36
(lp34011
sg341
(lp34012
sg42
(lp34013
I893
asg104
(lp34014
sg313
(lp34015
sg149
(lp34016
ssS'rwn'
p34017
(dp34018
g230
(lp34019
I2780
assS'yy'
p34020
(dp34021
g72
(lp34022
I2151
asg63
(lp34023
ssS'yx'
p34024
(dp34025
g174
(lp34026
I937
asg145
(lp34027
ssS'subtask'
p34028
(dp34029
g460
(lp34030
I361
assS'sussmann'
p34031
(dp34032
g341
(lp34033
I2927
assS'yp'
p34034
(dp34035
g76
(lp34036
sg354
(lp34037
I912
assS'ys'
p34038
(dp34039
g230
(lp34040
sg4
(lp34041
I1554
assS'yr'
p34042
(dp34043
g354
(lp34044
I1414
assS'pettet'
p34045
(dp34046
g149
(lp34047
I2617
assS'yt'
p34048
(dp34049
g230
(lp34050
sg318
(lp34051
sg235
(lp34052
sg36
(lp34053
sg341
(lp34054
I112
asg104
(lp34055
ssS'gandolfo'
p34056
(dp34057
g99
(lp34058
I3149
assS'yv'
p34059
(dp34060
g318
(lp34061
sg163
(lp34062
sg130
(lp34063
I1464
assS'goldstein'
p34064
(dp34065
g174
(lp34066
I2652
asg22
(lp34067
ssS'solver'
p34068
(dp34069
g34
(lp34070
I741
assS'iff'
p34071
(dp34072
g145
(lp34073
sg8
(lp34074
I944
assS'alexandr'
p34075
(dp34076
g303
(lp34077
I11
assS'radian'
p34078
(dp34079
g128
(lp34080
I2296
assS'braitenberg'
p34081
(dp34082
g48
(lp34083
I277
assS'radial'
p34084
(dp34085
g30
(lp34086
sg80
(lp34087
sg293
(lp34088
sg99
(lp34089
sg91
(lp34090
sg78
(lp34091
sg14
(lp34092
sg16
(lp34093
sg221
(lp34094
sg354
(lp34095
I1245
assS'ifl'
p34096
(dp34097
g130
(lp34098
I2733
assS'fulli'
p34099
(dp34100
g68
(lp34101
sg26
(lp34102
sg72
(lp34103
sg283
(lp34104
sg176
(lp34105
sg76
(lp34106
sg78
(lp34107
sg83
(lp34108
sg303
(lp34109
sg94
(lp34110
sg20
(lp34111
sg110
(lp34112
sg114
(lp34113
sg440
(lp34114
sg178
(lp34115
sg4
(lp34116
sg384
(lp34117
sg124
(lp34118
sg126
(lp34119
sg10
(lp34120
sg14
(lp34121
sg16
(lp34122
sg50
(lp34123
I98
assS'ifn'
p34124
(dp34125
g281
(lp34126
I1803
assS'ifi'
p34127
(dp34128
g344
(lp34129
sg223
(lp34130
I1386
assS'dig'
p34131
(dp34132
g20
(lp34133
I2586
assS'ifit'
p34134
(dp34135
g287
(lp34136
I2929
asg83
(lp34137
ssS'heavi'
p34138
(dp34139
g283
(lp34140
sg181
(lp34141
sg44
(lp34142
I1979
assS'dorsolater'
p34143
(dp34144
g116
(lp34145
I371
assS'everchang'
p34146
(dp34147
g114
(lp34148
I199
assS'todd'
p34149
(dp34150
g14
(lp34151
sg16
(lp34152
I44
asg4
(lp34153
ssS'beyond'
p34154
(dp34155
g174
(lp34156
sg318
(lp34157
sg178
(lp34158
sg235
(lp34159
sg384
(lp34160
sg484
(lp34161
sg303
(lp34162
sg42
(lp34163
I3057
asg176
(lp34164
sg89
(lp34165
sg70
(lp34166
sg108
(lp34167
sg99
(lp34168
sg59
(lp34169
ssS'event'
p34170
(dp34171
g30
(lp34172
sg174
(lp34173
sg440
(lp34174
sg70
(lp34175
sg4
(lp34176
sg262
(lp34177
sg83
(lp34178
sg85
(lp34179
sg429
(lp34180
sg132
(lp34181
sg106
(lp34182
I2762
assS'noiseless'
p34183
(dp34184
g163
(lp34185
I2077
assS'perceptu'
p34186
(dp34187
g116
(lp34188
sg174
(lp34189
sg440
(lp34190
sg332
(lp34191
sg70
(lp34192
sg293
(lp34193
sg74
(lp34194
sg245
(lp34195
sg429
(lp34196
sg318
(lp34197
sg102
(lp34198
sg48
(lp34199
sg52
(lp34200
sg149
(lp34201
I2713
assS'miss'
p34202
(dp34203
g145
(lp34204
sg26
(lp34205
sg277
(lp34206
sg78
(lp34207
sg59
(lp34208
sg484
(lp34209
sg72
(lp34210
sg85
(lp34211
sg91
(lp34212
sg130
(lp34213
sg135
(lp34214
I2223
asg221
(lp34215
ssS'robert'
p34216
(dp34217
g429
(lp34218
sg83
(lp34219
sg221
(lp34220
I2709
asg22
(lp34221
sg281
(lp34222
ssS'cubic'
p34223
(dp34224
g138
(lp34225
I638
assS'publish'
p34226
(dp34227
g26
(lp34228
sg163
(lp34229
sg30
(lp34230
sg287
(lp34231
sg74
(lp34232
sg183
(lp34233
sg59
(lp34234
sg42
(lp34235
I1645
asg87
(lp34236
sg94
(lp34237
sg48
(lp34238
sg223
(lp34239
sg174
(lp34240
sg318
(lp34241
sg63
(lp34242
sg230
(lp34243
sg438
(lp34244
sg440
(lp34245
sg332
(lp34246
sg34
(lp34247
sg281
(lp34248
sg130
(lp34249
sg135
(lp34250
sg138
(lp34251
sg354
(lp34252
ssS'eardrum'
p34253
(dp34254
g174
(lp34255
I212
assS'sustain'
p34256
(dp34257
g116
(lp34258
sg4
(lp34259
I251
asg10
(lp34260
sg350
(lp34261
ssS'jolla'
p34262
(dp34263
g116
(lp34264
sg318
(lp34265
sg8
(lp34266
sg262
(lp34267
sg303
(lp34268
sg91
(lp34269
sg18
(lp34270
sg50
(lp34271
I40
asg350
(lp34272
ssS'schaik'
p34273
(dp34274
g174
(lp34275
I473
assS'polysillicon'
p34276
(dp34277
g20
(lp34278
I1864
assS'wickelfeatur'
p34279
(dp34280
g181
(lp34281
I2178
assS'ruedger'
p34282
(dp34283
g138
(lp34284
I3405
assS'mathc'
p34285
(dp34286
g344
(lp34287
I18
assS'team'
p34288
(dp34289
g83
(lp34290
I107
assS'pub'
p34291
(dp34292
g145
(lp34293
sg76
(lp34294
sg8
(lp34295
sg484
(lp34296
sg126
(lp34297
sg121
(lp34298
I1487
assS'selfloop'
p34299
(dp34300
g20
(lp34301
I123
assS'hmll'
p34302
(dp34303
g72
(lp34304
I1461
assS'reason'
p34305
(dp34306
g124
(lp34307
sg26
(lp34308
sg72
(lp34309
sg68
(lp34310
sg283
(lp34311
sg30
(lp34312
sg74
(lp34313
sg176
(lp34314
sg145
(lp34315
sg76
(lp34316
sg262
(lp34317
sg295
(lp34318
sg183
(lp34319
sg80
(lp34320
sg85
(lp34321
sg306
(lp34322
sg89
(lp34323
sg91
(lp34324
sg12
(lp34325
sg94
(lp34326
sg18
(lp34327
sg221
(lp34328
sg223
(lp34329
sg293
(lp34330
sg429
(lp34331
sg318
(lp34332
sg104
(lp34333
sg63
(lp34334
sg329
(lp34335
sg48
(lp34336
sg121
(lp34337
sg22
(lp34338
sg8
(lp34339
sg34
(lp34340
sg384
(lp34341
sg235
(lp34342
sg126
(lp34343
sg40
(lp34344
sg344
(lp34345
sg128
(lp34346
sg132
(lp34347
sg14
(lp34348
sg16
(lp34349
sg50
(lp34350
I967
assS'base'
p34351
(dp34352
g344
(lp34353
sg329
(lp34354
sg70
(lp34355
sg78
(lp34356
sg277
(lp34357
sg163
(lp34358
sg72
(lp34359
sg281
(lp34360
sg283
(lp34361
sg85
(lp34362
sg36
(lp34363
sg181
(lp34364
sg40
(lp34365
sg26
(lp34366
sg30
(lp34367
sg350
(lp34368
sg74
(lp34369
sg145
(lp34370
sg80
(lp34371
sg76
(lp34372
sg293
(lp34373
sg295
(lp34374
sg183
(lp34375
sg59
(lp34376
sg484
(lp34377
sg83
(lp34378
sg114
(lp34379
sg63
(lp34380
sg42
(lp34381
I302
asg306
(lp34382
sg87
(lp34383
sg89
(lp34384
sg91
(lp34385
sg12
(lp34386
sg94
(lp34387
sg96
(lp34388
sg48
(lp34389
sg99
(lp34390
sg313
(lp34391
sg44
(lp34392
sg149
(lp34393
sg230
(lp34394
sg174
(lp34395
sg116
(lp34396
sg32
(lp34397
sg178
(lp34398
sg245
(lp34399
sg429
(lp34400
sg318
(lp34401
sg46
(lp34402
sg104
(lp34403
sg106
(lp34404
sg110
(lp34405
sg20
(lp34406
sg52
(lp34407
sg22
(lp34408
sg216
(lp34409
sg438
(lp34410
sg440
(lp34411
sg332
(lp34412
sg121
(lp34413
sg4
(lp34414
sg6
(lp34415
sg8
(lp34416
sg34
(lp34417
sg221
(lp34418
sg303
(lp34419
sg126
(lp34420
sg341
(lp34421
sg10
(lp34422
sg535
(lp34423
sg287
(lp34424
sg223
(lp34425
sg128
(lp34426
sg130
(lp34427
sg132
(lp34428
sg14
(lp34429
sg16
(lp34430
sg135
(lp34431
sg50
(lp34432
sg138
(lp34433
sg140
(lp34434
sg354
(lp34435
ssS'ask'
p34436
(dp34437
g32
(lp34438
sg4
(lp34439
sg277
(lp34440
sg293
(lp34441
sg183
(lp34442
sg40
(lp34443
sg99
(lp34444
I2092
asg91
(lp34445
sg78
(lp34446
sg303
(lp34447
sg110
(lp34448
sg63
(lp34449
ssS'earliest'
p34450
(dp34451
g132
(lp34452
I186
assS'aso'
p34453
(dp34454
g176
(lp34455
I961
assS'costto'
p34456
(dp34457
g306
(lp34458
I71
assS'asm'
p34459
(dp34460
g108
(lp34461
I2518
assS'basi'
p34462
(dp34463
g26
(lp34464
sg163
(lp34465
sg72
(lp34466
sg30
(lp34467
sg74
(lp34468
sg145
(lp34469
sg80
(lp34470
sg293
(lp34471
sg295
(lp34472
sg183
(lp34473
sg59
(lp34474
sg83
(lp34475
sg303
(lp34476
sg306
(lp34477
sg89
(lp34478
sg91
(lp34479
sg46
(lp34480
sg20
(lp34481
sg114
(lp34482
sg221
(lp34483
sg313
(lp34484
sg44
(lp34485
sg149
(lp34486
sg110
(lp34487
sg96
(lp34488
sg52
(lp34489
sg22
(lp34490
sg32
(lp34491
sg332
(lp34492
sg4
(lp34493
sg6
(lp34494
sg235
(lp34495
sg99
(lp34496
sg126
(lp34497
sg344
(lp34498
sg130
(lp34499
sg50
(lp34500
sg138
(lp34501
sg140
(lp34502
sg354
(lp34503
I1246
assS'veloc'
p34504
(dp34505
g216
(lp34506
sg32
(lp34507
sg460
(lp34508
sg89
(lp34509
sg245
(lp34510
sg114
(lp34511
sg99
(lp34512
I673
asg350
(lp34513
ssS'american'
p34514
(dp34515
g30
(lp34516
sg438
(lp34517
sg178
(lp34518
sg4
(lp34519
sg277
(lp34520
sg295
(lp34521
sg183
(lp34522
sg116
(lp34523
sg341
(lp34524
sg42
(lp34525
I548
asg87
(lp34526
ssS'doursat'
p34527
(dp34528
g484
(lp34529
sg140
(lp34530
I3139
assS'velop'
p34531
(dp34532
g116
(lp34533
I688
assS'lifetim'
p34534
(dp34535
g223
(lp34536
I315
assS'assign'
p34537
(dp34538
g26
(lp34539
sg277
(lp34540
sg30
(lp34541
sg287
(lp34542
sg74
(lp34543
sg76
(lp34544
sg295
(lp34545
sg183
(lp34546
sg38
(lp34547
sg83
(lp34548
sg85
(lp34549
sg42
(lp34550
I1444
asg89
(lp34551
sg96
(lp34552
sg313
(lp34553
sg116
(lp34554
sg104
(lp34555
sg110
(lp34556
sg114
(lp34557
sg216
(lp34558
sg329
(lp34559
sg124
(lp34560
sg281
(lp34561
sg130
(lp34562
sg132
(lp34563
ssS'singleton'
p34564
(dp34565
g145
(lp34566
I2641
assS'fischer'
p34567
(dp34568
g303
(lp34569
I2873
assS'uninterest'
p34570
(dp34571
g132
(lp34572
I1761
asg145
(lp34573
ssS'membran'
p34574
(dp34575
g116
(lp34576
sg174
(lp34577
sg106
(lp34578
I1200
asg176
(lp34579
sg262
(lp34580
ssS'zkij'
p34581
(dp34582
g121
(lp34583
I710
assS'fwction'
p34584
(dp34585
g110
(lp34586
I1009
assS'przep'
p34587
(dp34588
g85
(lp34589
I743
assS'transcendent'
p34590
(dp34591
g130
(lp34592
I1495
assS'scheme'
p34593
(dp34594
g283
(lp34595
sg70
(lp34596
sg26
(lp34597
sg72
(lp34598
sg74
(lp34599
sg76
(lp34600
sg295
(lp34601
sg183
(lp34602
sg306
(lp34603
sg87
(lp34604
sg89
(lp34605
sg245
(lp34606
sg20
(lp34607
sg44
(lp34608
sg429
(lp34609
sg63
(lp34610
sg230
(lp34611
sg118
(lp34612
sg440
(lp34613
sg121
(lp34614
sg22
(lp34615
sg8
(lp34616
sg384
(lp34617
sg126
(lp34618
sg130
(lp34619
sg135
(lp34620
sg50
(lp34621
sg138
(lp34622
sg140
(lp34623
I155
assS'disagr'
p34624
(dp34625
g221
(lp34626
sg140
(lp34627
I71
assS'schema'
p34628
(dp34629
g96
(lp34630
I534
asg332
(lp34631
ssS'adhes'
p34632
(dp34633
g484
(lp34634
I1758
assS'adher'
p34635
(dp34636
g332
(lp34637
I625
asg110
(lp34638
ssS'tfi'
p34639
(dp34640
g130
(lp34641
I2298
assS'behind'
p34642
(dp34643
g74
(lp34644
sg121
(lp34645
I569
asg78
(lp34646
sg72
(lp34647
sg70
(lp34648
sg63
(lp34649
sg350
(lp34650
ssS'basal'
p34651
(dp34652
g70
(lp34653
I1280
assS'bower'
p34654
(dp34655
g48
(lp34656
I2477
asg110
(lp34657
ssS'gnomon'
p34658
(dp34659
g32
(lp34660
I1813
assS'incorrec'
p34661
(dp34662
g4
(lp34663
I1572
assS'tft'
p34664
(dp34665
g14
(lp34666
I4488
assS'tive'
p34667
(dp34668
g295
(lp34669
sg183
(lp34670
sg106
(lp34671
I950
asg344
(lp34672
ssS'ncc'
p34673
(dp34674
g110
(lp34675
I3050
assS'nce'
p34676
(dp34677
g4
(lp34678
I1540
assS'rawl'
p34679
(dp34680
g344
(lp34681
I3615
assS'std'
p34682
(dp34683
g126
(lp34684
sg128
(lp34685
I1543
assS'passeriform'
p34686
(dp34687
g116
(lp34688
I128
assS'grew'
p34689
(dp34690
g96
(lp34691
I2134
assS'chausse'
p34692
(dp34693
g36
(lp34694
I249
assS'niddk'
p34695
(dp34696
g70
(lp34697
I11
assS'stm'
p34698
(dp34699
g535
(lp34700
I282
assS'grey'
p34701
(dp34702
g216
(lp34703
sg318
(lp34704
sg256
(lp34705
sg283
(lp34706
sg63
(lp34707
sg149
(lp34708
I1616
assS'dtd'
p34709
(dp34710
g384
(lp34711
I1239
assS'mussa'
p34712
(dp34713
g99
(lp34714
I18
assS'stu'
p34715
(dp34716
g132
(lp34717
I681
assS'wojj'
p34718
(dp34719
g36
(lp34720
I794
assS'unrt'
p34721
(dp34722
g295
(lp34723
I1220
asg183
(lp34724
ssS'str'
p34725
(dp34726
g245
(lp34727
I39
assS'tthose'
p34728
(dp34729
g221
(lp34730
I1095
assS'bridl'
p34731
(dp34732
g178
(lp34733
I1444
asg26
(lp34734
ssS'consumpt'
p34735
(dp34736
g20
(lp34737
sg135
(lp34738
I513
asg221
(lp34739
sg22
(lp34740
ssS'toward'
p34741
(dp34742
g70
(lp34743
sg26
(lp34744
sg277
(lp34745
sg74
(lp34746
sg76
(lp34747
sg80
(lp34748
sg38
(lp34749
sg83
(lp34750
sg303
(lp34751
sg89
(lp34752
sg91
(lp34753
sg46
(lp34754
sg20
(lp34755
sg18
(lp34756
sg223
(lp34757
sg350
(lp34758
sg318
(lp34759
sg230
(lp34760
sg118
(lp34761
sg332
(lp34762
sg121
(lp34763
sg22
(lp34764
sg8
(lp34765
sg36
(lp34766
sg40
(lp34767
sg44
(lp34768
I1822
assS'dtw'
p34769
(dp34770
g96
(lp34771
I1507
assS'dtu'
p34772
(dp34773
g26
(lp34774
I38
assS'hachmann'
p34775
(dp34776
g10
(lp34777
I2863
assS'dvgt'
p34778
(dp34779
g262
(lp34780
I1255
assS'cabelli'
p34781
(dp34782
g132
(lp34783
I3603
assS'valenstein'
p34784
(dp34785
g303
(lp34786
I2822
assS'null'
p34787
(dp34788
g99
(lp34789
I590
asg6
(lp34790
ssS'irjicd'
p34791
(dp34792
g183
(lp34793
I5295
assS'lid'
p34794
(dp34795
g306
(lp34796
I3034
asg74
(lp34797
ssS'lie'
p34798
(dp34799
g30
(lp34800
sg118
(lp34801
sg32
(lp34802
sg332
(lp34803
sg181
(lp34804
sg34
(lp34805
sg36
(lp34806
sg59
(lp34807
sg281
(lp34808
sg85
(lp34809
sg306
(lp34810
sg102
(lp34811
sg104
(lp34812
sg138
(lp34813
I999
asg350
(lp34814
ssS'jour'
p34815
(dp34816
g42
(lp34817
I3496
assS'lib'
p34818
(dp34819
g126
(lp34820
I2955
assS'milner'
p34821
(dp34822
g350
(lp34823
I3093
assS'lil'
p34824
(dp34825
g18
(lp34826
I820
assS'lim'
p34827
(dp34828
g102
(lp34829
I2143
asg287
(lp34830
sg384
(lp34831
sg341
(lp34832
ssS'lin'
p34833
(dp34834
g440
(lp34835
sg22
(lp34836
sg293
(lp34837
sg256
(lp34838
sg163
(lp34839
sg72
(lp34840
sg89
(lp34841
sg130
(lp34842
sg96
(lp34843
sg135
(lp34844
I438
asg114
(lp34845
ssS'scotland'
p34846
(dp34847
g174
(lp34848
I19
assS'lii'
p34849
(dp34850
g216
(lp34851
I2205
assS'lij'
p34852
(dp34853
g438
(lp34854
I290
assS'lik'
p34855
(dp34856
g74
(lp34857
I1038
assS'lit'
p34858
(dp34859
g36
(lp34860
sg429
(lp34861
sg108
(lp34862
I1178
asg178
(lp34863
sg68
(lp34864
ssS'liu'
p34865
(dp34866
g174
(lp34867
I467
asg22
(lp34868
sg256
(lp34869
ssS'lip'
p34870
(dp34871
g303
(lp34872
sg68
(lp34873
sg8
(lp34874
I1514
assS'useless'
p34875
(dp34876
g70
(lp34877
sg72
(lp34878
sg350
(lp34879
I1704
assS'lis'
p34880
(dp34881
g108
(lp34882
I1650
asg535
(lp34883
ssS'sherrington'
p34884
(dp34885
g384
(lp34886
I49
assS'lix'
p34887
(dp34888
g440
(lp34889
I1691
assS'liy'
p34890
(dp34891
g230
(lp34892
I2029
asg460
(lp34893
ssS'kaa'
p34894
(dp34895
g176
(lp34896
I137
assS'exampi'
p34897
(dp34898
g140
(lp34899
I2125
assS'kai'
p34900
(dp34901
g68
(lp34902
sg22
(lp34903
sg140
(lp34904
I3064
assS'choos'
p34905
(dp34906
g30
(lp34907
sg74
(lp34908
sg145
(lp34909
sg80
(lp34910
sg295
(lp34911
sg183
(lp34912
sg85
(lp34913
sg306
(lp34914
sg91
(lp34915
sg46
(lp34916
sg221
(lp34917
sg535
(lp34918
sg223
(lp34919
sg102
(lp34920
sg110
(lp34921
sg230
(lp34922
sg329
(lp34923
sg440
(lp34924
sg22
(lp34925
sg235
(lp34926
sg34
(lp34927
sg384
(lp34928
sg68
(lp34929
sg281
(lp34930
sg313
(lp34931
sg344
(lp34932
sg44
(lp34933
sg140
(lp34934
sg354
(lp34935
I2096
assS'homolog'
p34936
(dp34937
g26
(lp34938
I1150
assS'mixtur'
p34939
(dp34940
g68
(lp34941
sg277
(lp34942
sg30
(lp34943
sg74
(lp34944
sg295
(lp34945
sg183
(lp34946
sg59
(lp34947
sg303
(lp34948
sg87
(lp34949
sg91
(lp34950
sg94
(lp34951
sg96
(lp34952
sg221
(lp34953
sg313
(lp34954
sg329
(lp34955
sg318
(lp34956
sg460
(lp34957
sg124
(lp34958
sg72
(lp34959
sg281
(lp34960
sg130
(lp34961
sg138
(lp34962
sg140
(lp34963
I1085
assS'fllllction'
p34964
(dp34965
g40
(lp34966
I1244
assS'watt'
p34967
(dp34968
g22
(lp34969
I244
assS'alpha'
p34970
(dp34971
g329
(lp34972
I1291
assS'kay'
p34973
(dp34974
g283
(lp34975
I32
assS'ntatlon'
p34976
(dp34977
g72
(lp34978
I987
assS'mach'
p34979
(dp34980
g42
(lp34981
I3437
asg235
(lp34982
ssS'clear'
p34983
(dp34984
g30
(lp34985
sg287
(lp34986
sg74
(lp34987
sg176
(lp34988
sg76
(lp34989
sg262
(lp34990
sg78
(lp34991
sg59
(lp34992
sg85
(lp34993
sg303
(lp34994
sg94
(lp34995
sg20
(lp34996
sg18
(lp34997
sg329
(lp34998
sg108
(lp34999
sg110
(lp35000
sg63
(lp35001
sg52
(lp35002
sg114
(lp35003
sg230
(lp35004
sg174
(lp35005
sg32
(lp35006
sg332
(lp35007
sg4
(lp35008
sg181
(lp35009
sg235
(lp35010
sg36
(lp35011
sg384
(lp35012
sg68
(lp35013
sg72
(lp35014
sg341
(lp35015
sg40
(lp35016
sg128
(lp35017
sg14
(lp35018
sg16
(lp35019
sg22
(lp35020
sg140
(lp35021
I309
assS'syal'
p35022
(dp35023
g14
(lp35024
I4309
assS'physica'
p35025
(dp35026
g354
(lp35027
I3163
assS'filler'
p35028
(dp35029
g183
(lp35030
I4108
assS'clean'
p35031
(dp35032
g174
(lp35033
sg484
(lp35034
sg313
(lp35035
sg20
(lp35036
sg63
(lp35037
sg354
(lp35038
I3128
assS'jordan'
p35039
(dp35040
g230
(lp35041
sg329
(lp35042
sg59
(lp35043
sg293
(lp35044
sg295
(lp35045
sg183
(lp35046
sg460
(lp35047
sg30
(lp35048
sg313
(lp35049
sg72
(lp35050
sg87
(lp35051
sg91
(lp35052
sg99
(lp35053
sg138
(lp35054
I3430
assS'usual'
p35055
(dp35056
g68
(lp35057
sg70
(lp35058
sg277
(lp35059
sg163
(lp35060
sg30
(lp35061
sg287
(lp35062
sg74
(lp35063
sg176
(lp35064
sg76
(lp35065
sg262
(lp35066
sg295
(lp35067
sg183
(lp35068
sg484
(lp35069
sg83
(lp35070
sg85
(lp35071
sg42
(lp35072
I166
asg91
(lp35073
sg94
(lp35074
sg20
(lp35075
sg221
(lp35076
sg535
(lp35077
sg223
(lp35078
sg32
(lp35079
sg429
(lp35080
sg102
(lp35081
sg104
(lp35082
sg108
(lp35083
sg96
(lp35084
sg52
(lp35085
sg114
(lp35086
sg116
(lp35087
sg174
(lp35088
sg440
(lp35089
sg121
(lp35090
sg181
(lp35091
sg235
(lp35092
sg384
(lp35093
sg124
(lp35094
sg72
(lp35095
sg10
(lp35096
sg128
(lp35097
sg132
(lp35098
sg14
(lp35099
sg16
(lp35100
sg140
(lp35101
sg354
(lp35102
ssS'blend'
p35103
(dp35104
g30
(lp35105
I580
asg295
(lp35106
sg183
(lp35107
ssS'abab'
p35108
(dp35109
g332
(lp35110
I1836
assS'hyper'
p35111
(dp35112
g221
(lp35113
I2517
assS'phenomenon'
p35114
(dp35115
g176
(lp35116
sg181
(lp35117
sg38
(lp35118
sg341
(lp35119
sg85
(lp35120
sg130
(lp35121
I737
asg99
(lp35122
sg535
(lp35123
ssS'wehmeier'
p35124
(dp35125
g12
(lp35126
I2801
assS'retinotopi'
p35127
(dp35128
g303
(lp35129
sg149
(lp35130
I307
assS'corticon'
p35131
(dp35132
g245
(lp35133
I36
assS'seguinot'
p35134
(dp35135
g80
(lp35136
I488
assS'ayij'
p35137
(dp35138
g118
(lp35139
I1096
assS'copyright'
p35140
(dp35141
g108
(lp35142
I266
assS'flesh'
p35143
(dp35144
g116
(lp35145
I2308
assS'roitblat'
p35146
(dp35147
g313
(lp35148
I2198
assS'queu'
p35149
(dp35150
g306
(lp35151
I2435
assS'edelman'
p35152
(dp35153
g176
(lp35154
sg181
(lp35155
sg223
(lp35156
I3210
assS'morgenstern'
p35157
(dp35158
g91
(lp35159
I1677
assS'albeit'
p35160
(dp35161
g76
(lp35162
I1871
assS'inaijmaim'
p35163
(dp35164
g429
(lp35165
I704
assS'intensifi'
p35166
(dp35167
g283
(lp35168
sg138
(lp35169
I2150
assS'famous'
p35170
(dp35171
g132
(lp35172
I197
assS'liiqbc'
p35173
(dp35174
g102
(lp35175
I2145
assS'duq'
p35176
(dp35177
g344
(lp35178
I19
assS'politiora'
p35179
(dp35180
g63
(lp35181
I1996
assS'vestibula'
p35182
(dp35183
g350
(lp35184
I743
assS'gelb'
p35185
(dp35186
g108
(lp35187
I392
assS'oxi'
p35188
(dp35189
g130
(lp35190
I1383
assS'neuromorph'
p35191
(dp35192
g40
(lp35193
I2434
assS'switzerland'
p35194
(dp35195
g174
(lp35196
I2546
asg293
(lp35197
ssS'x'
p35198
(dp35199
g344
(lp35200
sg329
(lp35201
sg70
(lp35202
sg78
(lp35203
sg277
(lp35204
sg163
(lp35205
sg72
(lp35206
sg303
(lp35207
sg281
(lp35208
sg283
(lp35209
sg85
(lp35210
sg181
(lp35211
sg40
(lp35212
sg26
(lp35213
sg30
(lp35214
sg350
(lp35215
sg176
(lp35216
sg145
(lp35217
sg256
(lp35218
sg76
(lp35219
sg262
(lp35220
sg295
(lp35221
sg183
(lp35222
sg59
(lp35223
sg484
(lp35224
sg38
(lp35225
sg83
(lp35226
sg114
(lp35227
sg124
(lp35228
sg42
(lp35229
I100
asg306
(lp35230
sg87
(lp35231
sg89
(lp35232
sg91
(lp35233
sg12
(lp35234
sg94
(lp35235
sg96
(lp35236
sg48
(lp35237
sg221
(lp35238
sg313
(lp35239
sg44
(lp35240
sg149
(lp35241
sg118
(lp35242
sg116
(lp35243
sg174
(lp35244
sg18
(lp35245
sg32
(lp35246
sg245
(lp35247
sg429
(lp35248
sg318
(lp35249
sg46
(lp35250
sg104
(lp35251
sg108
(lp35252
sg110
(lp35253
sg178
(lp35254
sg52
(lp35255
sg22
(lp35256
sg230
(lp35257
sg438
(lp35258
sg440
(lp35259
sg332
(lp35260
sg121
(lp35261
sg4
(lp35262
sg6
(lp35263
sg8
(lp35264
sg34
(lp35265
sg36
(lp35266
sg460
(lp35267
sg235
(lp35268
sg126
(lp35269
sg341
(lp35270
sg10
(lp35271
sg535
(lp35272
sg287
(lp35273
sg63
(lp35274
sg223
(lp35275
sg128
(lp35276
sg130
(lp35277
sg14
(lp35278
sg16
(lp35279
sg135
(lp35280
sg50
(lp35281
sg138
(lp35282
sg140
(lp35283
sg354
(lp35284
ssS'earlbaum'
p35285
(dp35286
g68
(lp35287
I3382
asg80
(lp35288
ssS'vestibulo'
p35289
(dp35290
g350
(lp35291
I9
assS'grain'
p35292
(dp35293
g70
(lp35294
I1497
assS'pseudogradi'
p35295
(dp35296
g306
(lp35297
I2225
assS'mantra'
p35298
(dp35299
g174
(lp35300
I2535
assS'schizophrenia'
p35301
(dp35302
g4
(lp35303
I773
assS'close'
p35304
(dp35305
g68
(lp35306
sg70
(lp35307
sg26
(lp35308
sg72
(lp35309
sg30
(lp35310
sg74
(lp35311
sg145
(lp35312
sg460
(lp35313
sg38
(lp35314
sg83
(lp35315
sg85
(lp35316
sg306
(lp35317
sg91
(lp35318
sg96
(lp35319
sg535
(lp35320
sg350
(lp35321
sg230
(lp35322
sg329
(lp35323
sg63
(lp35324
sg52
(lp35325
sg114
(lp35326
sg216
(lp35327
sg438
(lp35328
I1100
asg32
(lp35329
sg332
(lp35330
sg22
(lp35331
sg181
(lp35332
sg34
(lp35333
sg384
(lp35334
sg124
(lp35335
sg126
(lp35336
sg313
(lp35337
sg128
(lp35338
sg132
(lp35339
sg14
(lp35340
sg16
(lp35341
sg149
(lp35342
sg50
(lp35343
sg138
(lp35344
sg140
(lp35345
sg354
(lp35346
ssS'momenta'
p35347
(dp35348
g124
(lp35349
sg126
(lp35350
I1012
assS'maxqm'
p35351
(dp35352
g72
(lp35353
I1548
assS'opper'
p35354
(dp35355
g140
(lp35356
I3175
assS'chart'
p35357
(dp35358
g32
(lp35359
I1147
assS'particip'
p35360
(dp35361
g176
(lp35362
I2484
assS'pisa'
p35363
(dp35364
g44
(lp35365
I19
assS'loglikelihood'
p35366
(dp35367
g74
(lp35368
sg221
(lp35369
I2028
assS'wol'
p35370
(dp35371
g36
(lp35372
I1279
assS'engel'
p35373
(dp35374
g70
(lp35375
sg149
(lp35376
I3037
assS'won'
p35377
(dp35378
g132
(lp35379
I3006
asg116
(lp35380
ssS'woi'
p35381
(dp35382
g36
(lp35383
sg341
(lp35384
I1188
assS'niversitat'
p35385
(dp35386
g48
(lp35387
sg130
(lp35388
I14
assS'gmk'
p35389
(dp35390
g78
(lp35391
I29
assS'isod'
p35392
(dp35393
g30
(lp35394
I1177
assS'numer'
p35395
(dp35396
g32
(lp35397
sg181
(lp35398
sg293
(lp35399
sg34
(lp35400
sg221
(lp35401
sg384
(lp35402
sg38
(lp35403
sg74
(lp35404
sg281
(lp35405
sg63
(lp35406
sg87
(lp35407
sg36
(lp35408
sg102
(lp35409
sg14
(lp35410
sg16
(lp35411
sg350
(lp35412
sg50
(lp35413
sg96
(lp35414
sg140
(lp35415
sg354
(lp35416
I122
assS'sts'
p35417
(dp35418
g535
(lp35419
I1556
assS'isol'
p35420
(dp35421
g116
(lp35422
sg332
(lp35423
sg121
(lp35424
sg181
(lp35425
sg6
(lp35426
sg295
(lp35427
sg183
(lp35428
sg63
(lp35429
sg128
(lp35430
sg46
(lp35431
sg245
(lp35432
sg14
(lp35433
sg99
(lp35434
sg138
(lp35435
I958
asg44
(lp35436
ssS'multilay'
p35437
(dp35438
g440
(lp35439
sg108
(lp35440
sg181
(lp35441
sg36
(lp35442
sg68
(lp35443
sg38
(lp35444
sg85
(lp35445
sg87
(lp35446
sg91
(lp35447
sg128
(lp35448
sg14
(lp35449
sg16
(lp35450
sg135
(lp35451
sg354
(lp35452
I1241
assS'effec'
p35453
(dp35454
g4
(lp35455
I2860
assS'idpost'
p35456
(dp35457
g106
(lp35458
I1982
assS'saggit'
p35459
(dp35460
g116
(lp35461
I320
assS'distinguish'
p35462
(dp35463
g26
(lp35464
sg181
(lp35465
sg30
(lp35466
sg287
(lp35467
sg76
(lp35468
sg262
(lp35469
sg183
(lp35470
sg38
(lp35471
sg303
(lp35472
sg42
(lp35473
I2071
asg18
(lp35474
sg223
(lp35475
sg318
(lp35476
sg104
(lp35477
sg116
(lp35478
sg32
(lp35479
sg332
(lp35480
sg121
(lp35481
sg4
(lp35482
sg6
(lp35483
sg128
(lp35484
sg78
(lp35485
sg135
(lp35486
sg354
(lp35487
ssS'both'
p35488
(dp35489
g344
(lp35490
sg329
(lp35491
sg70
(lp35492
sg78
(lp35493
sg277
(lp35494
sg163
(lp35495
sg72
(lp35496
sg68
(lp35497
sg80
(lp35498
sg293
(lp35499
sg283
(lp35500
sg85
(lp35501
sg460
(lp35502
sg40
(lp35503
sg26
(lp35504
sg30
(lp35505
sg350
(lp35506
sg74
(lp35507
sg176
(lp35508
sg145
(lp35509
sg256
(lp35510
sg76
(lp35511
sg262
(lp35512
sg295
(lp35513
sg183
(lp35514
sg59
(lp35515
sg484
(lp35516
sg38
(lp35517
sg83
(lp35518
sg114
(lp35519
sg124
(lp35520
sg87
(lp35521
sg89
(lp35522
sg91
(lp35523
sg12
(lp35524
sg94
(lp35525
sg96
(lp35526
sg48
(lp35527
sg221
(lp35528
sg313
(lp35529
sg44
(lp35530
sg149
(lp35531
sg118
(lp35532
sg230
(lp35533
sg303
(lp35534
sg174
(lp35535
sg18
(lp35536
sg32
(lp35537
sg178
(lp35538
sg245
(lp35539
sg429
(lp35540
sg318
(lp35541
sg46
(lp35542
sg102
(lp35543
sg104
(lp35544
sg106
(lp35545
sg110
(lp35546
sg20
(lp35547
sg52
(lp35548
sg22
(lp35549
sg216
(lp35550
sg438
(lp35551
I740
asg440
(lp35552
sg332
(lp35553
sg121
(lp35554
sg4
(lp35555
sg6
(lp35556
sg8
(lp35557
sg36
(lp35558
sg384
(lp35559
sg235
(lp35560
sg126
(lp35561
sg10
(lp35562
sg535
(lp35563
sg287
(lp35564
sg63
(lp35565
sg223
(lp35566
sg128
(lp35567
sg130
(lp35568
sg132
(lp35569
sg135
(lp35570
sg50
(lp35571
sg138
(lp35572
sg140
(lp35573
sg354
(lp35574
ssS'mega'
p35575
(dp35576
g135
(lp35577
I1010
assS'rijn'
p35578
(dp35579
g32
(lp35580
I338
assS'ttra'
p35581
(dp35582
g230
(lp35583
I2770
assS'ejit'
p35584
(dp35585
g138
(lp35586
I547
assS'jeff'
p35587
(dp35588
g344
(lp35589
I3345
assS'priorit'
p35590
(dp35591
g80
(lp35592
I1075
assS'ehrenfeucht'
p35593
(dp35594
g287
(lp35595
I3510
assS'cartwright'
p35596
(dp35597
g80
(lp35598
I2444
assS'vessel'
p35599
(dp35600
g102
(lp35601
sg14
(lp35602
sg16
(lp35603
I421
asg78
(lp35604
ssS'eenter'
p35605
(dp35606
g63
(lp35607
I1216
assS'zizj'
p35608
(dp35609
g128
(lp35610
I250
assS'damp'
p35611
(dp35612
g245
(lp35613
I1130
asg460
(lp35614
ssS'coggin'
p35615
(dp35616
g135
(lp35617
I11
assS'jolesz'
p35618
(dp35619
g318
(lp35620
I3059
assS'empti'
p35621
(dp35622
g145
(lp35623
sg76
(lp35624
sg83
(lp35625
sg40
(lp35626
sg42
(lp35627
I1199
asg306
(lp35628
sg44
(lp35629
ssS'tomlinson'
p35630
(dp35631
g6
(lp35632
I2002
assS'gestalt'
p35633
(dp35634
g332
(lp35635
sg149
(lp35636
I2376
assS'komaba'
p35637
(dp35638
g18
(lp35639
I23
assS'ganglion'
p35640
(dp35641
g174
(lp35642
I292
asg118
(lp35643
ssS'anthoni'
p35644
(dp35645
g287
(lp35646
sg40
(lp35647
sg6
(lp35648
I2207
asg262
(lp35649
ssS'clll'
p35650
(dp35651
g135
(lp35652
I921
assS'imai'
p35653
(dp35654
g20
(lp35655
I2717
assS'intersci'
p35656
(dp35657
g40
(lp35658
I2565
assS'imag'
p35659
(dp35660
g283
(lp35661
sg26
(lp35662
sg30
(lp35663
sg256
(lp35664
sg76
(lp35665
sg293
(lp35666
sg295
(lp35667
sg183
(lp35668
sg59
(lp35669
sg484
(lp35670
sg42
(lp35671
I1690
asg245
(lp35672
sg20
(lp35673
sg48
(lp35674
sg221
(lp35675
sg223
(lp35676
sg350
(lp35677
sg118
(lp35678
sg429
(lp35679
sg104
(lp35680
sg63
(lp35681
sg52
(lp35682
sg114
(lp35683
sg174
(lp35684
sg318
(lp35685
sg178
(lp35686
sg181
(lp35687
sg8
(lp35688
sg99
(lp35689
sg235
(lp35690
sg281
(lp35691
sg40
(lp35692
sg44
(lp35693
sg149
(lp35694
sg50
(lp35695
sg138
(lp35696
sg140
(lp35697
ssS'deter'
p35698
(dp35699
g145
(lp35700
I2763
assS'coordin'
p35701
(dp35702
g287
(lp35703
sg32
(lp35704
sg176
(lp35705
sg80
(lp35706
sg163
(lp35707
sg34
(lp35708
sg460
(lp35709
sg68
(lp35710
sg38
(lp35711
sg40
(lp35712
sg429
(lp35713
sg130
(lp35714
sg132
(lp35715
sg14
(lp35716
sg16
(lp35717
I439
asg48
(lp35718
sg99
(lp35719
sg59
(lp35720
sg303
(lp35721
sg350
(lp35722
ssS'liver'
p35723
(dp35724
g484
(lp35725
sg221
(lp35726
I2201
assS'blakemor'
p35727
(dp35728
g12
(lp35729
I2751
assS'uperimpo'
p35730
(dp35731
g4
(lp35732
I1542
assS'looo'
p35733
(dp35734
g332
(lp35735
I1838
assS'loom'
p35736
(dp35737
g350
(lp35738
I2398
assS'utmost'
p35739
(dp35740
g145
(lp35741
I1156
assS'look'
p35742
(dp35743
g30
(lp35744
sg329
(lp35745
sg32
(lp35746
sg178
(lp35747
sg256
(lp35748
sg181
(lp35749
sg118
(lp35750
sg34
(lp35751
sg183
(lp35752
sg124
(lp35753
sg116
(lp35754
sg440
(lp35755
sg293
(lp35756
sg74
(lp35757
sg429
(lp35758
sg91
(lp35759
sg40
(lp35760
sg12
(lp35761
I548
asg18
(lp35762
sg163
(lp35763
sg63
(lp35764
ssS'lood'
p35765
(dp35766
g18
(lp35767
I1103
assS'invit'
p35768
(dp35769
g72
(lp35770
I3588
assS'fapp'
p35771
(dp35772
g163
(lp35773
I1059
assS'wyatt'
p35774
(dp35775
g245
(lp35776
I68
assS'pace'
p35777
(dp35778
g135
(lp35779
I2438
assS'while'
p35780
(dp35781
g329
(lp35782
sg70
(lp35783
sg78
(lp35784
sg277
(lp35785
sg116
(lp35786
sg283
(lp35787
sg85
(lp35788
sg181
(lp35789
sg303
(lp35790
sg26
(lp35791
sg30
(lp35792
sg74
(lp35793
sg145
(lp35794
sg256
(lp35795
sg76
(lp35796
sg262
(lp35797
sg295
(lp35798
sg183
(lp35799
sg59
(lp35800
sg484
(lp35801
sg38
(lp35802
sg83
(lp35803
sg114
(lp35804
sg124
(lp35805
sg42
(lp35806
I227
asg306
(lp35807
sg89
(lp35808
sg68
(lp35809
sg245
(lp35810
sg94
(lp35811
sg96
(lp35812
sg48
(lp35813
sg99
(lp35814
sg313
(lp35815
sg44
(lp35816
sg149
(lp35817
sg118
(lp35818
sg230
(lp35819
sg174
(lp35820
sg32
(lp35821
sg318
(lp35822
sg46
(lp35823
sg102
(lp35824
sg104
(lp35825
sg106
(lp35826
sg108
(lp35827
sg20
(lp35828
sg22
(lp35829
sg216
(lp35830
sg438
(lp35831
sg440
(lp35832
sg18
(lp35833
sg121
(lp35834
sg4
(lp35835
sg6
(lp35836
sg8
(lp35837
sg36
(lp35838
sg460
(lp35839
sg235
(lp35840
sg126
(lp35841
sg10
(lp35842
sg535
(lp35843
sg344
(lp35844
sg63
(lp35845
sg223
(lp35846
sg130
(lp35847
sg132
(lp35848
sg14
(lp35849
sg16
(lp35850
sg135
(lp35851
sg50
(lp35852
sg138
(lp35853
sg140
(lp35854
ssS'looz'
p35855
(dp35856
g83
(lp35857
I2829
assS'fiser'
p35858
(dp35859
g181
(lp35860
I2312
assS'eabjjij'
p35861
(dp35862
g149
(lp35863
I847
assS'loor'
p35864
(dp35865
g8
(lp35866
I2443
assS'loos'
p35867
(dp35868
g70
(lp35869
sg181
(lp35870
sg344
(lp35871
sg68
(lp35872
sg40
(lp35873
sg132
(lp35874
I1718
asg108
(lp35875
ssS'pwin'
p35876
(dp35877
g14
(lp35878
I3766
assS'neocognitron'
p35879
(dp35880
g181
(lp35881
I2423
asg114
(lp35882
ssS'psec'
p35883
(dp35884
g20
(lp35885
I1510
assS'rewri'
p35886
(dp35887
g42
(lp35888
I1001
assS'ifth'
p35889
(dp35890
g18
(lp35891
I1151
assS'fllter'
p35892
(dp35893
g104
(lp35894
I1243
assS'unintuit'
p35895
(dp35896
g221
(lp35897
I1762
assS'confound'
p35898
(dp35899
g102
(lp35900
I2795
asg318
(lp35901
sg277
(lp35902
ssS'quadrat'
p35903
(dp35904
g230
(lp35905
sg287
(lp35906
sg74
(lp35907
sg318
(lp35908
sg8
(lp35909
sg295
(lp35910
sg183
(lp35911
sg163
(lp35912
sg38
(lp35913
sg34
(lp35914
sg89
(lp35915
sg110
(lp35916
sg128
(lp35917
sg135
(lp35918
sg50
(lp35919
sg535
(lp35920
sg140
(lp35921
I600
assS'invertera'
p35922
(dp35923
g20
(lp35924
I972
assS'bogazi'
p35925
(dp35926
g178
(lp35927
I13
assS'octav'
p35928
(dp35929
g181
(lp35930
I914
assS'shaw'
p35931
(dp35932
g256
(lp35933
I2160
asg40
(lp35934
ssS'grant'
p35935
(dp35936
g124
(lp35937
sg26
(lp35938
sg277
(lp35939
sg126
(lp35940
sg281
(lp35941
sg287
(lp35942
sg145
(lp35943
sg344
(lp35944
sg59
(lp35945
sg484
(lp35946
sg38
(lp35947
sg83
(lp35948
sg306
(lp35949
sg89
(lp35950
sg46
(lp35951
sg20
(lp35952
sg313
(lp35953
sg149
(lp35954
sg230
(lp35955
sg429
(lp35956
sg106
(lp35957
I2489
asg110
(lp35958
sg96
(lp35959
sg216
(lp35960
sg178
(lp35961
sg68
(lp35962
sg72
(lp35963
sg341
(lp35964
sg10
(lp35965
sg40
(lp35966
sg135
(lp35967
sg354
(lp35968
ssS'belong'
p35969
(dp35970
g30
(lp35971
sg440
(lp35972
sg332
(lp35973
sg121
(lp35974
sg26
(lp35975
sg76
(lp35976
sg59
(lp35977
sg74
(lp35978
sg42
(lp35979
I2158
asg130
(lp35980
sg108
(lp35981
sg221
(lp35982
ssS'albani'
p35983
(dp35984
g114
(lp35985
I2362
assS'elong'
p35986
(dp35987
g438
(lp35988
I968
asg16
(lp35989
sg138
(lp35990
sg181
(lp35991
sg14
(lp35992
ssS'grand'
p35993
(dp35994
g132
(lp35995
I1210
assS'shah'
p35996
(dp35997
g59
(lp35998
I3156
assS'conflict'
p35999
(dp36000
g99
(lp36001
sg85
(lp36002
sg354
(lp36003
I3101
assS'itrol'
p36004
(dp36005
g106
(lp36006
I1994
assS'magicmirror'
p36007
(dp36008
g293
(lp36009
I573
assS'goodwin'
p36010
(dp36011
g230
(lp36012
I1188
assS'triendl'
p36013
(dp36014
g42
(lp36015
I3502
assS'optic'
p36016
(dp36017
g118
(lp36018
sg32
(lp36019
sg283
(lp36020
sg22
(lp36021
sg181
(lp36022
sg183
(lp36023
sg245
(lp36024
sg104
(lp36025
sg48
(lp36026
sg149
(lp36027
I2912
assS'imagin'
p36028
(dp36029
g12
(lp36030
sg30
(lp36031
sg63
(lp36032
sg44
(lp36033
I2486
asg26
(lp36034
ssS'optim'
p36035
(dp36036
g124
(lp36037
sg26
(lp36038
sg163
(lp36039
sg281
(lp36040
sg30
(lp36041
sg74
(lp36042
sg262
(lp36043
sg295
(lp36044
sg183
(lp36045
sg59
(lp36046
sg484
(lp36047
sg38
(lp36048
sg83
(lp36049
sg85
(lp36050
sg306
(lp36051
sg89
(lp36052
sg91
(lp36053
sg12
(lp36054
sg96
(lp36055
sg18
(lp36056
sg221
(lp36057
sg313
(lp36058
sg44
(lp36059
sg293
(lp36060
sg245
(lp36061
sg429
(lp36062
sg68
(lp36063
sg102
(lp36064
sg106
(lp36065
I555
asg108
(lp36066
sg110
(lp36067
sg20
(lp36068
sg52
(lp36069
sg230
(lp36070
sg329
(lp36071
sg440
(lp36072
sg318
(lp36073
sg121
(lp36074
sg22
(lp36075
sg8
(lp36076
sg34
(lp36077
sg36
(lp36078
sg235
(lp36079
sg341
(lp36080
sg10
(lp36081
sg40
(lp36082
sg344
(lp36083
sg130
(lp36084
sg132
(lp36085
sg14
(lp36086
sg16
(lp36087
sg50
(lp36088
sg138
(lp36089
sg140
(lp36090
sg354
(lp36091
ssS'easur'
p36092
(dp36093
g163
(lp36094
I1335
assS'holtz'
p36095
(dp36096
g72
(lp36097
I2432
assS'temporari'
p36098
(dp36099
g20
(lp36100
sg99
(lp36101
I3001
asg4
(lp36102
ssS'user'
p36103
(dp36104
g145
(lp36105
sg277
(lp36106
sg293
(lp36107
sg183
(lp36108
sg126
(lp36109
sg91
(lp36110
sg94
(lp36111
I129
asg96
(lp36112
sg223
(lp36113
ssS'ffrs'
p36114
(dp36115
g78
(lp36116
I1341
assS'mlmma'
p36117
(dp36118
g438
(lp36119
I1784
assS'mlla'
p36120
(dp36121
g114
(lp36122
I1164
assS'grossli'
p36123
(dp36124
g74
(lp36125
sg85
(lp36126
sg181
(lp36127
I1868
assS'grind'
p36128
(dp36129
g216
(lp36130
I620
assS'todorov'
p36131
(dp36132
g118
(lp36133
sg99
(lp36134
I3158
assS'realiz'
p36135
(dp36136
g230
(lp36137
sg52
(lp36138
sg440
(lp36139
sg163
(lp36140
sg329
(lp36141
sg295
(lp36142
sg183
(lp36143
sg59
(lp36144
sg68
(lp36145
sg38
(lp36146
sg341
(lp36147
sg85
(lp36148
sg42
(lp36149
I1652
asg287
(lp36150
sg91
(lp36151
sg36
(lp36152
sg245
(lp36153
sg96
(lp36154
sg221
(lp36155
sg223
(lp36156
ssS'five'
p36157
(dp36158
g70
(lp36159
sg277
(lp36160
sg30
(lp36161
sg74
(lp36162
sg256
(lp36163
sg76
(lp36164
sg78
(lp36165
sg89
(lp36166
sg91
(lp36167
sg18
(lp36168
sg221
(lp36169
sg223
(lp36170
sg114
(lp36171
sg116
(lp36172
sg80
(lp36173
sg181
(lp36174
sg124
(lp36175
sg126
(lp36176
sg281
(lp36177
sg14
(lp36178
sg135
(lp36179
sg138
(lp36180
sg140
(lp36181
sg354
(lp36182
I2522
assS'coman'
p36183
(dp36184
g163
(lp36185
I1344
assS'predetermin'
p36186
(dp36187
g287
(lp36188
sg63
(lp36189
sg138
(lp36190
I1101
asg281
(lp36191
ssS'effector'
p36192
(dp36193
g99
(lp36194
I543
assS'oscienc'
p36195
(dp36196
g106
(lp36197
I294
assS'administ'
p36198
(dp36199
g145
(lp36200
I2976
assS'ierf'
p36201
(dp36202
g262
(lp36203
I1663
assS'marqui'
p36204
(dp36205
g80
(lp36206
I2519
assS'predomin'
p36207
(dp36208
g118
(lp36209
sg135
(lp36210
I382
asg149
(lp36211
ssS'kluwer'
p36212
(dp36213
g287
(lp36214
sg440
(lp36215
sg48
(lp36216
sg145
(lp36217
sg26
(lp36218
sg76
(lp36219
sg10
(lp36220
sg87
(lp36221
sg135
(lp36222
I2614
asg223
(lp36223
sg149
(lp36224
ssS'shortcut'
p36225
(dp36226
g44
(lp36227
I2189
assS'informat'
p36228
(dp36229
g535
(lp36230
I19
assS'fredericksen'
p36231
(dp36232
g216
(lp36233
I617
assS'supersparc'
p36234
(dp36235
g10
(lp36236
I2318
assS'ocorrel'
p36237
(dp36238
g48
(lp36239
I952
assS'jerom'
p36240
(dp36241
g429
(lp36242
I2402
assS'subsequ'
p36243
(dp36244
g216
(lp36245
sg332
(lp36246
sg70
(lp36247
sg22
(lp36248
sg181
(lp36249
sg76
(lp36250
sg295
(lp36251
sg256
(lp36252
sg384
(lp36253
sg68
(lp36254
sg38
(lp36255
sg83
(lp36256
sg42
(lp36257
I1384
asg183
(lp36258
sg245
(lp36259
sg94
(lp36260
sg106
(lp36261
sg99
(lp36262
sg59
(lp36263
sg52
(lp36264
sg354
(lp36265
ssS'informax'
p36266
(dp36267
g72
(lp36268
I1850
assS'mustererkennung'
p36269
(dp36270
g59
(lp36271
I3242
assS'marco'
p36272
(dp36273
g341
(lp36274
I2870
assS'march'
p36275
(dp36276
g230
(lp36277
sg174
(lp36278
sg283
(lp36279
sg126
(lp36280
sg429
(lp36281
sg102
(lp36282
I3651
assS'innat'
p36283
(dp36284
g116
(lp36285
I199
assS'glutam'
p36286
(dp36287
g106
(lp36288
I1204
assS'jacobian'
p36289
(dp36290
g99
(lp36291
I845
asg44
(lp36292
ssS'kybernetik'
p36293
(dp36294
g216
(lp36295
sg149
(lp36296
I3195
assS'game'
p36297
(dp36298
g132
(lp36299
I5
asg89
(lp36300
sg91
(lp36301
sg277
(lp36302
ssS'amari'
p36303
(dp36304
g36
(lp36305
sg384
(lp36306
sg176
(lp36307
sg72
(lp36308
sg535
(lp36309
I311
assS'resid'
p36310
(dp36311
g85
(lp36312
I712
assS'characterist'
p36313
(dp36314
g70
(lp36315
sg145
(lp36316
sg295
(lp36317
sg183
(lp36318
sg484
(lp36319
sg38
(lp36320
sg89
(lp36321
sg12
(lp36322
sg94
(lp36323
sg96
(lp36324
sg114
(lp36325
sg350
(lp36326
sg429
(lp36327
sg102
(lp36328
sg18
(lp36329
sg4
(lp36330
sg116
(lp36331
sg332
(lp36332
sg22
(lp36333
sg8
(lp36334
sg235
(lp36335
sg344
(lp36336
sg14
(lp36337
sg16
(lp36338
I1978
assS'backbon'
p36339
(dp36340
g26
(lp36341
I404
assS'yjli'
p36342
(dp36343
g87
(lp36344
I1160
assS'vpwm'
p36345
(dp36346
g14
(lp36347
I3649
assS'activev'
p36348
(dp36349
g8
(lp36350
I1009
assS'resolv'
p36351
(dp36352
g8
(lp36353
sg183
(lp36354
sg281
(lp36355
sg85
(lp36356
sg78
(lp36357
sg102
(lp36358
I2050
assS'elaps'
p36359
(dp36360
g83
(lp36361
I662
assS'eel'
p36362
(dp36363
g8
(lp36364
I1147
assS'mkva'
p36365
(dp36366
g130
(lp36367
I1141
assS'popular'
p36368
(dp36369
g32
(lp36370
sg295
(lp36371
sg183
(lp36372
sg384
(lp36373
sg72
(lp36374
sg138
(lp36375
sg89
(lp36376
sg128
(lp36377
sg108
(lp36378
sg313
(lp36379
sg44
(lp36380
sg354
(lp36381
I2076
assS'eec'
p36382
(dp36383
g114
(lp36384
I2494
assS'ipw'
p36385
(dp36386
g245
(lp36387
I1793
assS'sketch'
p36388
(dp36389
g116
(lp36390
sg176
(lp36391
sg295
(lp36392
sg183
(lp36393
sg68
(lp36394
sg245
(lp36395
sg104
(lp36396
I2627
asg223
(lp36397
ssS'neumann'
p36398
(dp36399
g118
(lp36400
sg262
(lp36401
I1087
assS'creation'
p36402
(dp36403
g295
(lp36404
I1550
asg183
(lp36405
ssS'some'
p36406
(dp36407
g124
(lp36408
sg70
(lp36409
sg78
(lp36410
sg277
(lp36411
sg72
(lp36412
sg68
(lp36413
sg281
(lp36414
sg283
(lp36415
sg181
(lp36416
sg40
(lp36417
sg26
(lp36418
sg30
(lp36419
sg287
(lp36420
sg74
(lp36421
sg176
(lp36422
sg145
(lp36423
sg256
(lp36424
sg76
(lp36425
sg262
(lp36426
sg295
(lp36427
sg183
(lp36428
sg59
(lp36429
sg83
(lp36430
sg85
(lp36431
sg63
(lp36432
sg42
(lp36433
I45
asg306
(lp36434
sg89
(lp36435
sg91
(lp36436
sg12
(lp36437
sg94
(lp36438
sg96
(lp36439
sg48
(lp36440
sg99
(lp36441
sg313
(lp36442
sg44
(lp36443
sg149
(lp36444
sg303
(lp36445
sg174
(lp36446
sg18
(lp36447
sg32
(lp36448
sg350
(lp36449
sg429
(lp36450
sg318
(lp36451
sg46
(lp36452
sg102
(lp36453
sg104
(lp36454
sg106
(lp36455
sg108
(lp36456
sg110
(lp36457
sg20
(lp36458
sg52
(lp36459
sg114
(lp36460
sg230
(lp36461
sg438
(lp36462
sg440
(lp36463
sg332
(lp36464
sg121
(lp36465
sg4
(lp36466
sg6
(lp36467
sg8
(lp36468
sg34
(lp36469
sg36
(lp36470
sg384
(lp36471
sg235
(lp36472
sg126
(lp36473
sg341
(lp36474
sg10
(lp36475
sg535
(lp36476
sg344
(lp36477
sg223
(lp36478
sg128
(lp36479
sg130
(lp36480
sg132
(lp36481
sg14
(lp36482
sg16
(lp36483
sg135
(lp36484
sg50
(lp36485
sg138
(lp36486
sg140
(lp36487
sg354
(lp36488
ssS'footfal'
p36489
(dp36490
g277
(lp36491
I3197
assS'urgent'
p36492
(dp36493
g34
(lp36494
sg52
(lp36495
I124
assS'syndrom'
p36496
(dp36497
g4
(lp36498
I470
asg303
(lp36499
ssS'whitlock'
p36500
(dp36501
g354
(lp36502
I1478
assS'singularit'
p36503
(dp36504
g48
(lp36505
I1212
assS'goldman'
p36506
(dp36507
g4
(lp36508
I353
asg40
(lp36509
ssS'rhs'
p36510
(dp36511
g12
(lp36512
I2750
assS'xand'
p36513
(dp36514
g108
(lp36515
I1679
assS'ruj'
p36516
(dp36517
g104
(lp36518
I1239
assS'wkfk'
p36519
(dp36520
g235
(lp36521
I417
assS'nelll'
p36522
(dp36523
g174
(lp36524
I2385
assS'rul'
p36525
(dp36526
g174
(lp36527
I2386
assS'cga'
p36528
(dp36529
g295
(lp36530
I29
asg183
(lp36531
ssS'stem'
p36532
(dp36533
g223
(lp36534
I361
asg350
(lp36535
ssS'sacrif'
p36536
(dp36537
g132
(lp36538
I2710
asg121
(lp36539
ssS'step'
p36540
(dp36541
g26
(lp36542
sg72
(lp36543
sg74
(lp36544
sg76
(lp36545
sg262
(lp36546
sg460
(lp36547
sg183
(lp36548
sg59
(lp36549
sg38
(lp36550
sg85
(lp36551
sg303
(lp36552
sg42
(lp36553
I1428
asg306
(lp36554
sg89
(lp36555
sg91
(lp36556
sg46
(lp36557
sg18
(lp36558
sg221
(lp36559
sg44
(lp36560
sg149
(lp36561
sg118
(lp36562
sg116
(lp36563
sg329
(lp36564
sg293
(lp36565
sg32
(lp36566
sg102
(lp36567
sg178
(lp36568
sg63
(lp36569
sg230
(lp36570
sg174
(lp36571
sg440
(lp36572
sg318
(lp36573
sg121
(lp36574
sg4
(lp36575
sg8
(lp36576
sg34
(lp36577
sg36
(lp36578
sg384
(lp36579
sg124
(lp36580
sg126
(lp36581
sg40
(lp36582
sg128
(lp36583
sg130
(lp36584
sg132
(lp36585
sg14
(lp36586
sg16
(lp36587
sg138
(lp36588
sg354
(lp36589
ssS'backprop'
p36590
(dp36591
g30
(lp36592
sg277
(lp36593
sg344
(lp36594
sg126
(lp36595
sg89
(lp36596
sg108
(lp36597
I2070
assS'subtract'
p36598
(dp36599
g174
(lp36600
sg70
(lp36601
sg256
(lp36602
sg6
(lp36603
sg14
(lp36604
sg96
(lp36605
sg135
(lp36606
sg140
(lp36607
I622
assS'shine'
p36608
(dp36609
g8
(lp36610
I1232
assS'faith'
p36611
(dp36612
g42
(lp36613
I2194
asg295
(lp36614
sg183
(lp36615
sg130
(lp36616
ssS'anftrmeianprosc'
p36617
(dp36618
g174
(lp36619
I1859
assS'keynot'
p36620
(dp36621
g72
(lp36622
I3542
assS'iiiiiu'
p36623
(dp36624
g6
(lp36625
I1739
assS'iiiiii'
p36626
(dp36627
g6
(lp36628
I1831
assS'duda'
p36629
(dp36630
g318
(lp36631
I840
asg281
(lp36632
sg63
(lp36633
ssS'idl'
p36634
(dp36635
g94
(lp36636
I506
assS'idm'
p36637
(dp36638
g76
(lp36639
I344
assS'faitt'
p36640
(dp36641
g130
(lp36642
I1551
assS'block'
p36643
(dp36644
g118
(lp36645
sg438
(lp36646
I1571
asg440
(lp36647
sg178
(lp36648
sg22
(lp36649
sg181
(lp36650
sg8
(lp36651
sg78
(lp36652
sg59
(lp36653
sg68
(lp36654
sg10
(lp36655
sg40
(lp36656
sg287
(lp36657
sg63
(lp36658
sg130
(lp36659
sg245
(lp36660
sg14
(lp36661
sg106
(lp36662
sg135
(lp36663
sg16
(lp36664
ssS'weizmann'
p36665
(dp36666
g32
(lp36667
sg223
(lp36668
I3231
assS'ndseg'
p36669
(dp36670
g89
(lp36671
I2336
assS'ppis'
p36672
(dp36673
g72
(lp36674
I3598
assS'within'
p36675
(dp36676
g283
(lp36677
sg70
(lp36678
sg181
(lp36679
sg74
(lp36680
sg176
(lp36681
sg76
(lp36682
sg293
(lp36683
sg344
(lp36684
sg59
(lp36685
sg85
(lp36686
sg303
(lp36687
sg42
(lp36688
I3043
asg87
(lp36689
sg89
(lp36690
sg12
(lp36691
sg94
(lp36692
sg20
(lp36693
sg48
(lp36694
sg535
(lp36695
sg223
(lp36696
sg149
(lp36697
sg118
(lp36698
sg18
(lp36699
sg350
(lp36700
sg102
(lp36701
sg104
(lp36702
sg108
(lp36703
sg110
(lp36704
sg96
(lp36705
sg52
(lp36706
sg22
(lp36707
sg216
(lp36708
sg438
(lp36709
sg32
(lp36710
sg332
(lp36711
sg4
(lp36712
sg6
(lp36713
sg8
(lp36714
sg34
(lp36715
sg124
(lp36716
sg281
(lp36717
sg10
(lp36718
sg40
(lp36719
sg128
(lp36720
sg14
(lp36721
sg16
(lp36722
sg135
(lp36723
sg138
(lp36724
ssS'drain'
p36725
(dp36726
g135
(lp36727
I1104
assS'sirosh'
p36728
(dp36729
g149
(lp36730
I18
assS'ensur'
p36731
(dp36732
g283
(lp36733
sg70
(lp36734
sg26
(lp36735
sg30
(lp36736
sg74
(lp36737
sg145
(lp36738
sg344
(lp36739
sg303
(lp36740
sg12
(lp36741
sg223
(lp36742
sg230
(lp36743
sg329
(lp36744
sg216
(lp36745
sg174
(lp36746
sg332
(lp36747
sg22
(lp36748
sg34
(lp36749
sg460
(lp36750
sg281
(lp36751
sg10
(lp36752
sg14
(lp36753
sg16
(lp36754
sg50
(lp36755
sg140
(lp36756
I2551
assS'eurospeech'
p36757
(dp36758
g174
(lp36759
I2852
assS'institut'
p36760
(dp36761
g78
(lp36762
sg116
(lp36763
sg40
(lp36764
sg26
(lp36765
sg30
(lp36766
sg287
(lp36767
sg74
(lp36768
sg256
(lp36769
sg262
(lp36770
sg183
(lp36771
sg484
(lp36772
sg38
(lp36773
sg303
(lp36774
sg42
(lp36775
I549
asg306
(lp36776
sg91
(lp36777
sg20
(lp36778
sg18
(lp36779
sg221
(lp36780
sg535
(lp36781
sg223
(lp36782
sg350
(lp36783
sg230
(lp36784
sg329
(lp36785
sg32
(lp36786
sg318
(lp36787
sg106
(lp36788
sg216
(lp36789
sg438
(lp36790
sg440
(lp36791
sg332
(lp36792
sg6
(lp36793
sg36
(lp36794
sg68
(lp36795
sg126
(lp36796
sg10
(lp36797
sg313
(lp36798
sg48
(lp36799
sg128
(lp36800
sg130
(lp36801
sg50
(lp36802
sg138
(lp36803
sg140
(lp36804
ssS'propel'
p36805
(dp36806
g429
(lp36807
I1393
assS'carnegi'
p36808
(dp36809
g4
(lp36810
sg80
(lp36811
sg277
(lp36812
sg306
(lp36813
sg89
(lp36814
sg91
(lp36815
sg132
(lp36816
I3492
asg94
(lp36817
sg221
(lp36818
sg313
(lp36819
sg223
(lp36820
ssS'diagr'
p36821
(dp36822
g18
(lp36823
I729
assS'harel'
p36824
(dp36825
g149
(lp36826
I2898
assS'frost'
p36827
(dp36828
g26
(lp36829
I3206
assS'sampleswehav'
p36830
(dp36831
g85
(lp36832
I1968
assS'russian'
p36833
(dp36834
g14
(lp36835
sg16
(lp36836
I293
assS'pharmacolog'
p36837
(dp36838
g106
(lp36839
I2650
assS'tachycardia'
p36840
(dp36841
g135
(lp36842
I285
assS'info'
p36843
(dp36844
g102
(lp36845
I346
asg104
(lp36846
sg178
(lp36847
sg114
(lp36848
ssS'tum'
p36849
(dp36850
g78
(lp36851
sg256
(lp36852
I500
assS'utd'
p36853
(dp36854
g341
(lp36855
I2229
assS'uti'
p36856
(dp36857
g341
(lp36858
I2226
asg181
(lp36859
ssS'transmitt'
p36860
(dp36861
g106
(lp36862
I2425
asg4
(lp36863
ssS'fdn'
p36864
(dp36865
g384
(lp36866
I968
assS'kuhn'
p36867
(dp36868
g78
(lp36869
sg128
(lp36870
I2963
assS'nonconvex'
p36871
(dp36872
g8
(lp36873
I1379
assS'ehoshua'
p36874
(dp36875
g40
(lp36876
I8
assS'similar'
p36877
(dp36878
g329
(lp36879
sg70
(lp36880
sg78
(lp36881
sg277
(lp36882
sg163
(lp36883
sg72
(lp36884
sg283
(lp36885
sg181
(lp36886
sg303
(lp36887
sg26
(lp36888
sg30
(lp36889
sg287
(lp36890
sg74
(lp36891
sg256
(lp36892
sg118
(lp36893
sg295
(lp36894
sg183
(lp36895
sg59
(lp36896
sg484
(lp36897
sg38
(lp36898
sg83
(lp36899
sg85
(lp36900
sg124
(lp36901
sg42
(lp36902
I2318
asg306
(lp36903
sg87
(lp36904
sg89
(lp36905
sg91
(lp36906
sg12
(lp36907
sg94
(lp36908
sg48
(lp36909
sg99
(lp36910
sg535
(lp36911
sg44
(lp36912
sg149
(lp36913
sg116
(lp36914
sg174
(lp36915
sg293
(lp36916
sg245
(lp36917
sg68
(lp36918
sg46
(lp36919
sg104
(lp36920
sg108
(lp36921
sg110
(lp36922
sg178
(lp36923
sg52
(lp36924
sg114
(lp36925
sg216
(lp36926
sg438
(lp36927
sg440
(lp36928
sg332
(lp36929
sg121
(lp36930
sg4
(lp36931
sg6
(lp36932
sg8
(lp36933
sg34
(lp36934
sg235
(lp36935
sg126
(lp36936
sg341
(lp36937
sg10
(lp36938
sg344
(lp36939
sg63
(lp36940
sg223
(lp36941
sg128
(lp36942
sg130
(lp36943
sg132
(lp36944
sg135
(lp36945
sg138
(lp36946
sg140
(lp36947
sg354
(lp36948
ssS'tomographi'
p36949
(dp36950
g318
(lp36951
I2305
assS'iiiiiiiiiii'
p36952
(dp36953
g6
(lp36954
I1740
assS'repres'
p36955
(dp36956
g68
(lp36957
sg70
(lp36958
sg26
(lp36959
sg163
(lp36960
sg283
(lp36961
sg181
(lp36962
sg303
(lp36963
sg30
(lp36964
sg74
(lp36965
sg176
(lp36966
sg145
(lp36967
sg80
(lp36968
sg76
(lp36969
sg293
(lp36970
sg295
(lp36971
sg183
(lp36972
sg59
(lp36973
sg484
(lp36974
sg83
(lp36975
sg85
(lp36976
sg63
(lp36977
sg306
(lp36978
sg87
(lp36979
sg89
(lp36980
sg12
(lp36981
sg94
(lp36982
sg20
(lp36983
sg99
(lp36984
sg535
(lp36985
sg44
(lp36986
sg149
(lp36987
sg329
(lp36988
sg32
(lp36989
sg245
(lp36990
sg429
(lp36991
sg46
(lp36992
sg104
(lp36993
sg108
(lp36994
sg110
(lp36995
sg178
(lp36996
sg52
(lp36997
sg22
(lp36998
sg230
(lp36999
sg438
(lp37000
I423
asg440
(lp37001
sg332
(lp37002
sg121
(lp37003
sg4
(lp37004
sg6
(lp37005
sg8
(lp37006
sg34
(lp37007
sg460
(lp37008
sg124
(lp37009
sg126
(lp37010
sg281
(lp37011
sg10
(lp37012
sg344
(lp37013
sg223
(lp37014
sg128
(lp37015
sg130
(lp37016
sg132
(lp37017
sg14
(lp37018
sg16
(lp37019
sg350
(lp37020
sg138
(lp37021
sg354
(lp37022
ssS'incomplet'
p37023
(dp37024
g440
(lp37025
sg460
(lp37026
sg74
(lp37027
sg83
(lp37028
sg42
(lp37029
I195
asg91
(lp37030
sg130
(lp37031
sg132
(lp37032
sg221
(lp37033
sg313
(lp37034
ssS'nonsymmetr'
p37035
(dp37036
g384
(lp37037
I79
assS'sato'
p37038
(dp37039
g46
(lp37040
I3716
asg114
(lp37041
ssS'naa'
p37042
(dp37043
g429
(lp37044
I1014
assS'blockneuron'
p37045
(dp37046
g8
(lp37047
I890
assS'pronounc'
p37048
(dp37049
g102
(lp37050
I1846
asg174
(lp37051
sg318
(lp37052
sg85
(lp37053
ssS'hafo'
p37054
(dp37055
g130
(lp37056
I1561
assS'afford'
p37057
(dp37058
g78
(lp37059
I552
assS'unidirect'
p37060
(dp37061
g216
(lp37062
I421
assS'qualitat'
p37063
(dp37064
g48
(lp37065
I703
assS'nat'
p37066
(dp37067
g106
(lp37068
I2555
asg48
(lp37069
ssS'naz'
p37070
(dp37071
g85
(lp37072
I1061
assS'osedloop'
p37073
(dp37074
g230
(lp37075
I2400
assS'bmax'
p37076
(dp37077
g34
(lp37078
I1986
assS'draw'
p37079
(dp37080
g329
(lp37081
sg283
(lp37082
sg178
(lp37083
sg256
(lp37084
sg295
(lp37085
sg183
(lp37086
sg59
(lp37087
sg484
(lp37088
sg42
(lp37089
I2470
asg344
(lp37090
sg128
(lp37091
sg36
(lp37092
sg132
(lp37093
ssS'lroblem'
p37094
(dp37095
g221
(lp37096
I375
assS'william'
p37097
(dp37098
g216
(lp37099
sg329
(lp37100
sg178
(lp37101
sg76
(lp37102
sg6
(lp37103
sg118
(lp37104
sg344
(lp37105
sg36
(lp37106
sg124
(lp37107
sg126
(lp37108
sg30
(lp37109
sg89
(lp37110
sg78
(lp37111
sg70
(lp37112
sg108
(lp37113
sg138
(lp37114
I14
asg223
(lp37115
sg114
(lp37116
ssS'amplitud'
p37117
(dp37118
g116
(lp37119
sg174
(lp37120
sg22
(lp37121
sg14
(lp37122
sg106
(lp37123
I701
asg350
(lp37124
ssS'brookhaven'
p37125
(dp37126
g26
(lp37127
I1379
assS'neocortex'
p37128
(dp37129
g149
(lp37130
I3216
assS'belsley'
p37131
(dp37132
g295
(lp37133
I1788
asg183
(lp37134
ssS'lowerdimension'
p37135
(dp37136
g306
(lp37137
I1195
assS'gower'
p37138
(dp37139
g130
(lp37140
I778
assS'e'
p37141
(dp37142
g80
(lp37143
sg293
(lp37144
sg344
(lp37145
sg78
(lp37146
sg59
(lp37147
sg484
(lp37148
sg38
(lp37149
sg83
(lp37150
sg85
(lp37151
sg303
(lp37152
sg438
(lp37153
sg116
(lp37154
sg118
(lp37155
sg34
(lp37156
sg36
(lp37157
sg460
(lp37158
sg68
(lp37159
sg72
(lp37160
sg281
(lp37161
sg10
(lp37162
sg40
(lp37163
sg283
(lp37164
sg70
(lp37165
sg26
(lp37166
sg277
(lp37167
sg163
(lp37168
sg89
(lp37169
sg91
(lp37170
sg12
(lp37171
sg94
(lp37172
sg96
(lp37173
sg48
(lp37174
sg99
(lp37175
sg313
(lp37176
sg44
(lp37177
sg149
(lp37178
sg429
(lp37179
sg102
(lp37180
sg104
(lp37181
sg106
(lp37182
sg108
(lp37183
sg110
(lp37184
sg63
(lp37185
sg52
(lp37186
sg114
(lp37187
sg128
(lp37188
sg130
(lp37189
sg132
(lp37190
sg14
(lp37191
sg16
(lp37192
sg135
(lp37193
sg50
(lp37194
sg138
(lp37195
sg140
(lp37196
sg354
(lp37197
sg306
(lp37198
sg87
(lp37199
sg245
(lp37200
sg46
(lp37201
sg20
(lp37202
sg18
(lp37203
sg221
(lp37204
sg535
(lp37205
sg223
(lp37206
sg350
(lp37207
sg216
(lp37208
sg174
(lp37209
sg440
(lp37210
sg332
(lp37211
sg121
(lp37212
sg4
(lp37213
sg6
(lp37214
sg8
(lp37215
sg126
(lp37216
sg341
(lp37217
sg30
(lp37218
sg287
(lp37219
sg74
(lp37220
sg176
(lp37221
sg145
(lp37222
sg256
(lp37223
sg76
(lp37224
sg262
(lp37225
sg295
(lp37226
sg183
(lp37227
sg42
(lp37228
I606
asg230
(lp37229
sg329
(lp37230
sg32
(lp37231
sg318
(lp37232
sg178
(lp37233
sg22
(lp37234
sg181
(lp37235
sg235
(lp37236
sg384
(lp37237
sg124
(lp37238
ssS'felin'
p37239
(dp37240
g70
(lp37241
I2506
assS'viewbas'
p37242
(dp37243
g293
(lp37244
I952
assS'dvf'
p37245
(dp37246
g318
(lp37247
I1688
assS'depth'
p37248
(dp37249
g287
(lp37250
sg145
(lp37251
sg181
(lp37252
sg118
(lp37253
sg68
(lp37254
sg102
(lp37255
sg132
(lp37256
I1869
asg121
(lp37257
ssS'diagrammat'
p37258
(dp37259
g235
(lp37260
I1546
assS'brugnara'
p37261
(dp37262
g96
(lp37263
I2718
assS'xtpnx'
p37264
(dp37265
g295
(lp37266
I1695
asg183
(lp37267
ssS'svr'
p37268
(dp37269
g76
(lp37270
I907
assS'gw'
p37271
(dp37272
g40
(lp37273
I1423
assS'katz'
p37274
(dp37275
g149
(lp37276
I215
assS'gt'
p37277
(dp37278
g34
(lp37279
sg318
(lp37280
sg91
(lp37281
I2159
asg341
(lp37282
ssS'gs'
p37283
(dp37284
g14
(lp37285
sg16
(lp37286
I663
assS'gr'
p37287
(dp37288
g99
(lp37289
I756
asg535
(lp37290
ssS'funct'
p37291
(dp37292
g48
(lp37293
I1167
assS'eckert'
p37294
(dp37295
g245
(lp37296
I236
assS'gz'
p37297
(dp37298
g110
(lp37299
sg76
(lp37300
I3338
assS'irrisou'
p37301
(dp37302
g10
(lp37303
I2635
assS'gc'
p37304
(dp37305
g102
(lp37306
I1006
assS'ftourosc'
p37307
(dp37308
g91
(lp37309
I2143
assS'ga'
p37310
(dp37311
g295
(lp37312
sg183
(lp37313
sg121
(lp37314
sg4
(lp37315
I3594
asg262
(lp37316
ssS'go'
p37317
(dp37318
g30
(lp37319
sg438
(lp37320
I1515
asg32
(lp37321
sg145
(lp37322
sg4
(lp37323
sg174
(lp37324
sg116
(lp37325
sg59
(lp37326
sg72
(lp37327
sg12
(lp37328
sg306
(lp37329
sg89
(lp37330
sg132
(lp37331
sg14
(lp37332
sg16
(lp37333
sg102
(lp37334
sg44
(lp37335
ssS'gn'
p37336
(dp37337
g108
(lp37338
I418
asg68
(lp37339
ssS'gl'
p37340
(dp37341
g132
(lp37342
sg287
(lp37343
sg106
(lp37344
I2598
asg535
(lp37345
ssS'gk'
p37346
(dp37347
g91
(lp37348
I1424
assS'gj'
p37349
(dp37350
g174
(lp37351
sg332
(lp37352
sg91
(lp37353
I1417
asg287
(lp37354
ssS'gi'
p37355
(dp37356
g329
(lp37357
sg287
(lp37358
sg36
(lp37359
sg59
(lp37360
sg8
(lp37361
sg341
(lp37362
sg40
(lp37363
sg313
(lp37364
I939
assS'gh'
p37365
(dp37366
g245
(lp37367
sg8
(lp37368
I1069
assS'lkn'
p37369
(dp37370
g438
(lp37371
I318
assS'lko'
p37372
(dp37373
g14
(lp37374
I3189
assS'nationalacademi'
p37375
(dp37376
g149
(lp37377
I3030
assS'vacuous'
p37378
(dp37379
g85
(lp37380
I3340
assS'lkd'
p37381
(dp37382
g18
(lp37383
I1795
assS'ladner'
p37384
(dp37385
g313
(lp37386
I2081
assS'econometr'
p37387
(dp37388
g96
(lp37389
sg313
(lp37390
sg354
(lp37391
I3228
assS'reorganis'
p37392
(dp37393
g176
(lp37394
I1
assS'aris'
p37395
(dp37396
g68
(lp37397
sg176
(lp37398
sg262
(lp37399
sg295
(lp37400
sg183
(lp37401
sg38
(lp37402
sg85
(lp37403
sg42
(lp37404
I3145
asg306
(lp37405
sg91
(lp37406
sg221
(lp37407
sg44
(lp37408
sg116
(lp37409
sg318
(lp37410
sg230
(lp37411
sg332
(lp37412
sg4
(lp37413
sg6
(lp37414
sg8
(lp37415
sg34
(lp37416
sg384
(lp37417
sg235
(lp37418
sg130
(lp37419
sg14
(lp37420
sg16
(lp37421
sg135
(lp37422
ssS'flicker'
p37423
(dp37424
g256
(lp37425
I1147
assS'adaf'
p37426
(dp37427
g344
(lp37428
I1231
assS'wave'
p37429
(dp37430
g116
(lp37431
sg174
(lp37432
sg22
(lp37433
sg181
(lp37434
sg293
(lp37435
sg384
(lp37436
sg245
(lp37437
sg128
(lp37438
sg12
(lp37439
sg20
(lp37440
sg48
(lp37441
sg313
(lp37442
sg140
(lp37443
I1131
asg256
(lp37444
ssS'trough'
p37445
(dp37446
g118
(lp37447
I2261
assS'cellular'
p37448
(dp37449
g106
(lp37450
I1582
asg4
(lp37451
ssS'mizumori'
p37452
(dp37453
g80
(lp37454
I2510
assS'sych'
p37455
(dp37456
g313
(lp37457
I22
assS'stiff'
p37458
(dp37459
g99
(lp37460
I1329
assS'gender'
p37461
(dp37462
g96
(lp37463
I1848
assS'button'
p37464
(dp37465
g178
(lp37466
sg4
(lp37467
I875
asg83
(lp37468
ssS'huaiyu'
p37469
(dp37470
g124
(lp37471
I3013
assS'michael'
p37472
(dp37473
g329
(lp37474
sg332
(lp37475
sg145
(lp37476
sg277
(lp37477
sg6
(lp37478
sg460
(lp37479
sg85
(lp37480
sg313
(lp37481
sg89
(lp37482
sg132
(lp37483
sg138
(lp37484
sg140
(lp37485
I270
assS'ryan'
p37486
(dp37487
g104
(lp37488
I49
assS'convolv'
p37489
(dp37490
g216
(lp37491
sg174
(lp37492
I796
asg178
(lp37493
sg22
(lp37494
ssS'hemispher'
p37495
(dp37496
g32
(lp37497
I1817
asg303
(lp37498
ssS'burden'
p37499
(dp37500
g329
(lp37501
I205
assS'kuh'
p37502
(dp37503
g295
(lp37504
I1789
asg183
(lp37505
ssS'zl'
p37506
(dp37507
g438
(lp37508
I1324
asg108
(lp37509
sg145
(lp37510
sg128
(lp37511
ssS'zm'
p37512
(dp37513
g145
(lp37514
I776
assS'zn'
p37515
(dp37516
g40
(lp37517
I449
assS'zo'
p37518
(dp37519
g121
(lp37520
sg46
(lp37521
sg102
(lp37522
sg14
(lp37523
sg16
(lp37524
sg108
(lp37525
sg138
(lp37526
I2813
assS'zi'
p37527
(dp37528
g118
(lp37529
sg145
(lp37530
sg91
(lp37531
sg128
(lp37532
sg46
(lp37533
sg138
(lp37534
I2817
assS'zj'
p37535
(dp37536
g121
(lp37537
sg50
(lp37538
I1350
asg44
(lp37539
ssS'zk'
p37540
(dp37541
g102
(lp37542
sg438
(lp37543
I1219
assS'ulv'
p37544
(dp37545
g287
(lp37546
I2311
assS'zf'
p37547
(dp37548
g91
(lp37549
I1499
assS'intervisit'
p37550
(dp37551
g83
(lp37552
I2142
assS'za'
p37553
(dp37554
g85
(lp37555
I4288
assS'uniqu'
p37556
(dp37557
g216
(lp37558
sg287
(lp37559
sg32
(lp37560
sg145
(lp37561
sg116
(lp37562
sg460
(lp37563
sg68
(lp37564
sg38
(lp37565
sg535
(lp37566
sg42
(lp37567
I404
asg306
(lp37568
sg128
(lp37569
sg104
(lp37570
sg102
(lp37571
sg94
(lp37572
sg20
(lp37573
sg40
(lp37574
sg46
(lp37575
ssS'jump'
p37576
(dp37577
g30
(lp37578
sg78
(lp37579
sg18
(lp37580
sg89
(lp37581
I1893
asg262
(lp37582
ssS'zt'
p37583
(dp37584
g329
(lp37585
sg306
(lp37586
I2506
assS'zq'
p37587
(dp37588
g14
(lp37589
sg16
(lp37590
I1589
assS'bootc'
p37591
(dp37592
g484
(lp37593
I464
assS'ycx'
p37594
(dp37595
g354
(lp37596
I1621
assS'soup'
p37597
(dp37598
g181
(lp37599
I509
assS'cell'
p37600
(dp37601
g70
(lp37602
sg277
(lp37603
sg181
(lp37604
sg256
(lp37605
sg80
(lp37606
sg262
(lp37607
sg484
(lp37608
sg149
(lp37609
sg303
(lp37610
sg245
(lp37611
sg20
(lp37612
sg99
(lp37613
sg350
(lp37614
sg116
(lp37615
sg118
(lp37616
sg12
(lp37617
sg102
(lp37618
sg106
(lp37619
sg174
(lp37620
sg216
(lp37621
sg438
(lp37622
I44
asg4
(lp37623
sg6
(lp37624
sg281
(lp37625
sg14
(lp37626
sg135
(lp37627
sg138
(lp37628
ssS'experiment'
p37629
(dp37630
g124
(lp37631
sg70
(lp37632
sg283
(lp37633
sg85
(lp37634
sg74
(lp37635
sg176
(lp37636
sg145
(lp37637
sg76
(lp37638
sg262
(lp37639
sg78
(lp37640
sg80
(lp37641
sg83
(lp37642
sg114
(lp37643
sg303
(lp37644
sg42
(lp37645
I2042
asg91
(lp37646
sg12
(lp37647
sg20
(lp37648
sg48
(lp37649
sg99
(lp37650
sg313
(lp37651
sg223
(lp37652
sg149
(lp37653
sg116
(lp37654
sg429
(lp37655
sg104
(lp37656
sg110
(lp37657
sg63
(lp37658
sg22
(lp37659
sg216
(lp37660
sg118
(lp37661
sg440
(lp37662
sg18
(lp37663
sg121
(lp37664
sg4
(lp37665
sg6
(lp37666
sg8
(lp37667
sg235
(lp37668
sg72
(lp37669
sg281
(lp37670
sg10
(lp37671
sg40
(lp37672
sg128
(lp37673
sg130
(lp37674
sg132
(lp37675
sg14
(lp37676
sg16
(lp37677
sg350
(lp37678
sg138
(lp37679
sg140
(lp37680
sg354
(lp37681
ssS'playa'
p37682
(dp37683
g318
(lp37684
sg4
(lp37685
I760
assS'microelectrod'
p37686
(dp37687
g106
(lp37688
I1949
assS'modelnumh'
p37689
(dp37690
g48
(lp37691
I976
assS'brunei'
p37692
(dp37693
g283
(lp37694
I1897
assS'ozl'
p37695
(dp37696
g32
(lp37697
I1447
assS'ribrari'
p37698
(dp37699
g176
(lp37700
I2608
assS'becom'
p37701
(dp37702
g124
(lp37703
sg70
(lp37704
sg277
(lp37705
sg72
(lp37706
sg283
(lp37707
sg36
(lp37708
sg30
(lp37709
sg287
(lp37710
sg74
(lp37711
sg176
(lp37712
sg256
(lp37713
sg76
(lp37714
sg295
(lp37715
sg183
(lp37716
sg38
(lp37717
sg83
(lp37718
sg85
(lp37719
sg63
(lp37720
sg42
(lp37721
I680
asg306
(lp37722
sg87
(lp37723
sg89
(lp37724
sg91
(lp37725
sg245
(lp37726
sg94
(lp37727
sg20
(lp37728
sg99
(lp37729
sg44
(lp37730
sg149
(lp37731
sg230
(lp37732
sg329
(lp37733
sg429
(lp37734
sg68
(lp37735
sg46
(lp37736
sg102
(lp37737
sg104
(lp37738
sg106
(lp37739
sg108
(lp37740
sg96
(lp37741
sg52
(lp37742
sg114
(lp37743
sg216
(lp37744
sg438
(lp37745
sg32
(lp37746
sg121
(lp37747
sg4
(lp37748
sg8
(lp37749
sg34
(lp37750
sg221
(lp37751
sg384
(lp37752
sg235
(lp37753
sg126
(lp37754
sg341
(lp37755
sg223
(lp37756
sg130
(lp37757
sg460
(lp37758
sg140
(lp37759
ssS'convert'
p37760
(dp37761
g116
(lp37762
sg329
(lp37763
sg76
(lp37764
sg6
(lp37765
sg344
(lp37766
sg183
(lp37767
sg80
(lp37768
sg10
(lp37769
sg429
(lp37770
sg78
(lp37771
sg14
(lp37772
sg16
(lp37773
sg135
(lp37774
I500
asg20
(lp37775
ssS'scalabl'
p37776
(dp37777
g283
(lp37778
sg10
(lp37779
I921
assS'convers'
p37780
(dp37781
g30
(lp37782
sg287
(lp37783
sg32
(lp37784
sg318
(lp37785
sg22
(lp37786
sg10
(lp37787
sg176
(lp37788
sg85
(lp37789
sg106
(lp37790
I1655
asg135
(lp37791
sg52
(lp37792
ssS'stereogram'
p37793
(dp37794
g318
(lp37795
I2799
assS'converg'
p37796
(dp37797
g68
(lp37798
sg277
(lp37799
sg30
(lp37800
sg74
(lp37801
sg76
(lp37802
sg262
(lp37803
sg38
(lp37804
sg149
(lp37805
sg85
(lp37806
sg42
(lp37807
I699
asg306
(lp37808
sg89
(lp37809
sg91
(lp37810
sg245
(lp37811
sg46
(lp37812
sg96
(lp37813
sg18
(lp37814
sg99
(lp37815
sg535
(lp37816
sg350
(lp37817
sg293
(lp37818
sg429
(lp37819
sg106
(lp37820
sg108
(lp37821
sg230
(lp37822
sg329
(lp37823
sg440
(lp37824
sg318
(lp37825
sg121
(lp37826
sg8
(lp37827
sg34
(lp37828
sg36
(lp37829
sg460
(lp37830
sg124
(lp37831
sg72
(lp37832
sg281
(lp37833
sg40
(lp37834
sg135
(lp37835
sg140
(lp37836
ssS'repel'
p37837
(dp37838
g18
(lp37839
I2394
assS'uld'
p37840
(dp37841
g230
(lp37842
I1375
assS'dopamin'
p37843
(dp37844
g4
(lp37845
I2953
assS'chang'
p37846
(dp37847
g329
(lp37848
sg26
(lp37849
sg277
(lp37850
sg72
(lp37851
sg68
(lp37852
sg283
(lp37853
sg181
(lp37854
sg30
(lp37855
sg287
(lp37856
sg176
(lp37857
sg145
(lp37858
sg256
(lp37859
sg76
(lp37860
sg118
(lp37861
sg295
(lp37862
sg183
(lp37863
sg80
(lp37864
sg38
(lp37865
sg83
(lp37866
sg85
(lp37867
sg42
(lp37868
I1986
asg306
(lp37869
sg87
(lp37870
sg89
(lp37871
sg91
(lp37872
sg12
(lp37873
sg94
(lp37874
sg20
(lp37875
sg48
(lp37876
sg99
(lp37877
sg313
(lp37878
sg223
(lp37879
sg149
(lp37880
sg174
(lp37881
sg350
(lp37882
sg318
(lp37883
sg46
(lp37884
sg102
(lp37885
sg18
(lp37886
sg106
(lp37887
sg63
(lp37888
sg52
(lp37889
sg114
(lp37890
sg230
(lp37891
sg438
(lp37892
sg32
(lp37893
sg332
(lp37894
sg178
(lp37895
sg4
(lp37896
sg6
(lp37897
sg235
(lp37898
sg34
(lp37899
sg460
(lp37900
sg124
(lp37901
sg126
(lp37902
sg10
(lp37903
sg128
(lp37904
sg78
(lp37905
sg132
(lp37906
sg14
(lp37907
sg16
(lp37908
sg135
(lp37909
sg50
(lp37910
sg138
(lp37911
ssS'gene'
p37912
(dp37913
g429
(lp37914
I9
assS'chanc'
p37915
(dp37916
g329
(lp37917
sg145
(lp37918
sg181
(lp37919
sg78
(lp37920
sg59
(lp37921
sg484
(lp37922
sg94
(lp37923
I2212
asg110
(lp37924
ssS'gufc'
p37925
(dp37926
g176
(lp37927
I2532
assS'wig'
p37928
(dp37929
g76
(lp37930
I1813
assS'flanneri'
p37931
(dp37932
g34
(lp37933
I2897
assS'wid'
p37934
(dp37935
g42
(lp37936
I2272
assS'clark'
p37937
(dp37938
g350
(lp37939
I1224
assS'nearest'
p37940
(dp37941
g30
(lp37942
sg277
(lp37943
sg181
(lp37944
sg293
(lp37945
sg281
(lp37946
sg83
(lp37947
sg44
(lp37948
I316
asg18
(lp37949
sg63
(lp37950
sg223
(lp37951
sg26
(lp37952
ssS'win'
p37953
(dp37954
g132
(lp37955
sg104
(lp37956
sg135
(lp37957
I727
asg116
(lp37958
sg303
(lp37959
ssS'wil'
p37960
(dp37961
g230
(lp37962
sg108
(lp37963
I1082
assS'wij'
p37964
(dp37965
g116
(lp37966
sg76
(lp37967
sg149
(lp37968
I1075
assS'wii'
p37969
(dp37970
g121
(lp37971
I657
assS'austria'
p37972
(dp37973
g68
(lp37974
sg341
(lp37975
I2745
assS'wit'
p37976
(dp37977
g110
(lp37978
sg59
(lp37979
sg48
(lp37980
I753
asg89
(lp37981
ssS'revow'
p37982
(dp37983
g30
(lp37984
sg138
(lp37985
I114
assS'wir'
p37986
(dp37987
g354
(lp37988
I1784
assS'cloud'
p37989
(dp37990
g70
(lp37991
I1824
assS'metaphor'
p37992
(dp37993
g293
(lp37994
I574
assS'fibr'
p37995
(dp37996
g174
(lp37997
I311
asg283
(lp37998
ssS'implic'
p37999
(dp38000
g287
(lp38001
sg6
(lp38002
sg183
(lp38003
sg85
(lp38004
sg40
(lp38005
sg12
(lp38006
I78
asg535
(lp38007
sg350
(lp38008
ssS'generos'
p38009
(dp38010
g126
(lp38011
I2898
assS'camera'
p38012
(dp38013
g283
(lp38014
sg293
(lp38015
sg59
(lp38016
sg42
(lp38017
I2140
asg223
(lp38018
sg114
(lp38019
ssS'postprocess'
p38020
(dp38021
g59
(lp38022
sg138
(lp38023
I1233
assS'panel'
p38024
(dp38025
g329
(lp38026
sg4
(lp38027
sg80
(lp38028
sg6
(lp38029
sg14
(lp38030
sg16
(lp38031
I2107
assS'remaind'
p38032
(dp38033
g74
(lp38034
sg484
(lp38035
sg121
(lp38036
sg22
(lp38037
sg183
(lp38038
sg68
(lp38039
sg126
(lp38040
sg306
(lp38041
sg283
(lp38042
sg135
(lp38043
I1891
asg63
(lp38044
ssS'symposia'
p38045
(dp38046
g149
(lp38047
I3055
assS'ecol'
p38048
(dp38049
g287
(lp38050
I15
asg40
(lp38051
ssS'benchmark'
p38052
(dp38053
g132
(lp38054
I382
asg283
(lp38055
ssS'cmli'
p38056
(dp38057
g22
(lp38058
I698
assS'cmls'
p38059
(dp38060
g245
(lp38061
I2474
assS'gribkoff'
p38062
(dp38063
g106
(lp38064
I2806
assS'retriev'
p38065
(dp38066
g20
(lp38067
I213
asg384
(lp38068
sg262
(lp38069
ssS'perceiv'
p38070
(dp38071
g216
(lp38072
sg118
(lp38073
sg332
(lp38074
sg80
(lp38075
sg116
(lp38076
sg245
(lp38077
sg12
(lp38078
I581
assS'ridg'
p38079
(dp38080
g295
(lp38081
sg183
(lp38082
sg138
(lp38083
I1027
assS'haussler'
p38084
(dp38085
g344
(lp38086
sg287
(lp38087
I570
asg110
(lp38088
sg85
(lp38089
ssS'meet'
p38090
(dp38091
g78
(lp38092
sg135
(lp38093
I62
asg4
(lp38094
sg149
(lp38095
ssS'control'
p38096
(dp38097
g68
(lp38098
sg78
(lp38099
sg283
(lp38100
sg145
(lp38101
sg256
(lp38102
sg80
(lp38103
sg293
(lp38104
sg295
(lp38105
sg183
(lp38106
sg59
(lp38107
sg38
(lp38108
sg83
(lp38109
sg303
(lp38110
sg42
(lp38111
I2258
asg306
(lp38112
sg89
(lp38113
sg460
(lp38114
sg12
(lp38115
sg46
(lp38116
sg20
(lp38117
sg114
(lp38118
sg99
(lp38119
sg313
(lp38120
sg223
(lp38121
sg350
(lp38122
sg116
(lp38123
sg245
(lp38124
sg429
(lp38125
sg102
(lp38126
sg104
(lp38127
sg106
(lp38128
sg178
(lp38129
sg22
(lp38130
sg230
(lp38131
sg118
(lp38132
sg32
(lp38133
sg121
(lp38134
sg4
(lp38135
sg8
(lp38136
sg34
(lp38137
sg36
(lp38138
sg384
(lp38139
sg124
(lp38140
sg126
(lp38141
sg10
(lp38142
sg535
(lp38143
sg128
(lp38144
sg130
(lp38145
sg132
(lp38146
sg14
(lp38147
sg16
(lp38148
sg135
(lp38149
sg50
(lp38150
sg138
(lp38151
sg354
(lp38152
ssS'coincident'
p38153
(dp38154
g245
(lp38155
I1437
asg145
(lp38156
ssS'tiibitak'
p38157
(dp38158
g178
(lp38159
I2436
assS'superposit'
p38160
(dp38161
g96
(lp38162
I772
asg68
(lp38163
sg85
(lp38164
ssS'wklikfjk'
p38165
(dp38166
g74
(lp38167
I631
assS'sought'
p38168
(dp38169
g145
(lp38170
sg44
(lp38171
I388
asg303
(lp38172
ssS'papouli'
p38173
(dp38174
g163
(lp38175
I547
assS'mclachlan'
p38176
(dp38177
g295
(lp38178
sg183
(lp38179
sg91
(lp38180
I725
assS'acycl'
p38181
(dp38182
g287
(lp38183
I1214
assS'petersen'
p38184
(dp38185
g176
(lp38186
sg178
(lp38187
I2668
assS'dissolv'
p38188
(dp38189
g281
(lp38190
I1969
assS'filament'
p38191
(dp38192
g14
(lp38193
sg16
(lp38194
I2261
assS'circular'
p38195
(dp38196
g438
(lp38197
I469
asg256
(lp38198
sg181
(lp38199
sg130
(lp38200
sg14
(lp38201
sg16
(lp38202
sg48
(lp38203
ssS'sernicircular'
p38204
(dp38205
g350
(lp38206
I811
assS'hashem'
p38207
(dp38208
g484
(lp38209
sg235
(lp38210
I3083
assS'blegdamsdvej'
p38211
(dp38212
g38
(lp38213
I36
assS'cornpon'
p38214
(dp38215
g350
(lp38216
I825
assS'tadahiro'
p38217
(dp38218
g20
(lp38219
I23
assS'revowand'
p38220
(dp38221
g138
(lp38222
I17
assS'tfiv'
p38223
(dp38224
g130
(lp38225
I2358
assS'paab'
p38226
(dp38227
g354
(lp38228
I1334
assS'baird'
p38229
(dp38230
g63
(lp38231
sg181
(lp38232
I2472
assS'subtyp'
p38233
(dp38234
g106
(lp38235
I1213
assS'fastest'
p38236
(dp38237
g38
(lp38238
I2847
asg83
(lp38239
ssS'jnw'
p38240
(dp38241
g235
(lp38242
I1008
assS'sigmoid'
p38243
(dp38244
g283
(lp38245
sg277
(lp38246
sg104
(lp38247
sg287
(lp38248
sg176
(lp38249
sg76
(lp38250
sg262
(lp38251
sg295
(lp38252
sg183
(lp38253
sg83
(lp38254
sg85
(lp38255
sg303
(lp38256
sg18
(lp38257
sg149
(lp38258
sg102
(lp38259
sg178
(lp38260
sg114
(lp38261
sg121
(lp38262
sg8
(lp38263
sg36
(lp38264
sg68
(lp38265
sg341
(lp38266
sg10
(lp38267
sg128
(lp38268
sg14
(lp38269
sg16
(lp38270
sg138
(lp38271
I1897
assS'spert'
p38272
(dp38273
g10
(lp38274
I1
assS'newsweed'
p38275
(dp38276
g277
(lp38277
I3125
assS'ohtain'
p38278
(dp38279
g108
(lp38280
I1656
assS'holmdel'
p38281
(dp38282
g183
(lp38283
I4042
asg38
(lp38284
sg63
(lp38285
ssS'jnm'
p38286
(dp38287
g26
(lp38288
I2251
assS'blake'
p38289
(dp38290
g216
(lp38291
I464
assS'outer'
p38292
(dp38293
g30
(lp38294
sg174
(lp38295
sg256
(lp38296
sg8
(lp38297
sg344
(lp38298
sg78
(lp38299
sg10
(lp38300
sg46
(lp38301
sg14
(lp38302
sg16
(lp38303
I422
asg63
(lp38304
ssS'broom'
p38305
(dp38306
g114
(lp38307
I297
assS'brook'
p38308
(dp38309
g42
(lp38310
I3441
assS'bcsjfcs'
p38311
(dp38312
g118
(lp38313
I2289
assS'nyi'
p38314
(dp38315
g76
(lp38316
I352
assS'trumpington'
p38317
(dp38318
g87
(lp38319
I23
asg76
(lp38320
ssS'pixel'
p38321
(dp38322
g318
(lp38323
sg178
(lp38324
sg256
(lp38325
sg183
(lp38326
sg59
(lp38327
sg281
(lp38328
sg44
(lp38329
sg63
(lp38330
sg283
(lp38331
sg223
(lp38332
sg130
(lp38333
sg245
(lp38334
sg70
(lp38335
sg138
(lp38336
I826
asg52
(lp38337
sg114
(lp38338
ssS'handl'
p38339
(dp38340
g80
(lp38341
sg8
(lp38342
sg10
(lp38343
sg429
(lp38344
sg87
(lp38345
sg91
(lp38346
sg245
(lp38347
sg108
(lp38348
I237
asg99
(lp38349
sg63
(lp38350
ssS'frontal'
p38351
(dp38352
g59
(lp38353
sg4
(lp38354
I3530
asg303
(lp38355
ssS'tile'
p38356
(dp38357
g10
(lp38358
I2077
assS'front'
p38359
(dp38360
g174
(lp38361
sg32
(lp38362
sg22
(lp38363
sg293
(lp38364
sg74
(lp38365
sg42
(lp38366
I2404
asg94
(lp38367
sg14
(lp38368
sg16
(lp38369
sg20
(lp38370
sg96
(lp38371
sg350
(lp38372
ssS'ccgs'
p38373
(dp38374
g6
(lp38375
I112
assS'monopolar'
p38376
(dp38377
g256
(lp38378
I427
assS'axon'
p38379
(dp38380
g106
(lp38381
I576
assS'universita'
p38382
(dp38383
g44
(lp38384
I17
assS'mode'
p38385
(dp38386
g329
(lp38387
sg283
(lp38388
sg22
(lp38389
sg295
(lp38390
sg183
(lp38391
sg126
(lp38392
sg535
(lp38393
sg42
(lp38394
I3019
asg38
(lp38395
sg104
(lp38396
sg245
(lp38397
sg14
(lp38398
sg96
(lp38399
sg135
(lp38400
sg20
(lp38401
sg46
(lp38402
sg256
(lp38403
ssS'velocityl'
p38404
(dp38405
g245
(lp38406
I2072
assS'ylxt'
p38407
(dp38408
g354
(lp38409
I1630
assS'modi'
p38410
(dp38411
g106
(lp38412
I1071
assS'upward'
p38413
(dp38414
g32
(lp38415
I1872
asg59
(lp38416
sg83
(lp38417
ssS'baxter'
p38418
(dp38419
g30
(lp38420
sg48
(lp38421
I259
asg223
(lp38422
ssS'chung'
p38423
(dp38424
g121
(lp38425
I11
assS'snapp'
p38426
(dp38427
g281
(lp38428
I10
assS'nonparametr'
p38429
(dp38430
g318
(lp38431
sg295
(lp38432
sg183
(lp38433
sg281
(lp38434
sg96
(lp38435
sg354
(lp38436
I3223
assS'subfram'
p38437
(dp38438
g59
(lp38439
I1117
assS'ylogo'
p38440
(dp38441
g72
(lp38442
I2229
assS'illq'
p38443
(dp38444
g99
(lp38445
I1043
assS'mijk'
p38446
(dp38447
g74
(lp38448
I1492
assS'combinatori'
p38449
(dp38450
g287
(lp38451
sg74
(lp38452
sg178
(lp38453
sg22
(lp38454
sg8
(lp38455
sg40
(lp38456
sg130
(lp38457
I40
assS'hlm'
p38458
(dp38459
g178
(lp38460
I706
assS'hll'
p38461
(dp38462
g108
(lp38463
I623
assS'hlj'
p38464
(dp38465
g174
(lp38466
I2039
assS'oybernet'
p38467
(dp38468
g114
(lp38469
I2349
assS'hyperbol'
p38470
(dp38471
g108
(lp38472
I1737
asg341
(lp38473
sg256
(lp38474
ssS'illd'
p38475
(dp38476
g174
(lp38477
I2802
assS'special'
p38478
(dp38479
g70
(lp38480
sg287
(lp38481
sg145
(lp38482
sg293
(lp38483
sg344
(lp38484
sg59
(lp38485
sg484
(lp38486
sg38
(lp38487
sg83
(lp38488
sg303
(lp38489
sg306
(lp38490
sg91
(lp38491
sg94
(lp38492
sg48
(lp38493
sg535
(lp38494
sg350
(lp38495
sg429
(lp38496
sg104
(lp38497
sg63
(lp38498
sg114
(lp38499
sg174
(lp38500
sg32
(lp38501
sg121
(lp38502
sg235
(lp38503
sg72
(lp38504
sg10
(lp38505
sg128
(lp38506
sg135
(lp38507
sg138
(lp38508
I2710
assS'kapoor'
p38509
(dp38510
g277
(lp38511
I3150
assS'informmutu'
p38512
(dp38513
g72
(lp38514
I2433
assS'influenc'
p38515
(dp38516
g216
(lp38517
sg30
(lp38518
sg332
(lp38519
sg4
(lp38520
sg245
(lp38521
sg295
(lp38522
sg256
(lp38523
sg59
(lp38524
sg12
(lp38525
sg87
(lp38526
sg91
(lp38527
sg183
(lp38528
sg132
(lp38529
sg18
(lp38530
sg135
(lp38531
sg99
(lp38532
sg313
(lp38533
sg223
(lp38534
sg354
(lp38535
I324
assS'maffei'
p38536
(dp38537
g12
(lp38538
I2866
assS'discharg'
p38539
(dp38540
g245
(lp38541
sg256
(lp38542
sg6
(lp38543
I2363
assS'suitabl'
p38544
(dp38545
g32
(lp38546
sg22
(lp38547
sg76
(lp38548
sg68
(lp38549
sg42
(lp38550
I2819
asg14
(lp38551
sg16
(lp38552
sg96
(lp38553
sg52
(lp38554
ssS'hardwar'
p38555
(dp38556
g283
(lp38557
sg78
(lp38558
sg59
(lp38559
sg68
(lp38560
sg10
(lp38561
sg63
(lp38562
sg42
(lp38563
I65
asg104
(lp38564
sg245
(lp38565
sg14
(lp38566
sg16
(lp38567
sg20
(lp38568
sg44
(lp38569
ssS'syrinx'
p38570
(dp38571
g116
(lp38572
I356
assS'calif'
p38573
(dp38574
g341
(lp38575
I2784
asg40
(lp38576
ssS'ece'
p38577
(dp38578
g22
(lp38579
I32
asg83
(lp38580
ssS'ecd'
p38581
(dp38582
g149
(lp38583
I859
assS'eca'
p38584
(dp38585
g318
(lp38586
I1565
assS'sander'
p38587
(dp38588
g26
(lp38589
I3270
assS'torrey'
p38590
(dp38591
g318
(lp38592
sg91
(lp38593
I25
asg350
(lp38594
ssS'eck'
p38595
(dp38596
g14
(lp38597
sg16
(lp38598
I2608
assS'transfer'
p38599
(dp38600
g30
(lp38601
sg68
(lp38602
sg121
(lp38603
sg22
(lp38604
sg8
(lp38605
sg256
(lp38606
sg59
(lp38607
sg262
(lp38608
sg72
(lp38609
sg341
(lp38610
sg10
(lp38611
sg245
(lp38612
sg283
(lp38613
sg102
(lp38614
sg14
(lp38615
sg16
(lp38616
sg135
(lp38617
I810
asg99
(lp38618
sg96
(lp38619
sg223
(lp38620
sg114
(lp38621
ssS'ecv'
p38622
(dp38623
g85
(lp38624
I2377
assS'jill'
p38625
(dp38626
g130
(lp38627
I2466
assS'amsotropi'
p38628
(dp38629
g48
(lp38630
I2013
assS'hypercub'
p38631
(dp38632
g283
(lp38633
I1982
assS'cadera'
p38634
(dp38635
g350
(lp38636
I1007
assS'chatham'
p38637
(dp38638
g114
(lp38639
I2274
assS'etern'
p38640
(dp38641
g42
(lp38642
I341
assS'vjwj'
p38643
(dp38644
g216
(lp38645
I1299
assS'saberi'
p38646
(dp38647
g535
(lp38648
I2187
assS'unwant'
p38649
(dp38650
g438
(lp38651
I1848
asg68
(lp38652
ssS'raghavan'
p38653
(dp38654
g341
(lp38655
I2820
asg40
(lp38656
ssS'segreg'
p38657
(dp38658
g116
(lp38659
sg48
(lp38660
I2256
assS'timer'
p38661
(dp38662
g10
(lp38663
I1045
assS'keep'
p38664
(dp38665
g329
(lp38666
sg74
(lp38667
sg484
(lp38668
sg34
(lp38669
sg460
(lp38670
sg124
(lp38671
sg126
(lp38672
sg83
(lp38673
sg350
(lp38674
sg87
(lp38675
sg89
(lp38676
sg128
(lp38677
sg132
(lp38678
I2152
asg48
(lp38679
sg149
(lp38680
ssS'counterpart'
p38681
(dp38682
g295
(lp38683
sg183
(lp38684
sg20
(lp38685
I428
asg72
(lp38686
sg344
(lp38687
ssS'informatiqu'
p38688
(dp38689
g287
(lp38690
I12
assS'geometri'
p38691
(dp38692
g32
(lp38693
sg293
(lp38694
sg460
(lp38695
sg72
(lp38696
sg245
(lp38697
sg14
(lp38698
sg16
(lp38699
I1295
asg44
(lp38700
sg350
(lp38701
ssS'rheinisch'
p38702
(dp38703
g130
(lp38704
I10
assS'austin'
p38705
(dp38706
g149
(lp38707
I30
assS'abdallah'
p38708
(dp38709
g46
(lp38710
I16
assS'shorthand'
p38711
(dp38712
g318
(lp38713
I641
asg85
(lp38714
ssS'christoph'
p38715
(dp38716
g116
(lp38717
sg438
(lp38718
I18
asg70
(lp38719
sg295
(lp38720
sg183
(lp38721
sg124
(lp38722
sg429
(lp38723
sg138
(lp38724
ssS'uncorrel'
p38725
(dp38726
g235
(lp38727
sg38
(lp38728
sg130
(lp38729
I1655
asg102
(lp38730
sg99
(lp38731
sg149
(lp38732
ssS'runnngmum'
p38733
(dp38734
g135
(lp38735
I1216
assS'date'
p38736
(dp38737
g76
(lp38738
sg295
(lp38739
sg183
(lp38740
sg83
(lp38741
sg344
(lp38742
sg89
(lp38743
sg78
(lp38744
sg14
(lp38745
I2820
asg48
(lp38746
ssS'barr'
p38747
(dp38748
g429
(lp38749
I890
assS'ilwil'
p38750
(dp38751
g34
(lp38752
I2484
assS'forgeri'
p38753
(dp38754
g114
(lp38755
I74
assS'medisgroup'
p38756
(dp38757
g277
(lp38758
I3159
assS'foveat'
p38759
(dp38760
g178
(lp38761
I2354
asg293
(lp38762
ssS'mtmw'
p38763
(dp38764
g306
(lp38765
I2049
assS'attach'
p38766
(dp38767
g460
(lp38768
I798
asg10
(lp38769
ssS'attack'
p38770
(dp38771
g132
(lp38772
I862
asg63
(lp38773
ssS'foveal'
p38774
(dp38775
g245
(lp38776
I2799
assS'baveli'
p38777
(dp38778
g130
(lp38779
I3062
assS'wilkinson'
p38780
(dp38781
g114
(lp38782
I24
assS'final'
p38783
(dp38784
g124
(lp38785
sg78
(lp38786
sg163
(lp38787
sg72
(lp38788
sg26
(lp38789
sg287
(lp38790
sg74
(lp38791
sg76
(lp38792
sg118
(lp38793
sg295
(lp38794
sg183
(lp38795
sg59
(lp38796
sg85
(lp38797
sg42
(lp38798
I1547
asg87
(lp38799
sg89
(lp38800
sg68
(lp38801
sg12
(lp38802
sg94
(lp38803
sg48
(lp38804
sg221
(lp38805
sg44
(lp38806
sg149
(lp38807
sg329
(lp38808
sg293
(lp38809
sg32
(lp38810
sg245
(lp38811
sg429
(lp38812
sg318
(lp38813
sg46
(lp38814
sg102
(lp38815
sg104
(lp38816
sg108
(lp38817
sg110
(lp38818
sg178
(lp38819
sg114
(lp38820
sg438
(lp38821
sg440
(lp38822
sg18
(lp38823
sg121
(lp38824
sg80
(lp38825
sg6
(lp38826
sg8
(lp38827
sg384
(lp38828
sg235
(lp38829
sg126
(lp38830
sg341
(lp38831
sg10
(lp38832
sg40
(lp38833
sg344
(lp38834
sg128
(lp38835
sg130
(lp38836
sg132
(lp38837
sg14
(lp38838
sg16
(lp38839
sg350
(lp38840
sg50
(lp38841
sg138
(lp38842
sg140
(lp38843
sg354
(lp38844
ssS'prone'
p38845
(dp38846
g94
(lp38847
I1259
asg121
(lp38848
ssS'rse'
p38849
(dp38850
g6
(lp38851
I408
assS'methodolog'
p38852
(dp38853
g277
(lp38854
sg181
(lp38855
sg344
(lp38856
sg429
(lp38857
sg42
(lp38858
I738
asg306
(lp38859
sg87
(lp38860
sg128
(lp38861
ssS'coutsia'
p38862
(dp38863
g46
(lp38864
I3579
assS'mariani'
p38865
(dp38866
g174
(lp38867
I2857
assS'mclennan'
p38868
(dp38869
g106
(lp38870
I2602
assS'tissu'
p38871
(dp38872
g318
(lp38873
I2341
assS'rst'
p38874
(dp38875
g6
(lp38876
I519
assS'rsp'
p38877
(dp38878
g176
(lp38879
I2450
assS'beh'
p38880
(dp38881
g32
(lp38882
I2199
assS'bei'
p38883
(dp38884
g72
(lp38885
I2456
assS'bel'
p38886
(dp38887
g256
(lp38888
I821
assS'repuls'
p38889
(dp38890
g12
(lp38891
I1446
assS'bee'
p38892
(dp38893
g70
(lp38894
I2573
assS'ss'
p38895
(dp38896
g341
(lp38897
I1075
assS'signifi'
p38898
(dp38899
g145
(lp38900
sg99
(lp38901
I1761
asg293
(lp38902
ssS'perrett'
p38903
(dp38904
g181
(lp38905
I285
assS'bet'
p38906
(dp38907
g48
(lp38908
I1664
assS'exhibit'
p38909
(dp38910
g230
(lp38911
sg30
(lp38912
sg332
(lp38913
sg121
(lp38914
sg256
(lp38915
sg181
(lp38916
sg118
(lp38917
sg34
(lp38918
sg130
(lp38919
sg38
(lp38920
sg10
(lp38921
sg303
(lp38922
sg245
(lp38923
sg85
(lp38924
sg14
(lp38925
sg102
(lp38926
sg94
(lp38927
sg106
(lp38928
I1811
asg48
(lp38929
sg344
(lp38930
sg535
(lp38931
ssS'rhythmic'
p38932
(dp38933
g295
(lp38934
sg183
(lp38935
sg106
(lp38936
I612
assS'kroghgsang'
p38937
(dp38938
g235
(lp38939
I30
assS'vatanab'
p38940
(dp38941
g63
(lp38942
I1599
assS'arg'
p38943
(dp38944
g74
(lp38945
sg178
(lp38946
sg293
(lp38947
sg72
(lp38948
sg306
(lp38949
sg130
(lp38950
I1068
assS'msec'
p38951
(dp38952
g116
(lp38953
sg4
(lp38954
sg6
(lp38955
sg262
(lp38956
sg106
(lp38957
I1098
asg99
(lp38958
sg535
(lp38959
ssS'goldberg'
p38960
(dp38961
g287
(lp38962
I782
assS'keller'
p38963
(dp38964
g132
(lp38965
I3599
asg48
(lp38966
sg149
(lp38967
ssS'need'
p38968
(dp38969
g124
(lp38970
sg70
(lp38971
sg26
(lp38972
sg163
(lp38973
sg72
(lp38974
sg287
(lp38975
sg74
(lp38976
sg145
(lp38977
sg256
(lp38978
sg76
(lp38979
sg293
(lp38980
sg295
(lp38981
sg183
(lp38982
sg59
(lp38983
sg38
(lp38984
sg83
(lp38985
sg303
(lp38986
sg306
(lp38987
sg87
(lp38988
sg89
(lp38989
sg91
(lp38990
sg46
(lp38991
sg114
(lp38992
sg99
(lp38993
sg313
(lp38994
sg223
(lp38995
sg350
(lp38996
sg174
(lp38997
sg68
(lp38998
sg94
(lp38999
sg102
(lp39000
sg108
(lp39001
sg63
(lp39002
sg52
(lp39003
sg22
(lp39004
sg230
(lp39005
sg438
(lp39006
I1617
asg32
(lp39007
sg332
(lp39008
sg178
(lp39009
sg4
(lp39010
sg8
(lp39011
sg34
(lp39012
sg235
(lp39013
sg126
(lp39014
sg281
(lp39015
sg10
(lp39016
sg40
(lp39017
sg344
(lp39018
sg44
(lp39019
sg78
(lp39020
sg132
(lp39021
sg135
(lp39022
sg50
(lp39023
sg140
(lp39024
ssS'border'
p39025
(dp39026
g8
(lp39027
sg176
(lp39028
sg354
(lp39029
I2773
assS'nadarayawatson'
p39030
(dp39031
g96
(lp39032
I906
assS'screw'
p39033
(dp39034
g350
(lp39035
I661
assS'drucker'
p39036
(dp39037
g183
(lp39038
I4037
assS'expos'
p39039
(dp39040
g10
(lp39041
I583
assS'berthommi'
p39042
(dp39043
g174
(lp39044
I539
assS'sh'
p39045
(dp39046
g80
(lp39047
I36
assS'tactil'
p39048
(dp39049
g176
(lp39050
I6
assS'singh'
p39051
(dp39052
g89
(lp39053
I1540
asg293
(lp39054
ssS'barn'
p39055
(dp39056
g80
(lp39057
I2514
assS'singl'
p39058
(dp39059
g329
(lp39060
sg70
(lp39061
sg26
(lp39062
sg277
(lp39063
sg163
(lp39064
sg72
(lp39065
sg80
(lp39066
sg283
(lp39067
sg85
(lp39068
sg30
(lp39069
sg350
(lp39070
sg74
(lp39071
sg176
(lp39072
sg256
(lp39073
sg76
(lp39074
sg262
(lp39075
sg295
(lp39076
sg183
(lp39077
sg59
(lp39078
sg484
(lp39079
sg38
(lp39080
sg83
(lp39081
sg114
(lp39082
sg303
(lp39083
sg42
(lp39084
I1054
asg87
(lp39085
sg89
(lp39086
sg68
(lp39087
sg12
(lp39088
sg94
(lp39089
sg96
(lp39090
sg18
(lp39091
sg221
(lp39092
sg44
(lp39093
sg149
(lp39094
sg116
(lp39095
sg174
(lp39096
sg293
(lp39097
sg32
(lp39098
sg245
(lp39099
sg429
(lp39100
sg318
(lp39101
sg46
(lp39102
sg102
(lp39103
sg104
(lp39104
sg106
(lp39105
sg63
(lp39106
sg52
(lp39107
sg22
(lp39108
sg216
(lp39109
sg438
(lp39110
sg440
(lp39111
sg332
(lp39112
sg181
(lp39113
sg6
(lp39114
sg8
(lp39115
sg384
(lp39116
sg235
(lp39117
sg126
(lp39118
sg341
(lp39119
sg10
(lp39120
sg40
(lp39121
sg344
(lp39122
sg223
(lp39123
sg128
(lp39124
sg78
(lp39125
sg14
(lp39126
sg16
(lp39127
sg135
(lp39128
sg50
(lp39129
sg140
(lp39130
sg354
(lp39131
ssS'pyx'
p39132
(dp39133
g329
(lp39134
I506
asg72
(lp39135
ssS'textur'
p39136
(dp39137
g181
(lp39138
sg52
(lp39139
I578
assS'sloman'
p39140
(dp39141
g110
(lp39142
I3057
assS'awt'
p39143
(dp39144
g34
(lp39145
I684
assS'discov'
p39146
(dp39147
g74
(lp39148
sg318
(lp39149
sg145
(lp39150
sg110
(lp39151
sg85
(lp39152
sg176
(lp39153
sg130
(lp39154
sg132
(lp39155
sg70
(lp39156
sg50
(lp39157
sg138
(lp39158
I3525
assS'awm'
p39159
(dp39160
g89
(lp39161
I29
assS'awl'
p39162
(dp39163
g341
(lp39164
I2617
assS'rigor'
p39165
(dp39166
g74
(lp39167
sg85
(lp39168
sg535
(lp39169
sg140
(lp39170
I350
assS'monochrom'
p39171
(dp39172
g174
(lp39173
I1059
assS'ure'
p39174
(dp39175
g59
(lp39176
sg4
(lp39177
sg128
(lp39178
I2247
assS'stereo'
p39179
(dp39180
g42
(lp39181
I3451
asg59
(lp39182
ssS'ura'
p39183
(dp39184
g4
(lp39185
I1547
assS'url'
p39186
(dp39187
g76
(lp39188
sg8
(lp39189
I2635
assS'jijaj'
p39190
(dp39191
g384
(lp39192
I398
assS'dxjexp'
p39193
(dp39194
g130
(lp39195
I502
assS'uri'
p39196
(dp39197
g440
(lp39198
I2488
asg10
(lp39199
ssS'schulten'
p39200
(dp39201
g124
(lp39202
sg36
(lp39203
sg48
(lp39204
sg176
(lp39205
sg149
(lp39206
I2988
assS'inde'
p39207
(dp39208
g287
(lp39209
sg440
(lp39210
sg18
(lp39211
sg145
(lp39212
sg76
(lp39213
sg235
(lp39214
sg34
(lp39215
sg293
(lp39216
sg384
(lp39217
sg40
(lp39218
sg72
(lp39219
sg32
(lp39220
sg85
(lp39221
sg74
(lp39222
sg42
(lp39223
I682
asg12
(lp39224
sg89
(lp39225
sg132
(lp39226
sg48
(lp39227
sg63
(lp39228
sg44
(lp39229
ssS'laurenc'
p39230
(dp39231
g283
(lp39232
I37
assS'constrain'
p39233
(dp39234
g283
(lp39235
sg26
(lp39236
sg535
(lp39237
sg74
(lp39238
sg38
(lp39239
sg89
(lp39240
sg91
(lp39241
sg96
(lp39242
sg48
(lp39243
sg313
(lp39244
sg429
(lp39245
sg102
(lp39246
sg110
(lp39247
sg329
(lp39248
sg4
(lp39249
sg8
(lp39250
sg384
(lp39251
sg235
(lp39252
sg40
(lp39253
sg128
(lp39254
sg50
(lp39255
sg138
(lp39256
sg140
(lp39257
I512
assS'llarbjk'
p39258
(dp39259
g46
(lp39260
I2457
assS'parker'
p39261
(dp39262
g99
(lp39263
I41
assS'noway'
p39264
(dp39265
g87
(lp39266
I909
assS'verbos'
p39267
(dp39268
g94
(lp39269
I671
assS'stij'
p39270
(dp39271
g174
(lp39272
I1860
assS'anyway'
p39273
(dp39274
g83
(lp39275
sg44
(lp39276
I1574
asg26
(lp39277
ssS'yaea'
p39278
(dp39279
g318
(lp39280
I930
assS'wor'
p39281
(dp39282
g10
(lp39283
I482
assS'poggio'
p39284
(dp39285
g293
(lp39286
sg36
(lp39287
sg124
(lp39288
sg96
(lp39289
sg138
(lp39290
I3286
asg223
(lp39291
ssS'higherlevel'
p39292
(dp39293
g178
(lp39294
I2411
assS'computationallearn'
p39295
(dp39296
g295
(lp39297
sg287
(lp39298
I445
asg183
(lp39299
ssS'winner'
p39300
(dp39301
g30
(lp39302
sg178
(lp39303
sg8
(lp39304
sg303
(lp39305
sg429
(lp39306
sg70
(lp39307
sg20
(lp39308
sg135
(lp39309
I125
asg52
(lp39310
ssS'songbird'
p39311
(dp39312
g116
(lp39313
I84
assS'oneparamet'
p39314
(dp39315
g32
(lp39316
I2996
assS'manufactur'
p39317
(dp39318
g78
(lp39319
I821
asg283
(lp39320
ssS'brighter'
p39321
(dp39322
g318
(lp39323
I2633
assS'maginu'
p39324
(dp39325
g384
(lp39326
I623
assS'fuel'
p39327
(dp39328
g78
(lp39329
I306
assS'discontinu'
p39330
(dp39331
g332
(lp39332
sg178
(lp39333
sg8
(lp39334
sg68
(lp39335
sg429
(lp39336
sg87
(lp39337
sg89
(lp39338
sg70
(lp39339
sg48
(lp39340
I2149
assS'pronzato'
p39341
(dp39342
g354
(lp39343
I332
assS'matsukoto'
p39344
(dp39345
g96
(lp39346
I387
assS'marnag'
p39347
(dp39348
g72
(lp39349
I2909
assS'michigan'
p39350
(dp39351
g99
(lp39352
I3363
assS'joint'
p39353
(dp39354
g68
(lp39355
sg72
(lp39356
sg176
(lp39357
sg344
(lp39358
sg183
(lp39359
sg91
(lp39360
sg12
(lp39361
sg99
(lp39362
sg313
(lp39363
sg44
(lp39364
sg102
(lp39365
sg52
(lp39366
sg32
(lp39367
sg318
(lp39368
sg384
(lp39369
sg124
(lp39370
sg126
(lp39371
sg10
(lp39372
sg295
(lp39373
sg78
(lp39374
sg14
(lp39375
sg50
(lp39376
sg354
(lp39377
I2891
assS'tbk'
p39378
(dp39379
g99
(lp39380
I36
assS'bambang'
p39381
(dp39382
g484
(lp39383
I7
assS'tbe'
p39384
(dp39385
g149
(lp39386
I2794
assS'endless'
p39387
(dp39388
g42
(lp39389
I2096
assS'gray'
p39390
(dp39391
g26
(lp39392
sg118
(lp39393
sg48
(lp39394
sg256
(lp39395
sg130
(lp39396
I1587
asg149
(lp39397
sg99
(lp39398
sg63
(lp39399
sg22
(lp39400
ssS'swansea'
p39401
(dp39402
g52
(lp39403
I2591
assS'enabl'
p39404
(dp39405
g216
(lp39406
sg32
(lp39407
sg332
(lp39408
sg145
(lp39409
sg80
(lp39410
sg295
(lp39411
sg183
(lp39412
sg68
(lp39413
sg281
(lp39414
sg10
(lp39415
sg176
(lp39416
sg89
(lp39417
sg36
(lp39418
sg20
(lp39419
sg18
(lp39420
sg110
(lp39421
sg140
(lp39422
I1282
assS'grayscal'
p39423
(dp39424
g44
(lp39425
I1886
assS'itrr'
p39426
(dp39427
g295
(lp39428
I2129
asg183
(lp39429
ssS'jasa'
p39430
(dp39431
g89
(lp39432
I2715
assS'gran'
p39433
(dp39434
g96
(lp39435
I156
assS'contain'
p39436
(dp39437
g68
(lp39438
sg70
(lp39439
sg26
(lp39440
sg277
(lp39441
sg116
(lp39442
sg281
(lp39443
sg74
(lp39444
sg295
(lp39445
sg183
(lp39446
sg59
(lp39447
sg484
(lp39448
sg83
(lp39449
sg85
(lp39450
sg303
(lp39451
sg87
(lp39452
sg89
(lp39453
sg91
(lp39454
sg46
(lp39455
sg96
(lp39456
sg48
(lp39457
sg223
(lp39458
sg350
(lp39459
sg230
(lp39460
sg429
(lp39461
sg318
(lp39462
sg94
(lp39463
sg104
(lp39464
sg106
(lp39465
sg108
(lp39466
sg63
(lp39467
sg52
(lp39468
sg216
(lp39469
sg438
(lp39470
I331
asg32
(lp39471
sg18
(lp39472
sg121
(lp39473
sg22
(lp39474
sg181
(lp39475
sg235
(lp39476
sg460
(lp39477
sg124
(lp39478
sg126
(lp39479
sg341
(lp39480
sg10
(lp39481
sg44
(lp39482
sg78
(lp39483
sg14
(lp39484
sg16
(lp39485
sg140
(lp39486
sg354
(lp39487
ssS'grab'
p39488
(dp39489
g59
(lp39490
sg283
(lp39491
sg181
(lp39492
I1178
asg293
(lp39493
ssS'nodulus'
p39494
(dp39495
g350
(lp39496
I2814
assS'grad'
p39497
(dp39498
g14
(lp39499
sg16
(lp39500
I661
assS'graf'
p39501
(dp39502
g14
(lp39503
I4662
asg63
(lp39504
sg114
(lp39505
ssS'vincentelli'
p39506
(dp39507
g34
(lp39508
I2869
assS'bbbb'
p39509
(dp39510
g110
(lp39511
I1355
assS'ttl'
p39512
(dp39513
g230
(lp39514
I1673
assS'afterhyperpolar'
p39515
(dp39516
g106
(lp39517
I1920
assS'computer'
p39518
(dp39519
g91
(lp39520
I2427
assS'peterson'
p39521
(dp39522
g178
(lp39523
I514
assS'disjunct'
p39524
(dp39525
g110
(lp39526
I1751
asg63
(lp39527
ssS'aros'
p39528
(dp39529
g59
(lp39530
I659
assS'teno'
p39531
(dp39532
g106
(lp39533
I2431
assS'tenn'
p39534
(dp39535
g85
(lp39536
I2549
assS'dewees'
p39537
(dp39538
g262
(lp39539
I555
assS'wta'
p39540
(dp39541
g70
(lp39542
sg20
(lp39543
sg135
(lp39544
I689
asg178
(lp39545
ssS'tend'
p39546
(dp39547
g174
(lp39548
sg68
(lp39549
sg4
(lp39550
sg163
(lp39551
sg183
(lp39552
sg124
(lp39553
sg38
(lp39554
sg83
(lp39555
sg484
(lp39556
sg102
(lp39557
sg48
(lp39558
sg138
(lp39559
I2892
asg149
(lp39560
ssS'state'
p39561
(dp39562
g124
(lp39563
sg70
(lp39564
sg26
(lp39565
sg293
(lp39566
sg281
(lp39567
sg176
(lp39568
sg256
(lp39569
sg76
(lp39570
sg262
(lp39571
sg295
(lp39572
sg183
(lp39573
sg80
(lp39574
sg83
(lp39575
sg85
(lp39576
sg63
(lp39577
sg306
(lp39578
sg87
(lp39579
sg89
(lp39580
sg460
(lp39581
sg12
(lp39582
sg46
(lp39583
sg96
(lp39584
sg48
(lp39585
sg99
(lp39586
sg535
(lp39587
sg44
(lp39588
sg230
(lp39589
sg118
(lp39590
sg18
(lp39591
sg32
(lp39592
sg245
(lp39593
sg429
(lp39594
sg68
(lp39595
sg102
(lp39596
sg104
(lp39597
sg108
(lp39598
sg110
(lp39599
sg20
(lp39600
sg22
(lp39601
sg216
(lp39602
sg438
(lp39603
I438
asg440
(lp39604
sg332
(lp39605
sg4
(lp39606
sg8
(lp39607
sg34
(lp39608
sg384
(lp39609
sg235
(lp39610
sg126
(lp39611
sg341
(lp39612
sg40
(lp39613
sg223
(lp39614
sg128
(lp39615
sg130
(lp39616
sg132
(lp39617
sg14
(lp39618
sg50
(lp39619
sg138
(lp39620
sg140
(lp39621
ssS'wavelet'
p39622
(dp39623
g22
(lp39624
I7
asg293
(lp39625
ssS'lui'
p39626
(dp39627
g87
(lp39628
I681
assS'neither'
p39629
(dp39630
g329
(lp39631
sg318
(lp39632
sg4
(lp39633
sg36
(lp39634
sg281
(lp39635
sg303
(lp39636
sg78
(lp39637
sg14
(lp39638
I4550
asg63
(lp39639
ssS'illusori'
p39640
(dp39641
g216
(lp39642
I418
asg118
(lp39643
sg70
(lp39644
ssS'kltjkl'
p39645
(dp39646
g149
(lp39647
I944
assS'sold'
p39648
(dp39649
g42
(lp39650
I605
assS'helg'
p39651
(dp39652
g59
(lp39653
I44
assS'herbster'
p39654
(dp39655
g341
(lp39656
I14
assS'maybank'
p39657
(dp39658
g281
(lp39659
I935
assS'doyl'
p39660
(dp39661
g484
(lp39662
I40
assS'gilmor'
p39663
(dp39664
g32
(lp39665
I1492
assS'key'
p39666
(dp39667
g216
(lp39668
sg329
(lp39669
sg440
(lp39670
sg176
(lp39671
sg22
(lp39672
sg80
(lp39673
sg262
(lp39674
sg183
(lp39675
sg163
(lp39676
sg116
(lp39677
sg277
(lp39678
sg94
(lp39679
sg14
(lp39680
sg106
(lp39681
I2478
asg20
(lp39682
sg16
(lp39683
sg223
(lp39684
ssS'chipset'
p39685
(dp39686
g10
(lp39687
I2356
assS'nettalk'
p39688
(dp39689
g114
(lp39690
I1005
assS'thank'
p39691
(dp39692
g283
(lp39693
sg26
(lp39694
sg277
(lp39695
sg30
(lp39696
sg74
(lp39697
sg256
(lp39698
sg76
(lp39699
sg59
(lp39700
sg484
(lp39701
sg83
(lp39702
sg85
(lp39703
sg89
(lp39704
sg94
(lp39705
sg96
(lp39706
sg48
(lp39707
sg313
(lp39708
sg44
(lp39709
sg350
(lp39710
sg174
(lp39711
sg32
(lp39712
sg429
(lp39713
sg46
(lp39714
sg106
(lp39715
sg110
(lp39716
sg63
(lp39717
sg116
(lp39718
sg438
(lp39719
I2347
asg440
(lp39720
sg181
(lp39721
sg6
(lp39722
sg36
(lp39723
sg124
(lp39724
sg341
(lp39725
sg10
(lp39726
sg128
(lp39727
sg130
(lp39728
sg132
(lp39729
sg14
(lp39730
sg16
(lp39731
sg135
(lp39732
sg50
(lp39733
sg138
(lp39734
sg140
(lp39735
sg354
(lp39736
ssS'wilenski'
p39737
(dp39738
g44
(lp39739
I514
assS'britten'
p39740
(dp39741
g6
(lp39742
I1288
assS'articulatori'
p39743
(dp39744
g174
(lp39745
I2848
assS'admit'
p39746
(dp39747
g59
(lp39748
sg277
(lp39749
sg44
(lp39750
I2472
asg40
(lp39751
ssS'admir'
p39752
(dp39753
g14
(lp39754
I4343
assS'schoppmann'
p39755
(dp39756
g176
(lp39757
I2596
assS'qfj'
p39758
(dp39759
g26
(lp39760
I2015
assS'lowerleft'
p39761
(dp39762
g12
(lp39763
I1504
assS'jersey'
p39764
(dp39765
g183
(lp39766
sg63
(lp39767
sg114
(lp39768
I2394
assS'hesselmo'
p39769
(dp39770
g126
(lp39771
I3043
assS'nondeterminist'
p39772
(dp39773
g89
(lp39774
sg313
(lp39775
I2309
assS'johann'
p39776
(dp39777
g132
(lp39778
I3705
assS'universitat'
p39779
(dp39780
g30
(lp39781
sg34
(lp39782
sg59
(lp39783
sg68
(lp39784
sg48
(lp39785
I1960
asg221
(lp39786
sg223
(lp39787
ssS'quit'
p39788
(dp39789
g68
(lp39790
sg26
(lp39791
sg287
(lp39792
sg74
(lp39793
sg59
(lp39794
sg484
(lp39795
sg85
(lp39796
sg89
(lp39797
sg12
(lp39798
sg94
(lp39799
sg20
(lp39800
sg44
(lp39801
sg318
(lp39802
sg102
(lp39803
sg110
(lp39804
sg52
(lp39805
sg230
(lp39806
sg332
(lp39807
sg178
(lp39808
sg181
(lp39809
sg384
(lp39810
sg124
(lp39811
sg72
(lp39812
sg341
(lp39813
sg130
(lp39814
sg132
(lp39815
sg135
(lp39816
sg140
(lp39817
sg354
(lp39818
I2387
assS'slowli'
p39819
(dp39820
g30
(lp39821
sg438
(lp39822
sg318
(lp39823
sg262
(lp39824
sg174
(lp39825
sg295
(lp39826
sg183
(lp39827
sg124
(lp39828
sg126
(lp39829
sg303
(lp39830
sg42
(lp39831
I2697
asg108
(lp39832
ssS'quenc'
p39833
(dp39834
g48
(lp39835
I1381
assS'yau'
p39836
(dp39837
g281
(lp39838
I931
assS'breiman'
p39839
(dp39840
g183
(lp39841
sg484
(lp39842
sg221
(lp39843
I100
assS'sara'
p39844
(dp39845
g38
(lp39846
I27
assS'malinow'
p39847
(dp39848
g106
(lp39849
I147
assS'willi'
p39850
(dp39851
g40
(lp39852
I1051
assS'cenk'
p39853
(dp39854
g178
(lp39855
I2446
assS'treat'
p39856
(dp39857
g74
(lp39858
sg277
(lp39859
sg76
(lp39860
sg235
(lp39861
sg36
(lp39862
sg262
(lp39863
sg72
(lp39864
sg306
(lp39865
sg87
(lp39866
sg102
(lp39867
sg46
(lp39868
sg48
(lp39869
I2427
asg535
(lp39870
ssS'dure'
p39871
(dp39872
g26
(lp39873
sg277
(lp39874
sg181
(lp39875
sg350
(lp39876
sg74
(lp39877
sg145
(lp39878
sg80
(lp39879
sg76
(lp39880
sg262
(lp39881
sg295
(lp39882
sg183
(lp39883
sg484
(lp39884
sg38
(lp39885
sg83
(lp39886
sg63
(lp39887
sg87
(lp39888
sg89
(lp39889
sg91
(lp39890
sg12
(lp39891
sg94
(lp39892
sg99
(lp39893
sg535
(lp39894
sg44
(lp39895
sg149
(lp39896
sg116
(lp39897
sg329
(lp39898
sg32
(lp39899
sg245
(lp39900
sg429
(lp39901
sg102
(lp39902
sg104
(lp39903
sg106
(lp39904
sg108
(lp39905
sg110
(lp39906
sg178
(lp39907
sg52
(lp39908
sg216
(lp39909
sg438
(lp39910
I997
asg440
(lp39911
sg332
(lp39912
sg121
(lp39913
sg4
(lp39914
sg6
(lp39915
sg8
(lp39916
sg34
(lp39917
sg36
(lp39918
sg124
(lp39919
sg126
(lp39920
sg223
(lp39921
sg78
(lp39922
sg132
(lp39923
sg14
(lp39924
sg16
(lp39925
sg135
(lp39926
sg138
(lp39927
sg140
(lp39928
sg354
(lp39929
ssS'yab'
p39930
(dp39931
g102
(lp39932
I846
assS'oti'
p39933
(dp39934
g126
(lp39935
I2519
asg40
(lp39936
ssS'informlearn'
p39937
(dp39938
g72
(lp39939
I2441
assS'winnertak'
p39940
(dp39941
g52
(lp39942
I61
assS'maxwel'
p39943
(dp39944
g68
(lp39945
I3352
assS'maim'
p39946
(dp39947
g429
(lp39948
I1284
assS'marcantonio'
p39949
(dp39950
g78
(lp39951
I13
assS'plausibl'
p39952
(dp39953
g30
(lp39954
sg174
(lp39955
sg332
(lp39956
sg178
(lp39957
sg4
(lp39958
sg118
(lp39959
sg116
(lp39960
sg85
(lp39961
sg429
(lp39962
sg283
(lp39963
sg91
(lp39964
I3091
asg102
(lp39965
ssS'replic'
p39966
(dp39967
g116
(lp39968
sg332
(lp39969
sg121
(lp39970
sg484
(lp39971
sg303
(lp39972
sg149
(lp39973
I388
asg350
(lp39974
ssS'novel'
p39975
(dp39976
g26
(lp39977
sg163
(lp39978
sg74
(lp39979
sg76
(lp39980
sg293
(lp39981
sg78
(lp39982
sg59
(lp39983
sg38
(lp39984
sg20
(lp39985
sg18
(lp39986
sg99
(lp39987
sg44
(lp39988
sg149
(lp39989
sg106
(lp39990
I2274
asg110
(lp39991
sg52
(lp39992
sg116
(lp39993
sg329
(lp39994
sg318
(lp39995
sg4
(lp39996
sg181
(lp39997
sg8
(lp39998
sg384
(lp39999
sg235
(lp40000
sg40
(lp40001
sg14
(lp40002
sg16
(lp40003
sg138
(lp40004
ssS'inm'
p40005
(dp40006
g72
(lp40007
I1310
assS'harder'
p40008
(dp40009
g132
(lp40010
I1833
asg89
(lp40011
ssS'contextu'
p40012
(dp40013
g12
(lp40014
I2897
asg87
(lp40015
sg70
(lp40016
sg4
(lp40017
sg293
(lp40018
ssS'prenticehal'
p40019
(dp40020
g230
(lp40021
I3295
assS'engag'
p40022
(dp40023
g4
(lp40024
I973
assS'rtin'
p40025
(dp40026
g52
(lp40027
I11
assS'neuroo'
p40028
(dp40029
g106
(lp40030
I340
assS'neuron'
p40031
(dp40032
g68
(lp40033
sg70
(lp40034
sg283
(lp40035
sg287
(lp40036
sg176
(lp40037
sg256
(lp40038
sg80
(lp40039
sg262
(lp40040
sg78
(lp40041
sg59
(lp40042
sg149
(lp40043
sg303
(lp40044
sg12
(lp40045
sg20
(lp40046
sg535
(lp40047
sg350
(lp40048
sg116
(lp40049
sg174
(lp40050
sg429
(lp40051
sg318
(lp40052
sg102
(lp40053
sg104
(lp40054
sg106
(lp40055
sg52
(lp40056
sg114
(lp40057
sg216
(lp40058
sg438
(lp40059
I148
asg118
(lp40060
sg332
(lp40061
sg121
(lp40062
sg4
(lp40063
sg6
(lp40064
sg8
(lp40065
sg384
(lp40066
sg124
(lp40067
sg341
(lp40068
sg40
(lp40069
sg128
(lp40070
sg14
(lp40071
sg135
(lp40072
sg50
(lp40073
ssS'aml'
p40074
(dp40075
g72
(lp40076
I2217
assS'hypoglossi'
p40077
(dp40078
g350
(lp40079
I991
assS'revis'
p40080
(dp40081
g36
(lp40082
I3335
assS'scienc'
p40083
(dp40084
g329
(lp40085
sg70
(lp40086
sg26
(lp40087
sg277
(lp40088
sg72
(lp40089
sg281
(lp40090
sg283
(lp40091
sg36
(lp40092
sg181
(lp40093
sg287
(lp40094
sg74
(lp40095
sg176
(lp40096
sg145
(lp40097
sg256
(lp40098
sg80
(lp40099
sg118
(lp40100
sg344
(lp40101
sg183
(lp40102
sg59
(lp40103
sg484
(lp40104
sg83
(lp40105
sg63
(lp40106
sg42
(lp40107
I3369
asg89
(lp40108
sg91
(lp40109
sg94
(lp40110
sg96
(lp40111
sg48
(lp40112
sg99
(lp40113
sg313
(lp40114
sg223
(lp40115
sg149
(lp40116
sg230
(lp40117
sg174
(lp40118
sg116
(lp40119
sg32
(lp40120
sg429
(lp40121
sg104
(lp40122
sg106
(lp40123
sg110
(lp40124
sg20
(lp40125
sg114
(lp40126
sg216
(lp40127
sg438
(lp40128
sg440
(lp40129
sg18
(lp40130
sg178
(lp40131
sg4
(lp40132
sg6
(lp40133
sg8
(lp40134
sg221
(lp40135
sg460
(lp40136
sg124
(lp40137
sg126
(lp40138
sg341
(lp40139
sg10
(lp40140
sg40
(lp40141
sg128
(lp40142
sg130
(lp40143
sg132
(lp40144
sg14
(lp40145
sg16
(lp40146
sg350
(lp40147
sg50
(lp40148
sg138
(lp40149
sg354
(lp40150
ssS'generic'
p40151
(dp40152
g96
(lp40153
sg135
(lp40154
I2417
asg70
(lp40155
sg59
(lp40156
ssS'parti'
p40157
(dp40158
g318
(lp40159
I253
assS'began'
p40160
(dp40161
g281
(lp40162
I1831
assS'ornstein'
p40163
(dp40164
g262
(lp40165
I900
assS'maxml'
p40166
(dp40167
g72
(lp40168
I1169
assS'speaker'
p40169
(dp40170
g30
(lp40171
sg96
(lp40172
I2
asg121
(lp40173
sg10
(lp40174
ssS'lthis'
p40175
(dp40176
g329
(lp40177
sg138
(lp40178
I784
assS'romerstra'
p40179
(dp40180
g130
(lp40181
I19
assS'gielen'
p40182
(dp40183
g32
(lp40184
I3122
assS'anfarmeian'
p40185
(dp40186
g174
(lp40187
I1813
assS'absorb'
p40188
(dp40189
g74
(lp40190
sg89
(lp40191
I525
asg262
(lp40192
ssS'effect'
p40193
(dp40194
g329
(lp40195
sg78
(lp40196
sg277
(lp40197
sg283
(lp40198
sg85
(lp40199
sg36
(lp40200
sg303
(lp40201
sg74
(lp40202
sg176
(lp40203
sg80
(lp40204
sg118
(lp40205
sg295
(lp40206
sg183
(lp40207
sg484
(lp40208
sg38
(lp40209
sg83
(lp40210
sg114
(lp40211
sg124
(lp40212
sg42
(lp40213
I1158
asg87
(lp40214
sg12
(lp40215
sg94
(lp40216
sg20
(lp40217
sg18
(lp40218
sg99
(lp40219
sg68
(lp40220
sg350
(lp40221
sg116
(lp40222
sg174
(lp40223
sg293
(lp40224
sg245
(lp40225
sg318
(lp40226
sg46
(lp40227
sg102
(lp40228
sg104
(lp40229
sg106
(lp40230
sg110
(lp40231
sg96
(lp40232
sg52
(lp40233
sg22
(lp40234
sg216
(lp40235
sg438
(lp40236
sg332
(lp40237
sg178
(lp40238
sg4
(lp40239
sg6
(lp40240
sg8
(lp40241
sg34
(lp40242
sg221
(lp40243
sg460
(lp40244
sg235
(lp40245
sg126
(lp40246
sg10
(lp40247
sg63
(lp40248
sg130
(lp40249
sg132
(lp40250
sg135
(lp40251
sg50
(lp40252
sg354
(lp40253
ssS'lcorrel'
p40254
(dp40255
g384
(lp40256
I795
assS'micro'
p40257
(dp40258
g283
(lp40259
I702
assS'excruci'
p40260
(dp40261
g63
(lp40262
I2962
assS'sorter'
p40263
(dp40264
g20
(lp40265
I2100
assS'cauwenbergh'
p40266
(dp40267
g22
(lp40268
I14
assS'i'
p40269
(dp40270
g80
(lp40271
sg293
(lp40272
sg344
(lp40273
sg78
(lp40274
sg59
(lp40275
sg484
(lp40276
sg38
(lp40277
sg83
(lp40278
sg85
(lp40279
sg303
(lp40280
sg438
(lp40281
sg116
(lp40282
sg118
(lp40283
sg34
(lp40284
sg36
(lp40285
sg460
(lp40286
sg68
(lp40287
sg72
(lp40288
sg281
(lp40289
sg10
(lp40290
sg40
(lp40291
sg283
(lp40292
sg70
(lp40293
sg26
(lp40294
sg277
(lp40295
sg163
(lp40296
sg89
(lp40297
sg91
(lp40298
sg12
(lp40299
sg94
(lp40300
sg96
(lp40301
sg48
(lp40302
sg99
(lp40303
sg313
(lp40304
sg44
(lp40305
sg149
(lp40306
sg429
(lp40307
sg102
(lp40308
sg104
(lp40309
sg106
(lp40310
sg108
(lp40311
sg110
(lp40312
sg63
(lp40313
sg52
(lp40314
sg114
(lp40315
sg128
(lp40316
sg130
(lp40317
sg132
(lp40318
sg14
(lp40319
sg16
(lp40320
sg135
(lp40321
sg50
(lp40322
sg138
(lp40323
sg140
(lp40324
sg354
(lp40325
sg306
(lp40326
sg87
(lp40327
sg245
(lp40328
sg46
(lp40329
sg20
(lp40330
sg18
(lp40331
sg221
(lp40332
sg535
(lp40333
sg223
(lp40334
sg350
(lp40335
sg216
(lp40336
sg174
(lp40337
sg440
(lp40338
sg332
(lp40339
sg121
(lp40340
sg4
(lp40341
sg6
(lp40342
sg8
(lp40343
sg126
(lp40344
sg341
(lp40345
sg30
(lp40346
sg287
(lp40347
sg74
(lp40348
sg176
(lp40349
sg145
(lp40350
sg256
(lp40351
sg76
(lp40352
sg262
(lp40353
sg295
(lp40354
sg183
(lp40355
sg42
(lp40356
I763
asg230
(lp40357
sg329
(lp40358
sg32
(lp40359
sg318
(lp40360
sg178
(lp40361
sg22
(lp40362
sg181
(lp40363
sg235
(lp40364
sg384
(lp40365
sg124
(lp40366
ssS'well'
p40367
(dp40368
g124
(lp40369
sg70
(lp40370
sg26
(lp40371
sg277
(lp40372
sg163
(lp40373
sg72
(lp40374
sg68
(lp40375
sg80
(lp40376
sg85
(lp40377
sg303
(lp40378
sg30
(lp40379
sg287
(lp40380
sg74
(lp40381
sg176
(lp40382
sg145
(lp40383
sg256
(lp40384
sg76
(lp40385
sg293
(lp40386
sg295
(lp40387
sg183
(lp40388
sg484
(lp40389
sg38
(lp40390
sg83
(lp40391
sg114
(lp40392
sg63
(lp40393
sg42
(lp40394
I319
asg306
(lp40395
sg87
(lp40396
sg89
(lp40397
sg91
(lp40398
sg12
(lp40399
sg46
(lp40400
sg96
(lp40401
sg48
(lp40402
sg221
(lp40403
sg44
(lp40404
sg149
(lp40405
sg118
(lp40406
sg350
(lp40407
sg429
(lp40408
sg318
(lp40409
sg106
(lp40410
sg108
(lp40411
sg110
(lp40412
sg20
(lp40413
sg22
(lp40414
sg230
(lp40415
sg329
(lp40416
sg440
(lp40417
sg332
(lp40418
sg178
(lp40419
sg4
(lp40420
sg181
(lp40421
sg8
(lp40422
sg34
(lp40423
sg36
(lp40424
sg384
(lp40425
sg235
(lp40426
sg126
(lp40427
sg341
(lp40428
sg10
(lp40429
sg40
(lp40430
sg344
(lp40431
sg223
(lp40432
sg128
(lp40433
sg78
(lp40434
sg132
(lp40435
sg14
(lp40436
sg16
(lp40437
sg135
(lp40438
sg50
(lp40439
sg138
(lp40440
sg140
(lp40441
sg354
(lp40442
ssS'bohr'
p40443
(dp40444
g38
(lp40445
I34
asg26
(lp40446
ssS'bohm'
p40447
(dp40448
g59
(lp40449
I3388
assS'mccabe'
p40450
(dp40451
g332
(lp40452
I8
assS'undefin'
p40453
(dp40454
g344
(lp40455
sg72
(lp40456
I1143
asg293
(lp40457
ssS'glial'
p40458
(dp40459
g256
(lp40460
I538
assS'pulnix'
p40461
(dp40462
g59
(lp40463
I840
assS'kauffmann'
p40464
(dp40465
g135
(lp40466
sg138
(lp40467
I3366
assS'distanc'
p40468
(dp40469
g68
(lp40470
sg283
(lp40471
sg30
(lp40472
sg287
(lp40473
sg74
(lp40474
sg176
(lp40475
sg80
(lp40476
sg295
(lp40477
sg183
(lp40478
sg59
(lp40479
sg42
(lp40480
I1869
asg306
(lp40481
sg48
(lp40482
sg12
(lp40483
sg46
(lp40484
sg18
(lp40485
sg99
(lp40486
sg223
(lp40487
sg350
(lp40488
sg429
(lp40489
sg102
(lp40490
sg110
(lp40491
sg52
(lp40492
sg116
(lp40493
sg438
(lp40494
sg318
(lp40495
sg181
(lp40496
sg124
(lp40497
sg44
(lp40498
sg130
(lp40499
sg14
(lp40500
sg16
(lp40501
sg149
(lp40502
sg138
(lp40503
ssS'nemon'
p40504
(dp40505
g174
(lp40506
I1245
assS'sanner'
p40507
(dp40508
g230
(lp40509
I362
assS'spta'
p40510
(dp40511
g63
(lp40512
I3198
assS'hese'
p40513
(dp40514
g48
(lp40515
I698
assS'mistaken'
p40516
(dp40517
g72
(lp40518
I1873
assS'unattract'
p40519
(dp40520
g50
(lp40521
I82
assS'distant'
p40522
(dp40523
g42
(lp40524
I673
asg350
(lp40525
ssS'skill'
p40526
(dp40527
g178
(lp40528
sg26
(lp40529
sg104
(lp40530
sg132
(lp40531
I3202
asg94
(lp40532
sg99
(lp40533
sg223
(lp40534
ssS'hess'
p40535
(dp40536
g350
(lp40537
I2581
assS'kaihara'
p40538
(dp40539
g20
(lp40540
I19
assS'ofcollinear'
p40541
(dp40542
g295
(lp40543
I3845
asg183
(lp40544
ssS'feedfarward'
p40545
(dp40546
g72
(lp40547
I2072
assS'oat'
p40548
(dp40549
g78
(lp40550
I2156
assS'densiti'
p40551
(dp40552
g26
(lp40553
sg72
(lp40554
sg30
(lp40555
sg176
(lp40556
sg295
(lp40557
sg183
(lp40558
sg85
(lp40559
sg87
(lp40560
sg91
(lp40561
sg96
(lp40562
sg18
(lp40563
sg221
(lp40564
sg329
(lp40565
sg102
(lp40566
sg438
(lp40567
I1148
asg318
(lp40568
sg36
(lp40569
sg460
(lp40570
sg126
(lp40571
sg281
(lp40572
sg48
(lp40573
sg130
(lp40574
sg138
(lp40575
sg354
(lp40576
ssS'seus'
p40577
(dp40578
g10
(lp40579
I436
assS'ought'
p40580
(dp40581
g176
(lp40582
I1686
assS'warrant'
p40583
(dp40584
g104
(lp40585
I2843
asg277
(lp40586
ssS'vidv'
p40587
(dp40588
g46
(lp40589
I1736
assS'jesper'
p40590
(dp40591
g140
(lp40592
I15
assS'vanant'
p40593
(dp40594
g72
(lp40595
I2794
assS'fate'
p40596
(dp40597
g332
(lp40598
I423
assS'scientifica'
p40599
(dp40600
g96
(lp40601
I15
assS'utan'
p40602
(dp40603
g429
(lp40604
I2355
assS'munich'
p40605
(dp40606
g59
(lp40607
sg10
(lp40608
I2818
assS'sweep'
p40609
(dp40610
g14
(lp40611
I4299
asg108
(lp40612
sg83
(lp40613
ssS'kudrimoti'
p40614
(dp40615
g80
(lp40616
I2466
assS'unseen'
p40617
(dp40618
g287
(lp40619
sg126
(lp40620
I403
assS'unphys'
p40621
(dp40622
g102
(lp40623
I3193
assS'psychiatr'
p40624
(dp40625
g4
(lp40626
I769
assS'roock'
p40627
(dp40628
g281
(lp40629
I1560
assS'doher'
p40630
(dp40631
g332
(lp40632
I2287
assS'lost'
p40633
(dp40634
g114
(lp40635
I211
assS'sager'
p40636
(dp40637
g59
(lp40638
I3229
assS'necessari'
p40639
(dp40640
g70
(lp40641
sg78
(lp40642
sg281
(lp40643
sg287
(lp40644
sg74
(lp40645
sg176
(lp40646
sg256
(lp40647
sg295
(lp40648
sg183
(lp40649
sg59
(lp40650
sg484
(lp40651
sg63
(lp40652
sg42
(lp40653
I2873
asg306
(lp40654
sg91
(lp40655
sg94
(lp40656
sg535
(lp40657
sg350
(lp40658
sg429
(lp40659
sg104
(lp40660
sg106
(lp40661
sg110
(lp40662
sg178
(lp40663
sg52
(lp40664
sg230
(lp40665
sg318
(lp40666
sg121
(lp40667
sg4
(lp40668
sg181
(lp40669
sg34
(lp40670
sg36
(lp40671
sg460
(lp40672
sg68
(lp40673
sg126
(lp40674
sg341
(lp40675
sg130
(lp40676
sg132
(lp40677
sg14
(lp40678
sg16
(lp40679
sg50
(lp40680
sg354
(lp40681
ssS'lose'
p40682
(dp40683
g132
(lp40684
I3229
asg68
(lp40685
sg80
(lp40686
ssS'async'
p40687
(dp40688
g34
(lp40689
I18
assS'rote'
p40690
(dp40691
g287
(lp40692
I724
assS'page'
p40693
(dp40694
g68
(lp40695
sg70
(lp40696
sg283
(lp40697
sg145
(lp40698
sg80
(lp40699
sg293
(lp40700
sg59
(lp40701
sg85
(lp40702
sg303
(lp40703
sg96
(lp40704
sg223
(lp40705
sg429
(lp40706
sg318
(lp40707
sg121
(lp40708
sg76
(lp40709
sg34
(lp40710
sg124
(lp40711
sg341
(lp40712
sg10
(lp40713
sg40
(lp40714
sg128
(lp40715
sg130
(lp40716
sg132
(lp40717
sg50
(lp40718
sg138
(lp40719
sg140
(lp40720
I3192
assS'therein'
p40721
(dp40722
g124
(lp40723
I1202
assS'jlw'
p40724
(dp40725
g245
(lp40726
I2565
assS'didn'
p40727
(dp40728
g329
(lp40729
I2442
assS'jls'
p40730
(dp40731
g14
(lp40732
I3318
asg20
(lp40733
ssS'crrterion'
p40734
(dp40735
g110
(lp40736
I2367
assS'phenomena'
p40737
(dp40738
g118
(lp40739
sg48
(lp40740
sg36
(lp40741
sg85
(lp40742
sg12
(lp40743
I1307
asg96
(lp40744
sg18
(lp40745
sg535
(lp40746
ssS'jlx'
p40747
(dp40748
g313
(lp40749
I987
assS'gerrit'
p40750
(dp40751
g118
(lp40752
I779
assS'batteri'
p40753
(dp40754
g135
(lp40755
I346
assS'homo'
p40756
(dp40757
g106
(lp40758
I394
assS'shep'
p40759
(dp40760
g223
(lp40761
I2436
assS'cleveland'
p40762
(dp40763
g295
(lp40764
sg183
(lp40765
sg89
(lp40766
sg313
(lp40767
I1283
asg91
(lp40768
ssS'jlm'
p40769
(dp40770
g106
(lp40771
I1028
assS'home'
p40772
(dp40773
g277
(lp40774
sg128
(lp40775
sg46
(lp40776
sg94
(lp40777
sg18
(lp40778
sg138
(lp40779
I693
assS'peter'
p40780
(dp40781
g30
(lp40782
sg70
(lp40783
sg26
(lp40784
sg235
(lp40785
sg124
(lp40786
sg341
(lp40787
sg91
(lp40788
sg14
(lp40789
sg16
(lp40790
sg135
(lp40791
sg138
(lp40792
sg140
(lp40793
I3043
assS'jli'
p40794
(dp40795
g221
(lp40796
sg313
(lp40797
I990
assS'librari'
p40798
(dp40799
g26
(lp40800
sg10
(lp40801
I671
assS'reticular'
p40802
(dp40803
g350
(lp40804
I1188
assS'competitor'
p40805
(dp40806
g126
(lp40807
sg121
(lp40808
I2248
assS'broad'
p40809
(dp40810
g216
(lp40811
sg70
(lp40812
sg277
(lp40813
sg76
(lp40814
sg124
(lp40815
sg63
(lp40816
sg128
(lp40817
sg46
(lp40818
sg110
(lp40819
sg138
(lp40820
I1025
asg44
(lp40821
sg149
(lp40822
ssS'hannon'
p40823
(dp40824
g22
(lp40825
I1268
assS'overlap'
p40826
(dp40827
g30
(lp40828
sg74
(lp40829
sg76
(lp40830
sg295
(lp40831
sg183
(lp40832
sg484
(lp40833
sg38
(lp40834
sg87
(lp40835
sg245
(lp40836
sg149
(lp40837
sg116
(lp40838
sg102
(lp40839
sg110
(lp40840
sg114
(lp40841
sg216
(lp40842
sg440
(lp40843
sg332
(lp40844
sg121
(lp40845
sg235
(lp40846
sg384
(lp40847
sg68
(lp40848
sg126
(lp40849
sg344
(lp40850
sg78
(lp40851
sg135
(lp40852
sg140
(lp40853
I1251
assS'mcnulti'
p40854
(dp40855
g83
(lp40856
I2768
assS'diagno'
p40857
(dp40858
g277
(lp40859
I280
assS'estim'
p40860
(dp40861
g124
(lp40862
sg78
(lp40863
sg277
(lp40864
sg163
(lp40865
sg72
(lp40866
sg36
(lp40867
sg181
(lp40868
sg40
(lp40869
sg26
(lp40870
sg30
(lp40871
sg287
(lp40872
sg74
(lp40873
sg145
(lp40874
sg80
(lp40875
sg76
(lp40876
sg295
(lp40877
sg183
(lp40878
sg59
(lp40879
sg484
(lp40880
sg83
(lp40881
sg85
(lp40882
sg42
(lp40883
I151
asg87
(lp40884
sg89
(lp40885
sg91
(lp40886
sg12
(lp40887
sg94
(lp40888
sg96
(lp40889
sg99
(lp40890
sg313
(lp40891
sg223
(lp40892
sg230
(lp40893
sg329
(lp40894
sg116
(lp40895
sg46
(lp40896
sg102
(lp40897
sg108
(lp40898
sg110
(lp40899
sg63
(lp40900
sg52
(lp40901
sg216
(lp40902
sg438
(lp40903
sg440
(lp40904
sg318
(lp40905
sg121
(lp40906
sg4
(lp40907
sg6
(lp40908
sg8
(lp40909
sg34
(lp40910
sg221
(lp40911
sg460
(lp40912
sg235
(lp40913
sg126
(lp40914
sg281
(lp40915
sg535
(lp40916
sg128
(lp40917
sg130
(lp40918
sg132
(lp40919
sg138
(lp40920
sg140
(lp40921
sg354
(lp40922
ssS'wmin'
p40923
(dp40924
g295
(lp40925
I2737
asg183
(lp40926
ssS'wnew'
p40927
(dp40928
g341
(lp40929
I588
assS'aspart'
p40930
(dp40931
g106
(lp40932
I1211
assS'encourag'
p40933
(dp40934
g329
(lp40935
sg178
(lp40936
sg256
(lp40937
sg124
(lp40938
sg83
(lp40939
sg94
(lp40940
sg138
(lp40941
I2231
asg26
(lp40942
ssS'dui'
p40943
(dp40944
g68
(lp40945
I346
assS'journal'
p40946
(dp40947
g283
(lp40948
sg26
(lp40949
sg277
(lp40950
sg145
(lp40951
sg80
(lp40952
sg344
(lp40953
sg183
(lp40954
sg303
(lp40955
sg87
(lp40956
sg91
(lp40957
sg245
(lp40958
sg46
(lp40959
sg18
(lp40960
sg221
(lp40961
sg313
(lp40962
sg223
(lp40963
sg350
(lp40964
sg116
(lp40965
sg32
(lp40966
sg110
(lp40967
sg4
(lp40968
sg216
(lp40969
sg118
(lp40970
sg440
(lp40971
sg318
(lp40972
sg22
(lp40973
sg8
(lp40974
sg460
(lp40975
sg235
(lp40976
sg10
(lp40977
sg132
(lp40978
sg14
(lp40979
sg149
(lp40980
sg50
(lp40981
I1752
assS'reza'
p40982
(dp40983
g99
(lp40984
I11
assS'lissom'
p40985
(dp40986
g149
(lp40987
I438
assS'parasit'
p40988
(dp40989
g245
(lp40990
I1236
asg70
(lp40991
sg22
(lp40992
ssS'ofneur'
p40993
(dp40994
g130
(lp40995
I3193
assS'offset'
p40996
(dp40997
g216
(lp40998
sg174
(lp40999
sg318
(lp41000
sg256
(lp41001
sg235
(lp41002
sg14
(lp41003
sg16
(lp41004
sg135
(lp41005
I90
asg96
(lp41006
ssS'instinct'
p41007
(dp41008
g116
(lp41009
I2483
assS'monetari'
p41010
(dp41011
g114
(lp41012
I651
assS'unvoic'
p41013
(dp41014
g74
(lp41015
I2434
assS'quarter'
p41016
(dp41017
g216
(lp41018
I2288
asg178
(lp41019
sg126
(lp41020
ssS'mlps'
p41021
(dp41022
g87
(lp41023
sg10
(lp41024
sg354
(lp41025
I487
assS'dun'
p41026
(dp41027
g85
(lp41028
I4338
assS'freedom'
p41029
(dp41030
g32
(lp41031
sg121
(lp41032
I2297
asg181
(lp41033
sg76
(lp41034
sg460
(lp41035
sg83
(lp41036
sg429
(lp41037
ssS'pz'
p41038
(dp41039
g87
(lp41040
I27
assS'iixll'
p41041
(dp41042
g96
(lp41043
I1029
assS'hiooenunit'
p41044
(dp41045
g110
(lp41046
I1120
assS'mlrumum'
p41047
(dp41048
g72
(lp41049
I2969
assS'territori'
p41050
(dp41051
g176
(lp41052
I365
assS'washington'
p41053
(dp41054
g68
(lp41055
sg6
(lp41056
I19
assS'diagon'
p41057
(dp41058
g318
(lp41059
sg26
(lp41060
sg8
(lp41061
sg295
(lp41062
sg183
(lp41063
sg235
(lp41064
sg38
(lp41065
sg34
(lp41066
sg163
(lp41067
sg91
(lp41068
sg130
(lp41069
I1781
asg96
(lp41070
sg221
(lp41071
ssS'primat'
p41072
(dp41073
g176
(lp41074
sg4
(lp41075
sg181
(lp41076
sg245
(lp41077
sg149
(lp41078
I2922
asg350
(lp41079
ssS'receptor'
p41080
(dp41081
g176
(lp41082
sg256
(lp41083
sg106
(lp41084
I1205
asg149
(lp41085
sg535
(lp41086
sg4
(lp41087
ssS'titterington'
p41088
(dp41089
g354
(lp41090
I2965
assS'gerbil'
p41091
(dp41092
g80
(lp41093
I950
assS'whom'
p41094
(dp41095
g132
(lp41096
sg135
(lp41097
sg140
(lp41098
I233
asg318
(lp41099
ssS'inner'
p41100
(dp41101
g174
(lp41102
sg145
(lp41103
sg256
(lp41104
sg8
(lp41105
sg34
(lp41106
sg63
(lp41107
sg130
(lp41108
I2689
asg313
(lp41109
sg44
(lp41110
sg350
(lp41111
ssS'suzuki'
p41112
(dp41113
g42
(lp41114
I10
assS'elata'
p41115
(dp41116
g108
(lp41117
I208
assS'eak'
p41118
(dp41119
g6
(lp41120
I1042
assS'north'
p41121
(dp41122
g32
(lp41123
sg318
(lp41124
sg22
(lp41125
sg281
(lp41126
sg42
(lp41127
I2565
asg283
(lp41128
sg91
(lp41129
sg350
(lp41130
ssS'hr'
p41131
(dp41132
g183
(lp41133
I5393
asg38
(lp41134
ssS'hs'
p41135
(dp41136
g277
(lp41137
I3041
assS'hp'
p41138
(dp41139
g87
(lp41140
I1847
assS'eab'
p41141
(dp41142
g149
(lp41143
I857
assS'hv'
p41144
(dp41145
g116
(lp41146
sg221
(lp41147
I978
assS'triangular'
p41148
(dp41149
g163
(lp41150
sg295
(lp41151
sg183
(lp41152
sg42
(lp41153
I2865
asg130
(lp41154
sg14
(lp41155
sg16
(lp41156
ssS'ht'
p41157
(dp41158
g230
(lp41159
sg183
(lp41160
sg108
(lp41161
I1484
asg221
(lp41162
sg341
(lp41163
ssS'histogram'
p41164
(dp41165
g318
(lp41166
sg6
(lp41167
I1010
asg181
(lp41168
sg295
(lp41169
sg183
(lp41170
sg283
(lp41171
sg18
(lp41172
ssS'hj'
p41173
(dp41174
g12
(lp41175
I2663
assS'franklinstr'
p41176
(dp41177
g34
(lp41178
I15
assS'neutral'
p41179
(dp41180
g221
(lp41181
sg6
(lp41182
I1618
asg293
(lp41183
ssS'hi'
p41184
(dp41185
g329
(lp41186
sg26
(lp41187
sg34
(lp41188
sg183
(lp41189
sg384
(lp41190
sg85
(lp41191
sg344
(lp41192
sg130
(lp41193
sg132
(lp41194
I2365
asg108
(lp41195
sg221
(lp41196
sg313
(lp41197
sg52
(lp41198
ssS'hn'
p41199
(dp41200
g14
(lp41201
I2712
asg108
(lp41202
sg26
(lp41203
ssS'ho'
p41204
(dp41205
g183
(lp41206
sg135
(lp41207
I245
asg68
(lp41208
sg36
(lp41209
ssS'hl'
p41210
(dp41211
g245
(lp41212
I2515
asg34
(lp41213
sg38
(lp41214
sg85
(lp41215
ssS'hm'
p41216
(dp41217
g72
(lp41218
sg281
(lp41219
I2006
assS'hb'
p41220
(dp41221
g12
(lp41222
sg281
(lp41223
sg130
(lp41224
I1533
assS'hc'
p41225
(dp41226
g59
(lp41227
I3311
assS'eas'
p41228
(dp41229
g14
(lp41230
sg16
(lp41231
I1863
asg176
(lp41232
sg10
(lp41233
sg52
(lp41234
ssS'ear'
p41235
(dp41236
g174
(lp41237
I189
asg350
(lp41238
ssS'hf'
p41239
(dp41240
g221
(lp41241
sg26
(lp41242
sg130
(lp41243
I1542
assS'nicol'
p41244
(dp41245
g318
(lp41246
sg50
(lp41247
I11
assS'hd'
p41248
(dp41249
g80
(lp41250
sg85
(lp41251
I468
assS'strabismus'
p41252
(dp41253
g48
(lp41254
sg149
(lp41255
I1530
assS'llfjt'
p41256
(dp41257
g230
(lp41258
I1374
assS'ttansmiss'
p41259
(dp41260
g106
(lp41261
I2571
assS'kessel'
p41262
(dp41263
g14
(lp41264
sg16
(lp41265
I2612
assS'dobbin'
p41266
(dp41267
g70
(lp41268
I2663
assS'asynchron'
p41269
(dp41270
g132
(lp41271
sg14
(lp41272
I4744
asg83
(lp41273
ssS'limit'
p41274
(dp41275
g329
(lp41276
sg78
(lp41277
sg163
(lp41278
sg281
(lp41279
sg283
(lp41280
sg36
(lp41281
sg181
(lp41282
sg26
(lp41283
sg287
(lp41284
sg74
(lp41285
sg176
(lp41286
sg76
(lp41287
sg262
(lp41288
sg295
(lp41289
sg183
(lp41290
sg59
(lp41291
sg484
(lp41292
sg38
(lp41293
sg124
(lp41294
sg42
(lp41295
I2114
asg306
(lp41296
sg245
(lp41297
sg96
(lp41298
sg68
(lp41299
sg99
(lp41300
sg535
(lp41301
sg44
(lp41302
sg149
(lp41303
sg116
(lp41304
sg174
(lp41305
sg293
(lp41306
sg350
(lp41307
sg318
(lp41308
sg102
(lp41309
sg104
(lp41310
sg108
(lp41311
sg110
(lp41312
sg52
(lp41313
sg230
(lp41314
sg438
(lp41315
sg332
(lp41316
sg121
(lp41317
sg4
(lp41318
sg6
(lp41319
sg8
(lp41320
sg34
(lp41321
sg221
(lp41322
sg384
(lp41323
sg235
(lp41324
sg126
(lp41325
sg341
(lp41326
sg10
(lp41327
sg40
(lp41328
sg344
(lp41329
sg128
(lp41330
sg130
(lp41331
sg132
(lp41332
sg14
(lp41333
sg16
(lp41334
sg135
(lp41335
sg50
(lp41336
sg138
(lp41337
sg140
(lp41338
sg354
(lp41339
ssS'indefinit'
p41340
(dp41341
g42
(lp41342
I267
assS'desimon'
p41343
(dp41344
g18
(lp41345
I172
assS'steepest'
p41346
(dp41347
g46
(lp41348
sg306
(lp41349
sg283
(lp41350
sg99
(lp41351
I3458
asg8
(lp41352
ssS'strategi'
p41353
(dp41354
g30
(lp41355
sg74
(lp41356
sg59
(lp41357
sg293
(lp41358
sg110
(lp41359
sg384
(lp41360
sg68
(lp41361
sg341
(lp41362
sg36
(lp41363
sg245
(lp41364
sg306
(lp41365
sg89
(lp41366
sg52
(lp41367
sg130
(lp41368
sg132
(lp41369
sg18
(lp41370
sg99
(lp41371
sg138
(lp41372
sg140
(lp41373
sg354
(lp41374
I110
assS'alilay'
p41375
(dp41376
g121
(lp41377
I1835
assS'display'
p41378
(dp41379
g216
(lp41380
sg118
(lp41381
sg318
(lp41382
sg70
(lp41383
sg26
(lp41384
sg293
(lp41385
sg78
(lp41386
sg130
(lp41387
sg281
(lp41388
sg303
(lp41389
sg42
(lp41390
I1509
asg429
(lp41391
sg104
(lp41392
sg94
(lp41393
sg99
(lp41394
sg223
(lp41395
ssS'zoubinqpsych'
p41396
(dp41397
g313
(lp41398
I18
assS'tikochinski'
p41399
(dp41400
g130
(lp41401
I220
assS'reciproc'
p41402
(dp41403
g245
(lp41404
sg83
(lp41405
sg350
(lp41406
sg256
(lp41407
sg149
(lp41408
I644
assS'evalu'
p41409
(dp41410
g283
(lp41411
sg277
(lp41412
sg145
(lp41413
sg76
(lp41414
sg293
(lp41415
sg295
(lp41416
sg183
(lp41417
sg59
(lp41418
sg484
(lp41419
sg303
(lp41420
sg306
(lp41421
sg87
(lp41422
sg89
(lp41423
sg91
(lp41424
sg245
(lp41425
sg94
(lp41426
sg20
(lp41427
sg48
(lp41428
sg313
(lp41429
sg223
(lp41430
sg46
(lp41431
sg102
(lp41432
sg116
(lp41433
sg318
(lp41434
sg4
(lp41435
sg34
(lp41436
sg36
(lp41437
sg124
(lp41438
sg126
(lp41439
sg281
(lp41440
sg10
(lp41441
sg344
(lp41442
sg130
(lp41443
sg132
(lp41444
sg138
(lp41445
I434
assS'neurochess'
p41446
(dp41447
g132
(lp41448
I33
assS'restecg'
p41449
(dp41450
g91
(lp41451
I2092
assS'xabx'
p41452
(dp41453
g332
(lp41454
I2289
assS'eric'
p41455
(dp41456
g429
(lp41457
sg283
(lp41458
sg8
(lp41459
I21
assS'diego'
p41460
(dp41461
g116
(lp41462
sg18
(lp41463
sg8
(lp41464
sg110
(lp41465
sg484
(lp41466
sg63
(lp41467
sg91
(lp41468
sg132
(lp41469
sg46
(lp41470
sg106
(lp41471
I320
asg108
(lp41472
sg50
(lp41473
sg96
(lp41474
sg350
(lp41475
ssS'barto'
p41476
(dp41477
g89
(lp41478
I459
asg83
(lp41479
ssS'bostonldordrechtilondon'
p41480
(dp41481
g223
(lp41482
I3114
assS'ghahramani'
p41483
(dp41484
g74
(lp41485
sg99
(lp41486
I3472
asg313
(lp41487
sg91
(lp41488
ssS'futur'
p41489
(dp41490
g70
(lp41491
sg26
(lp41492
sg277
(lp41493
sg287
(lp41494
sg295
(lp41495
sg183
(lp41496
sg83
(lp41497
sg42
(lp41498
I2132
asg306
(lp41499
sg87
(lp41500
sg89
(lp41501
sg91
(lp41502
sg245
(lp41503
sg94
(lp41504
sg20
(lp41505
sg48
(lp41506
sg44
(lp41507
sg149
(lp41508
sg12
(lp41509
sg102
(lp41510
sg116
(lp41511
sg34
(lp41512
sg36
(lp41513
sg460
(lp41514
sg124
(lp41515
sg281
(lp41516
sg10
(lp41517
sg40
(lp41518
sg344
(lp41519
sg78
(lp41520
sg135
(lp41521
sg354
(lp41522
ssS'rememb'
p41523
(dp41524
g18
(lp41525
sg332
(lp41526
I263
asg178
(lp41527
sg80
(lp41528
sg85
(lp41529
ssS'adxt'
p41530
(dp41531
g76
(lp41532
I2268
assS'pedroni'
p41533
(dp41534
g20
(lp41535
I2596
assS'ferdinando'
p41536
(dp41537
g99
(lp41538
I17
assS'stat'
p41539
(dp41540
g295
(lp41541
sg183
(lp41542
sg74
(lp41543
sg126
(lp41544
I2956
assS'star'
p41545
(dp41546
g235
(lp41547
I2371
assS'agmon'
p41548
(dp41549
g6
(lp41550
I1360
assS'shumeet'
p41551
(dp41552
g277
(lp41553
I19
assS'stan'
p41554
(dp41555
g32
(lp41556
sg128
(lp41557
I1040
assS'bcm'
p41558
(dp41559
g438
(lp41560
I1414
asg318
(lp41561
ssS'hnc'
p41562
(dp41563
g118
(lp41564
I2381
assS'sphinx'
p41565
(dp41566
g87
(lp41567
I2570
assS'shunt'
p41568
(dp41569
g535
(lp41570
I307
assS'katagiri'
p41571
(dp41572
g440
(lp41573
I794
assS'extran'
p41574
(dp41575
g429
(lp41576
sg332
(lp41577
sg138
(lp41578
I1685
asg63
(lp41579
ssS'inset'
p41580
(dp41581
g70
(lp41582
sg6
(lp41583
I1018
assS'littman'
p41584
(dp41585
g89
(lp41586
I2327
asg293
(lp41587
ssS'olyzo'
p41588
(dp41589
g108
(lp41590
I1351
assS'decemb'
p41591
(dp41592
g223
(lp41593
sg140
(lp41594
I253
assS'bcs'
p41595
(dp41596
g118
(lp41597
I824
assS'symmeter'
p41598
(dp41599
g74
(lp41600
I2108
assS'whose'
p41601
(dp41602
g283
(lp41603
sg74
(lp41604
sg145
(lp41605
sg76
(lp41606
sg262
(lp41607
sg295
(lp41608
sg183
(lp41609
sg38
(lp41610
sg83
(lp41611
sg306
(lp41612
sg89
(lp41613
sg91
(lp41614
sg46
(lp41615
sg96
(lp41616
sg48
(lp41617
sg223
(lp41618
sg429
(lp41619
sg102
(lp41620
sg104
(lp41621
sg110
(lp41622
sg216
(lp41623
sg329
(lp41624
sg32
(lp41625
sg318
(lp41626
sg181
(lp41627
sg8
(lp41628
sg34
(lp41629
sg68
(lp41630
sg40
(lp41631
sg44
(lp41632
sg128
(lp41633
sg14
(lp41634
sg16
(lp41635
sg50
(lp41636
sg138
(lp41637
sg354
(lp41638
I629
assS'accur'
p41639
(dp41640
g70
(lp41641
sg78
(lp41642
sg277
(lp41643
sg163
(lp41644
sg287
(lp41645
sg74
(lp41646
sg145
(lp41647
sg76
(lp41648
sg344
(lp41649
sg183
(lp41650
sg85
(lp41651
sg89
(lp41652
sg245
(lp41653
sg46
(lp41654
sg313
(lp41655
sg223
(lp41656
sg350
(lp41657
sg94
(lp41658
sg63
(lp41659
sg230
(lp41660
sg440
(lp41661
sg318
(lp41662
sg121
(lp41663
sg22
(lp41664
sg6
(lp41665
sg384
(lp41666
sg281
(lp41667
sg44
(lp41668
sg130
(lp41669
sg132
(lp41670
sg14
(lp41671
sg16
(lp41672
I126
assS'fokker'
p41673
(dp41674
g262
(lp41675
I924
assS'tobia'
p41676
(dp41677
g256
(lp41678
I2106
assS'miall'
p41679
(dp41680
g12
(lp41681
sg50
(lp41682
I1602
assS'uiti'
p41683
(dp41684
g68
(lp41685
I348
assS'aut'
p41686
(dp41687
g94
(lp41688
I847
asg48
(lp41689
ssS'preprocess'
p41690
(dp41691
g283
(lp41692
sg78
(lp41693
sg59
(lp41694
sg42
(lp41695
I2579
asg245
(lp41696
sg63
(lp41697
sg44
(lp41698
sg114
(lp41699
ssS'neuromodul'
p41700
(dp41701
g4
(lp41702
I144
assS'eori'
p41703
(dp41704
g287
(lp41705
I3456
assS'ropt'
p41706
(dp41707
g36
(lp41708
I2104
assS'voic'
p41709
(dp41710
g174
(lp41711
I2450
asg74
(lp41712
sg22
(lp41713
ssS'govern'
p41714
(dp41715
g216
(lp41716
sg116
(lp41717
sg4
(lp41718
sg8
(lp41719
sg68
(lp41720
sg126
(lp41721
sg429
(lp41722
sg104
(lp41723
sg14
(lp41724
sg16
(lp41725
I891
assS'affect'
p41726
(dp41727
g216
(lp41728
sg350
(lp41729
sg332
(lp41730
sg76
(lp41731
sg295
(lp41732
sg183
(lp41733
sg460
(lp41734
sg68
(lp41735
sg116
(lp41736
sg303
(lp41737
sg102
(lp41738
sg283
(lp41739
sg12
(lp41740
sg104
(lp41741
sg354
(lp41742
I1799
asg99
(lp41743
sg138
(lp41744
sg149
(lp41745
ssS'saard'
p41746
(dp41747
g36
(lp41748
I1407
assS'lefthand'
p41749
(dp41750
g124
(lp41751
I877
assS'vast'
p41752
(dp41753
g132
(lp41754
I1712
asg87
(lp41755
sg126
(lp41756
sg83
(lp41757
sg223
(lp41758
ssS'demerit'
p41759
(dp41760
g14
(lp41761
I4601
assS'natl'
p41762
(dp41763
g12
(lp41764
sg438
(lp41765
I2441
asg106
(lp41766
sg176
(lp41767
sg174
(lp41768
ssS'vector'
p41769
(dp41770
g124
(lp41771
sg70
(lp41772
sg78
(lp41773
sg163
(lp41774
sg116
(lp41775
sg281
(lp41776
sg36
(lp41777
sg40
(lp41778
sg26
(lp41779
sg30
(lp41780
sg287
(lp41781
sg145
(lp41782
sg80
(lp41783
sg293
(lp41784
sg295
(lp41785
sg183
(lp41786
sg59
(lp41787
sg484
(lp41788
sg38
(lp41789
sg63
(lp41790
sg306
(lp41791
sg87
(lp41792
sg91
(lp41793
sg12
(lp41794
sg46
(lp41795
sg96
(lp41796
sg48
(lp41797
sg99
(lp41798
sg313
(lp41799
sg44
(lp41800
sg149
(lp41801
sg230
(lp41802
sg329
(lp41803
sg32
(lp41804
sg318
(lp41805
sg102
(lp41806
sg108
(lp41807
sg110
(lp41808
sg20
(lp41809
sg52
(lp41810
sg216
(lp41811
sg438
(lp41812
I187
asg440
(lp41813
sg18
(lp41814
sg178
(lp41815
sg181
(lp41816
sg8
(lp41817
sg34
(lp41818
sg221
(lp41819
sg460
(lp41820
sg235
(lp41821
sg72
(lp41822
sg341
(lp41823
sg10
(lp41824
sg535
(lp41825
sg344
(lp41826
sg223
(lp41827
sg130
(lp41828
sg132
(lp41829
sg14
(lp41830
sg16
(lp41831
sg350
(lp41832
sg50
(lp41833
sg138
(lp41834
sg354
(lp41835
ssS'cholesterol'
p41836
(dp41837
g91
(lp41838
I2114
assS'tsioutsia'
p41839
(dp41840
g8
(lp41841
I11
assS'markov'
p41842
(dp41843
g30
(lp41844
sg174
(lp41845
sg440
(lp41846
sg178
(lp41847
sg76
(lp41848
sg293
(lp41849
sg460
(lp41850
sg124
(lp41851
sg72
(lp41852
sg74
(lp41853
sg83
(lp41854
sg306
(lp41855
sg87
(lp41856
sg89
(lp41857
sg94
(lp41858
sg96
(lp41859
sg18
(lp41860
sg70
(lp41861
sg354
(lp41862
I78
assS'initialis'
p41863
(dp41864
g283
(lp41865
sg87
(lp41866
sg126
(lp41867
I1490
asg124
(lp41868
ssS'upp'
p41869
(dp41870
g104
(lp41871
I3046
assS'bevan'
p41872
(dp41873
g283
(lp41874
I273
assS'natarajan'
p41875
(dp41876
g287
(lp41877
I415
assS'ruoer'
p41878
(dp41879
g34
(lp41880
I304
assS'diseas'
p41881
(dp41882
g91
(lp41883
I42
asg277
(lp41884
ssS'instantan'
p41885
(dp41886
g83
(lp41887
sg262
(lp41888
sg70
(lp41889
sg256
(lp41890
sg350
(lp41891
I624
assS'interspik'
p41892
(dp41893
g262
(lp41894
I996
assS'cesar'
p41895
(dp41896
g96
(lp41897
I9
assS'illumin'
p41898
(dp41899
g118
(lp41900
sg256
(lp41901
sg223
(lp41902
I3327
asg293
(lp41903
ssS'even'
p41904
(dp41905
g70
(lp41906
sg26
(lp41907
sg277
(lp41908
sg163
(lp41909
sg72
(lp41910
sg350
(lp41911
sg176
(lp41912
sg145
(lp41913
sg256
(lp41914
sg80
(lp41915
sg262
(lp41916
sg183
(lp41917
sg59
(lp41918
sg484
(lp41919
sg83
(lp41920
sg85
(lp41921
sg303
(lp41922
sg42
(lp41923
I406
asg306
(lp41924
sg87
(lp41925
sg89
(lp41926
sg91
(lp41927
sg12
(lp41928
sg94
(lp41929
sg48
(lp41930
sg99
(lp41931
sg313
(lp41932
sg44
(lp41933
sg149
(lp41934
sg329
(lp41935
sg293
(lp41936
sg245
(lp41937
sg429
(lp41938
sg46
(lp41939
sg102
(lp41940
sg104
(lp41941
sg108
(lp41942
sg110
(lp41943
sg63
(lp41944
sg114
(lp41945
sg230
(lp41946
sg174
(lp41947
sg32
(lp41948
sg235
(lp41949
sg34
(lp41950
sg221
(lp41951
sg384
(lp41952
sg68
(lp41953
sg126
(lp41954
sg341
(lp41955
sg10
(lp41956
sg40
(lp41957
sg287
(lp41958
sg223
(lp41959
sg128
(lp41960
sg36
(lp41961
sg132
(lp41962
sg14
(lp41963
sg16
(lp41964
sg135
(lp41965
sg50
(lp41966
sg460
(lp41967
sg140
(lp41968
sg354
(lp41969
ssS'jordon'
p41970
(dp41971
g313
(lp41972
I277
assS'fkiyi'
p41973
(dp41974
g18
(lp41975
I1712
assS'asif'
p41976
(dp41977
g83
(lp41978
I2772
assS'ned'
p41979
(dp41980
g132
(lp41981
I3327
assS'nee'
p41982
(dp41983
g128
(lp41984
I15
assS'nec'
p41985
(dp41986
g30
(lp41987
sg121
(lp41988
sg128
(lp41989
I26
assS'ofthes'
p41990
(dp41991
g128
(lp41992
I1253
assS'cheng'
p41993
(dp41994
g80
(lp41995
I2315
assS'new'
p41996
(dp41997
g68
(lp41998
sg70
(lp41999
sg78
(lp42000
sg163
(lp42001
sg72
(lp42002
sg281
(lp42003
sg283
(lp42004
sg303
(lp42005
sg26
(lp42006
sg30
(lp42007
sg287
(lp42008
sg74
(lp42009
sg176
(lp42010
sg76
(lp42011
sg118
(lp42012
sg295
(lp42013
sg183
(lp42014
sg59
(lp42015
sg80
(lp42016
sg83
(lp42017
sg85
(lp42018
sg63
(lp42019
sg42
(lp42020
I69
asg306
(lp42021
sg87
(lp42022
sg89
(lp42023
sg91
(lp42024
sg12
(lp42025
sg94
(lp42026
sg96
(lp42027
sg18
(lp42028
sg99
(lp42029
sg313
(lp42030
sg44
(lp42031
sg149
(lp42032
sg329
(lp42033
sg293
(lp42034
sg460
(lp42035
sg178
(lp42036
sg245
(lp42037
sg429
(lp42038
sg46
(lp42039
sg102
(lp42040
sg104
(lp42041
sg106
(lp42042
sg108
(lp42043
sg110
(lp42044
sg20
(lp42045
sg52
(lp42046
sg114
(lp42047
sg230
(lp42048
sg174
(lp42049
sg440
(lp42050
sg318
(lp42051
sg121
(lp42052
sg4
(lp42053
sg6
(lp42054
sg8
(lp42055
sg34
(lp42056
sg221
(lp42057
sg384
(lp42058
sg124
(lp42059
sg126
(lp42060
sg341
(lp42061
sg10
(lp42062
sg40
(lp42063
sg344
(lp42064
sg223
(lp42065
sg128
(lp42066
sg130
(lp42067
sg132
(lp42068
sg14
(lp42069
sg138
(lp42070
sg140
(lp42071
sg354
(lp42072
ssS'net'
p42073
(dp42074
g283
(lp42075
sg26
(lp42076
sg277
(lp42077
sg72
(lp42078
sg287
(lp42079
sg176
(lp42080
sg76
(lp42081
sg262
(lp42082
sg59
(lp42083
sg484
(lp42084
sg87
(lp42085
sg89
(lp42086
sg535
(lp42087
sg104
(lp42088
sg106
(lp42089
I914
asg108
(lp42090
sg110
(lp42091
sg63
(lp42092
sg114
(lp42093
sg440
(lp42094
sg332
(lp42095
sg4
(lp42096
sg8
(lp42097
sg34
(lp42098
sg460
(lp42099
sg124
(lp42100
sg126
(lp42101
sg10
(lp42102
sg128
(lp42103
sg50
(lp42104
sg138
(lp42105
ssS'ever'
p42106
(dp42107
g329
(lp42108
sg70
(lp42109
sg295
(lp42110
sg183
(lp42111
sg78
(lp42112
sg132
(lp42113
I3290
asg46
(lp42114
sg48
(lp42115
ssS'mee'
p42116
(dp42117
g59
(lp42118
I2173
assS'med'
p42119
(dp42120
g99
(lp42121
sg484
(lp42122
sg50
(lp42123
I503
assS'elimin'
p42124
(dp42125
g30
(lp42126
sg32
(lp42127
sg283
(lp42128
sg78
(lp42129
sg295
(lp42130
sg183
(lp42131
sg126
(lp42132
sg38
(lp42133
sg130
(lp42134
I626
asg96
(lp42135
sg114
(lp42136
sg110
(lp42137
sg63
(lp42138
sg52
(lp42139
sg149
(lp42140
ssS'mea'
p42141
(dp42142
g74
(lp42143
sg99
(lp42144
sg4
(lp42145
sg128
(lp42146
I1549
assS'behavior'
p42147
(dp42148
g40
(lp42149
sg256
(lp42150
sg80
(lp42151
sg262
(lp42152
sg78
(lp42153
sg38
(lp42154
sg85
(lp42155
sg303
(lp42156
sg306
(lp42157
sg89
(lp42158
sg91
(lp42159
sg12
(lp42160
sg46
(lp42161
sg18
(lp42162
sg99
(lp42163
sg535
(lp42164
sg350
(lp42165
sg116
(lp42166
sg329
(lp42167
sg429
(lp42168
sg102
(lp42169
sg104
(lp42170
sg110
(lp42171
sg216
(lp42172
sg438
(lp42173
I1068
asg318
(lp42174
sg4
(lp42175
sg181
(lp42176
sg8
(lp42177
sg34
(lp42178
sg36
(lp42179
sg293
(lp42180
sg126
(lp42181
sg281
(lp42182
sg313
(lp42183
ssS'meb'
p42184
(dp42185
g59
(lp42186
I2497
assS'rnidbrain'
p42187
(dp42188
g350
(lp42189
I1001
assS'mel'
p42190
(dp42191
g96
(lp42192
I1738
asg181
(lp42193
ssS'never'
p42194
(dp42195
g329
(lp42196
sg440
(lp42197
sg110
(lp42198
sg181
(lp42199
sg235
(lp42200
sg183
(lp42201
sg126
(lp42202
sg74
(lp42203
sg118
(lp42204
sg42
(lp42205
I679
asg89
(lp42206
sg91
(lp42207
sg128
(lp42208
sg36
(lp42209
sg50
(lp42210
ssS'drew'
p42211
(dp42212
g138
(lp42213
I2286
assS'meu'
p42214
(dp42215
g283
(lp42216
I390
assS'met'
p42217
(dp42218
g344
(lp42219
sg178
(lp42220
I1609
assS'mer'
p42221
(dp42222
g59
(lp42223
I2504
assS'cardboard'
p42224
(dp42225
g181
(lp42226
I1748
assS'dree'
p42227
(dp42228
g59
(lp42229
I14
assS'liebler'
p42230
(dp42231
g329
(lp42232
I647
assS'interpret'
p42233
(dp42234
g329
(lp42235
sg26
(lp42236
sg181
(lp42237
sg74
(lp42238
sg118
(lp42239
sg295
(lp42240
sg183
(lp42241
sg59
(lp42242
sg38
(lp42243
sg85
(lp42244
sg306
(lp42245
sg91
(lp42246
sg535
(lp42247
sg223
(lp42248
sg174
(lp42249
sg429
(lp42250
sg318
(lp42251
sg178
(lp42252
sg438
(lp42253
I1241
asg440
(lp42254
sg332
(lp42255
sg121
(lp42256
sg4
(lp42257
sg6
(lp42258
sg293
(lp42259
sg126
(lp42260
sg130
(lp42261
sg132
(lp42262
sg50
(lp42263
sg138
(lp42264
ssS'wacaa'
p42265
(dp42266
g140
(lp42267
I2039
assS'discretis'
p42268
(dp42269
g124
(lp42270
sg126
(lp42271
I1104
assS'drs'
p42272
(dp42273
g106
(lp42274
I2505
asg48
(lp42275
ssS'drw'
p42276
(dp42277
g104
(lp42278
I1451
assS'jame'
p42279
(dp42280
g46
(lp42281
I11
asg10
(lp42282
ssS'cbio'
p42283
(dp42284
g26
(lp42285
sg235
(lp42286
I295
assS'loop'
p42287
(dp42288
g230
(lp42289
sg30
(lp42290
sg70
(lp42291
sg145
(lp42292
sg8
(lp42293
sg34
(lp42294
sg68
(lp42295
sg10
(lp42296
sg63
(lp42297
sg42
(lp42298
I1553
asg245
(lp42299
sg14
(lp42300
sg16
(lp42301
sg135
(lp42302
sg20
(lp42303
sg44
(lp42304
ssS'permit'
p42305
(dp42306
g145
(lp42307
sg4
(lp42308
sg181
(lp42309
sg124
(lp42310
sg10
(lp42311
sg44
(lp42312
sg14
(lp42313
sg16
(lp42314
I1682
asg52
(lp42315
ssS'joshua'
p42316
(dp42317
g74
(lp42318
I6
assS'matocnt'
p42319
(dp42320
g277
(lp42321
I1986
assS'rosenberg'
p42322
(dp42323
g114
(lp42324
I893
assS'immin'
p42325
(dp42326
g78
(lp42327
I64
asg70
(lp42328
ssS'tertiari'
p42329
(dp42330
g26
(lp42331
I245
assS'avenu'
p42332
(dp42333
g12
(lp42334
sg87
(lp42335
sg44
(lp42336
I2447
assS'ackley'
p42337
(dp42338
g26
(lp42339
I3362
assS'instabl'
p42340
(dp42341
g34
(lp42342
sg121
(lp42343
sg384
(lp42344
sg89
(lp42345
I1295
assS'jpm'
p42346
(dp42347
g329
(lp42348
I752
assS'themodel'
p42349
(dp42350
g332
(lp42351
I744
assS'cybenko'
p42352
(dp42353
g38
(lp42354
I3308
assS'lwr'
p42355
(dp42356
g89
(lp42357
sg313
(lp42358
I1276
assS'leaki'
p42359
(dp42360
g174
(lp42361
I1252
asg332
(lp42362
sg118
(lp42363
sg262
(lp42364
sg176
(lp42365
sg350
(lp42366
ssS'kixi'
p42367
(dp42368
g130
(lp42369
I1461
assS'bike'
p42370
(dp42371
g174
(lp42372
I1929
asg181
(lp42373
ssS'call'
p42374
(dp42375
g78
(lp42376
sg303
(lp42377
sg26
(lp42378
sg30
(lp42379
sg287
(lp42380
sg74
(lp42381
sg145
(lp42382
sg256
(lp42383
sg344
(lp42384
sg183
(lp42385
sg484
(lp42386
sg83
(lp42387
sg85
(lp42388
sg63
(lp42389
sg42
(lp42390
I308
asg306
(lp42391
sg89
(lp42392
sg12
(lp42393
sg94
(lp42394
sg96
(lp42395
sg99
(lp42396
sg535
(lp42397
sg223
(lp42398
sg149
(lp42399
sg118
(lp42400
sg350
(lp42401
sg429
(lp42402
sg46
(lp42403
sg102
(lp42404
sg104
(lp42405
sg110
(lp42406
sg20
(lp42407
sg216
(lp42408
sg174
(lp42409
sg32
(lp42410
sg318
(lp42411
sg121
(lp42412
sg181
(lp42413
sg6
(lp42414
sg34
(lp42415
sg36
(lp42416
sg384
(lp42417
sg124
(lp42418
sg72
(lp42419
sg341
(lp42420
sg10
(lp42421
sg40
(lp42422
sg128
(lp42423
sg130
(lp42424
sg132
(lp42425
sg14
(lp42426
sg16
(lp42427
sg135
(lp42428
sg460
(lp42429
sg140
(lp42430
sg354
(lp42431
ssS'armi'
p42432
(dp42433
g438
(lp42434
I2374
assS'umass'
p42435
(dp42436
g83
(lp42437
I19
assS'lwa'
p42438
(dp42439
g140
(lp42440
I568
assS'type'
p42441
(dp42442
g283
(lp42443
sg70
(lp42444
sg26
(lp42445
sg72
(lp42446
sg287
(lp42447
sg74
(lp42448
sg176
(lp42449
sg80
(lp42450
sg118
(lp42451
sg78
(lp42452
sg59
(lp42453
sg484
(lp42454
sg303
(lp42455
sg42
(lp42456
I230
asg89
(lp42457
sg91
(lp42458
sg12
(lp42459
sg94
(lp42460
sg96
(lp42461
sg48
(lp42462
sg535
(lp42463
sg223
(lp42464
sg149
(lp42465
sg116
(lp42466
sg174
(lp42467
sg293
(lp42468
sg429
(lp42469
sg318
(lp42470
sg104
(lp42471
sg110
(lp42472
sg52
(lp42473
sg114
(lp42474
sg230
(lp42475
sg438
(lp42476
sg32
(lp42477
sg332
(lp42478
sg178
(lp42479
sg4
(lp42480
sg181
(lp42481
sg235
(lp42482
sg384
(lp42483
sg68
(lp42484
sg126
(lp42485
sg281
(lp42486
sg10
(lp42487
sg313
(lp42488
sg128
(lp42489
sg14
(lp42490
sg16
(lp42491
sg140
(lp42492
ssS'tell'
p42493
(dp42494
g85
(lp42495
sg124
(lp42496
sg176
(lp42497
sg140
(lp42498
I2474
assS'arma'
p42499
(dp42500
g124
(lp42501
I1186
assS'snowden'
p42502
(dp42503
g216
(lp42504
I2016
assS'metaproblem'
p42505
(dp42506
g34
(lp42507
I121
assS'logpm'
p42508
(dp42509
g72
(lp42510
I1065
assS'warp'
p42511
(dp42512
g96
(lp42513
I720
asg181
(lp42514
ssS'dogma'
p42515
(dp42516
g181
(lp42517
I209
assS'homosynapt'
p42518
(dp42519
g106
(lp42520
I690
assS'warm'
p42521
(dp42522
g36
(lp42523
I3059
asg85
(lp42524
ssS'preemphas'
p42525
(dp42526
g96
(lp42527
I1706
assS'adult'
p42528
(dp42529
g438
(lp42530
I958
asg176
(lp42531
ssS'ward'
p42532
(dp42533
g181
(lp42534
I840
assS'ware'
p42535
(dp42536
g36
(lp42537
I1181
assS'reinfcomp'
p42538
(dp42539
g329
(lp42540
I1960
assS'tympan'
p42541
(dp42542
g116
(lp42543
sg174
(lp42544
I210
assS'rook'
p42545
(dp42546
g132
(lp42547
I1735
assS'room'
p42548
(dp42549
g20
(lp42550
sg293
(lp42551
sg181
(lp42552
sg6
(lp42553
I21
asg63
(lp42554
ssS'setup'
p42555
(dp42556
g12
(lp42557
sg99
(lp42558
sg140
(lp42559
I1618
asg163
(lp42560
ssS'akin'
p42561
(dp42562
g106
(lp42563
I982
asg318
(lp42564
ssS'hansen'
p42565
(dp42566
g140
(lp42567
I3065
asg235
(lp42568
ssS'root'
p42569
(dp42570
g30
(lp42571
sg283
(lp42572
sg295
(lp42573
sg183
(lp42574
sg460
(lp42575
sg126
(lp42576
sg40
(lp42577
sg42
(lp42578
I1433
asg34
(lp42579
sg130
(lp42580
sg245
(lp42581
sg108
(lp42582
sg535
(lp42583
sg140
(lp42584
sg354
(lp42585
ssS'defer'
p42586
(dp42587
g145
(lp42588
I1460
assS'kinet'
p42589
(dp42590
g126
(lp42591
sg124
(lp42592
sg99
(lp42593
I3376
assS'give'
p42594
(dp42595
g283
(lp42596
sg26
(lp42597
sg163
(lp42598
sg72
(lp42599
sg460
(lp42600
sg287
(lp42601
sg74
(lp42602
sg145
(lp42603
sg80
(lp42604
sg76
(lp42605
sg262
(lp42606
sg295
(lp42607
sg183
(lp42608
sg59
(lp42609
sg484
(lp42610
sg83
(lp42611
sg85
(lp42612
sg63
(lp42613
sg42
(lp42614
I507
asg89
(lp42615
sg91
(lp42616
sg12
(lp42617
sg46
(lp42618
sg96
(lp42619
sg18
(lp42620
sg535
(lp42621
sg350
(lp42622
sg329
(lp42623
sg32
(lp42624
sg245
(lp42625
sg102
(lp42626
sg104
(lp42627
sg106
(lp42628
sg110
(lp42629
sg20
(lp42630
sg52
(lp42631
sg114
(lp42632
sg174
(lp42633
sg440
(lp42634
sg178
(lp42635
sg181
(lp42636
sg6
(lp42637
sg36
(lp42638
sg384
(lp42639
sg124
(lp42640
sg126
(lp42641
sg341
(lp42642
sg128
(lp42643
sg14
(lp42644
sg16
(lp42645
sg138
(lp42646
sg140
(lp42647
ssS'cursori'
p42648
(dp42649
g126
(lp42650
I929
assS'bellman'
p42651
(dp42652
g132
(lp42653
I2062
asg306
(lp42654
sg89
(lp42655
ssS'jlge'
p42656
(dp42657
g20
(lp42658
I1507
assS'cartan'
p42659
(dp42660
g32
(lp42661
I3031
assS'stanton'
p42662
(dp42663
g106
(lp42664
I19
assS'polystyren'
p42665
(dp42666
g283
(lp42667
I719
assS'tbat'
p42668
(dp42669
g181
(lp42670
I1921
assS'quot'
p42671
(dp42672
g287
(lp42673
I1032
assS'faulti'
p42674
(dp42675
g78
(lp42676
sg145
(lp42677
I1503
asg80
(lp42678
ssS'confin'
p42679
(dp42680
g438
(lp42681
I1249
asg235
(lp42682
sg295
(lp42683
sg183
(lp42684
sg460
(lp42685
sg38
(lp42686
sg34
(lp42687
sg14
(lp42688
sg16
(lp42689
ssS'pprox'
p42690
(dp42691
g85
(lp42692
I4158
assS'answer'
p42693
(dp42694
g287
(lp42695
sg32
(lp42696
sg36
(lp42697
sg40
(lp42698
sg42
(lp42699
I2005
asg91
(lp42700
sg94
(lp42701
ssS'equit'
p42702
(dp42703
g283
(lp42704
I1398
assS'confid'
p42705
(dp42706
g121
(lp42707
sg4
(lp42708
sg293
(lp42709
sg295
(lp42710
sg183
(lp42711
sg59
(lp42712
sg313
(lp42713
sg344
(lp42714
sg63
(lp42715
sg145
(lp42716
sg221
(lp42717
sg138
(lp42718
I3173
asg223
(lp42719
ssS'vrstv'
p42720
(dp42721
g14
(lp42722
I3672
assS'ministri'
p42723
(dp42724
g96
(lp42725
sg145
(lp42726
sg20
(lp42727
sg130
(lp42728
I3096
assS'synchroni'
p42729
(dp42730
g80
(lp42731
I922
assS'oscillatori'
p42732
(dp42733
g68
(lp42734
I415
assS'phasic'
p42735
(dp42736
g350
(lp42737
I1488
assS'passageway'
p42738
(dp42739
g42
(lp42740
I2270
assS'substant'
p42741
(dp42742
g74
(lp42743
I2774
assS'centersurround'
p42744
(dp42745
g256
(lp42746
I79
assS'biotechnolog'
p42747
(dp42748
g149
(lp42749
I3174
assS'attempt'
p42750
(dp42751
g283
(lp42752
sg26
(lp42753
sg126
(lp42754
sg30
(lp42755
sg74
(lp42756
sg80
(lp42757
sg293
(lp42758
sg78
(lp42759
sg83
(lp42760
sg303
(lp42761
sg91
(lp42762
sg245
(lp42763
sg99
(lp42764
sg44
(lp42765
sg350
(lp42766
sg429
(lp42767
sg102
(lp42768
sg108
(lp42769
sg329
(lp42770
sg4
(lp42771
sg72
(lp42772
sg132
(lp42773
sg135
(lp42774
sg138
(lp42775
I1574
assS'third'
p42776
(dp42777
g68
(lp42778
sg26
(lp42779
sg163
(lp42780
sg30
(lp42781
sg76
(lp42782
sg293
(lp42783
sg183
(lp42784
sg59
(lp42785
sg83
(lp42786
sg20
(lp42787
sg223
(lp42788
sg350
(lp42789
sg108
(lp42790
sg52
(lp42791
sg114
(lp42792
sg116
(lp42793
sg329
(lp42794
sg32
(lp42795
sg318
(lp42796
sg80
(lp42797
sg181
(lp42798
sg124
(lp42799
sg72
(lp42800
sg281
(lp42801
sg78
(lp42802
sg138
(lp42803
sg354
(lp42804
I453
assS'tutori'
p42805
(dp42806
g235
(lp42807
I3043
assS'think'
p42808
(dp42809
g26
(lp42810
sg235
(lp42811
sg344
(lp42812
sg183
(lp42813
sg306
(lp42814
sg140
(lp42815
I499
asg132
(lp42816
sg44
(lp42817
ssS'maintain'
p42818
(dp42819
g329
(lp42820
sg80
(lp42821
sg176
(lp42822
sg145
(lp42823
sg256
(lp42824
sg76
(lp42825
sg293
(lp42826
sg295
(lp42827
sg183
(lp42828
sg484
(lp42829
sg83
(lp42830
sg306
(lp42831
sg89
(lp42832
sg94
(lp42833
sg18
(lp42834
sg149
(lp42835
sg118
(lp42836
sg4
(lp42837
sg230
(lp42838
sg174
(lp42839
sg22
(lp42840
sg8
(lp42841
sg460
(lp42842
sg10
(lp42843
sg344
(lp42844
sg135
(lp42845
sg50
(lp42846
I850
assS'equiv'
p42847
(dp42848
g72
(lp42849
I2443
assS'deco'
p42850
(dp42851
g163
(lp42852
I125
assS'cylindr'
p42853
(dp42854
g80
(lp42855
I104
assS'oftbiolog'
p42856
(dp42857
g350
(lp42858
I44
assS'murray'
p42859
(dp42860
g132
(lp42861
sg14
(lp42862
I2683
asg183
(lp42863
ssS'keyboard'
p42864
(dp42865
g94
(lp42866
I265
assS'ilq'
p42867
(dp42868
g460
(lp42869
I1345
assS'ofhmm'
p42870
(dp42871
g87
(lp42872
I359
assS'better'
p42873
(dp42874
g283
(lp42875
sg70
(lp42876
sg26
(lp42877
sg277
(lp42878
sg163
(lp42879
sg72
(lp42880
sg36
(lp42881
sg303
(lp42882
sg287
(lp42883
sg74
(lp42884
sg145
(lp42885
sg80
(lp42886
sg76
(lp42887
sg344
(lp42888
sg183
(lp42889
sg59
(lp42890
sg484
(lp42891
sg83
(lp42892
sg63
(lp42893
sg42
(lp42894
I185
asg94
(lp42895
sg96
(lp42896
sg48
(lp42897
sg99
(lp42898
sg223
(lp42899
sg149
(lp42900
sg318
(lp42901
sg102
(lp42902
sg178
(lp42903
sg108
(lp42904
sg110
(lp42905
sg20
(lp42906
sg114
(lp42907
sg116
(lp42908
sg329
(lp42909
sg440
(lp42910
sg18
(lp42911
sg121
(lp42912
sg4
(lp42913
sg8
(lp42914
sg34
(lp42915
sg221
(lp42916
sg235
(lp42917
sg126
(lp42918
sg128
(lp42919
sg130
(lp42920
sg132
(lp42921
sg14
(lp42922
sg16
(lp42923
sg135
(lp42924
sg138
(lp42925
sg140
(lp42926
ssS'spoken'
p42927
(dp42928
g94
(lp42929
I3533
asg96
(lp42930
sg87
(lp42931
sg440
(lp42932
sg174
(lp42933
ssS'oolo'
p42934
(dp42935
g281
(lp42936
I2231
assS'persist'
p42937
(dp42938
g106
(lp42939
I158
asg124
(lp42940
sg126
(lp42941
sg80
(lp42942
sg235
(lp42943
ssS'schmidt'
p42944
(dp42945
g384
(lp42946
sg99
(lp42947
I2925
assS'aston'
p42948
(dp42949
g14
(lp42950
sg16
(lp42951
sg124
(lp42952
sg38
(lp42953
sg138
(lp42954
I279
assS'anim'
p42955
(dp42956
g116
(lp42957
sg438
(lp42958
I1332
asg48
(lp42959
sg4
(lp42960
sg6
(lp42961
sg174
(lp42962
sg78
(lp42963
sg80
(lp42964
sg293
(lp42965
sg181
(lp42966
sg18
(lp42967
ssS'setpc'
p42968
(dp42969
g44
(lp42970
I1164
assS'iset'
p42971
(dp42972
g14
(lp42973
I3481
assS'promin'
p42974
(dp42975
g318
(lp42976
sg145
(lp42977
sg6
(lp42978
sg8
(lp42979
sg48
(lp42980
sg50
(lp42981
I995
assS'overestim'
p42982
(dp42983
g384
(lp42984
I1581
assS'microfilm'
p42985
(dp42986
g12
(lp42987
I2707
assS'mxn'
p42988
(dp42989
g438
(lp42990
I2330
assS'tthe'
p42991
(dp42992
g221
(lp42993
sg128
(lp42994
I1609
assS'promis'
p42995
(dp42996
g74
(lp42997
sg124
(lp42998
sg83
(lp42999
sg10
(lp43000
sg40
(lp43001
sg132
(lp43002
sg14
(lp43003
sg16
(lp43004
I272
asg63
(lp43005
ssS'aniv'
p43006
(dp43007
g106
(lp43008
I1905
assS'grammat'
p43009
(dp43010
g128
(lp43011
I76
assS'grammar'
p43012
(dp43013
g30
(lp43014
I418
assS'phansalkar'
p43015
(dp43016
g329
(lp43017
I163
assS'localis'
p43018
(dp43019
g176
(lp43020
sg52
(lp43021
sg8
(lp43022
I697
assS'bonn'
p43023
(dp43024
g132
(lp43025
I12
asg36
(lp43026
sg223
(lp43027
sg130
(lp43028
ssS'qplxi'
p43029
(dp43030
g76
(lp43031
I2506
assS'went'
p43032
(dp43033
g145
(lp43034
sg178
(lp43035
I1819
assS'side'
p43036
(dp43037
g26
(lp43038
sg183
(lp43039
sg59
(lp43040
sg303
(lp43041
sg87
(lp43042
sg91
(lp43043
sg12
(lp43044
sg223
(lp43045
sg350
(lp43046
sg32
(lp43047
sg429
(lp43048
sg102
(lp43049
sg104
(lp43050
sg106
(lp43051
I592
asg63
(lp43052
sg116
(lp43053
sg118
(lp43054
sg440
(lp43055
sg36
(lp43056
sg44
(lp43057
sg130
(lp43058
sg132
(lp43059
sg149
(lp43060
sg354
(lp43061
ssS'bone'
p43062
(dp43063
g174
(lp43064
I223
asg318
(lp43065
ssS'mean'
p43066
(dp43067
g329
(lp43068
sg70
(lp43069
sg78
(lp43070
sg277
(lp43071
sg163
(lp43072
sg283
(lp43073
sg181
(lp43074
sg26
(lp43075
sg30
(lp43076
sg287
(lp43077
sg74
(lp43078
sg176
(lp43079
sg145
(lp43080
sg256
(lp43081
sg76
(lp43082
sg262
(lp43083
sg295
(lp43084
sg183
(lp43085
sg59
(lp43086
sg484
(lp43087
sg38
(lp43088
sg83
(lp43089
sg85
(lp43090
sg124
(lp43091
sg42
(lp43092
I139
asg306
(lp43093
sg87
(lp43094
sg91
(lp43095
sg12
(lp43096
sg94
(lp43097
sg96
(lp43098
sg18
(lp43099
sg99
(lp43100
sg313
(lp43101
sg44
(lp43102
sg350
(lp43103
sg230
(lp43104
sg174
(lp43105
sg460
(lp43106
sg318
(lp43107
sg46
(lp43108
sg102
(lp43109
sg108
(lp43110
sg20
(lp43111
sg114
(lp43112
sg216
(lp43113
sg438
(lp43114
sg32
(lp43115
sg332
(lp43116
sg178
(lp43117
sg4
(lp43118
sg6
(lp43119
sg8
(lp43120
sg34
(lp43121
sg36
(lp43122
sg384
(lp43123
sg235
(lp43124
sg126
(lp43125
sg281
(lp43126
sg535
(lp43127
sg63
(lp43128
sg223
(lp43129
sg128
(lp43130
sg130
(lp43131
sg14
(lp43132
sg135
(lp43133
sg50
(lp43134
sg138
(lp43135
sg140
(lp43136
sg354
(lp43137
ssS'iiiiiiii'
p43138
(dp43139
g6
(lp43140
I1797
assS'navi'
p43141
(dp43142
g78
(lp43143
I369
assS'fie'
p43144
(dp43145
g484
(lp43146
sg350
(lp43147
I796
assS'enorm'
p43148
(dp43149
g132
(lp43150
I843
asg283
(lp43151
sg63
(lp43152
sg223
(lp43153
ssS'journ'
p43154
(dp43155
g178
(lp43156
I2711
assS'rieger'
p43157
(dp43158
g384
(lp43159
I749
assS'sillito'
p43160
(dp43161
g70
(lp43162
I1533
assS'vote'
p43163
(dp43164
g344
(lp43165
sg138
(lp43166
I1541
asg223
(lp43167
sg293
(lp43168
ssS'taught'
p43169
(dp43170
g104
(lp43171
I2662
assS'sahar'
p43172
(dp43173
g145
(lp43174
I11
assS'framevill'
p43175
(dp43176
g429
(lp43177
I462
assS'degeneraci'
p43178
(dp43179
g32
(lp43180
I2498
assS'extract'
p43181
(dp43182
g283
(lp43183
sg163
(lp43184
sg256
(lp43185
sg59
(lp43186
sg306
(lp43187
sg96
(lp43188
sg102
(lp43189
sg178
(lp43190
sg63
(lp43191
sg52
(lp43192
sg114
(lp43193
sg174
(lp43194
sg332
(lp43195
sg121
(lp43196
sg281
(lp43197
sg10
(lp43198
sg128
(lp43199
sg132
(lp43200
sg14
(lp43201
sg16
(lp43202
sg50
(lp43203
sg138
(lp43204
I1401
assS'acous'
p43205
(dp43206
g96
(lp43207
I2334
assS'contend'
p43208
(dp43209
g344
(lp43210
I307
assS'linggard'
p43211
(dp43212
g52
(lp43213
I2628
assS'unbound'
p43214
(dp43215
g341
(lp43216
I230
asg52
(lp43217
ssS'gradient'
p43218
(dp43219
g124
(lp43220
sg26
(lp43221
sg163
(lp43222
sg287
(lp43223
sg295
(lp43224
sg183
(lp43225
sg38
(lp43226
sg303
(lp43227
sg306
(lp43228
sg91
(lp43229
sg46
(lp43230
sg96
(lp43231
sg48
(lp43232
sg223
(lp43233
sg116
(lp43234
sg68
(lp43235
sg102
(lp43236
sg178
(lp43237
sg108
(lp43238
sg110
(lp43239
sg230
(lp43240
sg329
(lp43241
sg440
(lp43242
sg121
(lp43243
sg8
(lp43244
sg34
(lp43245
sg36
(lp43246
sg460
(lp43247
sg235
(lp43248
sg126
(lp43249
sg341
(lp43250
sg44
(lp43251
sg128
(lp43252
sg14
(lp43253
sg16
(lp43254
sg50
(lp43255
sg138
(lp43256
sg140
(lp43257
sg354
(lp43258
I1807
assS'suboscin'
p43259
(dp43260
g116
(lp43261
I2466
assS'crucial'
p43262
(dp43263
g116
(lp43264
sg74
(lp43265
sg59
(lp43266
sg145
(lp43267
sg26
(lp43268
sg163
(lp43269
sg344
(lp43270
sg384
(lp43271
sg283
(lp43272
sg140
(lp43273
I528
asg70
(lp43274
sg96
(lp43275
sg18
(lp43276
sg99
(lp43277
sg63
(lp43278
sg44
(lp43279
ssS'content'
p43280
(dp43281
g438
(lp43282
I1685
asg50
(lp43283
sg178
(lp43284
sg52
(lp43285
sg114
(lp43286
ssS'rewrit'
p43287
(dp43288
g42
(lp43289
I894
asg102
(lp43290
sg306
(lp43291
sg293
(lp43292
ssS'ihop'
p43293
(dp43294
g63
(lp43295
I1981
assS'reader'
p43296
(dp43297
g230
(lp43298
I1908
asg76
(lp43299
sg235
(lp43300
ssS'quantifi'
p43301
(dp43302
g344
(lp43303
sg287
(lp43304
sg384
(lp43305
sg99
(lp43306
sg140
(lp43307
I69
assS'chniqu'
p43308
(dp43309
g130
(lp43310
I462
assS'maurer'
p43311
(dp43312
g80
(lp43313
I486
assS'linear'
p43314
(dp43315
g344
(lp43316
sg124
(lp43317
sg78
(lp43318
sg163
(lp43319
sg72
(lp43320
sg303
(lp43321
sg283
(lp43322
sg40
(lp43323
sg26
(lp43324
sg30
(lp43325
sg350
(lp43326
sg74
(lp43327
sg176
(lp43328
sg145
(lp43329
sg295
(lp43330
sg183
(lp43331
sg59
(lp43332
sg484
(lp43333
sg38
(lp43334
sg83
(lp43335
sg85
(lp43336
sg63
(lp43337
sg306
(lp43338
sg89
(lp43339
sg91
(lp43340
sg12
(lp43341
sg46
(lp43342
sg96
(lp43343
sg48
(lp43344
sg313
(lp43345
sg44
(lp43346
sg149
(lp43347
sg329
(lp43348
sg460
(lp43349
sg245
(lp43350
sg429
(lp43351
sg68
(lp43352
sg102
(lp43353
sg104
(lp43354
sg108
(lp43355
sg110
(lp43356
sg178
(lp43357
sg114
(lp43358
sg230
(lp43359
sg438
(lp43360
I755
asg32
(lp43361
sg18
(lp43362
sg121
(lp43363
sg22
(lp43364
sg8
(lp43365
sg36
(lp43366
sg384
(lp43367
sg235
(lp43368
sg126
(lp43369
sg281
(lp43370
sg10
(lp43371
sg535
(lp43372
sg287
(lp43373
sg128
(lp43374
sg130
(lp43375
sg132
(lp43376
sg14
(lp43377
sg16
(lp43378
sg135
(lp43379
sg50
(lp43380
sg138
(lp43381
sg140
(lp43382
ssS'mdef'
p43383
(dp43384
g295
(lp43385
I2139
asg183
(lp43386
ssS'nordita'
p43387
(dp43388
g26
(lp43389
sg140
(lp43390
I11
asg235
(lp43391
ssS'verif'
p43392
(dp43393
g70
(lp43394
sg114
(lp43395
I8
assS'situat'
p43396
(dp43397
g26
(lp43398
sg30
(lp43399
sg145
(lp43400
sg293
(lp43401
sg59
(lp43402
sg484
(lp43403
sg83
(lp43404
sg85
(lp43405
sg42
(lp43406
I689
asg306
(lp43407
sg46
(lp43408
sg223
(lp43409
sg94
(lp43410
sg102
(lp43411
sg63
(lp43412
sg216
(lp43413
sg438
(lp43414
sg121
(lp43415
sg181
(lp43416
sg34
(lp43417
sg36
(lp43418
sg68
(lp43419
sg132
(lp43420
sg354
(lp43421
ssS'infin'
p43422
(dp43423
g329
(lp43424
sg262
(lp43425
sg287
(lp43426
sg124
(lp43427
sg354
(lp43428
I1359
asg350
(lp43429
ssS'ineffici'
p43430
(dp43431
g94
(lp43432
sg26
(lp43433
sg44
(lp43434
I367
assS'ebsa'
p43435
(dp43436
g26
(lp43437
I2822
assS'trammg'
p43438
(dp43439
g460
(lp43440
I2229
assS'lineal'
p43441
(dp43442
g174
(lp43443
I2523
asg40
(lp43444
ssS'sigal'
p43445
(dp43446
g145
(lp43447
I10
assS'flect'
p43448
(dp43449
g42
(lp43450
I1096
assS'steward'
p43451
(dp43452
g106
(lp43453
I358
assS'isi'
p43454
(dp43455
g14
(lp43456
I3560
asg6
(lp43457
ssS'iso'
p43458
(dp43459
g295
(lp43460
sg183
(lp43461
sg48
(lp43462
I1444
assS'japkowicz'
p43463
(dp43464
g78
(lp43465
I2943
assS'isl'
p43466
(dp43467
g68
(lp43468
I2836
assS'xilinx'
p43469
(dp43470
g10
(lp43471
I460
assS'isa'
p43472
(dp43473
g429
(lp43474
sg10
(lp43475
I541
assS'gridworld'
p43476
(dp43477
g89
(lp43478
I822
assS'grade'
p43479
(dp43480
g287
(lp43481
sg332
(lp43482
I1440
asg38
(lp43483
sg256
(lp43484
ssS'ofwo'
p43485
(dp43486
g36
(lp43487
I1327
assS'biomed'
p43488
(dp43489
g99
(lp43490
I255
asg91
(lp43491
sg181
(lp43492
ssS'unlik'
p43493
(dp43494
g293
(lp43495
sg344
(lp43496
sg83
(lp43497
sg63
(lp43498
sg128
(lp43499
sg94
(lp43500
sg132
(lp43501
sg14
(lp43502
sg106
(lp43503
I2297
asg108
(lp43504
sg16
(lp43505
ssS'jji'
p43506
(dp43507
g384
(lp43508
I468
assS'multimod'
p43509
(dp43510
g138
(lp43511
I3143
assS'massiv'
p43512
(dp43513
g14
(lp43514
I4513
asg114
(lp43515
sg70
(lp43516
sg63
(lp43517
sg104
(lp43518
ssS'dmllz'
p43519
(dp43520
g85
(lp43521
I1604
assS'wherev'
p43522
(dp43523
g277
(lp43524
sg76
(lp43525
I970
assS'garri'
p43526
(dp43527
g178
(lp43528
I2460
assS'dipartimento'
p43529
(dp43530
g44
(lp43531
I14
assS'sometim'
p43532
(dp43533
g30
(lp43534
sg287
(lp43535
sg74
(lp43536
sg176
(lp43537
sg26
(lp43538
sg277
(lp43539
sg235
(lp43540
sg344
(lp43541
sg460
(lp43542
sg124
(lp43543
sg341
(lp43544
sg85
(lp43545
sg42
(lp43546
I500
asg102
(lp43547
sg132
(lp43548
sg140
(lp43549
ssS'grossberg'
p43550
(dp43551
g216
(lp43552
I1510
asg104
(lp43553
sg535
(lp43554
sg118
(lp43555
ssS'dwell'
p43556
(dp43557
g114
(lp43558
I853
assS'beymer'
p43559
(dp43560
g138
(lp43561
I1738
asg223
(lp43562
ssS'digm'
p43563
(dp43564
g106
(lp43565
I954
assS'uction'
p43566
(dp43567
g344
(lp43568
I128
assS'digi'
p43569
(dp43570
g52
(lp43571
I332
assS'somewhat'
p43572
(dp43573
g287
(lp43574
sg176
(lp43575
sg145
(lp43576
sg22
(lp43577
sg34
(lp43578
sg83
(lp43579
sg85
(lp43580
sg344
(lp43581
sg102
(lp43582
sg94
(lp43583
sg110
(lp43584
sg44
(lp43585
I572
assS'stuss'
p43586
(dp43587
g4
(lp43588
I405
assS'cccn'
p43589
(dp43590
g174
(lp43591
I8
assS'symptom'
p43592
(dp43593
g110
(lp43594
sg91
(lp43595
I171
asg277
(lp43596
ssS'charcter'
p43597
(dp43598
g104
(lp43599
I2720
assS'approachw'
p43600
(dp43601
g104
(lp43602
I2751
assS'photoreceptor'
p43603
(dp43604
g245
(lp43605
I1216
asg256
(lp43606
ssS'princeton'
p43607
(dp43608
g30
(lp43609
sg262
(lp43610
sg78
(lp43611
sg163
(lp43612
sg91
(lp43613
sg128
(lp43614
sg132
(lp43615
I3510
assS'kenji'
p43616
(dp43617
g18
(lp43618
I12
assS'connectionist'
p43619
(dp43620
g80
(lp43621
sg329
(lp43622
sg440
(lp43623
sg4
(lp43624
sg76
(lp43625
sg34
(lp43626
sg50
(lp43627
I852
asg68
(lp43628
sg74
(lp43629
sg87
(lp43630
sg89
(lp43631
sg110
(lp43632
sg36
(lp43633
sg132
(lp43634
sg94
(lp43635
sg96
(lp43636
sg99
(lp43637
sg313
(lp43638
ssS'unstimul'
p43639
(dp43640
g106
(lp43641
I2053
assS'matter'
p43642
(dp43643
g318
(lp43644
sg80
(lp43645
sg163
(lp43646
sg34
(lp43647
sg68
(lp43648
sg303
(lp43649
sg18
(lp43650
I2030
asg63
(lp43651
ssS'wiesel'
p43652
(dp43653
g12
(lp43654
sg48
(lp43655
sg63
(lp43656
sg149
(lp43657
I172
assS'microcomput'
p43658
(dp43659
g52
(lp43660
I81
assS'minh'
p43661
(dp43662
g85
(lp43663
I614
assS'mini'
p43664
(dp43665
g85
(lp43666
I1072
assS'minl'
p43667
(dp43668
g72
(lp43669
I2300
assS'rojer'
p43670
(dp43671
g48
(lp43672
I260
assS'modern'
p43673
(dp43674
g174
(lp43675
I337
asg384
(lp43676
sg63
(lp43677
sg10
(lp43678
ssS'mind'
p43679
(dp43680
g116
(lp43681
sg74
(lp43682
sg48
(lp43683
I482
asg63
(lp43684
ssS'mine'
p43685
(dp43686
g183
(lp43687
sg223
(lp43688
sg130
(lp43689
I3050
assS'kotani'
p43690
(dp43691
g20
(lp43692
I2532
assS'seed'
p43693
(dp43694
g72
(lp43695
sg121
(lp43696
I1859
assS'seen'
p43697
(dp43698
g116
(lp43699
sg287
(lp43700
sg256
(lp43701
sg76
(lp43702
sg293
(lp43703
sg295
(lp43704
sg183
(lp43705
sg59
(lp43706
sg83
(lp43707
sg85
(lp43708
sg87
(lp43709
sg89
(lp43710
sg12
(lp43711
sg94
(lp43712
sg20
(lp43713
sg18
(lp43714
sg221
(lp43715
sg350
(lp43716
sg230
(lp43717
sg329
(lp43718
sg429
(lp43719
sg178
(lp43720
sg108
(lp43721
sg63
(lp43722
sg114
(lp43723
sg216
(lp43724
sg174
(lp43725
sg32
(lp43726
sg332
(lp43727
sg121
(lp43728
sg4
(lp43729
sg181
(lp43730
sg235
(lp43731
sg99
(lp43732
sg384
(lp43733
sg68
(lp43734
sg126
(lp43735
sg78
(lp43736
sg132
(lp43737
sg135
(lp43738
sg460
(lp43739
sg354
(lp43740
I173
assS'seem'
p43741
(dp43742
g124
(lp43743
sg26
(lp43744
sg181
(lp43745
sg176
(lp43746
sg145
(lp43747
sg344
(lp43748
sg183
(lp43749
sg83
(lp43750
sg85
(lp43751
sg303
(lp43752
sg306
(lp43753
sg87
(lp43754
sg89
(lp43755
sg48
(lp43756
sg221
(lp43757
sg223
(lp43758
sg106
(lp43759
I1579
asg116
(lp43760
sg332
(lp43761
sg178
(lp43762
sg4
(lp43763
sg6
(lp43764
sg8
(lp43765
sg99
(lp43766
sg235
(lp43767
sg44
(lp43768
sg128
(lp43769
sg78
(lp43770
sg50
(lp43771
sg138
(lp43772
sg140
(lp43773
ssS'seek'
p43774
(dp43775
g30
(lp43776
sg74
(lp43777
sg176
(lp43778
sg70
(lp43779
sg8
(lp43780
sg102
(lp43781
sg96
(lp43782
sg50
(lp43783
I526
asg44
(lp43784
ssS'willshawmalsburg'
p43785
(dp43786
g176
(lp43787
I596
assS'noncircular'
p43788
(dp43789
g14
(lp43790
sg16
(lp43791
I374
assS'venkatesh'
p43792
(dp43793
g36
(lp43794
I3319
asg281
(lp43795
ssS'ricoh'
p43796
(dp43797
g44
(lp43798
I32
assS'pulsewidth'
p43799
(dp43800
g14
(lp43801
I4144
assS'chess'
p43802
(dp43803
g132
(lp43804
I7
asg223
(lp43805
ssS'davidson'
p43806
(dp43807
g18
(lp43808
I2683
assS'phosphonovaler'
p43809
(dp43810
g106
(lp43811
I1530
assS'marit'
p43812
(dp43813
g72
(lp43814
I593
assS'terrestri'
p43815
(dp43816
g14
(lp43817
sg16
(lp43818
I250
assS'boltzmann'
p43819
(dp43820
g34
(lp43821
sg329
(lp43822
I479
asg384
(lp43823
sg83
(lp43824
sg26
(lp43825
ssS'regular'
p43826
(dp43827
g26
(lp43828
sg295
(lp43829
sg183
(lp43830
sg85
(lp43831
sg235
(lp43832
sg221
(lp43833
sg44
(lp43834
sg429
(lp43835
sg102
(lp43836
sg104
(lp43837
sg174
(lp43838
sg440
(lp43839
sg178
(lp43840
sg8
(lp43841
sg34
(lp43842
sg36
(lp43843
sg124
(lp43844
sg10
(lp43845
sg132
(lp43846
sg138
(lp43847
sg140
(lp43848
sg354
(lp43849
I1970
assS'timat'
p43850
(dp43851
g132
(lp43852
I3321
assS'iterati'
p43853
(dp43854
g89
(lp43855
I1228
assS'vasken'
p43856
(dp43857
g40
(lp43858
I10
assS'christ'
p43859
(dp43860
g83
(lp43861
sg303
(lp43862
I2847
assS'tradit'
p43863
(dp43864
g216
(lp43865
sg174
(lp43866
sg74
(lp43867
sg277
(lp43868
sg118
(lp43869
sg34
(lp43870
sg293
(lp43871
sg83
(lp43872
sg40
(lp43873
sg306
(lp43874
sg91
(lp43875
I2432
asg535
(lp43876
sg223
(lp43877
ssS'simplic'
p43878
(dp43879
g438
(lp43880
I681
asg74
(lp43881
sg318
(lp43882
sg181
(lp43883
sg235
(lp43884
sg384
(lp43885
sg262
(lp43886
sg126
(lp43887
sg85
(lp43888
sg287
(lp43889
sg89
(lp43890
sg14
(lp43891
sg16
(lp43892
sg18
(lp43893
sg110
(lp43894
sg313
(lp43895
sg52
(lp43896
ssS'don'
p43897
(dp43898
g178
(lp43899
sg313
(lp43900
I248
asg63
(lp43901
ssS'entrain'
p43902
(dp43903
g176
(lp43904
I2271
assS'simplif'
p43905
(dp43906
g438
(lp43907
I2195
asg313
(lp43908
sg91
(lp43909
ssS'alarm'
p43910
(dp43911
g78
(lp43912
I2360
assS'ormoneit'
p43913
(dp43914
g221
(lp43915
I14
assS'm'
p43916
(dp43917
g80
(lp43918
sg293
(lp43919
sg344
(lp43920
sg78
(lp43921
sg59
(lp43922
sg484
(lp43923
sg38
(lp43924
sg83
(lp43925
sg85
(lp43926
sg303
(lp43927
sg438
(lp43928
sg116
(lp43929
sg118
(lp43930
sg34
(lp43931
sg36
(lp43932
sg460
(lp43933
sg68
(lp43934
sg72
(lp43935
sg281
(lp43936
sg10
(lp43937
sg40
(lp43938
sg283
(lp43939
sg70
(lp43940
sg26
(lp43941
sg277
(lp43942
sg163
(lp43943
sg89
(lp43944
sg91
(lp43945
sg12
(lp43946
sg96
(lp43947
sg48
(lp43948
sg99
(lp43949
sg313
(lp43950
sg44
(lp43951
sg149
(lp43952
sg429
(lp43953
sg102
(lp43954
sg104
(lp43955
sg106
(lp43956
sg108
(lp43957
sg110
(lp43958
sg63
(lp43959
sg52
(lp43960
sg128
(lp43961
sg130
(lp43962
sg132
(lp43963
sg14
(lp43964
sg16
(lp43965
sg135
(lp43966
sg50
(lp43967
sg138
(lp43968
sg140
(lp43969
sg354
(lp43970
sg306
(lp43971
sg87
(lp43972
sg245
(lp43973
sg46
(lp43974
sg20
(lp43975
sg18
(lp43976
sg221
(lp43977
sg535
(lp43978
sg223
(lp43979
sg350
(lp43980
sg216
(lp43981
sg174
(lp43982
sg440
(lp43983
sg332
(lp43984
sg121
(lp43985
sg6
(lp43986
sg8
(lp43987
sg126
(lp43988
sg341
(lp43989
sg30
(lp43990
sg287
(lp43991
sg74
(lp43992
sg176
(lp43993
sg145
(lp43994
sg256
(lp43995
sg76
(lp43996
sg262
(lp43997
sg295
(lp43998
sg183
(lp43999
sg42
(lp44000
I2224
asg230
(lp44001
sg329
(lp44002
sg32
(lp44003
sg318
(lp44004
sg178
(lp44005
sg22
(lp44006
sg181
(lp44007
sg235
(lp44008
sg384
(lp44009
sg124
(lp44010
ssS'dod'
p44011
(dp44012
g135
(lp44013
I616
assS'doe'
p44014
(dp44015
g329
(lp44016
sg70
(lp44017
sg26
(lp44018
sg277
(lp44019
sg163
(lp44020
sg281
(lp44021
sg74
(lp44022
sg145
(lp44023
sg256
(lp44024
sg76
(lp44025
sg293
(lp44026
sg295
(lp44027
sg183
(lp44028
sg484
(lp44029
sg85
(lp44030
sg303
(lp44031
sg42
(lp44032
I960
asg87
(lp44033
sg89
(lp44034
sg91
(lp44035
sg12
(lp44036
sg20
(lp44037
sg48
(lp44038
sg99
(lp44039
sg223
(lp44040
sg350
(lp44041
sg174
(lp44042
sg32
(lp44043
sg245
(lp44044
sg429
(lp44045
sg102
(lp44046
sg178
(lp44047
sg106
(lp44048
sg108
(lp44049
sg63
(lp44050
sg114
(lp44051
sg216
(lp44052
sg438
(lp44053
sg440
(lp44054
sg318
(lp44055
sg121
(lp44056
sg80
(lp44057
sg8
(lp44058
sg34
(lp44059
sg36
(lp44060
sg235
(lp44061
sg126
(lp44062
sg341
(lp44063
sg128
(lp44064
sg78
(lp44065
sg132
(lp44066
sg14
(lp44067
sg135
(lp44068
sg138
(lp44069
sg140
(lp44070
sg354
(lp44071
ssS'constrict'
p44072
(dp44073
g52
(lp44074
I1976
assS'hopfieldjgrossberg'
p44075
(dp44076
g8
(lp44077
I1058
assS'dot'
p44078
(dp44079
g74
(lp44080
sg145
(lp44081
sg59
(lp44082
sg303
(lp44083
sg42
(lp44084
I2016
asg12
(lp44085
sg46
(lp44086
sg350
(lp44087
sg102
(lp44088
sg63
(lp44089
sg52
(lp44090
sg216
(lp44091
sg329
(lp44092
sg318
(lp44093
sg22
(lp44094
sg235
(lp44095
sg34
(lp44096
sg384
(lp44097
sg124
(lp44098
sg149
(lp44099
sg140
(lp44100
sg354
(lp44101
ssS'probe'
p44102
(dp44103
g20
(lp44104
sg4
(lp44105
I84
asg80
(lp44106
ssS'speedup'
p44107
(dp44108
g94
(lp44109
sg87
(lp44110
sg138
(lp44111
I3132
asg10
(lp44112
ssS'syntax'
p44113
(dp44114
g440
(lp44115
I2055
assS'corticocort'
p44116
(dp44117
g12
(lp44118
sg303
(lp44119
sg149
(lp44120
I2786
assS'schenkel'
p44121
(dp44122
g303
(lp44123
I2870
assS'logieal'
p44124
(dp44125
g63
(lp44126
I1199
assS'fillingin'
p44127
(dp44128
g118
(lp44129
I948
assS'despit'
p44130
(dp44131
g230
(lp44132
sg118
(lp44133
sg76
(lp44134
sg460
(lp44135
sg85
(lp44136
sg306
(lp44137
sg89
(lp44138
sg132
(lp44139
I3167
asg48
(lp44140
ssS'acquir'
p44141
(dp44142
g277
(lp44143
sg235
(lp44144
sg295
(lp44145
sg183
(lp44146
sg293
(lp44147
sg38
(lp44148
sg91
(lp44149
sg96
(lp44150
sg99
(lp44151
sg140
(lp44152
I2244
assS'explain'
p44153
(dp44154
g26
(lp44155
sg163
(lp44156
sg74
(lp44157
sg176
(lp44158
sg76
(lp44159
sg295
(lp44160
sg183
(lp44161
sg80
(lp44162
sg38
(lp44163
sg303
(lp44164
sg245
(lp44165
sg20
(lp44166
sg221
(lp44167
sg44
(lp44168
sg149
(lp44169
sg116
(lp44170
sg12
(lp44171
sg102
(lp44172
sg110
(lp44173
sg216
(lp44174
sg118
(lp44175
sg32
(lp44176
sg318
(lp44177
sg178
(lp44178
sg4
(lp44179
sg181
(lp44180
sg235
(lp44181
sg99
(lp44182
sg72
(lp44183
sg344
(lp44184
sg130
(lp44185
sg132
(lp44186
sg138
(lp44187
I1122
assS'mustrat'
p44188
(dp44189
g106
(lp44190
I1266
assS'arpa'
p44191
(dp44192
g87
(lp44193
I114
asg277
(lp44194
sg10
(lp44195
ssS'sugar'
p44196
(dp44197
g91
(lp44198
I2117
assS'cccxabxcc'
p44199
(dp44200
g332
(lp44201
I2047
assS'biophys'
p44202
(dp44203
g106
(lp44204
I27
asg262
(lp44205
ssS'tdnns'
p44206
(dp44207
g440
(lp44208
I2364
assS'bifurc'
p44209
(dp44210
g18
(lp44211
I8
asg68
(lp44212
ssS'blur'
p44213
(dp44214
g70
(lp44215
sg440
(lp44216
sg178
(lp44217
sg4
(lp44218
I3312
assS'stop'
p44219
(dp44220
g230
(lp44221
sg174
(lp44222
sg74
(lp44223
sg70
(lp44224
sg277
(lp44225
sg235
(lp44226
sg344
(lp44227
sg36
(lp44228
sg484
(lp44229
sg83
(lp44230
sg42
(lp44231
I1558
asg89
(lp44232
sg91
(lp44233
sg128
(lp44234
sg108
(lp44235
ssS'compli'
p44236
(dp44237
g48
(lp44238
I695
assS'pattem'
p44239
(dp44240
g183
(lp44241
sg52
(lp44242
I1216
assS'barlow'
p44243
(dp44244
g12
(lp44245
sg245
(lp44246
sg50
(lp44247
I294
assS'stoc'
p44248
(dp44249
g40
(lp44250
I2519
assS'earn'
p44251
(dp44252
g104
(lp44253
I893
assS'bar'
p44254
(dp44255
g116
(lp44256
sg121
(lp44257
sg256
(lp44258
sg235
(lp44259
sg78
(lp44260
sg126
(lp44261
sg245
(lp44262
sg140
(lp44263
I481
asg12
(lp44264
sg70
(lp44265
sg110
(lp44266
sg223
(lp44267
ssS'shulz'
p44268
(dp44269
g106
(lp44270
I2625
assS'sacrific'
p44271
(dp44272
g132
(lp44273
I882
assS'bay'
p44274
(dp44275
g440
(lp44276
sg277
(lp44277
sg76
(lp44278
sg126
(lp44279
sg281
(lp44280
sg91
(lp44281
sg221
(lp44282
sg223
(lp44283
sg354
(lp44284
I705
assS'bag'
p44285
(dp44286
g183
(lp44287
sg484
(lp44288
sg221
(lp44289
I102
asg181
(lp44290
ssS'bad'
p44291
(dp44292
g70
(lp44293
sg26
(lp44294
sg78
(lp44295
sg89
(lp44296
sg132
(lp44297
I888
asg99
(lp44298
sg52
(lp44299
ssS'bao'
p44300
(dp44301
g145
(lp44302
I2171
asg83
(lp44303
ssS'bal'
p44304
(dp44305
g145
(lp44306
I2172
assS'linguist'
p44307
(dp44308
g114
(lp44309
sg130
(lp44310
I115
assS'takeal'
p44311
(dp44312
g20
(lp44313
I2667
asg303
(lp44314
ssS'realvalu'
p44315
(dp44316
g140
(lp44317
I358
assS'schaal'
p44318
(dp44319
g295
(lp44320
sg183
(lp44321
sg313
(lp44322
I2053
assS'subjecl'
p44323
(dp44324
g4
(lp44325
I1532
assS'pauan'
p44326
(dp44327
g183
(lp44328
I5164
assS'subtre'
p44329
(dp44330
g183
(lp44331
sg145
(lp44332
I2144
assS'taiwan'
p44333
(dp44334
g72
(lp44335
I3600
assS'subject'
p44336
(dp44337
g216
(lp44338
sg118
(lp44339
sg74
(lp44340
sg332
(lp44341
sg4
(lp44342
sg78
(lp44343
sg235
(lp44344
sg110
(lp44345
sg384
(lp44346
sg303
(lp44347
sg42
(lp44348
I989
asg429
(lp44349
sg91
(lp44350
sg36
(lp44351
sg102
(lp44352
sg94
(lp44353
sg96
(lp44354
sg99
(lp44355
sg535
(lp44356
sg140
(lp44357
ssS'brazil'
p44358
(dp44359
g118
(lp44360
I21
assS'sinwav'
p44361
(dp44362
g484
(lp44363
I105
assS'said'
p44364
(dp44365
g329
(lp44366
sg221
(lp44367
sg281
(lp44368
sg140
(lp44369
I1801
asg287
(lp44370
ssS'saic'
p44371
(dp44372
g104
(lp44373
I3093
assS'duff'
p44374
(dp44375
g83
(lp44376
I1191
assS'hmds'
p44377
(dp44378
g130
(lp44379
I428
assS'miesbach'
p44380
(dp44381
g163
(lp44382
I132
assS'stinchcomb'
p44383
(dp44384
g68
(lp44385
sg354
(lp44386
I1992
assS'simplest'
p44387
(dp44388
g174
(lp44389
sg74
(lp44390
sg178
(lp44391
sg78
(lp44392
sg221
(lp44393
sg384
(lp44394
sg85
(lp44395
sg42
(lp44396
I1174
asg183
(lp44397
sg102
(lp44398
sg50
(lp44399
sg114
(lp44400
ssS'attribut'
p44401
(dp44402
g283
(lp44403
sg121
(lp44404
sg22
(lp44405
sg277
(lp44406
sg183
(lp44407
sg126
(lp44408
sg89
(lp44409
sg94
(lp44410
sg12
(lp44411
sg14
(lp44412
sg16
(lp44413
sg138
(lp44414
I256
assS'triplet'
p44415
(dp44416
g344
(lp44417
I3177
assS'conjectur'
p44418
(dp44419
g68
(lp44420
sg223
(lp44421
sg140
(lp44422
I1329
asg277
(lp44423
ssS'physicist'
p44424
(dp44425
g384
(lp44426
I138
assS'reserach'
p44427
(dp44428
g293
(lp44429
I3255
assS'lazi'
p44430
(dp44431
g132
(lp44432
I310
assS'harvard'
p44433
(dp44434
g48
(lp44435
I2569
assS'vaud'
p44436
(dp44437
g110
(lp44438
I3090
assS'tangl'
p44439
(dp44440
g181
(lp44441
I1879
assS'against'
p44442
(dp44443
g116
(lp44444
sg118
(lp44445
sg332
(lp44446
sg70
(lp44447
sg4
(lp44448
sg80
(lp44449
sg235
(lp44450
sg256
(lp44451
sg293
(lp44452
sg72
(lp44453
sg83
(lp44454
sg10
(lp44455
sg42
(lp44456
I2701
asg283
(lp44457
sg78
(lp44458
sg132
(lp44459
sg104
(lp44460
sg48
(lp44461
sg99
(lp44462
sg63
(lp44463
sg114
(lp44464
ssS'und'
p44465
(dp44466
g535
(lp44467
I2137
assS'chernoff'
p44468
(dp44469
g85
(lp44470
I3499
assS'uni'
p44471
(dp44472
g176
(lp44473
sg76
(lp44474
sg59
(lp44475
sg42
(lp44476
I2555
asg130
(lp44477
sg132
(lp44478
ssS'uncertainti'
p44479
(dp44480
g230
(lp44481
sg76
(lp44482
sg183
(lp44483
sg460
(lp44484
sg124
(lp44485
sg306
(lp44486
sg14
(lp44487
sg16
(lp44488
sg313
(lp44489
sg354
(lp44490
I2836
assS'ecorrespon'
p44491
(dp44492
g230
(lp44493
I1657
assS'typist'
p44494
(dp44495
g94
(lp44496
I94
assS'ethem'
p44497
(dp44498
g178
(lp44499
I7
assS'nonstationar'
p44500
(dp44501
g83
(lp44502
I1663
assS'forag'
p44503
(dp44504
g70
(lp44505
I2574
assS'height'
p44506
(dp44507
g329
(lp44508
sg6
(lp44509
sg76
(lp44510
sg38
(lp44511
sg42
(lp44512
I2015
asg102
(lp44513
sg46
(lp44514
ssS'brigad'
p44515
(dp44516
g135
(lp44517
I111
assS'influenti'
p44518
(dp44519
g295
(lp44520
I3841
asg183
(lp44521
ssS'learnabl'
p44522
(dp44523
g287
(lp44524
I475
asg145
(lp44525
sg277
(lp44526
sg344
(lp44527
sg183
(lp44528
sg110
(lp44529
ssS'mcnaughton'
p44530
(dp44531
g80
(lp44532
I1962
assS'arbib'
p44533
(dp44534
g104
(lp44535
I860
asg429
(lp44536
ssS'three'
p44537
(dp44538
g68
(lp44539
sg70
(lp44540
sg78
(lp44541
sg277
(lp44542
sg72
(lp44543
sg283
(lp44544
sg181
(lp44545
sg303
(lp44546
sg26
(lp44547
sg30
(lp44548
sg74
(lp44549
sg176
(lp44550
sg76
(lp44551
sg293
(lp44552
sg295
(lp44553
sg183
(lp44554
sg59
(lp44555
sg484
(lp44556
sg38
(lp44557
sg83
(lp44558
sg85
(lp44559
sg63
(lp44560
sg42
(lp44561
I2244
asg87
(lp44562
sg89
(lp44563
sg245
(lp44564
sg94
(lp44565
sg96
(lp44566
sg18
(lp44567
sg221
(lp44568
sg535
(lp44569
sg44
(lp44570
sg350
(lp44571
sg429
(lp44572
sg46
(lp44573
sg104
(lp44574
sg108
(lp44575
sg110
(lp44576
sg20
(lp44577
sg52
(lp44578
sg114
(lp44579
sg116
(lp44580
sg329
(lp44581
sg32
(lp44582
sg318
(lp44583
sg4
(lp44584
sg6
(lp44585
sg8
(lp44586
sg34
(lp44587
sg124
(lp44588
sg126
(lp44589
sg281
(lp44590
sg10
(lp44591
sg344
(lp44592
sg128
(lp44593
sg130
(lp44594
sg132
(lp44595
sg14
(lp44596
sg16
(lp44597
sg135
(lp44598
sg50
(lp44599
sg138
(lp44600
sg354
(lp44601
ssS'ting'
p44602
(dp44603
g42
(lp44604
I1002
assS'trigger'
p44605
(dp44606
g106
(lp44607
I1937
asg70
(lp44608
ssS'interest'
p44609
(dp44610
g68
(lp44611
sg26
(lp44612
sg277
(lp44613
sg163
(lp44614
sg40
(lp44615
sg287
(lp44616
sg145
(lp44617
sg256
(lp44618
sg293
(lp44619
sg344
(lp44620
sg183
(lp44621
sg484
(lp44622
sg83
(lp44623
sg85
(lp44624
sg303
(lp44625
sg306
(lp44626
sg91
(lp44627
sg12
(lp44628
sg46
(lp44629
sg96
(lp44630
sg48
(lp44631
sg99
(lp44632
sg313
(lp44633
sg44
(lp44634
sg318
(lp44635
sg329
(lp44636
sg332
(lp44637
sg102
(lp44638
sg104
(lp44639
sg108
(lp44640
sg63
(lp44641
sg22
(lp44642
sg216
(lp44643
sg174
(lp44644
sg32
(lp44645
sg18
(lp44646
sg178
(lp44647
sg4
(lp44648
sg181
(lp44649
sg8
(lp44650
sg34
(lp44651
sg221
(lp44652
sg124
(lp44653
sg72
(lp44654
sg281
(lp44655
sg535
(lp44656
sg223
(lp44657
sg128
(lp44658
sg78
(lp44659
sg132
(lp44660
sg14
(lp44661
sg16
(lp44662
sg138
(lp44663
sg140
(lp44664
I273
assS'basic'
p44665
(dp44666
g277
(lp44667
sg163
(lp44668
sg287
(lp44669
sg74
(lp44670
sg176
(lp44671
sg145
(lp44672
sg295
(lp44673
sg183
(lp44674
sg484
(lp44675
sg83
(lp44676
sg87
(lp44677
sg12
(lp44678
sg535
(lp44679
sg223
(lp44680
sg350
(lp44681
sg178
(lp44682
sg108
(lp44683
sg110
(lp44684
sg63
(lp44685
sg52
(lp44686
sg32
(lp44687
sg332
(lp44688
sg121
(lp44689
sg181
(lp44690
sg384
(lp44691
sg68
(lp44692
sg40
(lp44693
sg344
(lp44694
sg44
(lp44695
sg78
(lp44696
sg132
(lp44697
sg14
(lp44698
sg16
(lp44699
sg138
(lp44700
sg140
(lp44701
I2328
assS'basin'
p44702
(dp44703
g36
(lp44704
I1055
assS'suppress'
p44705
(dp44706
g178
(lp44707
sg6
(lp44708
sg235
(lp44709
sg429
(lp44710
sg70
(lp44711
sg20
(lp44712
sg18
(lp44713
sg50
(lp44714
I226
assS'dismiss'
p44715
(dp44716
g223
(lp44717
I234
assS'efficaci'
p44718
(dp44719
g535
(lp44720
I198
assS'roychodhuri'
p44721
(dp44722
g145
(lp44723
I3161
assS'cdrom'
p44724
(dp44725
g138
(lp44726
I1998
assS'deepen'
p44727
(dp44728
g132
(lp44729
I1882
asg102
(lp44730
ssS'simard'
p44731
(dp44732
g132
(lp44733
I3755
asg183
(lp44734
sg223
(lp44735
sg44
(lp44736
ssS'tann'
p44737
(dp44738
g230
(lp44739
I1254
assS'tank'
p44740
(dp44741
g42
(lp44742
I3362
asg429
(lp44743
sg8
(lp44744
ssS'occam'
p44745
(dp44746
g30
(lp44747
I856
assS'tanh'
p44748
(dp44749
g126
(lp44750
sg14
(lp44751
sg16
(lp44752
I1465
asg121
(lp44753
sg384
(lp44754
ssS'servic'
p44755
(dp44756
g83
(lp44757
sg10
(lp44758
sg63
(lp44759
sg46
(lp44760
sg138
(lp44761
I2275
asg223
(lp44762
ssS'neal'
p44763
(dp44764
g74
(lp44765
sg70
(lp44766
sg126
(lp44767
sg124
(lp44768
sg72
(lp44769
sg14
(lp44770
sg354
(lp44771
I1335
assS'sll'
p44772
(dp44773
g102
(lp44774
I2326
assS'calcul'
p44775
(dp44776
g26
(lp44777
sg163
(lp44778
sg74
(lp44779
sg145
(lp44780
sg76
(lp44781
sg295
(lp44782
sg183
(lp44783
sg59
(lp44784
sg484
(lp44785
sg83
(lp44786
sg85
(lp44787
sg42
(lp44788
I2972
asg87
(lp44789
sg91
(lp44790
sg245
(lp44791
sg94
(lp44792
sg99
(lp44793
sg313
(lp44794
sg44
(lp44795
sg350
(lp44796
sg116
(lp44797
sg329
(lp44798
sg318
(lp44799
sg102
(lp44800
sg63
(lp44801
sg52
(lp44802
sg114
(lp44803
sg230
(lp44804
sg174
(lp44805
sg32
(lp44806
sg332
(lp44807
sg4
(lp44808
sg235
(lp44809
sg34
(lp44810
sg36
(lp44811
sg384
(lp44812
sg68
(lp44813
sg341
(lp44814
sg10
(lp44815
sg344
(lp44816
sg130
(lp44817
sg14
(lp44818
sg16
(lp44819
sg140
(lp44820
sg354
(lp44821
ssS'euler'
p44822
(dp44823
g34
(lp44824
I700
assS'occas'
p44825
(dp44826
g135
(lp44827
I2110
asg145
(lp44828
ssS'slf'
p44829
(dp44830
g341
(lp44831
I484
assS'anchor'
p44832
(dp44833
g303
(lp44834
I2892
assS'divergeri'
p44835
(dp44836
g89
(lp44837
I1232
assS'slc'
p44838
(dp44839
g350
(lp44840
I809
assS'iy'
p44841
(dp44842
g176
(lp44843
sg72
(lp44844
sg341
(lp44845
I254
asg85
(lp44846
ssS'ix'
p44847
(dp44848
g440
(lp44849
sg283
(lp44850
sg344
(lp44851
sg124
(lp44852
sg72
(lp44853
sg341
(lp44854
sg91
(lp44855
sg354
(lp44856
I2144
asg221
(lp44857
sg535
(lp44858
sg149
(lp44859
ssS'seven'
p44860
(dp44861
g116
(lp44862
sg74
(lp44863
sg283
(lp44864
sg178
(lp44865
sg26
(lp44866
sg59
(lp44867
sg281
(lp44868
sg40
(lp44869
sg128
(lp44870
sg70
(lp44871
sg135
(lp44872
sg99
(lp44873
sg138
(lp44874
I673
assS'iz'
p44875
(dp44876
g262
(lp44877
I1664
assS'krige'
p44878
(dp44879
g124
(lp44880
I1259
assS'mexico'
p44881
(dp44882
g46
(lp44883
I29
asg384
(lp44884
ssS'ip'
p44885
(dp44886
g344
(lp44887
sg96
(lp44888
I1099
asg221
(lp44889
ssS'is'
p44890
(dp44891
g80
(lp44892
sg293
(lp44893
sg344
(lp44894
sg78
(lp44895
sg59
(lp44896
sg484
(lp44897
sg38
(lp44898
sg83
(lp44899
sg85
(lp44900
sg303
(lp44901
sg438
(lp44902
sg116
(lp44903
sg118
(lp44904
sg34
(lp44905
sg36
(lp44906
sg460
(lp44907
sg68
(lp44908
sg72
(lp44909
sg281
(lp44910
sg10
(lp44911
sg40
(lp44912
sg283
(lp44913
sg70
(lp44914
sg26
(lp44915
sg277
(lp44916
sg163
(lp44917
sg89
(lp44918
sg91
(lp44919
sg12
(lp44920
sg94
(lp44921
sg96
(lp44922
sg48
(lp44923
sg99
(lp44924
sg313
(lp44925
sg44
(lp44926
sg149
(lp44927
sg429
(lp44928
sg102
(lp44929
sg104
(lp44930
sg106
(lp44931
sg108
(lp44932
sg110
(lp44933
sg63
(lp44934
sg52
(lp44935
sg114
(lp44936
sg128
(lp44937
sg130
(lp44938
sg132
(lp44939
sg14
(lp44940
sg16
(lp44941
sg135
(lp44942
sg50
(lp44943
sg138
(lp44944
sg140
(lp44945
sg354
(lp44946
sg306
(lp44947
sg87
(lp44948
sg245
(lp44949
sg46
(lp44950
sg20
(lp44951
sg18
(lp44952
sg221
(lp44953
sg535
(lp44954
sg223
(lp44955
sg350
(lp44956
sg216
(lp44957
sg174
(lp44958
sg440
(lp44959
sg332
(lp44960
sg121
(lp44961
sg4
(lp44962
sg6
(lp44963
sg8
(lp44964
sg126
(lp44965
sg341
(lp44966
sg30
(lp44967
sg287
(lp44968
sg74
(lp44969
sg176
(lp44970
sg145
(lp44971
sg256
(lp44972
sg76
(lp44973
sg262
(lp44974
sg295
(lp44975
sg183
(lp44976
sg42
(lp44977
I28
asg230
(lp44978
sg329
(lp44979
sg32
(lp44980
sg318
(lp44981
sg178
(lp44982
sg22
(lp44983
sg181
(lp44984
sg235
(lp44985
sg384
(lp44986
sg124
(lp44987
ssS'ir'
p44988
(dp44989
g230
(lp44990
sg287
(lp44991
sg295
(lp44992
sg183
(lp44993
sg34
(lp44994
sg130
(lp44995
sg46
(lp44996
sg99
(lp44997
sg354
(lp44998
I1143
assS'iu'
p44999
(dp45000
g460
(lp45001
I1355
assS'it'
p45002
(dp45003
g80
(lp45004
sg293
(lp45005
sg344
(lp45006
sg78
(lp45007
sg59
(lp45008
sg484
(lp45009
sg38
(lp45010
sg83
(lp45011
sg85
(lp45012
sg303
(lp45013
sg438
(lp45014
sg116
(lp45015
sg118
(lp45016
sg34
(lp45017
sg36
(lp45018
sg460
(lp45019
sg68
(lp45020
sg72
(lp45021
sg281
(lp45022
sg10
(lp45023
sg40
(lp45024
sg283
(lp45025
sg70
(lp45026
sg26
(lp45027
sg277
(lp45028
sg163
(lp45029
sg89
(lp45030
sg91
(lp45031
sg12
(lp45032
sg94
(lp45033
sg96
(lp45034
sg48
(lp45035
sg99
(lp45036
sg313
(lp45037
sg44
(lp45038
sg149
(lp45039
sg429
(lp45040
sg102
(lp45041
sg104
(lp45042
sg106
(lp45043
sg108
(lp45044
sg110
(lp45045
sg63
(lp45046
sg52
(lp45047
sg114
(lp45048
sg128
(lp45049
sg130
(lp45050
sg132
(lp45051
sg14
(lp45052
sg16
(lp45053
sg135
(lp45054
sg50
(lp45055
sg138
(lp45056
sg140
(lp45057
sg354
(lp45058
sg306
(lp45059
sg87
(lp45060
sg245
(lp45061
sg46
(lp45062
sg20
(lp45063
sg18
(lp45064
sg221
(lp45065
sg535
(lp45066
sg223
(lp45067
sg350
(lp45068
sg216
(lp45069
sg174
(lp45070
sg440
(lp45071
sg332
(lp45072
sg121
(lp45073
sg4
(lp45074
sg6
(lp45075
sg8
(lp45076
sg126
(lp45077
sg341
(lp45078
sg30
(lp45079
sg287
(lp45080
sg74
(lp45081
sg176
(lp45082
sg145
(lp45083
sg256
(lp45084
sg76
(lp45085
sg262
(lp45086
sg295
(lp45087
sg183
(lp45088
sg42
(lp45089
I7
asg230
(lp45090
sg329
(lp45091
sg32
(lp45092
sg318
(lp45093
sg178
(lp45094
sg22
(lp45095
sg181
(lp45096
sg235
(lp45097
sg384
(lp45098
sg124
(lp45099
ssS'iw'
p45100
(dp45101
g230
(lp45102
sg329
(lp45103
sg36
(lp45104
sg130
(lp45105
sg104
(lp45106
sg14
(lp45107
sg354
(lp45108
I572
assS'iv'
p45109
(dp45110
g438
(lp45111
I7
asg59
(lp45112
sg36
(lp45113
sg384
(lp45114
sg14
(lp45115
sg63
(lp45116
sg87
(lp45117
sg130
(lp45118
sg245
(lp45119
sg104
(lp45120
sg313
(lp45121
ssS'ii'
p45122
(dp45123
g68
(lp45124
sg78
(lp45125
sg163
(lp45126
sg283
(lp45127
sg85
(lp45128
sg181
(lp45129
sg145
(lp45130
sg256
(lp45131
sg76
(lp45132
sg183
(lp45133
sg484
(lp45134
sg114
(lp45135
sg303
(lp45136
sg42
(lp45137
I1284
asg306
(lp45138
sg12
(lp45139
sg46
(lp45140
sg20
(lp45141
sg48
(lp45142
sg221
(lp45143
sg313
(lp45144
sg44
(lp45145
sg149
(lp45146
sg116
(lp45147
sg329
(lp45148
sg245
(lp45149
sg102
(lp45150
sg104
(lp45151
sg108
(lp45152
sg110
(lp45153
sg52
(lp45154
sg22
(lp45155
sg230
(lp45156
sg174
(lp45157
sg440
(lp45158
sg332
(lp45159
sg121
(lp45160
sg4
(lp45161
sg6
(lp45162
sg235
(lp45163
sg36
(lp45164
sg384
(lp45165
sg124
(lp45166
sg72
(lp45167
sg10
(lp45168
sg535
(lp45169
sg223
(lp45170
sg130
(lp45171
sg132
(lp45172
sg14
(lp45173
sg16
(lp45174
sg350
(lp45175
sg50
(lp45176
sg138
(lp45177
sg354
(lp45178
ssS'ih'
p45179
(dp45180
g287
(lp45181
sg460
(lp45182
sg535
(lp45183
sg223
(lp45184
I2147
assS'ik'
p45185
(dp45186
g74
(lp45187
sg70
(lp45188
sg22
(lp45189
sg78
(lp45190
sg130
(lp45191
I1621
asg104
(lp45192
sg223
(lp45193
ssS'ij'
p45194
(dp45195
g283
(lp45196
sg30
(lp45197
sg145
(lp45198
sg78
(lp45199
sg46
(lp45200
sg96
(lp45201
sg221
(lp45202
sg535
(lp45203
sg350
(lp45204
sg118
(lp45205
sg429
(lp45206
sg104
(lp45207
sg108
(lp45208
sg174
(lp45209
sg121
(lp45210
sg4
(lp45211
sg8
(lp45212
sg99
(lp45213
sg384
(lp45214
sg68
(lp45215
sg40
(lp45216
sg130
(lp45217
sg50
(lp45218
I867
asg460
(lp45219
ssS'im'
p45220
(dp45221
g138
(lp45222
I499
asg6
(lp45223
sg256
(lp45224
ssS'il'
p45225
(dp45226
g344
(lp45227
sg32
(lp45228
sg68
(lp45229
sg145
(lp45230
sg78
(lp45231
sg8
(lp45232
sg295
(lp45233
sg221
(lp45234
sg262
(lp45235
sg38
(lp45236
sg74
(lp45237
sg10
(lp45238
sg40
(lp45239
sg245
(lp45240
sg110
(lp45241
sg85
(lp45242
sg183
(lp45243
sg12
(lp45244
sg48
(lp45245
sg99
(lp45246
I272
assS'io'
p45247
(dp45248
g245
(lp45249
sg30
(lp45250
sg74
(lp45251
sg70
(lp45252
sg354
(lp45253
I2265
assS'in'
p45254
(dp45255
g80
(lp45256
sg293
(lp45257
sg344
(lp45258
sg78
(lp45259
sg59
(lp45260
sg484
(lp45261
sg38
(lp45262
sg83
(lp45263
sg85
(lp45264
sg303
(lp45265
sg438
(lp45266
sg116
(lp45267
sg118
(lp45268
sg34
(lp45269
sg36
(lp45270
sg460
(lp45271
sg68
(lp45272
sg72
(lp45273
sg281
(lp45274
sg10
(lp45275
sg40
(lp45276
sg283
(lp45277
sg70
(lp45278
sg26
(lp45279
sg277
(lp45280
sg163
(lp45281
sg89
(lp45282
sg91
(lp45283
sg12
(lp45284
sg94
(lp45285
sg96
(lp45286
sg48
(lp45287
sg99
(lp45288
sg313
(lp45289
sg44
(lp45290
sg149
(lp45291
sg429
(lp45292
sg102
(lp45293
sg104
(lp45294
sg106
(lp45295
sg108
(lp45296
sg110
(lp45297
sg63
(lp45298
sg52
(lp45299
sg114
(lp45300
sg128
(lp45301
sg130
(lp45302
sg132
(lp45303
sg14
(lp45304
sg16
(lp45305
sg135
(lp45306
sg50
(lp45307
sg138
(lp45308
sg140
(lp45309
sg354
(lp45310
sg306
(lp45311
sg87
(lp45312
sg245
(lp45313
sg46
(lp45314
sg20
(lp45315
sg18
(lp45316
sg221
(lp45317
sg535
(lp45318
sg223
(lp45319
sg350
(lp45320
sg216
(lp45321
sg174
(lp45322
sg440
(lp45323
sg332
(lp45324
sg121
(lp45325
sg4
(lp45326
sg6
(lp45327
sg8
(lp45328
sg126
(lp45329
sg341
(lp45330
sg30
(lp45331
sg287
(lp45332
sg74
(lp45333
sg176
(lp45334
sg145
(lp45335
sg256
(lp45336
sg76
(lp45337
sg262
(lp45338
sg295
(lp45339
sg183
(lp45340
sg42
(lp45341
I47
asg230
(lp45342
sg329
(lp45343
sg32
(lp45344
sg318
(lp45345
sg178
(lp45346
sg22
(lp45347
sg181
(lp45348
sg235
(lp45349
sg384
(lp45350
sg124
(lp45351
ssS'ia'
p45352
(dp45353
g40
(lp45354
sg130
(lp45355
I1380
assS'vendor'
p45356
(dp45357
g78
(lp45358
I438
assS'ic'
p45359
(dp45360
g230
(lp45361
sg176
(lp45362
sg121
(lp45363
sg484
(lp45364
sg10
(lp45365
sg87
(lp45366
sg102
(lp45367
I2177
asg46
(lp45368
ssS'ib'
p45369
(dp45370
g230
(lp45371
sg318
(lp45372
I1120
asg70
(lp45373
sg303
(lp45374
ssS'ie'
p45375
(dp45376
g30
(lp45377
sg174
(lp45378
sg283
(lp45379
sg121
(lp45380
sg76
(lp45381
sg8
(lp45382
sg34
(lp45383
sg59
(lp45384
sg484
(lp45385
sg42
(lp45386
I2567
asg14
(lp45387
sg48
(lp45388
sg221
(lp45389
sg149
(lp45390
ssS'id'
p45391
(dp45392
g287
(lp45393
sg124
(lp45394
sg130
(lp45395
sg341
(lp45396
sg46
(lp45397
sg14
(lp45398
sg106
(lp45399
I1939
asg108
(lp45400
ssS'sever'
p45401
(dp45402
g124
(lp45403
sg70
(lp45404
sg26
(lp45405
sg72
(lp45406
sg281
(lp45407
sg181
(lp45408
sg40
(lp45409
sg74
(lp45410
sg145
(lp45411
sg80
(lp45412
sg76
(lp45413
sg295
(lp45414
sg183
(lp45415
sg59
(lp45416
sg484
(lp45417
sg83
(lp45418
sg85
(lp45419
sg303
(lp45420
sg306
(lp45421
sg89
(lp45422
sg91
(lp45423
sg94
(lp45424
sg48
(lp45425
sg221
(lp45426
sg313
(lp45427
sg223
(lp45428
sg149
(lp45429
sg32
(lp45430
sg429
(lp45431
sg68
(lp45432
sg102
(lp45433
sg63
(lp45434
sg52
(lp45435
sg22
(lp45436
sg118
(lp45437
sg440
(lp45438
sg4
(lp45439
sg6
(lp45440
sg8
(lp45441
sg34
(lp45442
sg235
(lp45443
sg126
(lp45444
sg341
(lp45445
sg10
(lp45446
sg535
(lp45447
sg344
(lp45448
sg44
(lp45449
sg128
(lp45450
sg78
(lp45451
sg132
(lp45452
sg14
(lp45453
sg16
(lp45454
sg350
(lp45455
sg138
(lp45456
sg140
(lp45457
sg354
(lp45458
I288
assS'if'
p45459
(dp45460
g80
(lp45461
sg293
(lp45462
sg344
(lp45463
sg78
(lp45464
sg59
(lp45465
sg484
(lp45466
sg38
(lp45467
sg83
(lp45468
sg85
(lp45469
sg303
(lp45470
sg438
(lp45471
sg116
(lp45472
sg118
(lp45473
sg34
(lp45474
sg36
(lp45475
sg460
(lp45476
sg68
(lp45477
sg72
(lp45478
sg281
(lp45479
sg10
(lp45480
sg40
(lp45481
sg283
(lp45482
sg70
(lp45483
sg26
(lp45484
sg277
(lp45485
sg163
(lp45486
sg89
(lp45487
sg91
(lp45488
sg12
(lp45489
sg94
(lp45490
sg96
(lp45491
sg48
(lp45492
sg99
(lp45493
sg44
(lp45494
sg429
(lp45495
sg102
(lp45496
sg104
(lp45497
sg108
(lp45498
sg110
(lp45499
sg63
(lp45500
sg52
(lp45501
sg114
(lp45502
sg128
(lp45503
sg130
(lp45504
sg132
(lp45505
sg14
(lp45506
sg16
(lp45507
sg135
(lp45508
sg50
(lp45509
sg138
(lp45510
sg140
(lp45511
sg354
(lp45512
sg306
(lp45513
sg245
(lp45514
sg46
(lp45515
sg20
(lp45516
sg18
(lp45517
sg221
(lp45518
sg535
(lp45519
sg223
(lp45520
sg174
(lp45521
sg440
(lp45522
sg332
(lp45523
sg121
(lp45524
sg6
(lp45525
sg8
(lp45526
sg126
(lp45527
sg341
(lp45528
sg30
(lp45529
sg287
(lp45530
sg74
(lp45531
sg176
(lp45532
sg145
(lp45533
sg256
(lp45534
sg76
(lp45535
sg262
(lp45536
sg295
(lp45537
sg183
(lp45538
sg42
(lp45539
I320
asg230
(lp45540
sg329
(lp45541
sg32
(lp45542
sg318
(lp45543
sg178
(lp45544
sg181
(lp45545
sg235
(lp45546
sg384
(lp45547
sg124
(lp45548
ssS'microstructur'
p45549
(dp45550
g344
(lp45551
sg108
(lp45552
I2581
asg40
(lp45553
sg76
(lp45554
sg114
(lp45555
ssS'receiv'
p45556
(dp45557
g329
(lp45558
sg277
(lp45559
sg176
(lp45560
sg145
(lp45561
sg256
(lp45562
sg76
(lp45563
sg295
(lp45564
sg183
(lp45565
sg38
(lp45566
sg83
(lp45567
sg114
(lp45568
sg12
(lp45569
sg20
(lp45570
sg18
(lp45571
sg44
(lp45572
sg149
(lp45573
sg116
(lp45574
sg118
(lp45575
sg110
(lp45576
sg63
(lp45577
sg174
(lp45578
sg216
(lp45579
sg438
(lp45580
I150
asg332
(lp45581
sg36
(lp45582
sg78
(lp45583
sg14
(lp45584
sg354
(lp45585
ssS'make'
p45586
(dp45587
g124
(lp45588
sg70
(lp45589
sg277
(lp45590
sg283
(lp45591
sg30
(lp45592
sg74
(lp45593
sg176
(lp45594
sg145
(lp45595
sg76
(lp45596
sg262
(lp45597
sg295
(lp45598
sg183
(lp45599
sg59
(lp45600
sg484
(lp45601
sg83
(lp45602
sg85
(lp45603
sg42
(lp45604
I158
asg306
(lp45605
sg87
(lp45606
sg89
(lp45607
sg91
(lp45608
sg12
(lp45609
sg94
(lp45610
sg48
(lp45611
sg99
(lp45612
sg223
(lp45613
sg149
(lp45614
sg116
(lp45615
sg329
(lp45616
sg293
(lp45617
sg350
(lp45618
sg318
(lp45619
sg46
(lp45620
sg102
(lp45621
sg104
(lp45622
sg106
(lp45623
sg63
(lp45624
sg52
(lp45625
sg230
(lp45626
sg174
(lp45627
sg440
(lp45628
sg332
(lp45629
sg178
(lp45630
sg4
(lp45631
sg8
(lp45632
sg34
(lp45633
sg384
(lp45634
sg235
(lp45635
sg126
(lp45636
sg341
(lp45637
sg10
(lp45638
sg344
(lp45639
sg128
(lp45640
sg130
(lp45641
sg14
(lp45642
sg16
(lp45643
sg135
(lp45644
sg138
(lp45645
sg140
(lp45646
ssS'viabil'
p45647
(dp45648
g80
(lp45649
I1567
assS'isotrop'
p45650
(dp45651
g12
(lp45652
I245
asg36
(lp45653
sg38
(lp45654
ssS'roland'
p45655
(dp45656
g99
(lp45657
I3283
assS'lqi'
p45658
(dp45659
g46
(lp45660
I1738
asg87
(lp45661
ssS'lqf'
p45662
(dp45663
g83
(lp45664
I2118
assS'sat'
p45665
(dp45666
g85
(lp45667
I4286
assS'sacramento'
p45668
(dp45669
g223
(lp45670
I3137
assS'kip'
p45671
(dp45672
g221
(lp45673
I419
assS'kit'
p45674
(dp45675
g68
(lp45676
I1997
assS'kij'
p45677
(dp45678
g124
(lp45679
sg121
(lp45680
I387
assS'kim'
p45681
(dp45682
g174
(lp45683
sg48
(lp45684
I2184
assS'overt'
p45685
(dp45686
g14
(lp45687
I4055
asg99
(lp45688
sg178
(lp45689
ssS'overs'
p45690
(dp45691
g78
(lp45692
I209
assS'facult'
p45693
(dp45694
g440
(lp45695
I226
assS'lefi'
p45696
(dp45697
g245
(lp45698
I1794
assS'stimuli'
p45699
(dp45700
g216
(lp45701
sg118
(lp45702
sg74
(lp45703
sg332
(lp45704
sg70
(lp45705
sg4
(lp45706
sg6
(lp45707
sg256
(lp45708
sg116
(lp45709
sg303
(lp45710
sg245
(lp45711
sg176
(lp45712
sg78
(lp45713
sg12
(lp45714
sg106
(lp45715
I551
asg18
(lp45716
sg110
(lp45717
sg535
(lp45718
sg149
(lp45719
ssS'xji'
p45720
(dp45721
g384
(lp45722
I1836
asg72
(lp45723
ssS'inherit'
p45724
(dp45725
g104
(lp45726
I3024
asg384
(lp45727
sg460
(lp45728
ssS'qualit'
p45729
(dp45730
g438
(lp45731
I97
asg4
(lp45732
sg181
(lp45733
sg329
(lp45734
sg262
(lp45735
sg384
(lp45736
sg235
(lp45737
sg38
(lp45738
sg85
(lp45739
sg429
(lp45740
sg12
(lp45741
sg46
(lp45742
sg18
(lp45743
ssS'rome'
p45744
(dp45745
g14
(lp45746
sg16
(lp45747
I2600
asg281
(lp45748
ssS'prefront'
p45749
(dp45750
g4
(lp45751
I5
assS'programm'
p45752
(dp45753
g14
(lp45754
I2904
asg104
(lp45755
sg10
(lp45756
sg94
(lp45757
ssS'paradigm'
p45758
(dp45759
g30
(lp45760
sg287
(lp45761
sg74
(lp45762
sg4
(lp45763
sg118
(lp45764
sg116
(lp45765
sg293
(lp45766
sg429
(lp45767
sg106
(lp45768
I535
asg99
(lp45769
ssS'xtist'
p45770
(dp45771
g76
(lp45772
I2715
assS'left'
p45773
(dp45774
g26
(lp45775
sg163
(lp45776
sg303
(lp45777
sg76
(lp45778
sg262
(lp45779
sg183
(lp45780
sg59
(lp45781
sg80
(lp45782
sg83
(lp45783
sg85
(lp45784
sg63
(lp45785
sg42
(lp45786
I1742
asg87
(lp45787
sg89
(lp45788
sg245
(lp45789
sg94
(lp45790
sg20
(lp45791
sg48
(lp45792
sg221
(lp45793
sg313
(lp45794
sg149
(lp45795
sg116
(lp45796
sg118
(lp45797
sg293
(lp45798
sg32
(lp45799
sg12
(lp45800
sg318
(lp45801
sg46
(lp45802
sg110
(lp45803
sg96
(lp45804
sg52
(lp45805
sg114
(lp45806
sg216
(lp45807
sg438
(lp45808
sg440
(lp45809
sg18
(lp45810
sg121
(lp45811
sg181
(lp45812
sg6
(lp45813
sg8
(lp45814
sg36
(lp45815
sg235
(lp45816
sg126
(lp45817
sg132
(lp45818
sg350
(lp45819
sg50
(lp45820
sg140
(lp45821
sg354
(lp45822
ssS'gipi'
p45823
(dp45824
g329
(lp45825
I1676
assS'facto'
p45826
(dp45827
g94
(lp45828
I445
assS'just'
p45829
(dp45830
g70
(lp45831
sg277
(lp45832
sg287
(lp45833
sg76
(lp45834
sg262
(lp45835
sg344
(lp45836
sg183
(lp45837
sg85
(lp45838
sg303
(lp45839
sg42
(lp45840
I2402
asg46
(lp45841
sg20
(lp45842
sg18
(lp45843
sg99
(lp45844
sg223
(lp45845
sg429
(lp45846
sg102
(lp45847
sg104
(lp45848
sg63
(lp45849
sg216
(lp45850
sg174
(lp45851
sg32
(lp45852
sg48
(lp45853
sg80
(lp45854
sg221
(lp45855
sg384
(lp45856
sg68
(lp45857
sg72
(lp45858
sg44
(lp45859
sg132
(lp45860
sg50
(lp45861
sg138
(lp45862
sg140
(lp45863
ssS'coefficientnam'
p45864
(dp45865
g145
(lp45866
I1057
assS'ommatidia'
p45867
(dp45868
g256
(lp45869
I281
assS'yee'
p45870
(dp45871
g89
(lp45872
I1461
assS'bandwidth'
p45873
(dp45874
g216
(lp45875
sg174
(lp45876
sg22
(lp45877
sg10
(lp45878
sg245
(lp45879
sg14
(lp45880
sg16
(lp45881
I1769
asg96
(lp45882
ssS'psych'
p45883
(dp45884
g329
(lp45885
sg74
(lp45886
sg332
(lp45887
I2642
asg178
(lp45888
sg460
(lp45889
ssS'human'
p45890
(dp45891
g30
(lp45892
sg176
(lp45893
sg145
(lp45894
sg293
(lp45895
sg295
(lp45896
sg183
(lp45897
sg59
(lp45898
sg83
(lp45899
sg303
(lp45900
sg42
(lp45901
I1141
asg91
(lp45902
sg245
(lp45903
sg94
(lp45904
sg18
(lp45905
sg99
(lp45906
sg313
(lp45907
sg223
(lp45908
sg350
(lp45909
sg12
(lp45910
sg110
(lp45911
sg63
(lp45912
sg52
(lp45913
sg216
(lp45914
sg332
(lp45915
sg178
(lp45916
sg4
(lp45917
sg181
(lp45918
sg8
(lp45919
sg40
(lp45920
sg344
(lp45921
sg44
(lp45922
sg78
(lp45923
sg132
(lp45924
sg149
(lp45925
ssS'hongo'
p45926
(dp45927
g36
(lp45928
I27
assS'yes'
p45929
(dp45930
g94
(lp45931
I368
asg91
(lp45932
ssS'yet'
p45933
(dp45934
g277
(lp45935
sg262
(lp45936
sg344
(lp45937
sg183
(lp45938
sg59
(lp45939
sg83
(lp45940
sg89
(lp45941
sg12
(lp45942
sg18
(lp45943
sg44
(lp45944
sg329
(lp45945
sg106
(lp45946
I169
asg63
(lp45947
sg114
(lp45948
sg174
(lp45949
sg22
(lp45950
sg6
(lp45951
sg36
(lp45952
sg384
(lp45953
sg68
(lp45954
sg128
(lp45955
sg14
(lp45956
sg135
(lp45957
sg460
(lp45958
ssS'languag'
p45959
(dp45960
g174
(lp45961
sg440
(lp45962
sg332
(lp45963
sg4
(lp45964
sg76
(lp45965
sg293
(lp45966
sg10
(lp45967
sg87
(lp45968
sg94
(lp45969
I1081
asg110
(lp45970
sg223
(lp45971
ssS'metropoli'
p45972
(dp45973
g124
(lp45974
sg126
(lp45975
sg354
(lp45976
I2323
assS'propuls'
p45977
(dp45978
g68
(lp45979
I260
assS'character'
p45980
(dp45981
g287
(lp45982
sg18
(lp45983
sg145
(lp45984
sg22
(lp45985
sg262
(lp45986
sg303
(lp45987
sg38
(lp45988
sg535
(lp45989
sg306
(lp45990
sg128
(lp45991
sg130
(lp45992
sg102
(lp45993
sg94
(lp45994
sg104
(lp45995
sg48
(lp45996
sg110
(lp45997
sg138
(lp45998
I327
asg223
(lp45999
ssS'llieu'
p46000
(dp46001
g104
(lp46002
I2495
assS'sensat'
p46003
(dp46004
g174
(lp46005
I2755
assS'friedrich'
p46006
(dp46007
g130
(lp46008
I11
assS'relearn'
p46009
(dp46010
g460
(lp46011
I2714
assS'save'
p46012
(dp46013
g287
(lp46014
sg440
(lp46015
sg318
(lp46016
sg293
(lp46017
sg124
(lp46018
sg126
(lp46019
sg85
(lp46020
sg429
(lp46021
sg132
(lp46022
I872
asg94
(lp46023
ssS'adress'
p46024
(dp46025
g36
(lp46026
I2785
assS'opt'
p46027
(dp46028
g245
(lp46029
sg14
(lp46030
sg16
(lp46031
I1951
asg36
(lp46032
ssS'applic'
p46033
(dp46034
g329
(lp46035
sg26
(lp46036
sg277
(lp46037
sg163
(lp46038
sg283
(lp46039
sg30
(lp46040
sg287
(lp46041
sg74
(lp46042
sg145
(lp46043
sg76
(lp46044
sg460
(lp46045
sg183
(lp46046
sg59
(lp46047
sg83
(lp46048
sg124
(lp46049
sg42
(lp46050
I8
asg306
(lp46051
sg87
(lp46052
sg91
(lp46053
sg12
(lp46054
sg94
(lp46055
sg96
(lp46056
sg68
(lp46057
sg221
(lp46058
sg313
(lp46059
sg44
(lp46060
sg230
(lp46061
sg174
(lp46062
sg32
(lp46063
sg429
(lp46064
sg318
(lp46065
sg102
(lp46066
sg104
(lp46067
sg106
(lp46068
sg108
(lp46069
sg110
(lp46070
sg20
(lp46071
sg52
(lp46072
sg114
(lp46073
sg216
(lp46074
sg438
(lp46075
sg440
(lp46076
sg332
(lp46077
sg121
(lp46078
sg22
(lp46079
sg181
(lp46080
sg8
(lp46081
sg36
(lp46082
sg384
(lp46083
sg235
(lp46084
sg126
(lp46085
sg281
(lp46086
sg10
(lp46087
sg40
(lp46088
sg344
(lp46089
sg63
(lp46090
sg223
(lp46091
sg128
(lp46092
sg132
(lp46093
sg14
(lp46094
sg16
(lp46095
sg138
(lp46096
sg140
(lp46097
ssS'dissip'
p46098
(dp46099
g174
(lp46100
sg20
(lp46101
sg135
(lp46102
I2075
asg22
(lp46103
ssS'coron'
p46104
(dp46105
g32
(lp46106
I1893
assS'angluin'
p46107
(dp46108
g104
(lp46109
I869
assS'thenfor'
p46110
(dp46111
g341
(lp46112
I2605
assS'boldfac'
p46113
(dp46114
g22
(lp46115
I1031
assS'interior'
p46116
(dp46117
g230
(lp46118
I1943
assS'esus'
p46119
(dp46120
g341
(lp46121
I1542
assS'background'
p46122
(dp46123
g118
(lp46124
sg174
(lp46125
sg32
(lp46126
sg332
(lp46127
sg256
(lp46128
sg181
(lp46129
sg76
(lp46130
sg78
(lp46131
sg384
(lp46132
sg484
(lp46133
sg277
(lp46134
sg313
(lp46135
sg59
(lp46136
sg138
(lp46137
I891
asg52
(lp46138
ssS'elabor'
p46139
(dp46140
g384
(lp46141
I2084
asg74
(lp46142
ssS'oph'
p46143
(dp46144
g72
(lp46145
I858
assS'shoulder'
p46146
(dp46147
g99
(lp46148
I303
assS'ommatidium'
p46149
(dp46150
g256
(lp46151
I284
assS'negat'
p46152
(dp46153
g26
(lp46154
sg163
(lp46155
sg176
(lp46156
sg262
(lp46157
sg344
(lp46158
sg484
(lp46159
sg38
(lp46160
sg85
(lp46161
sg42
(lp46162
I2513
asg245
(lp46163
sg46
(lp46164
sg114
(lp46165
sg99
(lp46166
sg535
(lp46167
sg223
(lp46168
sg350
(lp46169
sg329
(lp46170
sg293
(lp46171
sg12
(lp46172
sg102
(lp46173
sg106
(lp46174
sg174
(lp46175
sg230
(lp46176
sg438
(lp46177
sg118
(lp46178
sg121
(lp46179
sg4
(lp46180
sg8
(lp46181
sg36
(lp46182
sg124
(lp46183
sg72
(lp46184
sg14
(lp46185
sg135
(lp46186
sg138
(lp46187
sg140
(lp46188
ssS'vdd'
p46189
(dp46190
g20
(lp46191
I901
assS'trigonometr'
p46192
(dp46193
g59
(lp46194
I2101
assS'unnecessari'
p46195
(dp46196
g91
(lp46197
I336
asg235
(lp46198
ssS'stryker'
p46199
(dp46200
g106
(lp46201
I2447
asg48
(lp46202
sg176
(lp46203
sg149
(lp46204
ssS'www'
p46205
(dp46206
g295
(lp46207
sg183
(lp46208
sg124
(lp46209
sg121
(lp46210
I228
asg223
(lp46211
ssS'dean'
p46212
(dp46213
g94
(lp46214
I14
assS'deal'
p46215
(dp46216
g287
(lp46217
sg59
(lp46218
sg295
(lp46219
sg183
(lp46220
sg384
(lp46221
sg124
(lp46222
sg83
(lp46223
sg40
(lp46224
sg34
(lp46225
sg306
(lp46226
sg460
(lp46227
sg102
(lp46228
sg429
(lp46229
sg96
(lp46230
sg63
(lp46231
sg138
(lp46232
I1671
assS'hyperstriatum'
p46233
(dp46234
g116
(lp46235
I346
assS'interv'
p46236
(dp46237
g163
(lp46238
sg287
(lp46239
sg74
(lp46240
sg176
(lp46241
sg262
(lp46242
sg295
(lp46243
sg183
(lp46244
sg38
(lp46245
sg83
(lp46246
sg85
(lp46247
sg42
(lp46248
I2833
asg91
(lp46249
sg46
(lp46250
sg221
(lp46251
sg102
(lp46252
sg106
(lp46253
sg332
(lp46254
sg6
(lp46255
sg181
(lp46256
sg99
(lp46257
sg68
(lp46258
sg126
(lp46259
sg341
(lp46260
sg78
(lp46261
sg132
(lp46262
sg140
(lp46263
ssS'disord'
p46264
(dp46265
g4
(lp46266
sg303
(lp46267
sg429
(lp46268
sg91
(lp46269
sg48
(lp46270
I788
asg221
(lp46271
ssS'maxim'
p46272
(dp46273
g70
(lp46274
sg277
(lp46275
sg30
(lp46276
sg287
(lp46277
sg74
(lp46278
sg76
(lp46279
sg293
(lp46280
sg295
(lp46281
sg183
(lp46282
sg80
(lp46283
sg42
(lp46284
I1710
asg91
(lp46285
sg12
(lp46286
sg221
(lp46287
sg313
(lp46288
sg116
(lp46289
sg118
(lp46290
sg318
(lp46291
sg102
(lp46292
sg110
(lp46293
sg63
(lp46294
sg22
(lp46295
sg216
(lp46296
sg329
(lp46297
sg440
(lp46298
sg332
(lp46299
sg4
(lp46300
sg235
(lp46301
sg36
(lp46302
sg460
(lp46303
sg124
(lp46304
sg72
(lp46305
sg10
(lp46306
sg535
(lp46307
sg130
(lp46308
sg50
(lp46309
sg138
(lp46310
sg140
(lp46311
sg354
(lp46312
ssS'dead'
p46313
(dp46314
g230
(lp46315
I1285
assS'linford'
p46316
(dp46317
g52
(lp46318
I2625
assS'intern'
p46319
(dp46320
g70
(lp46321
sg78
(lp46322
sg277
(lp46323
sg104
(lp46324
sg30
(lp46325
sg287
(lp46326
sg74
(lp46327
sg76
(lp46328
sg293
(lp46329
sg344
(lp46330
sg183
(lp46331
sg38
(lp46332
sg303
(lp46333
sg87
(lp46334
sg91
(lp46335
sg245
(lp46336
sg99
(lp46337
sg223
(lp46338
sg149
(lp46339
sg12
(lp46340
sg178
(lp46341
sg108
(lp46342
sg110
(lp46343
sg114
(lp46344
sg174
(lp46345
sg440
(lp46346
sg318
(lp46347
sg121
(lp46348
sg80
(lp46349
sg181
(lp46350
sg34
(lp46351
sg68
(lp46352
sg10
(lp46353
sg130
(lp46354
sg132
(lp46355
sg50
(lp46356
sg138
(lp46357
I2673
assS'inact'
p46358
(dp46359
g106
(lp46360
I458
asg70
(lp46361
sg149
(lp46362
ssS'total'
p46363
(dp46364
g124
(lp46365
sg26
(lp46366
sg163
(lp46367
sg30
(lp46368
sg287
(lp46369
sg176
(lp46370
sg145
(lp46371
sg295
(lp46372
sg183
(lp46373
sg484
(lp46374
sg306
(lp46375
sg87
(lp46376
sg89
(lp46377
sg91
(lp46378
sg12
(lp46379
sg46
(lp46380
sg20
(lp46381
sg114
(lp46382
sg149
(lp46383
sg118
(lp46384
sg94
(lp46385
sg102
(lp46386
sg106
(lp46387
sg108
(lp46388
sg96
(lp46389
sg22
(lp46390
sg230
(lp46391
sg438
(lp46392
I1442
asg32
(lp46393
sg121
(lp46394
sg4
(lp46395
sg181
(lp46396
sg8
(lp46397
sg384
(lp46398
sg235
(lp46399
sg126
(lp46400
sg10
(lp46401
sg40
(lp46402
sg130
(lp46403
sg132
(lp46404
sg14
(lp46405
sg16
(lp46406
sg138
(lp46407
ssS'offaci'
p46408
(dp46409
g293
(lp46410
I1108
assS'personnaz'
p46411
(dp46412
g50
(lp46413
I1645
asg128
(lp46414
ssS'wittner'
p46415
(dp46416
g341
(lp46417
I2983
asg63
(lp46418
ssS'creatur'
p46419
(dp46420
g18
(lp46421
I1110
asg293
(lp46422
ssS'shavlik'
p46423
(dp46424
g344
(lp46425
I2775
assS'bold'
p46426
(dp46427
g12
(lp46428
I1080
asg34
(lp46429
sg429
(lp46430
sg48
(lp46431
sg223
(lp46432
ssS'vertag'
p46433
(dp46434
g52
(lp46435
I2600
assS'burn'
p46436
(dp46437
g124
(lp46438
I2135
assS'eee'
p46439
(dp46440
g223
(lp46441
I3129
assS'unreason'
p46442
(dp46443
g132
(lp46444
I1043
asg293
(lp46445
ssS'hubel'
p46446
(dp46447
g12
(lp46448
I2571
asg48
(lp46449
sg63
(lp46450
ssS'offeatur'
p46451
(dp46452
g96
(lp46453
I1262
assS'super'
p46454
(dp46455
g20
(lp46456
I2488
assS'freko'
p46457
(dp46458
g110
(lp46459
I3149
assS'simul'
p46460
(dp46461
g26
(lp46462
sg40
(lp46463
sg287
(lp46464
sg74
(lp46465
sg176
(lp46466
sg80
(lp46467
sg118
(lp46468
sg295
(lp46469
sg183
(lp46470
sg59
(lp46471
sg484
(lp46472
sg83
(lp46473
sg114
(lp46474
sg303
(lp46475
sg89
(lp46476
sg460
(lp46477
sg12
(lp46478
sg46
(lp46479
sg20
(lp46480
sg18
(lp46481
sg313
(lp46482
sg44
(lp46483
sg149
(lp46484
sg230
(lp46485
sg329
(lp46486
sg429
(lp46487
sg318
(lp46488
sg104
(lp46489
sg108
(lp46490
sg110
(lp46491
sg178
(lp46492
sg52
(lp46493
sg22
(lp46494
sg216
(lp46495
sg174
(lp46496
sg332
(lp46497
sg121
(lp46498
sg4
(lp46499
sg8
(lp46500
sg34
(lp46501
sg36
(lp46502
sg384
(lp46503
sg124
(lp46504
sg126
(lp46505
sg535
(lp46506
sg344
(lp46507
sg128
(lp46508
sg130
(lp46509
sg132
(lp46510
sg14
(lp46511
sg16
(lp46512
sg135
(lp46513
sg138
(lp46514
sg354
(lp46515
I1364
assS'afternoon'
p46516
(dp46517
g83
(lp46518
I517
assS'smolenski'
p46519
(dp46520
g132
(lp46521
I3865
asg36
(lp46522
sg83
(lp46523
ssS'commit'
p46524
(dp46525
g313
(lp46526
I514
assS'capacit'
p46527
(dp46528
g14
(lp46529
I2841
asg20
(lp46530
sg22
(lp46531
sg256
(lp46532
ssS'telephon'
p46533
(dp46534
g440
(lp46535
I2092
asg181
(lp46536
ssS'almeida'
p46537
(dp46538
g34
(lp46539
I2950
assS'alamito'
p46540
(dp46541
g293
(lp46542
I3124
assS'capril'
p46543
(dp46544
g96
(lp46545
I2720
assS'down'
p46546
(dp46547
g283
(lp46548
sg70
(lp46549
sg26
(lp46550
sg74
(lp46551
sg59
(lp46552
sg83
(lp46553
sg89
(lp46554
sg91
(lp46555
sg20
(lp46556
sg18
(lp46557
sg329
(lp46558
sg63
(lp46559
sg114
(lp46560
sg116
(lp46561
sg174
(lp46562
sg178
(lp46563
sg6
(lp46564
sg181
(lp46565
sg384
(lp46566
sg126
(lp46567
sg130
(lp46568
sg22
(lp46569
sg138
(lp46570
I171
assS'unthin'
p46571
(dp46572
g138
(lp46573
I1854
assS'amsterdam'
p46574
(dp46575
g34
(lp46576
sg384
(lp46577
sg354
(lp46578
I3115
assS'ception'
p46579
(dp46580
g74
(lp46581
I305
assS'manfr'
p46582
(dp46583
g341
(lp46584
I10
assS'cldl'
p46585
(dp46586
g132
(lp46587
I2426
assS'imik'
p46588
(dp46589
g223
(lp46590
I712
assS'imit'
p46591
(dp46592
g38
(lp46593
I2100
assS'editor'
p46594
(dp46595
g70
(lp46596
sg287
(lp46597
sg80
(lp46598
sg262
(lp46599
sg344
(lp46600
sg59
(lp46601
sg303
(lp46602
sg306
(lp46603
sg89
(lp46604
sg96
(lp46605
sg313
(lp46606
sg223
(lp46607
sg350
(lp46608
sg429
(lp46609
sg116
(lp46610
sg318
(lp46611
sg121
(lp46612
sg34
(lp46613
sg124
(lp46614
sg126
(lp46615
sg341
(lp46616
sg132
(lp46617
sg149
(lp46618
sg50
(lp46619
sg138
(lp46620
sg140
(lp46621
I3123
assS'fraction'
p46622
(dp46623
g484
(lp46624
sg70
(lp46625
sg108
(lp46626
sg178
(lp46627
sg22
(lp46628
sg6
(lp46629
sg235
(lp46630
sg295
(lp46631
sg183
(lp46632
sg384
(lp46633
sg262
(lp46634
sg85
(lp46635
sg68
(lp46636
sg145
(lp46637
sg135
(lp46638
I1304
asg313
(lp46639
sg277
(lp46640
sg26
(lp46641
ssS'luria'
p46642
(dp46643
g332
(lp46644
I270
assS'analysi'
p46645
(dp46646
g124
(lp46647
sg26
(lp46648
sg277
(lp46649
sg163
(lp46650
sg281
(lp46651
sg283
(lp46652
sg30
(lp46653
sg350
(lp46654
sg74
(lp46655
sg176
(lp46656
sg145
(lp46657
sg80
(lp46658
sg293
(lp46659
sg344
(lp46660
sg78
(lp46661
sg59
(lp46662
sg38
(lp46663
sg85
(lp46664
sg303
(lp46665
sg42
(lp46666
I3405
asg89
(lp46667
sg91
(lp46668
sg12
(lp46669
sg96
(lp46670
sg114
(lp46671
sg221
(lp46672
sg535
(lp46673
sg149
(lp46674
sg174
(lp46675
sg32
(lp46676
sg245
(lp46677
sg318
(lp46678
sg102
(lp46679
sg110
(lp46680
sg63
(lp46681
sg52
(lp46682
sg22
(lp46683
sg116
(lp46684
sg438
(lp46685
sg440
(lp46686
sg332
(lp46687
sg121
(lp46688
sg4
(lp46689
sg6
(lp46690
sg8
(lp46691
sg34
(lp46692
sg36
(lp46693
sg384
(lp46694
sg235
(lp46695
sg72
(lp46696
sg341
(lp46697
sg130
(lp46698
sg14
(lp46699
sg16
(lp46700
sg135
(lp46701
sg138
(lp46702
sg140
(lp46703
sg354
(lp46704
ssS'randolf'
p46705
(dp46706
g281
(lp46707
I2412
assS'form'
p46708
(dp46709
g329
(lp46710
sg78
(lp46711
sg163
(lp46712
sg72
(lp46713
sg68
(lp46714
sg283
(lp46715
sg85
(lp46716
sg460
(lp46717
sg181
(lp46718
sg303
(lp46719
sg26
(lp46720
sg30
(lp46721
sg287
(lp46722
sg74
(lp46723
sg176
(lp46724
sg145
(lp46725
sg76
(lp46726
sg118
(lp46727
sg295
(lp46728
sg183
(lp46729
sg59
(lp46730
sg80
(lp46731
sg38
(lp46732
sg114
(lp46733
sg63
(lp46734
sg42
(lp46735
I930
asg306
(lp46736
sg87
(lp46737
sg89
(lp46738
sg91
(lp46739
sg12
(lp46740
sg94
(lp46741
sg96
(lp46742
sg48
(lp46743
sg99
(lp46744
sg313
(lp46745
sg44
(lp46746
sg149
(lp46747
sg174
(lp46748
sg32
(lp46749
sg178
(lp46750
sg245
(lp46751
sg429
(lp46752
sg318
(lp46753
sg46
(lp46754
sg102
(lp46755
sg104
(lp46756
sg106
(lp46757
sg108
(lp46758
sg110
(lp46759
sg20
(lp46760
sg52
(lp46761
sg22
(lp46762
sg230
(lp46763
sg438
(lp46764
sg440
(lp46765
sg332
(lp46766
sg121
(lp46767
sg4
(lp46768
sg6
(lp46769
sg8
(lp46770
sg34
(lp46771
sg221
(lp46772
sg384
(lp46773
sg124
(lp46774
sg126
(lp46775
sg40
(lp46776
sg223
(lp46777
sg128
(lp46778
sg130
(lp46779
sg132
(lp46780
sg14
(lp46781
sg16
(lp46782
sg135
(lp46783
sg50
(lp46784
sg138
(lp46785
sg140
(lp46786
ssS'pinna'
p46787
(dp46788
g174
(lp46789
I199
assS'forc'
p46790
(dp46791
g72
(lp46792
sg287
(lp46793
sg145
(lp46794
sg76
(lp46795
sg183
(lp46796
sg83
(lp46797
sg20
(lp46798
sg99
(lp46799
sg223
(lp46800
sg350
(lp46801
sg118
(lp46802
sg429
(lp46803
sg104
(lp46804
sg63
(lp46805
sg329
(lp46806
sg440
(lp46807
sg318
(lp46808
sg4
(lp46809
sg6
(lp46810
sg460
(lp46811
sg126
(lp46812
sg281
(lp46813
sg14
(lp46814
sg16
(lp46815
sg149
(lp46816
sg50
(lp46817
I1014
assS'forb'
p46818
(dp46819
g344
(lp46820
I13
assS'lueschow'
p46821
(dp46822
g18
(lp46823
I2615
assS'descrip'
p46824
(dp46825
g138
(lp46826
I3031
assS'forg'
p46827
(dp46828
g262
(lp46829
I316
assS'vectorwo'
p46830
(dp46831
g341
(lp46832
I2597
assS'ford'
p46833
(dp46834
g354
(lp46835
I295
assS'substrat'
p46836
(dp46837
g80
(lp46838
sg262
(lp46839
I2032
assS'berg'
p46840
(dp46841
g32
(lp46842
I341
assS'salk'
p46843
(dp46844
g318
(lp46845
sg262
(lp46846
sg303
(lp46847
sg91
(lp46848
sg106
(lp46849
I314
asg50
(lp46850
sg350
(lp46851
ssS'granger'
p46852
(dp46853
g235
(lp46854
I3033
assS'technion'
p46855
(dp46856
g140
(lp46857
I3168
assS'analyst'
p46858
(dp46859
g130
(lp46860
I837
assS'photogaph'
p46861
(dp46862
g181
(lp46863
I547
assS'iqt'
p46864
(dp46865
g87
(lp46866
I1011
assS'yorktown'
p46867
(dp46868
g102
(lp46869
I21
asg76
(lp46870
ssS'unreg'
p46871
(dp46872
g221
(lp46873
I2110
assS'tucson'
p46874
(dp46875
g104
(lp46876
I19
assS'iqi'
p46877
(dp46878
g102
(lp46879
I1972
asg87
(lp46880
ssS'gould'
p46881
(dp46882
g8
(lp46883
I2507
assS'unrel'
p46884
(dp46885
g102
(lp46886
I2269
asg332
(lp46887
sg181
(lp46888
ssS'jabri'
p46889
(dp46890
g135
(lp46891
I13
assS'classif'
p46892
(dp46893
g68
(lp46894
sg78
(lp46895
sg281
(lp46896
sg283
(lp46897
sg30
(lp46898
sg287
(lp46899
sg76
(lp46900
sg293
(lp46901
sg344
(lp46902
sg183
(lp46903
sg59
(lp46904
sg484
(lp46905
sg38
(lp46906
sg85
(lp46907
sg91
(lp46908
sg94
(lp46909
sg96
(lp46910
sg221
(lp46911
sg535
(lp46912
sg44
(lp46913
sg329
(lp46914
sg178
(lp46915
sg110
(lp46916
sg63
(lp46917
sg52
(lp46918
sg114
(lp46919
sg174
(lp46920
sg121
(lp46921
sg22
(lp46922
sg181
(lp46923
sg124
(lp46924
sg341
(lp46925
sg128
(lp46926
sg130
(lp46927
sg135
(lp46928
sg140
(lp46929
I213
assS'featur'
p46930
(dp46931
g70
(lp46932
sg277
(lp46933
sg163
(lp46934
sg30
(lp46935
sg74
(lp46936
sg176
(lp46937
sg145
(lp46938
sg76
(lp46939
sg262
(lp46940
sg295
(lp46941
sg183
(lp46942
sg59
(lp46943
sg114
(lp46944
sg63
(lp46945
sg42
(lp46946
I3404
asg306
(lp46947
sg91
(lp46948
sg46
(lp46949
sg20
(lp46950
sg48
(lp46951
sg221
(lp46952
sg223
(lp46953
sg149
(lp46954
sg118
(lp46955
sg293
(lp46956
sg102
(lp46957
sg178
(lp46958
sg106
(lp46959
sg96
(lp46960
sg52
(lp46961
sg22
(lp46962
sg116
(lp46963
sg174
(lp46964
sg440
(lp46965
sg332
(lp46966
sg121
(lp46967
sg4
(lp46968
sg181
(lp46969
sg235
(lp46970
sg34
(lp46971
sg460
(lp46972
sg68
(lp46973
sg72
(lp46974
sg281
(lp46975
sg10
(lp46976
sg344
(lp46977
sg44
(lp46978
sg78
(lp46979
sg132
(lp46980
sg50
(lp46981
sg138
(lp46982
ssS'classic'
p46983
(dp46984
g118
(lp46985
sg74
(lp46986
sg176
(lp46987
sg70
(lp46988
sg78
(lp46989
sg295
(lp46990
sg183
(lp46991
sg68
(lp46992
sg72
(lp46993
sg440
(lp46994
sg85
(lp46995
sg83
(lp46996
sg306
(lp46997
sg130
(lp46998
I775
asg63
(lp46999
ssS'covert'
p47000
(dp47001
g178
(lp47002
I2358
assS'motorbik'
p47003
(dp47004
g174
(lp47005
I2141
assS'diagnosi'
p47006
(dp47007
g78
(lp47008
sg484
(lp47009
sg91
(lp47010
sg14
(lp47011
sg16
(lp47012
I2222
asg110
(lp47013
ssS'photocurr'
p47014
(dp47015
g256
(lp47016
I1008
assS'diagnost'
p47017
(dp47018
g295
(lp47019
sg14
(lp47020
sg16
(lp47021
I815
asg91
(lp47022
sg183
(lp47023
ssS'vista'
p47024
(dp47025
g70
(lp47026
I2470
assS'verbesserung'
p47027
(dp47028
g34
(lp47029
I2914
assS'alopex'
p47030
(dp47031
g163
(lp47032
I1830
assS'excel'
p47033
(dp47034
g287
(lp47035
sg277
(lp47036
sg295
(lp47037
sg183
(lp47038
sg124
(lp47039
sg126
(lp47040
sg132
(lp47041
sg14
(lp47042
sg16
(lp47043
I312
asg63
(lp47044
ssS'communic'
p47045
(dp47046
g74
(lp47047
sg235
(lp47048
sg344
(lp47049
sg293
(lp47050
sg83
(lp47051
sg63
(lp47052
sg245
(lp47053
sg429
(lp47054
sg87
(lp47055
sg102
(lp47056
sg94
(lp47057
sg20
(lp47058
sg108
(lp47059
I15
asg535
(lp47060
sg223
(lp47061
ssS'unlimit'
p47062
(dp47063
g110
(lp47064
I640
assS'garland'
p47065
(dp47066
g26
(lp47067
I3234
assS'subdivid'
p47068
(dp47069
g221
(lp47070
sg68
(lp47071
sg126
(lp47072
I1886
assS'cxjkj'
p47073
(dp47074
g96
(lp47075
I796
assS'montagu'
p47076
(dp47077
g70
(lp47078
I2544
assS'fell'
p47079
(dp47080
g108
(lp47081
I1901
assS'quinlan'
p47082
(dp47083
g344
(lp47084
sg183
(lp47085
sg126
(lp47086
sg91
(lp47087
I535
assS'femal'
p47088
(dp47089
g96
(lp47090
I1299
asg72
(lp47091
sg91
(lp47092
ssS'ofebnn'
p47093
(dp47094
g132
(lp47095
I1622
assS'furthermor'
p47096
(dp47097
g283
(lp47098
sg26
(lp47099
sg163
(lp47100
sg145
(lp47101
sg295
(lp47102
sg183
(lp47103
sg59
(lp47104
sg303
(lp47105
sg42
(lp47106
I1234
asg306
(lp47107
sg245
(lp47108
sg46
(lp47109
sg313
(lp47110
sg104
(lp47111
sg63
(lp47112
sg440
(lp47113
sg4
(lp47114
sg72
(lp47115
sg341
(lp47116
sg14
(lp47117
sg16
(lp47118
sg135
(lp47119
sg140
(lp47120
ssS'jackel'
p47121
(dp47122
g94
(lp47123
I3563
asg183
(lp47124
sg63
(lp47125
sg181
(lp47126
sg114
(lp47127
ssS'pseudo'
p47128
(dp47129
g295
(lp47130
I2185
asg183
(lp47131
sg277
(lp47132
ssS'fsm'
p47133
(dp47134
g128
(lp47135
I1064
assS'ignor'
p47136
(dp47137
g30
(lp47138
sg287
(lp47139
sg74
(lp47140
sg318
(lp47141
sg178
(lp47142
sg256
(lp47143
sg80
(lp47144
sg344
(lp47145
sg460
(lp47146
sg124
(lp47147
sg126
(lp47148
sg83
(lp47149
sg87
(lp47150
sg50
(lp47151
I1421
assS'daili'
p47152
(dp47153
g78
(lp47154
sg221
(lp47155
I2251
asg114
(lp47156
ssS'skip'
p47157
(dp47158
g94
(lp47159
I1777
assS'mild'
p47160
(dp47161
g429
(lp47162
sg535
(lp47163
I1024
asg85
(lp47164
ssS'customari'
p47165
(dp47166
g32
(lp47167
I1839
assS'unconstrain'
p47168
(dp47169
g230
(lp47170
sg293
(lp47171
sg59
(lp47172
sg126
(lp47173
sg429
(lp47174
sg138
(lp47175
I1051
assS'barkai'
p47176
(dp47177
g36
(lp47178
I1400
assS'hierarch'
p47179
(dp47180
g30
(lp47181
sg74
(lp47182
sg277
(lp47183
sg295
(lp47184
sg183
(lp47185
sg59
(lp47186
sg68
(lp47187
sg126
(lp47188
sg429
(lp47189
sg87
(lp47190
sg110
(lp47191
sg44
(lp47192
I454
assS'depend'
p47193
(dp47194
g329
(lp47195
sg78
(lp47196
sg163
(lp47197
sg72
(lp47198
sg283
(lp47199
sg26
(lp47200
sg30
(lp47201
sg350
(lp47202
sg74
(lp47203
sg176
(lp47204
sg145
(lp47205
sg262
(lp47206
sg460
(lp47207
sg183
(lp47208
sg59
(lp47209
sg484
(lp47210
sg38
(lp47211
sg83
(lp47212
sg85
(lp47213
sg303
(lp47214
sg42
(lp47215
I3167
asg87
(lp47216
sg89
(lp47217
sg91
(lp47218
sg12
(lp47219
sg46
(lp47220
sg96
(lp47221
sg48
(lp47222
sg99
(lp47223
sg44
(lp47224
sg149
(lp47225
sg118
(lp47226
sg116
(lp47227
sg174
(lp47228
sg293
(lp47229
sg245
(lp47230
sg68
(lp47231
sg106
(lp47232
sg108
(lp47233
sg110
(lp47234
sg63
(lp47235
sg52
(lp47236
sg230
(lp47237
sg438
(lp47238
sg32
(lp47239
sg332
(lp47240
sg178
(lp47241
sg181
(lp47242
sg235
(lp47243
sg34
(lp47244
sg36
(lp47245
sg384
(lp47246
sg124
(lp47247
sg126
(lp47248
sg281
(lp47249
sg40
(lp47250
sg287
(lp47251
sg223
(lp47252
sg128
(lp47253
sg130
(lp47254
sg132
(lp47255
sg135
(lp47256
sg344
(lp47257
sg138
(lp47258
sg140
(lp47259
sg354
(lp47260
ssS'april'
p47261
(dp47262
g245
(lp47263
sg223
(lp47264
sg44
(lp47265
I2718
asg256
(lp47266
ssS'intermedi'
p47267
(dp47268
g235
(lp47269
sg36
(lp47270
sg10
(lp47271
sg303
(lp47272
sg102
(lp47273
sg89
(lp47274
sg132
(lp47275
sg50
(lp47276
I404
asg63
(lp47277
ssS'inerti'
p47278
(dp47279
g99
(lp47280
I784
asg350
(lp47281
ssS'tanaka'
p47282
(dp47283
g48
(lp47284
I270
asg181
(lp47285
ssS'chequ'
p47286
(dp47287
g76
(lp47288
I143
assS'specht'
p47289
(dp47290
g313
(lp47291
I922
assS'cerebellum'
p47292
(dp47293
g99
(lp47294
I2624
asg350
(lp47295
ssS'string'
p47296
(dp47297
g104
(lp47298
sg283
(lp47299
sg145
(lp47300
sg26
(lp47301
sg128
(lp47302
I1237
assS'asymptot'
p47303
(dp47304
g230
(lp47305
sg438
(lp47306
I1452
asg262
(lp47307
sg295
(lp47308
sg183
(lp47309
sg384
(lp47310
sg484
(lp47311
sg38
(lp47312
sg281
(lp47313
sg85
(lp47314
sg34
(lp47315
sg36
(lp47316
sg535
(lp47317
sg44
(lp47318
sg354
(lp47319
ssS'arsjk'
p47320
(dp47321
g46
(lp47322
I2596
assS'pathologist'
p47323
(dp47324
g91
(lp47325
I496
assS'ukaea'
p47326
(dp47327
g14
(lp47328
sg16
(lp47329
I60
assS'lynch'
p47330
(dp47331
g106
(lp47332
I462
assS'join'
p47333
(dp47334
g68
(lp47335
sg8
(lp47336
I1054
assS'aowacid'
p47337
(dp47338
g135
(lp47339
I600
assS'wklik'
p47340
(dp47341
g74
(lp47342
I1122
assS'dim'
p47343
(dp47344
g287
(lp47345
sg295
(lp47346
sg183
(lp47347
sg85
(lp47348
sg40
(lp47349
sg245
(lp47350
I1341
assS'din'
p47351
(dp47352
g108
(lp47353
I643
asg38
(lp47354
ssS'dii'
p47355
(dp47356
g34
(lp47357
sg535
(lp47358
I1662
assS'dij'
p47359
(dp47360
g535
(lp47361
sg130
(lp47362
I2355
assS'dik'
p47363
(dp47364
g38
(lp47365
sg130
(lp47366
I372
assS'did'
p47367
(dp47368
g283
(lp47369
sg26
(lp47370
sg277
(lp47371
sg80
(lp47372
sg293
(lp47373
sg344
(lp47374
sg183
(lp47375
sg83
(lp47376
sg303
(lp47377
sg89
(lp47378
sg12
(lp47379
sg94
(lp47380
sg18
(lp47381
sg221
(lp47382
sg44
(lp47383
sg102
(lp47384
sg106
(lp47385
I481
asg110
(lp47386
sg216
(lp47387
sg178
(lp47388
sg4
(lp47389
sg126
(lp47390
sg295
(lp47391
sg128
(lp47392
ssS'die'
p47393
(dp47394
g30
(lp47395
sg174
(lp47396
I1595
asg22
(lp47397
sg277
(lp47398
sg34
(lp47399
sg484
(lp47400
sg10
(lp47401
sg256
(lp47402
ssS'dif'
p47403
(dp47404
g30
(lp47405
I278
assS'mb'
p47406
(dp47407
g332
(lp47408
I1193
asg10
(lp47409
ssS'iter'
p47410
(dp47411
g68
(lp47412
sg26
(lp47413
sg72
(lp47414
sg30
(lp47415
sg74
(lp47416
sg76
(lp47417
sg295
(lp47418
sg183
(lp47419
sg59
(lp47420
sg306
(lp47421
sg89
(lp47422
sg91
(lp47423
sg221
(lp47424
sg313
(lp47425
sg149
(lp47426
sg108
(lp47427
sg63
(lp47428
sg114
(lp47429
sg440
(lp47430
sg332
(lp47431
sg34
(lp47432
sg36
(lp47433
sg460
(lp47434
sg124
(lp47435
sg126
(lp47436
sg40
(lp47437
sg344
(lp47438
sg130
(lp47439
sg14
(lp47440
sg16
(lp47441
sg135
(lp47442
sg138
(lp47443
sg354
(lp47444
I550
assS'magnet'
p47445
(dp47446
g438
(lp47447
I524
asg16
(lp47448
sg318
(lp47449
sg99
(lp47450
sg14
(lp47451
ssS'item'
p47452
(dp47453
g132
(lp47454
I2208
asg484
(lp47455
sg18
(lp47456
sg72
(lp47457
sg303
(lp47458
ssS'jhu'
p47459
(dp47460
g99
(lp47461
I34
asg22
(lp47462
sg114
(lp47463
ssS'div'
p47464
(dp47465
g20
(lp47466
I2056
assS'jhz'
p47467
(dp47468
g332
(lp47469
I2269
assS'round'
p47470
(dp47471
g106
(lp47472
I193
asg83
(lp47473
sg181
(lp47474
ssS'dis'
p47475
(dp47476
g245
(lp47477
I1514
asg85
(lp47478
ssS'amput'
p47479
(dp47480
g176
(lp47481
I238
assS'bjsj'
p47482
(dp47483
g535
(lp47484
I552
assS'monocular'
p47485
(dp47486
g438
(lp47487
I1022
asg118
(lp47488
sg48
(lp47489
sg149
(lp47490
ssS'minimis'
p47491
(dp47492
g135
(lp47493
I2219
asg283
(lp47494
ssS'vfiv'
p47495
(dp47496
g130
(lp47497
I2208
assS'nonquadrat'
p47498
(dp47499
g34
(lp47500
I1136
assS'inputoutput'
p47501
(dp47502
g99
(lp47503
I1242
assS'sroka'
p47504
(dp47505
g87
(lp47506
I416
assS'favour'
p47507
(dp47508
g14
(lp47509
I4460
assS'dimac'
p47510
(dp47511
g287
(lp47512
I3427
assS'suspect'
p47513
(dp47514
g74
(lp47515
I2571
assS'carey'
p47516
(dp47517
g116
(lp47518
I2498
assS'componentwis'
p47519
(dp47520
g108
(lp47521
I1356
assS'mp'
p47522
(dp47523
g30
(lp47524
I808
assS'tangenc'
p47525
(dp47526
g18
(lp47527
I589
assS'deriv'
p47528
(dp47529
g68
(lp47530
sg78
(lp47531
sg281
(lp47532
sg40
(lp47533
sg287
(lp47534
sg176
(lp47535
sg145
(lp47536
sg76
(lp47537
sg262
(lp47538
sg295
(lp47539
sg183
(lp47540
sg80
(lp47541
sg38
(lp47542
sg85
(lp47543
sg63
(lp47544
sg89
(lp47545
sg91
(lp47546
sg12
(lp47547
sg46
(lp47548
sg96
(lp47549
sg48
(lp47550
sg221
(lp47551
sg313
(lp47552
sg44
(lp47553
sg350
(lp47554
sg245
(lp47555
sg429
(lp47556
sg102
(lp47557
sg104
(lp47558
sg108
(lp47559
sg178
(lp47560
sg22
(lp47561
sg230
(lp47562
sg329
(lp47563
sg440
(lp47564
sg318
(lp47565
sg121
(lp47566
sg181
(lp47567
sg6
(lp47568
sg8
(lp47569
sg34
(lp47570
sg384
(lp47571
sg124
(lp47572
sg126
(lp47573
sg341
(lp47574
sg535
(lp47575
sg223
(lp47576
sg130
(lp47577
sg132
(lp47578
sg135
(lp47579
sg50
(lp47580
sg460
(lp47581
sg140
(lp47582
sg354
(lp47583
I225
assS'nuanc'
p47584
(dp47585
g87
(lp47586
I241
assS'french'
p47587
(dp47588
g74
(lp47589
sg99
(lp47590
I3192
assS'tnm'
p47591
(dp47592
g38
(lp47593
I684
assS'occm'
p47594
(dp47595
g174
(lp47596
I1002
assS'espoo'
p47597
(dp47598
g121
(lp47599
I2676
assS'tangent'
p47600
(dp47601
g32
(lp47602
sg108
(lp47603
sg341
(lp47604
sg223
(lp47605
sg132
(lp47606
I1566
asg46
(lp47607
sg18
(lp47608
sg44
(lp47609
ssS'rdin'
p47610
(dp47611
g38
(lp47612
I2210
assS'coincid'
p47613
(dp47614
g332
(lp47615
sg70
(lp47616
sg6
(lp47617
sg8
(lp47618
I2010
asg484
(lp47619
sg221
(lp47620
ssS'viabl'
p47621
(dp47622
g221
(lp47623
I1871
asg80
(lp47624
ssS'hummel'
p47625
(dp47626
g429
(lp47627
sg281
(lp47628
I928
assS'wait'
p47629
(dp47630
g94
(lp47631
I507
asg83
(lp47632
ssS'box'
p47633
(dp47634
g332
(lp47635
sg145
(lp47636
sg181
(lp47637
sg183
(lp47638
sg126
(lp47639
sg42
(lp47640
I2292
asg104
(lp47641
sg106
(lp47642
sg114
(lp47643
sg138
(lp47644
sg350
(lp47645
ssS'tnn'
p47646
(dp47647
g38
(lp47648
I746
assS'archistriatum'
p47649
(dp47650
g116
(lp47651
I351
assS'canadian'
p47652
(dp47653
g138
(lp47654
I3262
assS'shift'
p47655
(dp47656
g216
(lp47657
sg438
(lp47658
sg18
(lp47659
sg80
(lp47660
sg6
(lp47661
sg8
(lp47662
sg72
(lp47663
sg59
(lp47664
sg262
(lp47665
sg329
(lp47666
sg10
(lp47667
sg42
(lp47668
I1720
asg38
(lp47669
sg130
(lp47670
sg135
(lp47671
sg44
(lp47672
sg350
(lp47673
ssS'unsupervis'
p47674
(dp47675
g116
(lp47676
sg74
(lp47677
sg318
(lp47678
sg70
(lp47679
sg163
(lp47680
sg72
(lp47681
sg96
(lp47682
sg50
(lp47683
I133
asg52
(lp47684
sg149
(lp47685
ssS'bos'
p47686
(dp47687
g36
(lp47688
I3045
assS'bot'
p47689
(dp47690
g59
(lp47691
sg48
(lp47692
I712
assS'merrick'
p47693
(dp47694
g145
(lp47695
I3025
assS'hbx'
p47696
(dp47697
g130
(lp47698
I1541
assS'boo'
p47699
(dp47700
g130
(lp47701
I2400
assS'extrem'
p47702
(dp47703
g287
(lp47704
sg318
(lp47705
sg145
(lp47706
sg181
(lp47707
sg34
(lp47708
sg384
(lp47709
sg68
(lp47710
sg83
(lp47711
sg10
(lp47712
sg40
(lp47713
sg306
(lp47714
sg87
(lp47715
sg85
(lp47716
sg59
(lp47717
sg63
(lp47718
sg221
(lp47719
I1540
asg535
(lp47720
sg114
(lp47721
ssS'bob'
p47722
(dp47723
g110
(lp47724
I3059
assS'kauffman'
p47725
(dp47726
g183
(lp47727
I6803
asg293
(lp47728
ssS'imbal'
p47729
(dp47730
g216
(lp47731
I518
assS'approachmg'
p47732
(dp47733
g83
(lp47734
I1977
assS'xsij'
p47735
(dp47736
g178
(lp47737
I871
assS'otenti'
p47738
(dp47739
g106
(lp47740
I1249
assS'lemlsl'
p47741
(dp47742
g245
(lp47743
I2523
assS'modul'
p47744
(dp47745
g116
(lp47746
sg174
(lp47747
sg118
(lp47748
sg68
(lp47749
sg121
(lp47750
sg4
(lp47751
sg80
(lp47752
sg329
(lp47753
sg256
(lp47754
sg59
(lp47755
sg262
(lp47756
sg293
(lp47757
sg303
(lp47758
sg87
(lp47759
sg14
(lp47760
sg16
(lp47761
I1937
asg22
(lp47762
sg96
(lp47763
sg350
(lp47764
ssS'evalual'
p47765
(dp47766
g108
(lp47767
I1615
assS'mscc'
p47768
(dp47769
g96
(lp47770
I1742
assS'yxih'
p47771
(dp47772
g145
(lp47773
I2436
assS'planck'
p47774
(dp47775
g216
(lp47776
I14
asg262
(lp47777
ssS'transplant'
p47778
(dp47779
g484
(lp47780
sg135
(lp47781
I355
assS'sako'
p47782
(dp47783
g96
(lp47784
I1509
assS'ayi'
p47785
(dp47786
g18
(lp47787
I970
assS'franco'
p47788
(dp47789
g87
(lp47790
I340
assS'sharper'
p47791
(dp47792
g87
(lp47793
I286
assS'sake'
p47794
(dp47795
g438
(lp47796
I1341
asg18
(lp47797
sg110
(lp47798
ssS'saddl'
p47799
(dp47800
g18
(lp47801
sg48
(lp47802
sg46
(lp47803
sg130
(lp47804
I539
assS'univers'
p47805
(dp47806
g329
(lp47807
sg26
(lp47808
sg277
(lp47809
sg72
(lp47810
sg303
(lp47811
sg283
(lp47812
sg85
(lp47813
sg40
(lp47814
sg30
(lp47815
sg350
(lp47816
sg145
(lp47817
sg80
(lp47818
sg76
(lp47819
sg262
(lp47820
sg344
(lp47821
sg183
(lp47822
sg484
(lp47823
sg38
(lp47824
sg83
(lp47825
sg114
(lp47826
sg124
(lp47827
sg42
(lp47828
I15
asg306
(lp47829
sg87
(lp47830
sg89
(lp47831
sg91
(lp47832
sg12
(lp47833
sg94
(lp47834
sg96
(lp47835
sg48
(lp47836
sg221
(lp47837
sg313
(lp47838
sg223
(lp47839
sg149
(lp47840
sg174
(lp47841
sg245
(lp47842
sg429
(lp47843
sg68
(lp47844
sg46
(lp47845
sg178
(lp47846
sg106
(lp47847
sg110
(lp47848
sg20
(lp47849
sg52
(lp47850
sg22
(lp47851
sg116
(lp47852
sg438
(lp47853
sg118
(lp47854
sg332
(lp47855
sg121
(lp47856
sg181
(lp47857
sg6
(lp47858
sg8
(lp47859
sg34
(lp47860
sg36
(lp47861
sg384
(lp47862
sg235
(lp47863
sg126
(lp47864
sg281
(lp47865
sg10
(lp47866
sg535
(lp47867
sg287
(lp47868
sg128
(lp47869
sg130
(lp47870
sg132
(lp47871
sg14
(lp47872
sg16
(lp47873
sg135
(lp47874
sg50
(lp47875
sg138
(lp47876
sg140
(lp47877
sg354
(lp47878
ssS'visit'
p47879
(dp47880
g145
(lp47881
sg277
(lp47882
sg83
(lp47883
sg306
(lp47884
sg178
(lp47885
sg44
(lp47886
I1326
assS'dejong'
p47887
(dp47888
g132
(lp47889
I3570
asg223
(lp47890
ssS'robl'
p47891
(dp47892
g176
(lp47893
I2533
assS'sharpen'
p47894
(dp47895
g216
(lp47896
I853
asg329
(lp47897
sg332
(lp47898
sg118
(lp47899
ssS'epsp'
p47900
(dp47901
g262
(lp47902
I2159
assS'helix'
p47903
(dp47904
g26
(lp47905
I151
assS'ooj'
p47906
(dp47907
g350
(lp47908
I717
assS'rigid'
p47909
(dp47910
g181
(lp47911
sg440
(lp47912
sg138
(lp47913
I1615
asg223
(lp47914
sg76
(lp47915
ssS'oop'
p47916
(dp47917
g163
(lp47918
I494
assS'examin'
p47919
(dp47920
g30
(lp47921
sg116
(lp47922
sg74
(lp47923
sg178
(lp47924
sg4
(lp47925
sg6
(lp47926
sg76
(lp47927
sg183
(lp47928
sg80
(lp47929
sg38
(lp47930
sg281
(lp47931
sg85
(lp47932
sg83
(lp47933
sg91
(lp47934
sg12
(lp47935
sg108
(lp47936
I1665
asg110
(lp47937
sg535
(lp47938
sg114
(lp47939
ssS'effort'
p47940
(dp47941
g287
(lp47942
sg74
(lp47943
sg295
(lp47944
sg183
(lp47945
sg59
(lp47946
sg72
(lp47947
sg34
(lp47948
sg91
(lp47949
sg128
(lp47950
sg94
(lp47951
sg354
(lp47952
I412
asg110
(lp47953
sg52
(lp47954
sg114
(lp47955
ssS'baysian'
p47956
(dp47957
g72
(lp47958
sg221
(lp47959
I2510
assS'tokyo'
p47960
(dp47961
g36
(lp47962
sg18
(lp47963
I22
asg114
(lp47964
ssS'cnap'
p47965
(dp47966
g10
(lp47967
I1631
assS'ssocia'
p47968
(dp47969
g106
(lp47970
I949
assS'flt'
p47971
(dp47972
g174
(lp47973
I2019
assS'flw'
p47974
(dp47975
g74
(lp47976
I1076
assS'flv'
p47977
(dp47978
g287
(lp47979
I2098
assS'fli'
p47980
(dp47981
g145
(lp47982
sg256
(lp47983
sg34
(lp47984
sg59
(lp47985
sg245
(lp47986
sg102
(lp47987
I576
assS'repro'
p47988
(dp47989
g223
(lp47990
I2434
assS'uli'
p47991
(dp47992
g230
(lp47993
I3031
assS'flm'
p47994
(dp47995
g281
(lp47996
I1197
assS'ulo'
p47997
(dp47998
g8
(lp47999
I2476
assS'ulm'
p48000
(dp48001
g59
(lp48002
I24
assS'imped'
p48003
(dp48004
g99
(lp48005
I1051
assS'twodimension'
p48006
(dp48007
g108
(lp48008
I122
asg341
(lp48009
sg52
(lp48010
ssS'flb'
p48011
(dp48012
g132
(lp48013
I2263
assS'fget'
p48014
(dp48015
g132
(lp48016
I717
assS'claim'
p48017
(dp48018
g344
(lp48019
sg183
(lp48020
sg85
(lp48021
sg40
(lp48022
sg128
(lp48023
I2531
asg114
(lp48024
ssS'genesi'
p48025
(dp48026
g70
(lp48027
I1919
assS'kenneth'
p48028
(dp48029
g6
(lp48030
I2199
assS'nunit'
p48031
(dp48032
g96
(lp48033
I2301
assS'predict'
p48034
(dp48035
g329
(lp48036
sg70
(lp48037
sg78
(lp48038
sg277
(lp48039
sg303
(lp48040
sg26
(lp48041
sg30
(lp48042
sg176
(lp48043
sg145
(lp48044
sg80
(lp48045
sg118
(lp48046
sg295
(lp48047
sg183
(lp48048
sg484
(lp48049
sg38
(lp48050
sg85
(lp48051
sg124
(lp48052
sg306
(lp48053
sg87
(lp48054
sg89
(lp48055
sg460
(lp48056
sg12
(lp48057
sg94
(lp48058
sg48
(lp48059
sg99
(lp48060
sg313
(lp48061
sg223
(lp48062
sg149
(lp48063
sg116
(lp48064
sg174
(lp48065
sg293
(lp48066
sg245
(lp48067
sg102
(lp48068
sg18
(lp48069
sg106
(lp48070
sg108
(lp48071
sg110
(lp48072
sg230
(lp48073
sg438
(lp48074
I2243
asg332
(lp48075
sg4
(lp48076
sg6
(lp48077
sg8
(lp48078
sg221
(lp48079
sg384
(lp48080
sg235
(lp48081
sg126
(lp48082
sg344
(lp48083
sg128
(lp48084
sg36
(lp48085
sg132
(lp48086
sg14
(lp48087
sg16
(lp48088
sg350
(lp48089
sg50
(lp48090
sg138
(lp48091
sg140
(lp48092
sg354
(lp48093
ssS'fricat'
p48094
(dp48095
g74
(lp48096
sg121
(lp48097
I1371
assS'winston'
p48098
(dp48099
g429
(lp48100
I2509
assS'infeng'
p48101
(dp48102
g87
(lp48103
I2554
assS'agent'
p48104
(dp48105
g30
(lp48106
sg438
(lp48107
I1572
asg293
(lp48108
sg83
(lp48109
sg89
(lp48110
sg18
(lp48111
ssS'council'
p48112
(dp48113
g14
(lp48114
I4647
asg121
(lp48115
ssS'heckerman'
p48116
(dp48117
g91
(lp48118
I483
assS'agenc'
p48119
(dp48120
g277
(lp48121
sg40
(lp48122
I2456
assS'uncorrect'
p48123
(dp48124
g318
(lp48125
I2630
assS'xtil'
p48126
(dp48127
g76
(lp48128
I2563
assS'arbor'
p48129
(dp48130
g12
(lp48131
sg102
(lp48132
sg99
(lp48133
I3365
asg344
(lp48134
ssS'tilt'
p48135
(dp48136
g12
(lp48137
I1327
asg329
(lp48138
sg350
(lp48139
ssS'thiiiiii'
p48140
(dp48141
g70
(lp48142
I1951
assS'pine'
p48143
(dp48144
g318
(lp48145
sg91
(lp48146
I26
asg350
(lp48147
ssS'nyu'
p48148
(dp48149
g6
(lp48150
I14
assS'till'
p48151
(dp48152
g178
(lp48153
I1278
assS'nmse'
p48154
(dp48155
g121
(lp48156
I1814
assS'selforgan'
p48157
(dp48158
g20
(lp48159
sg535
(lp48160
sg149
(lp48161
I2272
assS'foreach'
p48162
(dp48163
g277
(lp48164
I1702
assS'pure'
p48165
(dp48166
g174
(lp48167
sg332
(lp48168
sg70
(lp48169
sg80
(lp48170
sg329
(lp48171
sg295
(lp48172
sg183
(lp48173
sg262
(lp48174
sg83
(lp48175
sg303
(lp48176
sg287
(lp48177
sg132
(lp48178
I299
asg46
(lp48179
sg34
(lp48180
sg63
(lp48181
sg52
(lp48182
sg350
(lp48183
ssS'biologisch'
p48184
(dp48185
g216
(lp48186
I17
assS'tdnn'
p48187
(dp48188
g121
(lp48189
sg128
(lp48190
I306
assS'tild'
p48191
(dp48192
g295
(lp48193
I1396
asg183
(lp48194
ssS'pathway'
p48195
(dp48196
g116
(lp48197
sg438
(lp48198
I2294
asg70
(lp48199
sg6
(lp48200
sg118
(lp48201
sg12
(lp48202
sg106
(lp48203
sg350
(lp48204
ssS'kummert'
p48205
(dp48206
g59
(lp48207
I3218
assS'journalo'
p48208
(dp48209
g149
(lp48210
I2771
assS'map'
p48211
(dp48212
g329
(lp48213
sg277
(lp48214
sg163
(lp48215
sg303
(lp48216
sg30
(lp48217
sg287
(lp48218
sg176
(lp48219
sg76
(lp48220
sg118
(lp48221
sg295
(lp48222
sg183
(lp48223
sg59
(lp48224
sg80
(lp48225
sg38
(lp48226
sg85
(lp48227
sg63
(lp48228
sg42
(lp48229
I98
asg306
(lp48230
sg87
(lp48231
sg68
(lp48232
sg94
(lp48233
sg20
(lp48234
sg48
(lp48235
sg99
(lp48236
sg313
(lp48237
sg223
(lp48238
sg149
(lp48239
sg174
(lp48240
sg293
(lp48241
sg32
(lp48242
sg332
(lp48243
sg102
(lp48244
sg104
(lp48245
sg108
(lp48246
sg110
(lp48247
sg96
(lp48248
sg52
(lp48249
sg116
(lp48250
sg438
(lp48251
sg440
(lp48252
sg18
(lp48253
sg178
(lp48254
sg22
(lp48255
sg235
(lp48256
sg221
(lp48257
sg460
(lp48258
sg124
(lp48259
sg72
(lp48260
sg10
(lp48261
sg535
(lp48262
sg344
(lp48263
sg132
(lp48264
sg14
(lp48265
sg16
(lp48266
sg138
(lp48267
sg354
(lp48268
ssS'mas'
p48269
(dp48270
g245
(lp48271
sg89
(lp48272
I2936
assS'mar'
p48273
(dp48274
g124
(lp48275
sg126
(lp48276
I2371
assS'mat'
p48277
(dp48278
g341
(lp48279
I2749
asg85
(lp48280
ssS'may'
p48281
(dp48282
g329
(lp48283
sg70
(lp48284
sg26
(lp48285
sg277
(lp48286
sg72
(lp48287
sg68
(lp48288
sg293
(lp48289
sg281
(lp48290
sg283
(lp48291
sg303
(lp48292
sg30
(lp48293
sg287
(lp48294
sg74
(lp48295
sg176
(lp48296
sg145
(lp48297
sg76
(lp48298
sg262
(lp48299
sg344
(lp48300
sg183
(lp48301
sg484
(lp48302
sg38
(lp48303
sg83
(lp48304
sg85
(lp48305
sg63
(lp48306
sg42
(lp48307
I1744
asg306
(lp48308
sg87
(lp48309
sg89
(lp48310
sg91
(lp48311
sg94
(lp48312
sg96
(lp48313
sg48
(lp48314
sg221
(lp48315
sg313
(lp48316
sg44
(lp48317
sg149
(lp48318
sg116
(lp48319
sg174
(lp48320
sg18
(lp48321
sg32
(lp48322
sg350
(lp48323
sg429
(lp48324
sg318
(lp48325
sg46
(lp48326
sg102
(lp48327
sg104
(lp48328
sg106
(lp48329
sg108
(lp48330
sg110
(lp48331
sg178
(lp48332
sg52
(lp48333
sg114
(lp48334
sg230
(lp48335
sg438
(lp48336
sg440
(lp48337
sg332
(lp48338
sg121
(lp48339
sg181
(lp48340
sg6
(lp48341
sg8
(lp48342
sg34
(lp48343
sg460
(lp48344
sg124
(lp48345
sg126
(lp48346
sg341
(lp48347
sg10
(lp48348
sg535
(lp48349
sg223
(lp48350
sg128
(lp48351
sg132
(lp48352
sg135
(lp48353
sg50
(lp48354
sg138
(lp48355
sg354
(lp48356
ssS'max'
p48357
(dp48358
g216
(lp48359
sg230
(lp48360
sg332
(lp48361
sg121
(lp48362
sg26
(lp48363
sg293
(lp48364
sg295
(lp48365
sg183
(lp48366
sg126
(lp48367
sg10
(lp48368
sg102
(lp48369
sg318
(lp48370
sg72
(lp48371
sg91
(lp48372
sg132
(lp48373
I3485
asg34
(lp48374
sg221
(lp48375
ssS'snm'
p48376
(dp48377
g102
(lp48378
I1097
assS'usabl'
p48379
(dp48380
g14
(lp48381
I3113
assS'sni'
p48382
(dp48383
g102
(lp48384
I936
assS'maa'
p48385
(dp48386
g85
(lp48387
I2960
assS'intrus'
p48388
(dp48389
g94
(lp48390
I3502
assS'membership'
p48391
(dp48392
g104
(lp48393
I2131
asg74
(lp48394
sg145
(lp48395
ssS'cbms'
p48396
(dp48397
g124
(lp48398
I3214
assS'mae'
p48399
(dp48400
g216
(lp48401
I426
asg293
(lp48402
ssS'mai'
p48403
(dp48404
g429
(lp48405
I1002
assS'gros'
p48406
(dp48407
g22
(lp48408
I867
assS'inputi'
p48409
(dp48410
g116
(lp48411
I1153
assS'mam'
p48412
(dp48413
g10
(lp48414
I1015
assS'grow'
p48415
(dp48416
g30
(lp48417
sg329
(lp48418
sg145
(lp48419
sg256
(lp48420
sg183
(lp48421
sg460
(lp48422
sg68
(lp48423
sg126
(lp48424
sg341
(lp48425
sg40
(lp48426
sg42
(lp48427
I1356
asg306
(lp48428
sg89
(lp48429
sg91
(lp48430
sg130
(lp48431
sg38
(lp48432
sg108
(lp48433
sg50
(lp48434
sg354
(lp48435
ssS'man'
p48436
(dp48437
g118
(lp48438
sg332
(lp48439
I2722
asg277
(lp48440
sg181
(lp48441
sg183
(lp48442
sg460
(lp48443
sg63
(lp48444
sg59
(lp48445
sg535
(lp48446
sg114
(lp48447
ssS'noun'
p48448
(dp48449
g94
(lp48450
I1180
assS'neck'
p48451
(dp48452
g303
(lp48453
I2856
assS'johnson'
p48454
(dp48455
g10
(lp48456
sg52
(lp48457
sg223
(lp48458
I3283
assS'neci'
p48459
(dp48460
g121
(lp48461
I229
assS'consecu'
p48462
(dp48463
g344
(lp48464
I3179
assS'q'
p48465
(dp48466
g70
(lp48467
sg163
(lp48468
sg74
(lp48469
sg256
(lp48470
sg76
(lp48471
sg262
(lp48472
sg78
(lp48473
sg484
(lp48474
sg38
(lp48475
sg83
(lp48476
sg303
(lp48477
sg42
(lp48478
I3418
asg87
(lp48479
sg89
(lp48480
sg12
(lp48481
sg46
(lp48482
sg96
(lp48483
sg114
(lp48484
sg99
(lp48485
sg313
(lp48486
sg32
(lp48487
sg102
(lp48488
sg108
(lp48489
sg63
(lp48490
sg52
(lp48491
sg22
(lp48492
sg230
(lp48493
sg438
(lp48494
sg440
(lp48495
sg121
(lp48496
sg4
(lp48497
sg34
(lp48498
sg36
(lp48499
sg384
(lp48500
sg293
(lp48501
sg72
(lp48502
sg132
(lp48503
sg135
(lp48504
sg460
(lp48505
sg140
(lp48506
ssS'switch'
p48507
(dp48508
g440
(lp48509
sg145
(lp48510
sg22
(lp48511
sg8
(lp48512
sg295
(lp48513
sg183
(lp48514
sg68
(lp48515
sg38
(lp48516
sg40
(lp48517
sg128
(lp48518
sg78
(lp48519
sg245
(lp48520
sg14
(lp48521
sg135
(lp48522
I969
assS'deposit'
p48523
(dp48524
g14
(lp48525
I4091
assS'jain'
p48526
(dp48527
g94
(lp48528
I1252
assS'african'
p48529
(dp48530
g74
(lp48531
I398
assS'hassan'
p48532
(dp48533
g535
(lp48534
I2189
assS'wkiyi'
p48535
(dp48536
g121
(lp48537
I360
assS'talk'
p48538
(dp48539
g72
(lp48540
I3543
assS'lwaaa'
p48541
(dp48542
g140
(lp48543
I566
assS'enk'
p48544
(dp48545
g76
(lp48546
I3330
assS'schwartz'
p48547
(dp48548
g174
(lp48549
sg306
(lp48550
sg87
(lp48551
sg89
(lp48552
sg132
(lp48553
I3852
asg48
(lp48554
ssS'feb'
p48555
(dp48556
g20
(lp48557
I2589
asg26
(lp48558
ssS'gradual'
p48559
(dp48560
g116
(lp48561
sg174
(lp48562
sg332
(lp48563
sg26
(lp48564
sg8
(lp48565
sg59
(lp48566
sg235
(lp48567
sg329
(lp48568
sg83
(lp48569
sg42
(lp48570
I838
asg50
(lp48571
sg138
(lp48572
sg354
(lp48573
ssS'lsi'
p48574
(dp48575
g262
(lp48576
I998
assS'graduat'
p48577
(dp48578
g440
(lp48579
I2083
asg277
(lp48580
sg350
(lp48581
ssS'pitch'
p48582
(dp48583
g30
(lp48584
sg174
(lp48585
I2607
asg332
(lp48586
ssS'equip'
p48587
(dp48588
g78
(lp48589
I443
assS'entitl'
p48590
(dp48591
g440
(lp48592
I2235
assS'pointer'
p48593
(dp48594
g42
(lp48595
I1465
asg14
(lp48596
sg429
(lp48597
sg44
(lp48598
ssS'synerget'
p48599
(dp48600
g149
(lp48601
I434
assS'entiti'
p48602
(dp48603
g295
(lp48604
sg183
(lp48605
sg460
(lp48606
sg44
(lp48607
I830
asg149
(lp48608
ssS'hirst'
p48609
(dp48610
g283
(lp48611
I1887
assS'reinforc'
p48612
(dp48613
g30
(lp48614
sg329
(lp48615
sg4
(lp48616
sg76
(lp48617
sg293
(lp48618
sg116
(lp48619
sg83
(lp48620
sg306
(lp48621
sg89
(lp48622
sg91
(lp48623
sg132
(lp48624
I3540
asg313
(lp48625
sg223
(lp48626
sg350
(lp48627
ssS'group'
p48628
(dp48629
g283
(lp48630
sg70
(lp48631
sg277
(lp48632
sg30
(lp48633
sg293
(lp48634
sg83
(lp48635
sg245
(lp48636
sg94
(lp48637
sg99
(lp48638
sg223
(lp48639
sg149
(lp48640
sg429
(lp48641
sg110
(lp48642
sg63
(lp48643
sg52
(lp48644
sg114
(lp48645
sg116
(lp48646
sg174
(lp48647
sg32
(lp48648
sg332
(lp48649
sg22
(lp48650
sg181
(lp48651
sg235
(lp48652
sg124
(lp48653
sg126
(lp48654
sg44
(lp48655
sg128
(lp48656
sg130
(lp48657
sg14
(lp48658
sg16
(lp48659
I17
assS'monitor'
p48660
(dp48661
g283
(lp48662
sg78
(lp48663
sg38
(lp48664
sg126
(lp48665
sg94
(lp48666
sg132
(lp48667
sg14
(lp48668
sg16
(lp48669
I2125
asg48
(lp48670
sg99
(lp48671
ssS'macaqu'
p48672
(dp48673
g216
(lp48674
sg48
(lp48675
I103
asg181
(lp48676
sg6
(lp48677
ssS'iiih'
p48678
(dp48679
g70
(lp48680
I1952
assS'polici'
p48681
(dp48682
g89
(lp48683
I108
asg83
(lp48684
sg277
(lp48685
sg293
(lp48686
ssS'dysfunct'
p48687
(dp48688
g4
(lp48689
I3423
assS'gem'
p48690
(dp48691
g440
(lp48692
I1893
assS'hutchin'
p48693
(dp48694
g18
(lp48695
I2573
assS'mail'
p48696
(dp48697
g216
(lp48698
sg145
(lp48699
sg36
(lp48700
sg281
(lp48701
sg40
(lp48702
sg306
(lp48703
sg132
(lp48704
I22
asg63
(lp48705
ssS'main'
p48706
(dp48707
g68
(lp48708
sg287
(lp48709
sg145
(lp48710
sg262
(lp48711
sg183
(lp48712
sg85
(lp48713
sg63
(lp48714
sg42
(lp48715
I553
asg306
(lp48716
sg91
(lp48717
sg96
(lp48718
sg48
(lp48719
sg223
(lp48720
sg350
(lp48721
sg116
(lp48722
sg429
(lp48723
sg102
(lp48724
sg104
(lp48725
sg110
(lp48726
sg178
(lp48727
sg216
(lp48728
sg121
(lp48729
sg181
(lp48730
sg235
(lp48731
sg384
(lp48732
sg124
(lp48733
sg341
(lp48734
sg10
(lp48735
sg40
(lp48736
sg130
(lp48737
sg132
(lp48738
sg14
(lp48739
sg460
(lp48740
sg354
(lp48741
ssS'enu'
p48742
(dp48743
g183
(lp48744
I5412
assS'tonic'
p48745
(dp48746
g176
(lp48747
sg350
(lp48748
I1480
assS'janeiro'
p48749
(dp48750
g118
(lp48751
I16
assS'financi'
p48752
(dp48753
g78
(lp48754
sg283
(lp48755
sg50
(lp48756
I1583
asg277
(lp48757
ssS'automobil'
p48758
(dp48759
g94
(lp48760
I866
asg293
(lp48761
sg78
(lp48762
ssS'shatter'
p48763
(dp48764
g287
(lp48765
I1823
assS'initi'
p48766
(dp48767
g329
(lp48768
sg70
(lp48769
sg78
(lp48770
sg72
(lp48771
sg283
(lp48772
sg460
(lp48773
sg26
(lp48774
sg30
(lp48775
sg74
(lp48776
sg256
(lp48777
sg76
(lp48778
sg262
(lp48779
sg295
(lp48780
sg183
(lp48781
sg484
(lp48782
sg38
(lp48783
sg85
(lp48784
sg42
(lp48785
I748
asg306
(lp48786
sg91
(lp48787
sg245
(lp48788
sg94
(lp48789
sg20
(lp48790
sg18
(lp48791
sg99
(lp48792
sg313
(lp48793
sg44
(lp48794
sg149
(lp48795
sg116
(lp48796
sg174
(lp48797
sg32
(lp48798
sg429
(lp48799
sg68
(lp48800
sg46
(lp48801
sg102
(lp48802
sg104
(lp48803
sg108
(lp48804
sg110
(lp48805
sg178
(lp48806
sg114
(lp48807
sg230
(lp48808
sg438
(lp48809
sg440
(lp48810
sg332
(lp48811
sg121
(lp48812
sg80
(lp48813
sg181
(lp48814
sg34
(lp48815
sg36
(lp48816
sg384
(lp48817
sg124
(lp48818
sg126
(lp48819
sg10
(lp48820
sg118
(lp48821
sg344
(lp48822
sg223
(lp48823
sg128
(lp48824
sg130
(lp48825
sg132
(lp48826
sg14
(lp48827
sg16
(lp48828
sg135
(lp48829
sg50
(lp48830
sg138
(lp48831
sg140
(lp48832
ssS'johnw'
p48833
(dp48834
g10
(lp48835
I36
assS'possess'
p48836
(dp48837
g230
(lp48838
sg176
(lp48839
sg256
(lp48840
sg295
(lp48841
sg183
(lp48842
sg85
(lp48843
sg221
(lp48844
sg44
(lp48845
I1647
assS'xln'
p48846
(dp48847
g223
(lp48848
I1866
assS'xll'
p48849
(dp48850
g221
(lp48851
I1840
assS'xlj'
p48852
(dp48853
g313
(lp48854
I1101
assS'xli'
p48855
(dp48856
g72
(lp48857
sg484
(lp48858
sg221
(lp48859
sg313
(lp48860
I1097
assS'adclus'
p48861
(dp48862
g74
(lp48863
I77
assS'xle'
p48864
(dp48865
g221
(lp48866
I417
assS'oliv'
p48867
(dp48868
g174
(lp48869
I323
assS'focuss'
p48870
(dp48871
g332
(lp48872
sg48
(lp48873
I228
asg350
(lp48874
ssS'tropi'
p48875
(dp48876
g48
(lp48877
I2268
assS'massachusett'
p48878
(dp48879
g230
(lp48880
sg329
(lp48881
sg74
(lp48882
sg318
(lp48883
sg181
(lp48884
sg83
(lp48885
sg306
(lp48886
sg89
(lp48887
sg313
(lp48888
I31
assS'adjud'
p48889
(dp48890
g87
(lp48891
I2061
assS'eyelid'
p48892
(dp48893
g6
(lp48894
I1976
assS'zurich'
p48895
(dp48896
g293
(lp48897
I3181
assS'continu'
p48898
(dp48899
g163
(lp48900
sg460
(lp48901
sg287
(lp48902
sg176
(lp48903
sg80
(lp48904
sg293
(lp48905
sg295
(lp48906
sg183
(lp48907
sg59
(lp48908
sg484
(lp48909
sg38
(lp48910
sg83
(lp48911
sg42
(lp48912
I1566
asg87
(lp48913
sg89
(lp48914
sg91
(lp48915
sg12
(lp48916
sg94
(lp48917
sg20
(lp48918
sg48
(lp48919
sg221
(lp48920
sg313
(lp48921
sg44
(lp48922
sg149
(lp48923
sg32
(lp48924
sg245
(lp48925
sg429
(lp48926
sg46
(lp48927
sg102
(lp48928
sg104
(lp48929
sg110
(lp48930
sg96
(lp48931
sg22
(lp48932
sg230
(lp48933
sg174
(lp48934
sg440
(lp48935
sg332
(lp48936
sg178
(lp48937
sg4
(lp48938
sg181
(lp48939
sg8
(lp48940
sg34
(lp48941
sg36
(lp48942
sg384
(lp48943
sg68
(lp48944
sg341
(lp48945
sg10
(lp48946
sg78
(lp48947
sg135
(lp48948
sg138
(lp48949
sg140
(lp48950
ssS'tverski'
p48951
(dp48952
g74
(lp48953
I306
assS'redistribut'
p48954
(dp48955
g52
(lp48956
I1542
assS'medial'
p48957
(dp48958
g116
(lp48959
sg174
(lp48960
I318
assS'gullichsen'
p48961
(dp48962
g63
(lp48963
I3064
assS'iasi'
p48964
(dp48965
g14
(lp48966
I3551
assS'entropyl'
p48967
(dp48968
g318
(lp48969
I1073
assS'baselin'
p48970
(dp48971
g216
(lp48972
sg329
(lp48973
sg440
(lp48974
sg4
(lp48975
sg295
(lp48976
sg183
(lp48977
sg484
(lp48978
sg87
(lp48979
sg106
(lp48980
I1880
asg99
(lp48981
sg96
(lp48982
ssS'canada'
p48983
(dp48984
g174
(lp48985
sg124
(lp48986
sg126
(lp48987
sg138
(lp48988
I30
assS'monomi'
p48989
(dp48990
g429
(lp48991
I1583
assS'clks'
p48992
(dp48993
g22
(lp48994
I687
assS'jackson'
p48995
(dp48996
g344
(lp48997
sg145
(lp48998
I3028
assS'acollst'
p48999
(dp49000
g174
(lp49001
I2704
assS'hemmen'
p49002
(dp49003
g34
(lp49004
sg124
(lp49005
I3094
assS'airgap'
p49006
(dp49007
g78
(lp49008
I914
assS'chea'
p49009
(dp49010
g116
(lp49011
I310
assS'correct'
p49012
(dp49013
g26
(lp49014
sg163
(lp49015
sg85
(lp49016
sg303
(lp49017
sg30
(lp49018
sg287
(lp49019
sg74
(lp49020
sg145
(lp49021
sg76
(lp49022
sg295
(lp49023
sg183
(lp49024
sg80
(lp49025
sg114
(lp49026
sg63
(lp49027
sg42
(lp49028
I1953
asg89
(lp49029
sg91
(lp49030
sg245
(lp49031
sg46
(lp49032
sg96
(lp49033
sg48
(lp49034
sg221
(lp49035
sg313
(lp49036
sg223
(lp49037
sg149
(lp49038
sg118
(lp49039
sg350
(lp49040
sg429
(lp49041
sg94
(lp49042
sg104
(lp49043
sg110
(lp49044
sg20
(lp49045
sg22
(lp49046
sg216
(lp49047
sg174
(lp49048
sg440
(lp49049
sg318
(lp49050
sg4
(lp49051
sg181
(lp49052
sg36
(lp49053
sg460
(lp49054
sg72
(lp49055
sg281
(lp49056
sg10
(lp49057
sg44
(lp49058
sg78
(lp49059
sg14
(lp49060
sg16
(lp49061
sg135
(lp49062
sg138
(lp49063
ssS'ixj'
p49064
(dp49065
g440
(lp49066
sg354
(lp49067
I2192
assS'earlier'
p49068
(dp49069
g440
(lp49070
sg70
(lp49071
sg4
(lp49072
sg80
(lp49073
sg8
(lp49074
sg344
(lp49075
sg293
(lp49076
sg59
(lp49077
sg124
(lp49078
sg74
(lp49079
sg10
(lp49080
sg102
(lp49081
sg91
(lp49082
sg12
(lp49083
sg14
(lp49084
sg16
(lp49085
I1900
asg535
(lp49086
sg52
(lp49087
ssS'goto'
p49088
(dp49089
g42
(lp49090
I3463
assS'ryckebusch'
p49091
(dp49092
g20
(lp49093
I2658
assS'feartur'
p49094
(dp49095
g145
(lp49096
I431
assS'ort'
p49097
(dp49098
g48
(lp49099
I1156
assS'boahen'
p49100
(dp49101
g256
(lp49102
I13
assS'gotz'
p49103
(dp49104
g48
(lp49105
I258
assS'oro'
p49106
(dp49107
g104
(lp49108
I1219
asg329
(lp49109
ssS'orm'
p49110
(dp49111
g230
(lp49112
sg221
(lp49113
I184
assS'california'
p49114
(dp49115
g116
(lp49116
sg181
(lp49117
sg6
(lp49118
sg8
(lp49119
sg344
(lp49120
sg221
(lp49121
sg68
(lp49122
sg341
(lp49123
sg10
(lp49124
sg40
(lp49125
sg12
(lp49126
sg140
(lp49127
I3224
asg183
(lp49128
sg132
(lp49129
sg256
(lp49130
sg350
(lp49131
sg50
(lp49132
sg44
(lp49133
sg91
(lp49134
ssS'triangl'
p49135
(dp49136
g36
(lp49137
sg429
(lp49138
sg26
(lp49139
sg223
(lp49140
I1601
assS'atick'
p49141
(dp49142
g12
(lp49143
I2486
asg72
(lp49144
ssS'mainpart'
p49145
(dp49146
g429
(lp49147
I2009
assS'sejnowski'
p49148
(dp49149
g116
(lp49150
sg318
(lp49151
sg70
(lp49152
sg26
(lp49153
sg34
(lp49154
sg72
(lp49155
sg303
(lp49156
sg89
(lp49157
sg91
(lp49158
sg132
(lp49159
sg106
(lp49160
I23
asg114
(lp49161
sg50
(lp49162
sg350
(lp49163
ssS'sejnowskl'
p49164
(dp49165
g318
(lp49166
I1235
assS'frequenc'
p49167
(dp49168
g283
(lp49169
sg181
(lp49170
sg176
(lp49171
sg145
(lp49172
sg78
(lp49173
sg85
(lp49174
sg42
(lp49175
I2002
asg94
(lp49176
sg318
(lp49177
sg46
(lp49178
sg106
(lp49179
sg22
(lp49180
sg116
(lp49181
sg174
(lp49182
sg332
(lp49183
sg4
(lp49184
sg6
(lp49185
sg68
(lp49186
sg128
(lp49187
sg14
(lp49188
sg16
(lp49189
sg135
(lp49190
ssS'thing'
p49191
(dp49192
g223
(lp49193
I6
assS'vjl'
p49194
(dp49195
g130
(lp49196
I2785
assS'principl'
p49197
(dp49198
g70
(lp49199
sg26
(lp49200
sg163
(lp49201
sg74
(lp49202
sg262
(lp49203
sg59
(lp49204
sg85
(lp49205
sg303
(lp49206
sg42
(lp49207
I2752
asg89
(lp49208
sg91
(lp49209
sg245
(lp49210
sg48
(lp49211
sg313
(lp49212
sg149
(lp49213
sg12
(lp49214
sg332
(lp49215
sg102
(lp49216
sg104
(lp49217
sg52
(lp49218
sg4
(lp49219
sg116
(lp49220
sg118
(lp49221
sg440
(lp49222
sg318
(lp49223
sg22
(lp49224
sg6
(lp49225
sg68
(lp49226
sg281
(lp49227
sg130
(lp49228
sg138
(lp49229
sg354
(lp49230
ssS'wattl'
p49231
(dp49232
g135
(lp49233
I2578
assS'frequent'
p49234
(dp49235
g116
(lp49236
sg332
(lp49237
sg78
(lp49238
sg344
(lp49239
sg221
(lp49240
sg281
(lp49241
sg89
(lp49242
sg128
(lp49243
sg104
(lp49244
sg132
(lp49245
sg94
(lp49246
sg50
(lp49247
I136
assS'first'
p49248
(dp49249
g80
(lp49250
sg293
(lp49251
sg344
(lp49252
sg78
(lp49253
sg59
(lp49254
sg484
(lp49255
sg38
(lp49256
sg83
(lp49257
sg85
(lp49258
sg303
(lp49259
sg438
(lp49260
sg116
(lp49261
sg34
(lp49262
sg36
(lp49263
sg460
(lp49264
sg68
(lp49265
sg72
(lp49266
sg281
(lp49267
sg10
(lp49268
sg40
(lp49269
sg283
(lp49270
sg26
(lp49271
sg277
(lp49272
sg89
(lp49273
sg91
(lp49274
sg12
(lp49275
sg94
(lp49276
sg96
(lp49277
sg99
(lp49278
sg313
(lp49279
sg44
(lp49280
sg429
(lp49281
sg102
(lp49282
sg104
(lp49283
sg106
(lp49284
sg108
(lp49285
sg110
(lp49286
sg63
(lp49287
sg52
(lp49288
sg114
(lp49289
sg128
(lp49290
sg130
(lp49291
sg132
(lp49292
sg14
(lp49293
sg16
(lp49294
sg135
(lp49295
sg138
(lp49296
sg140
(lp49297
sg354
(lp49298
sg306
(lp49299
sg245
(lp49300
sg46
(lp49301
sg20
(lp49302
sg18
(lp49303
sg221
(lp49304
sg535
(lp49305
sg223
(lp49306
sg350
(lp49307
sg216
(lp49308
sg174
(lp49309
sg440
(lp49310
sg121
(lp49311
sg4
(lp49312
sg6
(lp49313
sg8
(lp49314
sg126
(lp49315
sg341
(lp49316
sg30
(lp49317
sg287
(lp49318
sg74
(lp49319
sg176
(lp49320
sg145
(lp49321
sg256
(lp49322
sg76
(lp49323
sg262
(lp49324
sg295
(lp49325
sg183
(lp49326
sg42
(lp49327
I49
asg230
(lp49328
sg329
(lp49329
sg32
(lp49330
sg318
(lp49331
sg178
(lp49332
sg22
(lp49333
sg181
(lp49334
sg235
(lp49335
sg384
(lp49336
sg124
(lp49337
ssS'oppon'
p49338
(dp49339
g132
(lp49340
I859
asg89
(lp49341
sg6
(lp49342
ssS'kappen'
p49343
(dp49344
g36
(lp49345
I1396
assS'carri'
p49346
(dp49347
g216
(lp49348
sg174
(lp49349
sg74
(lp49350
sg484
(lp49351
sg178
(lp49352
sg4
(lp49353
sg6
(lp49354
sg8
(lp49355
sg181
(lp49356
sg124
(lp49357
sg85
(lp49358
sg63
(lp49359
sg429
(lp49360
sg230
(lp49361
sg26
(lp49362
sg14
(lp49363
sg20
(lp49364
sg138
(lp49365
I2137
asg223
(lp49366
sg350
(lp49367
ssS'acquisit'
p49368
(dp49369
g59
(lp49370
sg63
(lp49371
sg130
(lp49372
sg245
(lp49373
sg14
(lp49374
sg16
(lp49375
I1872
asg99
(lp49376
sg313
(lp49377
ssS'long'
p49378
(dp49379
g283
(lp49380
sg70
(lp49381
sg26
(lp49382
sg277
(lp49383
sg303
(lp49384
sg287
(lp49385
sg74
(lp49386
sg176
(lp49387
sg80
(lp49388
sg59
(lp49389
sg83
(lp49390
sg85
(lp49391
sg63
(lp49392
sg42
(lp49393
I2798
asg306
(lp49394
sg89
(lp49395
sg12
(lp49396
sg94
(lp49397
sg20
(lp49398
sg18
(lp49399
sg99
(lp49400
sg535
(lp49401
sg149
(lp49402
sg102
(lp49403
sg104
(lp49404
sg106
(lp49405
sg96
(lp49406
sg52
(lp49407
sg116
(lp49408
sg121
(lp49409
sg4
(lp49410
sg6
(lp49411
sg34
(lp49412
sg126
(lp49413
sg14
(lp49414
sg16
(lp49415
ssS'oppos'
p49416
(dp49417
g216
(lp49418
sg8
(lp49419
I633
asg34
(lp49420
sg384
(lp49421
sg40
(lp49422
sg303
(lp49423
sg18
(lp49424
sg63
(lp49425
ssS'midbrain'
p49426
(dp49427
g350
(lp49428
I2965
assS'aposteriori'
p49429
(dp49430
g36
(lp49431
I2267
assS'daaal'
p49432
(dp49433
g429
(lp49434
I309
assS'formalj'
p49435
(dp49436
g262
(lp49437
I1873
assS'vatil'
p49438
(dp49439
g183
(lp49440
I5210
assS'averag'
p49441
(dp49442
g329
(lp49443
sg70
(lp49444
sg78
(lp49445
sg277
(lp49446
sg163
(lp49447
sg283
(lp49448
sg26
(lp49449
sg30
(lp49450
sg74
(lp49451
sg176
(lp49452
sg262
(lp49453
sg295
(lp49454
sg183
(lp49455
sg59
(lp49456
sg484
(lp49457
sg38
(lp49458
sg83
(lp49459
sg85
(lp49460
sg124
(lp49461
sg306
(lp49462
sg87
(lp49463
sg460
(lp49464
sg12
(lp49465
sg94
(lp49466
sg96
(lp49467
sg48
(lp49468
sg221
(lp49469
sg313
(lp49470
sg44
(lp49471
sg350
(lp49472
sg116
(lp49473
sg174
(lp49474
sg293
(lp49475
sg245
(lp49476
sg318
(lp49477
sg102
(lp49478
sg178
(lp49479
sg108
(lp49480
sg110
(lp49481
sg216
(lp49482
sg438
(lp49483
I425
asg18
(lp49484
sg121
(lp49485
sg6
(lp49486
sg8
(lp49487
sg34
(lp49488
sg36
(lp49489
sg384
(lp49490
sg235
(lp49491
sg126
(lp49492
sg281
(lp49493
sg344
(lp49494
sg223
(lp49495
sg128
(lp49496
sg130
(lp49497
sg135
(lp49498
sg50
(lp49499
sg138
(lp49500
sg140
(lp49501
sg354
(lp49502
ssS'deciph'
p49503
(dp49504
g114
(lp49505
I380
assS'proposit'
p49506
(dp49507
g287
(lp49508
I1942
assS'gorri'
p49509
(dp49510
g91
(lp49511
I1873
assS'warmuth'
p49512
(dp49513
g287
(lp49514
sg341
(lp49515
I12
assS'memo'
p49516
(dp49517
g96
(lp49518
sg178
(lp49519
sg138
(lp49520
I3295
asg256
(lp49521
ssS'posses'
p49522
(dp49523
g535
(lp49524
I700
assS'broadcast'
p49525
(dp49526
g438
(lp49527
I2095
assS'ttll'
p49528
(dp49529
g287
(lp49530
I3208
assS'anandan'
p49531
(dp49532
g429
(lp49533
I28
assS'greenberg'
p49534
(dp49535
g440
(lp49536
I2785
assS'deafen'
p49537
(dp49538
g116
(lp49539
I172
assS'duquesn'
p49540
(dp49541
g344
(lp49542
I11
assS'batali'
p49543
(dp49544
g116
(lp49545
sg18
(lp49546
I2580
assS'were'
p49547
(dp49548
g124
(lp49549
sg78
(lp49550
sg116
(lp49551
sg293
(lp49552
sg281
(lp49553
sg283
(lp49554
sg85
(lp49555
sg181
(lp49556
sg303
(lp49557
sg26
(lp49558
sg30
(lp49559
sg287
(lp49560
sg74
(lp49561
sg176
(lp49562
sg145
(lp49563
sg80
(lp49564
sg262
(lp49565
sg295
(lp49566
sg183
(lp49567
sg59
(lp49568
sg484
(lp49569
sg83
(lp49570
sg114
(lp49571
sg63
(lp49572
sg42
(lp49573
I2030
asg306
(lp49574
sg87
(lp49575
sg89
(lp49576
sg91
(lp49577
sg12
(lp49578
sg94
(lp49579
sg96
(lp49580
sg48
(lp49581
sg99
(lp49582
sg313
(lp49583
sg44
(lp49584
sg149
(lp49585
sg118
(lp49586
sg230
(lp49587
sg329
(lp49588
sg18
(lp49589
sg178
(lp49590
sg350
(lp49591
sg46
(lp49592
sg102
(lp49593
sg104
(lp49594
sg106
(lp49595
sg108
(lp49596
sg110
(lp49597
sg20
(lp49598
sg52
(lp49599
sg22
(lp49600
sg216
(lp49601
sg174
(lp49602
sg440
(lp49603
sg332
(lp49604
sg121
(lp49605
sg4
(lp49606
sg6
(lp49607
sg8
(lp49608
sg221
(lp49609
sg460
(lp49610
sg235
(lp49611
sg126
(lp49612
sg341
(lp49613
sg535
(lp49614
sg223
(lp49615
sg128
(lp49616
sg36
(lp49617
sg132
(lp49618
sg14
(lp49619
sg16
(lp49620
sg135
(lp49621
sg50
(lp49622
sg138
(lp49623
sg140
(lp49624
sg354
(lp49625
ssS'werk'
p49626
(dp49627
g78
(lp49628
I419
assS'tractor'
p49629
(dp49630
g68
(lp49631
I615
assS'sult'
p49632
(dp49633
g74
(lp49634
I1937
assS'dass'
p49635
(dp49636
g221
(lp49637
I2040
assS'dash'
p49638
(dp49639
g329
(lp49640
sg440
(lp49641
sg163
(lp49642
sg235
(lp49643
sg384
(lp49644
sg262
(lp49645
sg140
(lp49646
I1163
asg46
(lp49647
sg14
(lp49648
sg16
(lp49649
sg110
(lp49650
sg44
(lp49651
ssS'kevin'
p49652
(dp49653
g283
(lp49654
sg83
(lp49655
sg52
(lp49656
I24
assS'cassandra'
p49657
(dp49658
g83
(lp49659
I2771
asg293
(lp49660
ssS'otolith'
p49661
(dp49662
g350
(lp49663
I144
assS'squar'
p49664
(dp49665
g283
(lp49666
sg78
(lp49667
sg277
(lp49668
sg163
(lp49669
sg72
(lp49670
sg281
(lp49671
sg26
(lp49672
sg30
(lp49673
sg287
(lp49674
sg74
(lp49675
sg145
(lp49676
sg256
(lp49677
sg76
(lp49678
sg295
(lp49679
sg183
(lp49680
sg83
(lp49681
sg85
(lp49682
sg42
(lp49683
I1641
asg306
(lp49684
sg91
(lp49685
sg245
(lp49686
sg94
(lp49687
sg313
(lp49688
sg149
(lp49689
sg429
(lp49690
sg108
(lp49691
sg438
(lp49692
sg318
(lp49693
sg4
(lp49694
sg235
(lp49695
sg34
(lp49696
sg460
(lp49697
sg124
(lp49698
sg126
(lp49699
sg341
(lp49700
sg10
(lp49701
sg128
(lp49702
sg130
(lp49703
sg14
(lp49704
sg16
(lp49705
sg138
(lp49706
sg140
(lp49707
sg354
(lp49708
ssS'advic'
p49709
(dp49710
g116
(lp49711
sg18
(lp49712
sg135
(lp49713
I2447
asg10
(lp49714
ssS'haclsalihzad'
p49715
(dp49716
g178
(lp49717
I2203
assS'advis'
p49718
(dp49719
g132
(lp49720
I3426
asg124
(lp49721
ssS'bailey'
p49722
(dp49723
g96
(lp49724
I2708
assS'channel'
p49725
(dp49726
g174
(lp49727
sg332
(lp49728
sg22
(lp49729
sg181
(lp49730
sg72
(lp49731
sg102
(lp49732
sg12
(lp49733
sg14
(lp49734
sg16
(lp49735
I1935
asg96
(lp49736
sg256
(lp49737
ssS'pain'
p49738
(dp49739
g91
(lp49740
I2109
assS'trace'
p49741
(dp49742
g32
(lp49743
sg332
(lp49744
sg4
(lp49745
sg6
(lp49746
sg50
(lp49747
sg42
(lp49748
I2321
asg106
(lp49749
sg114
(lp49750
sg99
(lp49751
sg22
(lp49752
ssS'normal'
p49753
(dp49754
g70
(lp49755
sg181
(lp49756
sg30
(lp49757
sg287
(lp49758
sg256
(lp49759
sg76
(lp49760
sg262
(lp49761
sg78
(lp49762
sg59
(lp49763
sg80
(lp49764
sg38
(lp49765
sg114
(lp49766
sg87
(lp49767
sg91
(lp49768
sg245
(lp49769
sg46
(lp49770
sg96
(lp49771
sg48
(lp49772
sg221
(lp49773
sg313
(lp49774
sg44
(lp49775
sg149
(lp49776
sg329
(lp49777
sg94
(lp49778
sg102
(lp49779
sg104
(lp49780
sg110
(lp49781
sg63
(lp49782
sg52
(lp49783
sg22
(lp49784
sg116
(lp49785
sg438
(lp49786
I957
asg32
(lp49787
sg178
(lp49788
sg4
(lp49789
sg6
(lp49790
sg235
(lp49791
sg34
(lp49792
sg36
(lp49793
sg293
(lp49794
sg126
(lp49795
sg281
(lp49796
sg535
(lp49797
sg128
(lp49798
sg130
(lp49799
sg14
(lp49800
sg16
(lp49801
sg138
(lp49802
sg354
(lp49803
ssS'track'
p49804
(dp49805
g230
(lp49806
sg70
(lp49807
sg8
(lp49808
sg34
(lp49809
sg293
(lp49810
sg245
(lp49811
sg132
(lp49812
I2153
asg94
(lp49813
sg20
(lp49814
ssS'friedman'
p49815
(dp49816
g183
(lp49817
sg126
(lp49818
I2373
assS'iwl'
p49819
(dp49820
g313
(lp49821
I876
assS'iwj'
p49822
(dp49823
g91
(lp49824
I1064
assS'paid'
p49825
(dp49826
g384
(lp49827
I2193
assS'aloud'
p49828
(dp49829
g114
(lp49830
I2480
assS'beta'
p49831
(dp49832
g26
(lp49833
I3
assS'puma'
p49834
(dp49835
g59
(lp49836
I826
assS'milanes'
p49837
(dp49838
g178
(lp49839
I2110
assS'sunglass'
p49840
(dp49841
g223
(lp49842
I2192
assS'pair'
p49843
(dp49844
g68
(lp49845
sg26
(lp49846
sg30
(lp49847
sg176
(lp49848
sg80
(lp49849
sg293
(lp49850
sg344
(lp49851
sg59
(lp49852
sg484
(lp49853
sg42
(lp49854
I127
asg306
(lp49855
sg91
(lp49856
sg46
(lp49857
sg96
(lp49858
sg18
(lp49859
sg99
(lp49860
sg223
(lp49861
sg230
(lp49862
sg318
(lp49863
sg106
(lp49864
sg216
(lp49865
sg329
(lp49866
sg440
(lp49867
sg332
(lp49868
sg6
(lp49869
sg124
(lp49870
sg72
(lp49871
sg341
(lp49872
sg40
(lp49873
sg130
(lp49874
sg14
(lp49875
sg16
(lp49876
sg135
(lp49877
ssS'normat'
p49878
(dp49879
g91
(lp49880
I2988
assS'sutton'
p49881
(dp49882
g329
(lp49883
sg293
(lp49884
sg295
(lp49885
sg183
(lp49886
sg83
(lp49887
sg306
(lp49888
sg89
(lp49889
sg132
(lp49890
I3801
asg99
(lp49891
ssS'acknowleoo'
p49892
(dp49893
g438
(lp49894
I2342
assS'sigma'
p49895
(dp49896
g283
(lp49897
I340
assS'lexicon'
p49898
(dp49899
g87
(lp49900
I1784
asg76
(lp49901
ssS'shot'
p49902
(dp49903
g14
(lp49904
sg16
(lp49905
I745
assS'show'
p49906
(dp49907
g80
(lp49908
sg293
(lp49909
sg344
(lp49910
sg78
(lp49911
sg59
(lp49912
sg484
(lp49913
sg38
(lp49914
sg83
(lp49915
sg85
(lp49916
sg303
(lp49917
sg438
(lp49918
sg116
(lp49919
sg118
(lp49920
sg34
(lp49921
sg36
(lp49922
sg460
(lp49923
sg68
(lp49924
sg10
(lp49925
sg40
(lp49926
sg70
(lp49927
sg26
(lp49928
sg277
(lp49929
sg163
(lp49930
sg89
(lp49931
sg12
(lp49932
sg94
(lp49933
sg96
(lp49934
sg48
(lp49935
sg99
(lp49936
sg313
(lp49937
sg44
(lp49938
sg149
(lp49939
sg429
(lp49940
sg102
(lp49941
sg106
(lp49942
sg108
(lp49943
sg110
(lp49944
sg63
(lp49945
sg52
(lp49946
sg114
(lp49947
sg128
(lp49948
sg130
(lp49949
sg132
(lp49950
sg14
(lp49951
sg16
(lp49952
sg135
(lp49953
sg50
(lp49954
sg138
(lp49955
sg140
(lp49956
sg354
(lp49957
sg306
(lp49958
sg87
(lp49959
sg245
(lp49960
sg46
(lp49961
sg20
(lp49962
sg18
(lp49963
sg221
(lp49964
sg535
(lp49965
sg223
(lp49966
sg350
(lp49967
sg216
(lp49968
sg174
(lp49969
sg440
(lp49970
sg332
(lp49971
sg121
(lp49972
sg4
(lp49973
sg6
(lp49974
sg8
(lp49975
sg341
(lp49976
sg30
(lp49977
sg287
(lp49978
sg74
(lp49979
sg176
(lp49980
sg145
(lp49981
sg256
(lp49982
sg76
(lp49983
sg262
(lp49984
sg295
(lp49985
sg183
(lp49986
sg42
(lp49987
I1155
asg230
(lp49988
sg329
(lp49989
sg32
(lp49990
sg318
(lp49991
sg22
(lp49992
sg235
(lp49993
sg384
(lp49994
sg124
(lp49995
ssS'kwok'
p49996
(dp49997
g183
(lp49998
I6343
assS'unprocess'
p49999
(dp50000
g70
(lp50001
I2198
assS'shoe'
p50002
(dp50003
g181
(lp50004
sg223
(lp50005
I2189
assS'threshold'
p50006
(dp50007
g70
(lp50008
sg277
(lp50009
sg287
(lp50010
sg176
(lp50011
sg145
(lp50012
sg262
(lp50013
sg295
(lp50014
sg183
(lp50015
sg83
(lp50016
sg89
(lp50017
sg94
(lp50018
sg20
(lp50019
sg106
(lp50020
I1644
asg108
(lp50021
sg110
(lp50022
sg63
(lp50023
sg114
(lp50024
sg116
(lp50025
sg174
(lp50026
sg178
(lp50027
sg4
(lp50028
sg281
(lp50029
sg40
(lp50030
sg344
(lp50031
sg78
(lp50032
sg14
(lp50033
sg135
(lp50034
ssS'corner'
p50035
(dp50036
g178
(lp50037
sg6
(lp50038
sg181
(lp50039
sg460
(lp50040
sg80
(lp50041
sg89
(lp50042
sg70
(lp50043
sg135
(lp50044
I956
asg63
(lp50045
sg52
(lp50046
sg114
(lp50047
ssS'goetsch'
p50048
(dp50049
g132
(lp50050
I3519
assS'mse'
p50051
(dp50052
g295
(lp50053
sg183
(lp50054
sg460
(lp50055
sg94
(lp50056
sg96
(lp50057
sg313
(lp50058
I1815
assS'contemporari'
p50059
(dp50060
g74
(lp50061
I162
assS'ijeigenvalu'
p50062
(dp50063
g295
(lp50064
I2677
asg183
(lp50065
ssS'catecholamin'
p50066
(dp50067
g4
(lp50068
I3576
assS'laughlin'
p50069
(dp50070
g256
(lp50071
I2135
assS'apriori'
p50072
(dp50073
g108
(lp50074
I1407
assS'srihari'
p50075
(dp50076
g138
(lp50077
I3477
asg63
(lp50078
ssS'helic'
p50079
(dp50080
g26
(lp50081
I1464
assS'stormo'
p50082
(dp50083
g344
(lp50084
I359
assS'parametr'
p50085
(dp50086
g230
(lp50087
sg287
(lp50088
sg318
(lp50089
sg76
(lp50090
sg235
(lp50091
sg295
(lp50092
sg183
(lp50093
sg460
(lp50094
sg124
(lp50095
sg72
(lp50096
sg281
(lp50097
sg85
(lp50098
sg34
(lp50099
sg91
(lp50100
sg130
(lp50101
I2261
asg96
(lp50102
sg59
(lp50103
ssS'utgoff'
p50104
(dp50105
g132
(lp50106
I3632
asg126
(lp50107
sg277
(lp50108
sg223
(lp50109
ssS'black'
p50110
(dp50111
g216
(lp50112
sg174
(lp50113
sg145
(lp50114
sg26
(lp50115
sg80
(lp50116
sg118
(lp50117
sg183
(lp50118
sg59
(lp50119
sg126
(lp50120
sg114
(lp50121
sg63
(lp50122
sg42
(lp50123
I1749
asg12
(lp50124
sg132
(lp50125
sg104
(lp50126
sg116
(lp50127
sg48
(lp50128
sg110
(lp50129
sg138
(lp50130
sg149
(lp50131
ssS'siggraph'
p50132
(dp50133
g429
(lp50134
I2578
assS'cepstral'
p50135
(dp50136
g96
(lp50137
I1740
assS'extracellular'
p50138
(dp50139
g106
(lp50140
I648
asg6
(lp50141
ssS'fanti'
p50142
(dp50143
g440
(lp50144
I2634
asg429
(lp50145
ssS'ger'
p50146
(dp50147
g174
(lp50148
I1380
assS'get'
p50149
(dp50150
g68
(lp50151
sg72
(lp50152
sg30
(lp50153
sg145
(lp50154
sg262
(lp50155
sg295
(lp50156
sg183
(lp50157
sg83
(lp50158
sg303
(lp50159
sg89
(lp50160
sg91
(lp50161
sg12
(lp50162
sg94
(lp50163
sg99
(lp50164
sg535
(lp50165
sg63
(lp50166
sg114
(lp50167
sg216
(lp50168
sg329
(lp50169
sg440
(lp50170
sg178
(lp50171
sg22
(lp50172
sg8
(lp50173
sg34
(lp50174
sg235
(lp50175
sg126
(lp50176
sg341
(lp50177
sg40
(lp50178
sg132
(lp50179
sg140
(lp50180
sg354
(lp50181
I560
assS'kinemat'
p50182
(dp50183
g313
(lp50184
I1717
asg350
(lp50185
ssS'dewint'
p50186
(dp50187
g181
(lp50188
I2331
assS'repr'
p50189
(dp50190
g4
(lp50191
I1586
assS'secondari'
p50192
(dp50193
g38
(lp50194
I2160
asg26
(lp50195
sg277
(lp50196
ssS'irradi'
p50197
(dp50198
g256
(lp50199
I1941
assS'gee'
p50200
(dp50201
g10
(lp50202
I1417
assS'nunih'
p50203
(dp50204
g96
(lp50205
I2294
assS'mozer'
p50206
(dp50207
g80
(lp50208
sg36
(lp50209
sg124
(lp50210
sg126
(lp50211
sg83
(lp50212
sg132
(lp50213
I3863
asg350
(lp50214
ssS'sensor'
p50215
(dp50216
g277
(lp50217
sg293
(lp50218
I376
assS'median'
p50219
(dp50220
g20
(lp50221
sg283
(lp50222
sg313
(lp50223
I1956
assS'yield'
p50224
(dp50225
g68
(lp50226
sg70
(lp50227
sg26
(lp50228
sg277
(lp50229
sg44
(lp50230
sg283
(lp50231
sg74
(lp50232
sg145
(lp50233
sg262
(lp50234
sg59
(lp50235
sg38
(lp50236
sg83
(lp50237
sg85
(lp50238
sg63
(lp50239
sg20
(lp50240
sg221
(lp50241
sg223
(lp50242
sg293
(lp50243
sg429
(lp50244
sg102
(lp50245
sg106
(lp50246
I1718
asg96
(lp50247
sg52
(lp50248
sg230
(lp50249
sg440
(lp50250
sg22
(lp50251
sg181
(lp50252
sg235
(lp50253
sg34
(lp50254
sg124
(lp50255
sg281
(lp50256
sg6
(lp50257
sg128
(lp50258
sg130
(lp50259
sg14
(lp50260
sg140
(lp50261
sg354
(lp50262
ssS'mediat'
p50263
(dp50264
g216
(lp50265
sg256
(lp50266
sg80
(lp50267
sg293
(lp50268
sg78
(lp50269
sg350
(lp50270
sg50
(lp50271
I2
asg4
(lp50272
ssS'gammon'
p50273
(dp50274
g132
(lp50275
I241
asg83
(lp50276
ssS'anisotrop'
p50277
(dp50278
g48
(lp50279
I1068
assS'summari'
p50280
(dp50281
g440
(lp50282
sg277
(lp50283
sg484
(lp50284
sg32
(lp50285
sg85
(lp50286
sg89
(lp50287
sg102
(lp50288
sg14
(lp50289
I4520
asg96
(lp50290
sg48
(lp50291
sg313
(lp50292
ssS'kernel'
p50293
(dp50294
g216
(lp50295
sg118
(lp50296
sg318
(lp50297
sg178
(lp50298
sg22
(lp50299
sg384
(lp50300
sg10
(lp50301
sg63
(lp50302
sg59
(lp50303
sg96
(lp50304
sg313
(lp50305
I1277
asg44
(lp50306
ssS'autocorrel'
p50307
(dp50308
g174
(lp50309
sg70
(lp50310
sg6
(lp50311
sg245
(lp50312
sg48
(lp50313
sg50
(lp50314
I834
assS'sllch'
p50315
(dp50316
g108
(lp50317
I190
assS'sear'
p50318
(dp50319
g99
(lp50320
I2797
assS'enci'
p50321
(dp50322
g4
(lp50323
I1224
assS'mcxi'
p50324
(dp50325
g429
(lp50326
I1509
assS'sean'
p50327
(dp50328
g145
(lp50329
I3067
assS'schreckenberg'
p50330
(dp50331
g384
(lp50332
I2382
assS'august'
p50333
(dp50334
g110
(lp50335
sg429
(lp50336
sg145
(lp50337
I3084
asg40
(lp50338
ssS'infinit'
p50339
(dp50340
g32
(lp50341
sg124
(lp50342
sg121
(lp50343
sg277
(lp50344
sg8
(lp50345
sg34
(lp50346
sg293
(lp50347
sg235
(lp50348
sg341
(lp50349
sg281
(lp50350
sg42
(lp50351
I110
asg102
(lp50352
sg306
(lp50353
sg68
(lp50354
sg132
(lp50355
sg104
(lp50356
sg221
(lp50357
sg138
(lp50358
sg83
(lp50359
ssS'kurt'
p50360
(dp50361
g68
(lp50362
I24
assS'aimsworth'
p50363
(dp50364
g174
(lp50365
I2442
assS'innov'
p50366
(dp50367
g52
(lp50368
I2542
assS'enumer'
p50369
(dp50370
g110
(lp50371
I218
asg293
(lp50372
ssS'label'
p50373
(dp50374
g26
(lp50375
sg163
(lp50376
sg30
(lp50377
sg287
(lp50378
sg145
(lp50379
sg76
(lp50380
sg293
(lp50381
sg78
(lp50382
sg38
(lp50383
sg85
(lp50384
sg42
(lp50385
I3004
asg94
(lp50386
sg96
(lp50387
sg221
(lp50388
sg223
(lp50389
sg149
(lp50390
sg429
(lp50391
sg104
(lp50392
sg108
(lp50393
sg110
(lp50394
sg52
(lp50395
sg114
(lp50396
sg440
(lp50397
sg121
(lp50398
sg22
(lp50399
sg8
(lp50400
sg36
(lp50401
sg281
(lp50402
sg44
(lp50403
sg14
(lp50404
sg16
(lp50405
sg135
(lp50406
sg50
(lp50407
sg140
(lp50408
ssS'enough'
p50409
(dp50410
g163
(lp50411
sg287
(lp50412
sg74
(lp50413
sg176
(lp50414
sg76
(lp50415
sg183
(lp50416
sg85
(lp50417
sg303
(lp50418
sg42
(lp50419
I2699
asg87
(lp50420
sg89
(lp50421
sg91
(lp50422
sg18
(lp50423
sg223
(lp50424
sg102
(lp50425
sg104
(lp50426
sg116
(lp50427
sg174
(lp50428
sg48
(lp50429
sg178
(lp50430
sg235
(lp50431
sg34
(lp50432
sg72
(lp50433
sg128
(lp50434
sg354
(lp50435
ssS'mflop'
p50436
(dp50437
g10
(lp50438
I2325
assS'volatil'
p50439
(dp50440
g14
(lp50441
I2673
assS'across'
p50442
(dp50443
g283
(lp50444
sg70
(lp50445
sg30
(lp50446
sg74
(lp50447
sg176
(lp50448
sg256
(lp50449
sg76
(lp50450
sg80
(lp50451
sg303
(lp50452
sg48
(lp50453
sg18
(lp50454
sg223
(lp50455
sg118
(lp50456
sg104
(lp50457
sg116
(lp50458
sg174
(lp50459
sg332
(lp50460
sg6
(lp50461
sg181
(lp50462
sg68
(lp50463
sg10
(lp50464
sg128
(lp50465
sg14
(lp50466
I3713
assS'jx'
p50467
(dp50468
g329
(lp50469
I685
asg72
(lp50470
ssS'jy'
p50471
(dp50472
g72
(lp50473
sg130
(lp50474
I2365
assS'parent'
p50475
(dp50476
g18
(lp50477
I1858
asg145
(lp50478
ssS'jt'
p50479
(dp50480
g318
(lp50481
I810
asg38
(lp50482
sg114
(lp50483
sg124
(lp50484
ssS'ju'
p50485
(dp50486
g174
(lp50487
sg354
(lp50488
I1882
assS'jv'
p50489
(dp50490
g130
(lp50491
I1366
assS'jp'
p50492
(dp50493
g329
(lp50494
sg384
(lp50495
sg18
(lp50496
I33
asg176
(lp50497
ssS'jq'
p50498
(dp50499
g102
(lp50500
I2195
assS'jr'
p50501
(dp50502
g34
(lp50503
sg329
(lp50504
sg350
(lp50505
sg80
(lp50506
sg354
(lp50507
I1762
assS'js'
p50508
(dp50509
g281
(lp50510
I327
assS'jl'
p50511
(dp50512
g438
(lp50513
I311
asg68
(lp50514
sg178
(lp50515
sg235
(lp50516
sg262
(lp50517
sg484
(lp50518
sg130
(lp50519
sg14
(lp50520
sg106
(lp50521
sg20
(lp50522
ssS'jm'
p50523
(dp50524
g99
(lp50525
I3325
assS'pruiett'
p50526
(dp50527
g4
(lp50528
I566
assS'jo'
p50529
(dp50530
g14
(lp50531
sg16
(lp50532
I2439
asg178
(lp50533
sg384
(lp50534
sg36
(lp50535
ssS'tul'
p50536
(dp50537
g350
(lp50538
I1769
assS'tacit'
p50539
(dp50540
g20
(lp50541
I381
assS'jj'
p50542
(dp50543
g174
(lp50544
sg74
(lp50545
sg262
(lp50546
sg429
(lp50547
sg130
(lp50548
sg12
(lp50549
sg106
(lp50550
I971
asg221
(lp50551
sg535
(lp50552
sg149
(lp50553
ssS'tuo'
p50554
(dp50555
g354
(lp50556
I2264
assS'jd'
p50557
(dp50558
g85
(lp50559
sg38
(lp50560
sg341
(lp50561
I2288
asg76
(lp50562
sg63
(lp50563
ssS'ime'
p50564
(dp50565
g59
(lp50566
sg87
(lp50567
I1395
assS'jf'
p50568
(dp50569
g38
(lp50570
I820
assS'madarasz'
p50571
(dp50572
g42
(lp50573
I3482
assS'europhi'
p50574
(dp50575
g384
(lp50576
I2343
assS'ja'
p50577
(dp50578
g102
(lp50579
sg99
(lp50580
I3313
asg6
(lp50581
ssS'jb'
p50582
(dp50583
g130
(lp50584
I26
assS'jc'
p50585
(dp50586
g102
(lp50587
I1143
asg70
(lp50588
ssS'collingridg'
p50589
(dp50590
g106
(lp50591
I1221
assS'tuj'
p50592
(dp50593
g104
(lp50594
I1765
assS'improv'
p50595
(dp50596
g70
(lp50597
sg78
(lp50598
sg277
(lp50599
sg163
(lp50600
sg26
(lp50601
sg30
(lp50602
sg74
(lp50603
sg145
(lp50604
sg76
(lp50605
sg295
(lp50606
sg183
(lp50607
sg59
(lp50608
sg484
(lp50609
sg83
(lp50610
sg303
(lp50611
sg42
(lp50612
I1255
asg306
(lp50613
sg87
(lp50614
sg460
(lp50615
sg245
(lp50616
sg94
(lp50617
sg96
(lp50618
sg221
(lp50619
sg313
(lp50620
sg223
(lp50621
sg32
(lp50622
sg429
(lp50623
sg110
(lp50624
sg63
(lp50625
sg52
(lp50626
sg114
(lp50627
sg230
(lp50628
sg329
(lp50629
sg440
(lp50630
sg332
(lp50631
sg121
(lp50632
sg22
(lp50633
sg181
(lp50634
sg235
(lp50635
sg34
(lp50636
sg99
(lp50637
sg384
(lp50638
sg126
(lp50639
sg281
(lp50640
sg10
(lp50641
sg44
(lp50642
sg128
(lp50643
sg36
(lp50644
sg14
(lp50645
sg16
(lp50646
sg135
(lp50647
sg50
(lp50648
sg138
(lp50649
sg140
(lp50650
sg354
(lp50651
ssS'dkl'
p50652
(dp50653
g130
(lp50654
I238
assS'ji'
p50655
(dp50656
g38
(lp50657
sg178
(lp50658
I1189
asg63
(lp50659
sg80
(lp50660
ssS'among'
p50661
(dp50662
g287
(lp50663
sg74
(lp50664
sg70
(lp50665
sg181
(lp50666
sg293
(lp50667
sg295
(lp50668
sg183
(lp50669
sg484
(lp50670
sg38
(lp50671
sg429
(lp50672
sg10
(lp50673
sg63
(lp50674
sg306
(lp50675
sg44
(lp50676
sg102
(lp50677
sg104
(lp50678
sg20
(lp50679
sg48
(lp50680
sg313
(lp50681
sg140
(lp50682
I72
asg114
(lp50683
ssS'acceler'
p50684
(dp50685
g34
(lp50686
sg341
(lp50687
sg10
(lp50688
sg83
(lp50689
sg42
(lp50690
I1113
asg245
(lp50691
sg94
(lp50692
sg350
(lp50693
ssS'cancer'
p50694
(dp50695
g484
(lp50696
I116
assS'jk'
p50697
(dp50698
g46
(lp50699
I2564
asg74
(lp50700
sg38
(lp50701
ssS'karpinski'
p50702
(dp50703
g287
(lp50704
I1151
asg40
(lp50705
ssS'je'
p50706
(dp50707
g34
(lp50708
I1631
assS'cancel'
p50709
(dp50710
g20
(lp50711
sg135
(lp50712
I1051
asg303
(lp50713
ssS'prasad'
p50714
(dp50715
g132
(lp50716
I3813
asg44
(lp50717
ssS'ultim'
p50718
(dp50719
g429
(lp50720
sg104
(lp50721
sg440
(lp50722
I171
asg484
(lp50723
sg22
(lp50724
ssS'marc'
p50725
(dp50726
g89
(lp50727
I2329
assS'marl'
p50728
(dp50729
g174
(lp50730
I2680
assS'mari'
p50731
(dp50732
g89
(lp50733
I2324
asg85
(lp50734
ssS'mark'
p50735
(dp50736
g174
(lp50737
sg181
(lp50738
sg329
(lp50739
sg295
(lp50740
sg183
(lp50741
sg235
(lp50742
sg341
(lp50743
sg42
(lp50744
I609
asg344
(lp50745
sg429
(lp50746
sg102
(lp50747
sg106
(lp50748
sg354
(lp50749
sg110
(lp50750
sg63
(lp50751
sg44
(lp50752
sg149
(lp50753
ssS'workshop'
p50754
(dp50755
g440
(lp50756
sg332
(lp50757
sg293
(lp50758
sg295
(lp50759
sg183
(lp50760
sg87
(lp50761
sg132
(lp50762
sg20
(lp50763
sg108
(lp50764
sg138
(lp50765
sg140
(lp50766
I247
asg114
(lp50767
ssS'red'
p50768
(dp50769
g87
(lp50770
I2017
asg256
(lp50771
ssS'eeckman'
p50772
(dp50773
g48
(lp50774
I2473
assS'arti'
p50775
(dp50776
g74
(lp50777
I3180
assS'barri'
p50778
(dp50779
g135
(lp50780
I14
assS'curvatur'
p50781
(dp50782
g30
(lp50783
sg295
(lp50784
sg183
(lp50785
sg8
(lp50786
I404
assS'manipulandum'
p50787
(dp50788
g99
(lp50789
I516
assS'imw'
p50790
(dp50791
g306
(lp50792
I2326
assS'bme'
p50793
(dp50794
g99
(lp50795
I33
assS'bmj'
p50796
(dp50797
g104
(lp50798
I1233
assS'yixt'
p50799
(dp50800
g354
(lp50801
I1554
assS'suscept'
p50802
(dp50803
g329
(lp50804
I79
asg124
(lp50805
ssS'squash'
p50806
(dp50807
g36
(lp50808
I2236
assS'unperturb'
p50809
(dp50810
g176
(lp50811
I1727
assS'wake'
p50812
(dp50813
g74
(lp50814
I3145
asg70
(lp50815
ssS'bayesian'
p50816
(dp50817
g30
(lp50818
sg26
(lp50819
sg295
(lp50820
sg183
(lp50821
sg124
(lp50822
sg126
(lp50823
sg138
(lp50824
sg72
(lp50825
sg63
(lp50826
sg91
(lp50827
sg221
(lp50828
sg313
(lp50829
sg354
(lp50830
I1
assS'lectur'
p50831
(dp50832
g59
(lp50833
I3170
assS'odyssey'
p50834
(dp50835
g42
(lp50836
I611
assS'those'
p50837
(dp50838
g70
(lp50839
sg277
(lp50840
sg181
(lp50841
sg40
(lp50842
sg287
(lp50843
sg176
(lp50844
sg76
(lp50845
sg293
(lp50846
sg344
(lp50847
sg183
(lp50848
sg80
(lp50849
sg38
(lp50850
sg85
(lp50851
sg42
(lp50852
I1132
asg306
(lp50853
sg48
(lp50854
sg245
(lp50855
sg94
(lp50856
sg18
(lp50857
sg99
(lp50858
sg313
(lp50859
sg223
(lp50860
sg149
(lp50861
sg230
(lp50862
sg329
(lp50863
sg12
(lp50864
sg429
(lp50865
sg104
(lp50866
sg106
(lp50867
sg110
(lp50868
sg63
(lp50869
sg216
(lp50870
sg438
(lp50871
sg32
(lp50872
sg332
(lp50873
sg178
(lp50874
sg4
(lp50875
sg6
(lp50876
sg8
(lp50877
sg34
(lp50878
sg460
(lp50879
sg68
(lp50880
sg72
(lp50881
sg535
(lp50882
sg128
(lp50883
sg78
(lp50884
sg14
(lp50885
sg16
(lp50886
sg50
(lp50887
sg140
(lp50888
ssS'sound'
p50889
(dp50890
g116
(lp50891
sg174
(lp50892
sg332
(lp50893
sg78
(lp50894
sg87
(lp50895
sg96
(lp50896
I1251
assS'klaus'
p50897
(dp50898
g36
(lp50899
I254
assS'kanerva'
p50900
(dp50901
g108
(lp50902
I2532
asg181
(lp50903
ssS'onr'
p50904
(dp50905
g344
(lp50906
sg118
(lp50907
sg429
(lp50908
sg68
(lp50909
I3271
asg10
(lp50910
ssS'nonstationari'
p50911
(dp50912
g18
(lp50913
I149
asg83
(lp50914
ssS'posjtiv'
p50915
(dp50916
g106
(lp50917
I955
assS'invok'
p50918
(dp50919
g344
(lp50920
sg126
(lp50921
I1553
asg10
(lp50922
ssS'outcom'
p50923
(dp50924
g118
(lp50925
sg145
(lp50926
sg277
(lp50927
sg80
(lp50928
sg384
(lp50929
sg68
(lp50930
sg91
(lp50931
sg132
(lp50932
I44
assS'invoc'
p50933
(dp50934
g145
(lp50935
I1107
assS'margin'
p50936
(dp50937
g74
(lp50938
sg484
(lp50939
sg124
(lp50940
sg126
(lp50941
sg91
(lp50942
sg135
(lp50943
I1844
asg63
(lp50944
ssS'paraltelism'
p50945
(dp50946
g287
(lp50947
I14
assS'iolog'
p50948
(dp50949
g174
(lp50950
I2433
assS'theo'
p50951
(dp50952
g283
(lp50953
I21
assS'planar'
p50954
(dp50955
g460
(lp50956
sg99
(lp50957
I292
asg313
(lp50958
ssS'advantag'
p50959
(dp50960
g283
(lp50961
sg70
(lp50962
sg277
(lp50963
sg145
(lp50964
sg293
(lp50965
sg295
(lp50966
sg183
(lp50967
sg484
(lp50968
sg83
(lp50969
sg89
(lp50970
sg94
(lp50971
sg48
(lp50972
sg221
(lp50973
sg313
(lp50974
sg44
(lp50975
sg329
(lp50976
sg429
(lp50977
sg318
(lp50978
sg178
(lp50979
sg63
(lp50980
sg438
(lp50981
I1966
asg32
(lp50982
sg18
(lp50983
sg121
(lp50984
sg22
(lp50985
sg181
(lp50986
sg235
(lp50987
sg36
(lp50988
sg384
(lp50989
sg124
(lp50990
sg126
(lp50991
sg10
(lp50992
sg344
(lp50993
sg128
(lp50994
sg132
(lp50995
sg14
(lp50996
sg16
(lp50997
sg135
(lp50998
sg50
(lp50999
sg140
(lp51000
ssS'oftim'
p51001
(dp51002
g83
(lp51003
I1145
assS'marcel'
p51004
(dp51005
g295
(lp51006
sg183
(lp51007
sg96
(lp51008
I2841
asg91
(lp51009
ssS'destin'
p51010
(dp51011
g63
(lp51012
I2057
asg83
(lp51013
ssS'cluster'
p51014
(dp51015
g30
(lp51016
sg174
(lp51017
sg74
(lp51018
sg318
(lp51019
sg283
(lp51020
sg181
(lp51021
sg221
(lp51022
sg72
(lp51023
sg52
(lp51024
sg223
(lp51025
sg110
(lp51026
sg91
(lp51027
sg130
(lp51028
sg87
(lp51029
sg50
(lp51030
I705
asg63
(lp51031
sg44
(lp51032
sg149
(lp51033
ssS'sudden'
p51034
(dp51035
g174
(lp51036
I1530
asg306
(lp51037
sg18
(lp51038
sg350
(lp51039
ssS'ixdxa'
p51040
(dp51041
g145
(lp51042
I1408
assS'protein'
p51043
(dp51044
g344
(lp51045
sg26
(lp51046
sg130
(lp51047
I728
assS'ybeb'
p51048
(dp51049
g318
(lp51050
I1140
assS'everyday'
p51051
(dp51052
g174
(lp51053
I1007
asg118
(lp51054
sg114
(lp51055
ssS'verag'
p51056
(dp51057
g221
(lp51058
I2188
assS'defibrillalor'
p51059
(dp51060
g135
(lp51061
I1215
assS'pas'
p51062
(dp51063
g89
(lp51064
I1192
assS'pat'
p51065
(dp51066
g48
(lp51067
I747
assS'samm'
p51068
(dp51069
g78
(lp51070
I610
assS'doctor'
p51071
(dp51072
g183
(lp51073
sg332
(lp51074
I2730
asg277
(lp51075
ssS'hyperparametersj'
p51076
(dp51077
g124
(lp51078
I525
assS'pay'
p51079
(dp51080
g132
(lp51081
I1983
asg8
(lp51082
ssS'lave'
p51083
(dp51084
g277
(lp51085
I3147
assS'stepper'
p51086
(dp51087
g104
(lp51088
I367
assS'hecht'
p51089
(dp51090
g36
(lp51091
I215
assS'same'
p51092
(dp51093
g80
(lp51094
sg293
(lp51095
sg344
(lp51096
sg78
(lp51097
sg59
(lp51098
sg484
(lp51099
sg38
(lp51100
sg85
(lp51101
sg303
(lp51102
sg438
(lp51103
I387
asg116
(lp51104
sg118
(lp51105
sg34
(lp51106
sg36
(lp51107
sg460
(lp51108
sg68
(lp51109
sg72
(lp51110
sg10
(lp51111
sg40
(lp51112
sg283
(lp51113
sg70
(lp51114
sg26
(lp51115
sg277
(lp51116
sg163
(lp51117
sg89
(lp51118
sg91
(lp51119
sg12
(lp51120
sg94
(lp51121
sg96
(lp51122
sg48
(lp51123
sg99
(lp51124
sg313
(lp51125
sg44
(lp51126
sg149
(lp51127
sg429
(lp51128
sg102
(lp51129
sg104
(lp51130
sg106
(lp51131
sg108
(lp51132
sg110
(lp51133
sg63
(lp51134
sg52
(lp51135
sg114
(lp51136
sg128
(lp51137
sg130
(lp51138
sg132
(lp51139
sg14
(lp51140
sg16
(lp51141
sg135
(lp51142
sg50
(lp51143
sg138
(lp51144
sg140
(lp51145
sg354
(lp51146
sg87
(lp51147
sg46
(lp51148
sg20
(lp51149
sg18
(lp51150
sg535
(lp51151
sg223
(lp51152
sg350
(lp51153
sg216
(lp51154
sg174
(lp51155
sg440
(lp51156
sg332
(lp51157
sg121
(lp51158
sg6
(lp51159
sg8
(lp51160
sg126
(lp51161
sg341
(lp51162
sg30
(lp51163
sg287
(lp51164
sg176
(lp51165
sg145
(lp51166
sg256
(lp51167
sg76
(lp51168
sg262
(lp51169
sg295
(lp51170
sg183
(lp51171
sg230
(lp51172
sg329
(lp51173
sg32
(lp51174
sg318
(lp51175
sg178
(lp51176
sg22
(lp51177
sg235
(lp51178
sg124
(lp51179
ssS'pac'
p51180
(dp51181
g344
(lp51182
sg287
(lp51183
I194
asg183
(lp51184
ssS'speech'
p51185
(dp51186
g30
(lp51187
sg174
(lp51188
sg440
(lp51189
sg332
(lp51190
sg121
(lp51191
sg22
(lp51192
sg76
(lp51193
sg235
(lp51194
sg10
(lp51195
sg40
(lp51196
sg52
(lp51197
sg87
(lp51198
sg223
(lp51199
sg128
(lp51200
sg94
(lp51201
sg96
(lp51202
sg108
(lp51203
sg221
(lp51204
sg178
(lp51205
sg140
(lp51206
I3127
assS'differenc'
p51207
(dp51208
g132
(lp51209
I65
assS'pal'
p51210
(dp51211
g30
(lp51212
I1237
assS'exhaust'
p51213
(dp51214
g344
(lp51215
sg36
(lp51216
I2468
asg484
(lp51217
sg83
(lp51218
ssS'jeffrey'
p51219
(dp51220
g344
(lp51221
sg145
(lp51222
I3027
assS'rimey'
p51223
(dp51224
g178
(lp51225
I2107
assS'assist'
p51226
(dp51227
g70
(lp51228
sg26
(lp51229
sg76
(lp51230
sg8
(lp51231
sg83
(lp51232
sg44
(lp51233
I2552
asg63
(lp51234
sg223
(lp51235
ssS'companion'
p51236
(dp51237
g126
(lp51238
I2005
assS'capabl'
p51239
(dp51240
g283
(lp51241
sg163
(lp51242
sg59
(lp51243
sg89
(lp51244
sg245
(lp51245
sg46
(lp51246
sg20
(lp51247
sg221
(lp51248
sg223
(lp51249
sg429
(lp51250
sg104
(lp51251
sg106
(lp51252
I275
asg108
(lp51253
sg110
(lp51254
sg440
(lp51255
sg332
(lp51256
sg68
(lp51257
sg40
(lp51258
sg44
(lp51259
sg128
(lp51260
sg14
(lp51261
sg16
(lp51262
sg50
(lp51263
ssS'kubi'
p51264
(dp51265
g80
(lp51266
I2541
assS'solvabl'
p51267
(dp51268
g384
(lp51269
sg235
(lp51270
I2832
assS'appropri'
p51271
(dp51272
g277
(lp51273
sg163
(lp51274
sg30
(lp51275
sg287
(lp51276
sg74
(lp51277
sg293
(lp51278
sg295
(lp51279
sg183
(lp51280
sg85
(lp51281
sg303
(lp51282
sg42
(lp51283
I2508
asg306
(lp51284
sg91
(lp51285
sg46
(lp51286
sg96
(lp51287
sg48
(lp51288
sg221
(lp51289
sg223
(lp51290
sg350
(lp51291
sg460
(lp51292
sg429
(lp51293
sg94
(lp51294
sg110
(lp51295
sg230
(lp51296
sg174
(lp51297
sg332
(lp51298
sg178
(lp51299
sg4
(lp51300
sg235
(lp51301
sg384
(lp51302
sg126
(lp51303
sg344
(lp51304
sg128
(lp51305
sg130
(lp51306
sg132
(lp51307
sg14
(lp51308
sg16
(lp51309
sg50
(lp51310
sg138
(lp51311
sg354
(lp51312
ssS'macro'
p51313
(dp51314
g132
(lp51315
I3821
assS'sirnul'
p51316
(dp51317
g350
(lp51318
I777
assS'cvpr'
p51319
(dp51320
g178
(lp51321
I2627
assS'greuel'
p51322
(dp51323
g106
(lp51324
I2409
assS'outdoor'
p51325
(dp51326
g277
(lp51327
sg181
(lp51328
I524
assS'execut'
p51329
(dp51330
g4
(lp51331
sg293
(lp51332
sg68
(lp51333
sg83
(lp51334
sg10
(lp51335
sg89
(lp51336
I1812
asg104
(lp51337
ssS'niebur'
p51338
(dp51339
g48
(lp51340
I261
assS'iiiiiii'
p51341
(dp51342
g6
(lp51343
I1728
assS'mter'
p51344
(dp51345
g104
(lp51346
I1764
assS'multiplex'
p51347
(dp51348
g14
(lp51349
sg16
(lp51350
I2135
asg22
(lp51351
sg78
(lp51352
ssS'aspect'
p51353
(dp51354
g116
(lp51355
sg287
(lp51356
sg332
(lp51357
sg181
(lp51358
sg8
(lp51359
sg183
(lp51360
sg460
(lp51361
sg85
(lp51362
sg40
(lp51363
sg42
(lp51364
I63
asg102
(lp51365
sg63
(lp51366
sg91
(lp51367
sg12
(lp51368
sg303
(lp51369
sg535
(lp51370
sg223
(lp51371
sg114
(lp51372
ssS'flavor'
p51373
(dp51374
g68
(lp51375
I2163
assS'lluctuat'
p51376
(dp51377
g42
(lp51378
I2075
assS'offcent'
p51379
(dp51380
g350
(lp51381
I2690
assS'gauwenbergh'
p51382
(dp51383
g20
(lp51384
I2593
assS'param'
p51385
(dp51386
g178
(lp51387
I1991
assS'wherepn'
p51388
(dp51389
g295
(lp51390
I1681
asg183
(lp51391
ssS'pill'
p51392
(dp51393
g130
(lp51394
I2713
assS'grip'
p51395
(dp51396
g99
(lp51397
I308
assS'ramach'
p51398
(dp51399
g10
(lp51400
I1648
assS'mos'
p51401
(dp51402
g245
(lp51403
sg20
(lp51404
I2
assS'kolodni'
p51405
(dp51406
g48
(lp51407
I2563
assS'ricerca'
p51408
(dp51409
g96
(lp51410
I14
assS'temperatur'
p51411
(dp51412
g329
(lp51413
sg74
(lp51414
sg26
(lp51415
sg8
(lp51416
sg235
(lp51417
sg83
(lp51418
sg130
(lp51419
sg14
(lp51420
sg16
(lp51421
sg354
(lp51422
I683
assS'grid'
p51423
(dp51424
g70
(lp51425
sg59
(lp51426
sg484
(lp51427
sg126
(lp51428
sg306
(lp51429
sg89
(lp51430
sg52
(lp51431
sg18
(lp51432
sg44
(lp51433
sg354
(lp51434
I2614
assS'mon'
p51435
(dp51436
g440
(lp51437
I230
assS'mol'
p51438
(dp51439
g26
(lp51440
I3275
assS'mod'
p51441
(dp51442
g20
(lp51443
I1243
assS'englewood'
p51444
(dp51445
g102
(lp51446
I3608
asg46
(lp51447
ssS'zll'
p51448
(dp51449
g46
(lp51450
I2975
asg329
(lp51451
ssS'juxtaposit'
p51452
(dp51453
g118
(lp51454
I1224
assS'zli'
p51455
(dp51456
g138
(lp51457
I3011
assS'pauem'
p51458
(dp51459
g183
(lp51460
I5167
assS'tubingen'
p51461
(dp51462
g216
(lp51463
I21
assS'server'
p51464
(dp51465
g132
(lp51466
I2151
asg10
(lp51467
ssS'chamber'
p51468
(dp51469
g14
(lp51470
sg106
(lp51471
I1036
asg283
(lp51472
sg16
(lp51473
ssS'pauer'
p51474
(dp51475
g341
(lp51476
I22
assS'either'
p51477
(dp51478
g70
(lp51479
sg26
(lp51480
sg281
(lp51481
sg85
(lp51482
sg74
(lp51483
sg145
(lp51484
sg256
(lp51485
sg80
(lp51486
sg118
(lp51487
sg295
(lp51488
sg183
(lp51489
sg83
(lp51490
sg114
(lp51491
sg303
(lp51492
sg42
(lp51493
I1343
asg306
(lp51494
sg89
(lp51495
sg94
(lp51496
sg48
(lp51497
sg221
(lp51498
sg44
(lp51499
sg350
(lp51500
sg230
(lp51501
sg174
(lp51502
sg293
(lp51503
sg429
(lp51504
sg46
(lp51505
sg104
(lp51506
sg106
(lp51507
sg63
(lp51508
sg52
(lp51509
sg22
(lp51510
sg216
(lp51511
sg438
(lp51512
sg32
(lp51513
sg318
(lp51514
sg4
(lp51515
sg8
(lp51516
sg34
(lp51517
sg384
(lp51518
sg124
(lp51519
sg126
(lp51520
sg341
(lp51521
sg10
(lp51522
sg128
(lp51523
sg130
(lp51524
sg14
(lp51525
sg50
(lp51526
sg460
(lp51527
ssS'unresampl'
p51528
(dp51529
g221
(lp51530
I2383
assS'rcml'
p51531
(dp51532
g245
(lp51533
I1805
assS'fulfil'
p51534
(dp51535
g295
(lp51536
sg183
(lp51537
sg32
(lp51538
sg48
(lp51539
sg130
(lp51540
I1247
assS'thermodynam'
p51541
(dp51542
g235
(lp51543
I1234
assS'ascend'
p51544
(dp51545
g116
(lp51546
I2536
assS'xjxf'
p51547
(dp51548
g130
(lp51549
I545
assS'adequ'
p51550
(dp51551
g230
(lp51552
sg4
(lp51553
sg8
(lp51554
sg36
(lp51555
sg384
(lp51556
sg126
(lp51557
sg91
(lp51558
sg128
(lp51559
sg354
(lp51560
I1188
assS'ascent'
p51561
(dp51562
g102
(lp51563
sg178
(lp51564
sg460
(lp51565
sg50
(lp51566
I625
assS'spasm'
p51567
(dp51568
g135
(lp51569
I2647
assS'iiitili'
p51570
(dp51571
g6
(lp51572
I1727
assS'gross'
p51573
(dp51574
g313
(lp51575
I2068
assS'confirm'
p51576
(dp51577
g230
(lp51578
sg4
(lp51579
sg235
(lp51580
sg36
(lp51581
sg484
(lp51582
sg85
(lp51583
sg12
(lp51584
sg106
(lp51585
I2285
asg114
(lp51586
ssS'pioneer'
p51587
(dp51588
g126
(lp51589
I169
asg26
(lp51590
ssS'inject'
p51591
(dp51592
g106
(lp51593
I1945
asg6
(lp51594
sg149
(lp51595
ssS'donder'
p51596
(dp51597
g32
(lp51598
I95
assS'minimumcontain'
p51599
(dp51600
g341
(lp51601
I2238
assS'tomorrow'
p51602
(dp51603
g70
(lp51604
I567
assS'shashua'
p51605
(dp51606
g138
(lp51607
I3283
assS'kyoto'
p51608
(dp51609
g295
(lp51610
sg183
(lp51611
sg18
(lp51612
I43
assS'broken'
p51613
(dp51614
g116
(lp51615
sg174
(lp51616
sg6
(lp51617
I491
asg181
(lp51618
sg78
(lp51619
sg72
(lp51620
sg63
(lp51621
sg114
(lp51622
ssS'symp'
p51623
(dp51624
g42
(lp51625
I3458
asg287
(lp51626
sg96
(lp51627
ssS'delbriick'
p51628
(dp51629
g256
(lp51630
I632
assS'island'
p51631
(dp51632
g438
(lp51633
I32
assS'insect'
p51634
(dp51635
g256
(lp51636
I269
assS'goodhil'
p51637
(dp51638
g149
(lp51639
I317
assS'roan'
p51640
(dp51641
g42
(lp51642
I2553
assS'xnl'
p51643
(dp51644
g104
(lp51645
I1674
assS'road'
p51646
(dp51647
g318
(lp51648
sg277
(lp51649
sg163
(lp51650
sg78
(lp51651
sg384
(lp51652
sg42
(lp51653
I3527
asg91
(lp51654
sg44
(lp51655
sg350
(lp51656
ssS'yyt'
p51657
(dp51658
g130
(lp51659
I1479
assS'dagger'
p51660
(dp51661
g87
(lp51662
I2195
assS'eval'
p51663
(dp51664
g87
(lp51665
I2009
assS'terminolog'
p51666
(dp51667
g344
(lp51668
sg46
(lp51669
I493
asg287
(lp51670
ssS'berthod'
p51671
(dp51672
g63
(lp51673
I3245
assS'dram'
p51674
(dp51675
g10
(lp51676
I1777
assS'einstein'
p51677
(dp51678
g106
(lp51679
I298
assS'jerusalem'
p51680
(dp51681
g183
(lp51682
sg6
(lp51683
sg130
(lp51684
I3158
assS'spline'
p51685
(dp51686
g295
(lp51687
sg183
(lp51688
sg124
(lp51689
sg126
(lp51690
sg138
(lp51691
I601
assS'lando'
p51692
(dp51693
g223
(lp51694
I3207
assS'ymax'
p51695
(dp51696
g121
(lp51697
I1529
assS'maass'
p51698
(dp51699
g287
(lp51700
I421
assS'brute'
p51701
(dp51702
g287
(lp51703
sg145
(lp51704
sg223
(lp51705
I3288
assS'compliant'
p51706
(dp51707
g460
(lp51708
I50
assS'berthoz'
p51709
(dp51710
g32
(lp51711
I3211
assS'gors'
p51712
(dp51713
g283
(lp51714
I357
assS'kohavi'
p51715
(dp51716
g128
(lp51717
I2835
assS'watkin'
p51718
(dp51719
g89
(lp51720
I457
asg83
(lp51721
sg293
(lp51722
ssS'corrioli'
p51723
(dp51724
g99
(lp51725
I786
assS'offspr'
p51726
(dp51727
g18
(lp51728
I1849
assS'corso'
p51729
(dp51730
g44
(lp51731
I20
assS'apic'
p51732
(dp51733
g106
(lp51734
I656
asg70
(lp51735
ssS'dvi'
p51736
(dp51737
g12
(lp51738
I867
assS'shortterm'
p51739
(dp51740
g178
(lp51741
sg6
(lp51742
I517
assS'possibl'
p51743
(dp51744
g68
(lp51745
sg70
(lp51746
sg78
(lp51747
sg277
(lp51748
sg163
(lp51749
sg72
(lp51750
sg303
(lp51751
sg80
(lp51752
sg283
(lp51753
sg85
(lp51754
sg460
(lp51755
sg40
(lp51756
sg26
(lp51757
sg30
(lp51758
sg287
(lp51759
sg74
(lp51760
sg176
(lp51761
sg256
(lp51762
sg76
(lp51763
sg293
(lp51764
sg295
(lp51765
sg183
(lp51766
sg59
(lp51767
sg484
(lp51768
sg38
(lp51769
sg83
(lp51770
sg114
(lp51771
sg63
(lp51772
sg42
(lp51773
I378
asg306
(lp51774
sg89
(lp51775
sg91
(lp51776
sg12
(lp51777
sg94
(lp51778
sg96
(lp51779
sg48
(lp51780
sg99
(lp51781
sg313
(lp51782
sg44
(lp51783
sg350
(lp51784
sg230
(lp51785
sg174
(lp51786
sg18
(lp51787
sg32
(lp51788
sg178
(lp51789
sg245
(lp51790
sg429
(lp51791
sg318
(lp51792
sg46
(lp51793
sg104
(lp51794
sg106
(lp51795
sg108
(lp51796
sg110
(lp51797
sg20
(lp51798
sg52
(lp51799
sg22
(lp51800
sg216
(lp51801
sg438
(lp51802
sg440
(lp51803
sg332
(lp51804
sg121
(lp51805
sg4
(lp51806
sg235
(lp51807
sg34
(lp51808
sg221
(lp51809
sg384
(lp51810
sg124
(lp51811
sg126
(lp51812
sg281
(lp51813
sg10
(lp51814
sg535
(lp51815
sg344
(lp51816
sg128
(lp51817
sg130
(lp51818
sg132
(lp51819
sg14
(lp51820
sg16
(lp51821
sg135
(lp51822
sg50
(lp51823
sg138
(lp51824
sg140
(lp51825
sg354
(lp51826
ssS'crawford'
p51827
(dp51828
g350
(lp51829
I1006
assS'schrauoolph'
p51830
(dp51831
g318
(lp51832
I2215
assS'unusu'
p51833
(dp51834
g99
(lp51835
sg50
(lp51836
I1521
asg22
(lp51837
sg277
(lp51838
ssS'exemplar'
p51839
(dp51840
g87
(lp51841
I1562
asg283
(lp51842
sg181
(lp51843
sg293
(lp51844
ssS'artifki'
p51845
(dp51846
g108
(lp51847
I2369
assS'embed'
p51848
(dp51849
g32
(lp51850
sg130
(lp51851
sg87
(lp51852
sg104
(lp51853
sg14
(lp51854
I4452
asg223
(lp51855
ssS'filt'
p51856
(dp51857
g68
(lp51858
I2801
assS'conundrum'
p51859
(dp51860
g181
(lp51861
I1916
assS'departm'
p51862
(dp51863
g110
(lp51864
I16
assS'deep'
p51865
(dp51866
g72
(lp51867
sg145
(lp51868
I1685
asg262
(lp51869
ssS'general'
p51870
(dp51871
g329
(lp51872
sg70
(lp51873
sg26
(lp51874
sg277
(lp51875
sg163
(lp51876
sg72
(lp51877
sg68
(lp51878
sg281
(lp51879
sg460
(lp51880
sg40
(lp51881
sg30
(lp51882
sg287
(lp51883
sg74
(lp51884
sg176
(lp51885
sg76
(lp51886
sg262
(lp51887
sg295
(lp51888
sg183
(lp51889
sg59
(lp51890
sg484
(lp51891
sg38
(lp51892
sg83
(lp51893
sg85
(lp51894
sg124
(lp51895
sg306
(lp51896
sg89
(lp51897
sg91
(lp51898
sg12
(lp51899
sg94
(lp51900
sg96
(lp51901
sg18
(lp51902
sg221
(lp51903
sg313
(lp51904
sg44
(lp51905
sg149
(lp51906
sg118
(lp51907
sg174
(lp51908
sg293
(lp51909
sg32
(lp51910
sg429
(lp51911
sg318
(lp51912
sg46
(lp51913
sg102
(lp51914
sg178
(lp51915
sg108
(lp51916
sg110
(lp51917
sg20
(lp51918
sg52
(lp51919
sg114
(lp51920
sg230
(lp51921
sg438
(lp51922
I242
asg440
(lp51923
sg332
(lp51924
sg121
(lp51925
sg4
(lp51926
sg181
(lp51927
sg8
(lp51928
sg34
(lp51929
sg36
(lp51930
sg384
(lp51931
sg235
(lp51932
sg126
(lp51933
sg341
(lp51934
sg10
(lp51935
sg535
(lp51936
sg344
(lp51937
sg63
(lp51938
sg223
(lp51939
sg128
(lp51940
sg130
(lp51941
sg132
(lp51942
sg14
(lp51943
sg16
(lp51944
sg350
(lp51945
sg50
(lp51946
sg138
(lp51947
sg140
(lp51948
sg354
(lp51949
ssS'file'
p51950
(dp51951
g174
(lp51952
sg440
(lp51953
sg10
(lp51954
sg8
(lp51955
I2636
assS'generat'
p51956
(dp51957
g68
(lp51958
sg70
(lp51959
sg78
(lp51960
sg163
(lp51961
sg72
(lp51962
sg281
(lp51963
sg283
(lp51964
sg85
(lp51965
sg26
(lp51966
sg30
(lp51967
sg74
(lp51968
sg145
(lp51969
sg76
(lp51970
sg262
(lp51971
sg295
(lp51972
sg183
(lp51973
sg59
(lp51974
sg484
(lp51975
sg83
(lp51976
sg114
(lp51977
sg303
(lp51978
sg42
(lp51979
I3388
asg306
(lp51980
sg87
(lp51981
sg89
(lp51982
sg91
(lp51983
sg12
(lp51984
sg94
(lp51985
sg96
(lp51986
sg48
(lp51987
sg221
(lp51988
sg350
(lp51989
sg116
(lp51990
sg293
(lp51991
sg32
(lp51992
sg46
(lp51993
sg102
(lp51994
sg110
(lp51995
sg22
(lp51996
sg216
(lp51997
sg329
(lp51998
sg440
(lp51999
sg18
(lp52000
sg178
(lp52001
sg4
(lp52002
sg235
(lp52003
sg34
(lp52004
sg36
(lp52005
sg384
(lp52006
sg124
(lp52007
sg126
(lp52008
sg341
(lp52009
sg10
(lp52010
sg40
(lp52011
sg344
(lp52012
sg128
(lp52013
sg130
(lp52014
sg132
(lp52015
sg14
(lp52016
sg16
(lp52017
sg135
(lp52018
sg138
(lp52019
sg140
(lp52020
sg354
(lp52021
ssS'proport'
p52022
(dp52023
g283
(lp52024
sg287
(lp52025
sg74
(lp52026
sg256
(lp52027
sg295
(lp52028
sg183
(lp52029
sg484
(lp52030
sg38
(lp52031
sg303
(lp52032
sg42
(lp52033
I1357
asg245
(lp52034
sg99
(lp52035
sg350
(lp52036
sg110
(lp52037
sg52
(lp52038
sg438
(lp52039
sg318
(lp52040
sg181
(lp52041
sg10
(lp52042
sg14
(lp52043
sg16
(lp52044
sg50
(lp52045
sg138
(lp52046
ssS'film'
p52047
(dp52048
g14
(lp52049
I4486
assS'fill'
p52050
(dp52051
g116
(lp52052
sg118
(lp52053
sg8
(lp52054
sg40
(lp52055
sg91
(lp52056
sg102
(lp52057
sg106
(lp52058
I1045
asg108
(lp52059
ssS'tedious'
p52060
(dp52061
g126
(lp52062
I128
assS'again'
p52063
(dp52064
g68
(lp52065
sg277
(lp52066
sg283
(lp52067
sg145
(lp52068
sg76
(lp52069
sg118
(lp52070
sg78
(lp52071
sg59
(lp52072
sg38
(lp52073
sg83
(lp52074
sg85
(lp52075
sg303
(lp52076
sg89
(lp52077
sg245
(lp52078
sg46
(lp52079
sg18
(lp52080
sg221
(lp52081
sg313
(lp52082
sg44
(lp52083
sg329
(lp52084
sg12
(lp52085
sg94
(lp52086
sg63
(lp52087
sg114
(lp52088
sg230
(lp52089
sg174
(lp52090
sg32
(lp52091
sg22
(lp52092
sg8
(lp52093
sg99
(lp52094
sg235
(lp52095
sg126
(lp52096
sg128
(lp52097
I1718
assS'formant'
p52098
(dp52099
g30
(lp52100
I140
assS'arrowhead'
p52101
(dp52102
g80
(lp52103
I2947
assS'lanier'
p52104
(dp52105
g59
(lp52106
I3438
assS'istanbul'
p52107
(dp52108
g178
(lp52109
I19
assS'binocular'
p52110
(dp52111
g438
(lp52112
I988
asg318
(lp52113
sg118
(lp52114
sg12
(lp52115
sg106
(lp52116
sg63
(lp52117
sg149
(lp52118
ssS'hybrid'
p52119
(dp52120
g440
(lp52121
sg76
(lp52122
sg59
(lp52123
sg124
(lp52124
sg126
(lp52125
sg87
(lp52126
sg14
(lp52127
sg16
(lp52128
I205
asg221
(lp52129
sg44
(lp52130
ssS'workday'
p52131
(dp52132
g94
(lp52133
I3401
assS'field'
p52134
(dp52135
g70
(lp52136
sg78
(lp52137
sg116
(lp52138
sg26
(lp52139
sg74
(lp52140
sg176
(lp52141
sg256
(lp52142
sg76
(lp52143
sg293
(lp52144
sg295
(lp52145
sg183
(lp52146
sg59
(lp52147
sg303
(lp52148
sg306
(lp52149
sg91
(lp52150
sg245
(lp52151
sg46
(lp52152
sg18
(lp52153
sg99
(lp52154
sg535
(lp52155
sg223
(lp52156
sg149
(lp52157
sg230
(lp52158
sg118
(lp52159
sg12
(lp52160
sg429
(lp52161
sg318
(lp52162
sg102
(lp52163
sg104
(lp52164
sg106
(lp52165
sg63
(lp52166
sg52
(lp52167
sg216
(lp52168
sg438
(lp52169
I3
asg32
(lp52170
sg48
(lp52171
sg80
(lp52172
sg181
(lp52173
sg8
(lp52174
sg36
(lp52175
sg124
(lp52176
sg281
(lp52177
sg40
(lp52178
sg344
(lp52179
sg130
(lp52180
sg14
(lp52181
sg16
(lp52182
sg138
(lp52183
ssS'binomi'
p52184
(dp52185
g91
(lp52186
I1020
assS'formanc'
p52187
(dp52188
g295
(lp52189
I3501
asg183
(lp52190
ssS'gelman'
p52191
(dp52192
g116
(lp52193
I2501
assS'midlin'
p52194
(dp52195
g303
(lp52196
I2864
assS'effid'
p52197
(dp52198
g106
(lp52199
I55
assS'architectur'
p52200
(dp52201
g68
(lp52202
sg70
(lp52203
sg283
(lp52204
sg303
(lp52205
sg350
(lp52206
sg76
(lp52207
sg78
(lp52208
sg59
(lp52209
sg484
(lp52210
sg38
(lp52211
sg83
(lp52212
sg85
(lp52213
sg63
(lp52214
sg42
(lp52215
I916
asg306
(lp52216
sg87
(lp52217
sg12
(lp52218
sg94
(lp52219
sg96
(lp52220
sg18
(lp52221
sg221
(lp52222
sg313
(lp52223
sg44
(lp52224
sg149
(lp52225
sg329
(lp52226
sg178
(lp52227
sg245
(lp52228
sg104
(lp52229
sg110
(lp52230
sg20
(lp52231
sg22
(lp52232
sg216
(lp52233
sg438
(lp52234
sg440
(lp52235
sg121
(lp52236
sg4
(lp52237
sg181
(lp52238
sg460
(lp52239
sg124
(lp52240
sg126
(lp52241
sg10
(lp52242
sg287
(lp52243
sg128
(lp52244
sg132
(lp52245
sg14
(lp52246
sg16
(lp52247
sg135
(lp52248
sg50
(lp52249
sg138
(lp52250
sg354
(lp52251
ssS'bulthoff'
p52252
(dp52253
g181
(lp52254
I201
assS'sequenc'
p52255
(dp52256
g70
(lp52257
sg26
(lp52258
sg281
(lp52259
sg104
(lp52260
sg287
(lp52261
sg76
(lp52262
sg293
(lp52263
sg344
(lp52264
sg80
(lp52265
sg38
(lp52266
sg85
(lp52267
sg42
(lp52268
I1162
asg306
(lp52269
sg87
(lp52270
sg91
(lp52271
sg245
(lp52272
sg46
(lp52273
sg96
(lp52274
sg99
(lp52275
sg178
(lp52276
sg108
(lp52277
sg110
(lp52278
sg116
(lp52279
sg440
(lp52280
sg332
(lp52281
sg121
(lp52282
sg22
(lp52283
sg181
(lp52284
sg34
(lp52285
sg36
(lp52286
sg460
(lp52287
sg68
(lp52288
sg341
(lp52289
sg10
(lp52290
sg128
(lp52291
sg130
(lp52292
sg132
(lp52293
ssS'kirchhoff'
p52294
(dp52295
g20
(lp52296
I555
assS'bowurj'
p52297
(dp52298
g283
(lp52299
I591
assS'ansi'
p52300
(dp52301
g10
(lp52302
I1534
assS'chris'
p52303
(dp52304
g30
(lp52305
sg295
(lp52306
sg183
(lp52307
sg124
(lp52308
sg14
(lp52309
sg16
(lp52310
I11
assS'sequent'
p52311
(dp52312
g72
(lp52313
I1281
assS'utoronto'
p52314
(dp52315
g124
(lp52316
I2975
assS'descript'
p52317
(dp52318
g283
(lp52319
sg74
(lp52320
sg76
(lp52321
sg293
(lp52322
sg344
(lp52323
sg78
(lp52324
sg59
(lp52325
sg38
(lp52326
sg85
(lp52327
sg87
(lp52328
sg91
(lp52329
sg94
(lp52330
sg48
(lp52331
sg223
(lp52332
sg429
(lp52333
sg102
(lp52334
sg104
(lp52335
sg110
(lp52336
sg318
(lp52337
sg80
(lp52338
sg36
(lp52339
sg384
(lp52340
sg126
(lp52341
sg281
(lp52342
sg40
(lp52343
sg132
(lp52344
I1910
assS'handcheck'
p52345
(dp52346
g114
(lp52347
I228
assS'boe'
p52348
(dp52349
g46
(lp52350
I3566
assS'u'
p52351
(dp52352
g68
(lp52353
sg277
(lp52354
sg293
(lp52355
sg283
(lp52356
sg30
(lp52357
sg287
(lp52358
sg74
(lp52359
sg176
(lp52360
sg256
(lp52361
sg80
(lp52362
sg262
(lp52363
sg460
(lp52364
sg183
(lp52365
sg38
(lp52366
sg85
(lp52367
sg42
(lp52368
I1249
asg306
(lp52369
sg87
(lp52370
sg91
(lp52371
sg12
(lp52372
sg94
(lp52373
sg96
(lp52374
sg48
(lp52375
sg221
(lp52376
sg313
(lp52377
sg44
(lp52378
sg149
(lp52379
sg116
(lp52380
sg329
(lp52381
sg18
(lp52382
sg32
(lp52383
sg245
(lp52384
sg46
(lp52385
sg104
(lp52386
sg106
(lp52387
sg108
(lp52388
sg63
(lp52389
sg114
(lp52390
sg230
(lp52391
sg174
(lp52392
sg440
(lp52393
sg332
(lp52394
sg178
(lp52395
sg4
(lp52396
sg6
(lp52397
sg8
(lp52398
sg34
(lp52399
sg384
(lp52400
sg235
(lp52401
sg126
(lp52402
sg341
(lp52403
sg10
(lp52404
sg535
(lp52405
sg128
(lp52406
sg130
(lp52407
sg14
(lp52408
sg16
(lp52409
sg350
(lp52410
sg138
(lp52411
sg354
(lp52412
ssS'escap'
p52413
(dp52414
g132
(lp52415
I2664
asg245
(lp52416
sg72
(lp52417
sg34
(lp52418
sg38
(lp52419
ssS'ajr'
p52420
(dp52421
g87
(lp52422
I33
assS'represent'
p52423
(dp52424
g26
(lp52425
sg277
(lp52426
sg163
(lp52427
sg85
(lp52428
sg181
(lp52429
sg303
(lp52430
sg30
(lp52431
sg287
(lp52432
sg74
(lp52433
sg176
(lp52434
sg145
(lp52435
sg76
(lp52436
sg293
(lp52437
sg344
(lp52438
sg78
(lp52439
sg59
(lp52440
sg80
(lp52441
sg83
(lp52442
sg114
(lp52443
sg63
(lp52444
sg42
(lp52445
I3377
asg306
(lp52446
sg87
(lp52447
sg89
(lp52448
sg12
(lp52449
sg94
(lp52450
sg96
(lp52451
sg48
(lp52452
sg99
(lp52453
sg313
(lp52454
sg223
(lp52455
sg350
(lp52456
sg245
(lp52457
sg429
(lp52458
sg104
(lp52459
sg108
(lp52460
sg110
(lp52461
sg178
(lp52462
sg52
(lp52463
sg22
(lp52464
sg116
(lp52465
sg174
(lp52466
sg32
(lp52467
sg121
(lp52468
sg4
(lp52469
sg6
(lp52470
sg8
(lp52471
sg36
(lp52472
sg68
(lp52473
sg72
(lp52474
sg281
(lp52475
sg10
(lp52476
sg130
(lp52477
sg132
(lp52478
sg14
(lp52479
sg16
(lp52480
sg50
(lp52481
ssS'forget'
p52482
(dp52483
g295
(lp52484
sg183
(lp52485
sg99
(lp52486
I172
asg262
(lp52487
ssS'phonolog'
p52488
(dp52489
g440
(lp52490
I561
asg74
(lp52491
ssS'meddi'
p52492
(dp52493
g174
(lp52494
I502
asg332
(lp52495
ssS'dollar'
p52496
(dp52497
g114
(lp52498
I210
assS'atlant'
p52499
(dp52500
g295
(lp52501
I45
asg183
(lp52502
ssS'suni'
p52503
(dp52504
g245
(lp52505
I2806
asg183
(lp52506
ssS'rmbi'
p52507
(dp52508
g42
(lp52509
I2566
assS'wohlgemuth'
p52510
(dp52511
g216
(lp52512
I465
assS'children'
p52513
(dp52514
g145
(lp52515
I2210
assS'edg'
p52516
(dp52517
g174
(lp52518
I1057
asg178
(lp52519
sg256
(lp52520
sg80
(lp52521
sg163
(lp52522
sg245
(lp52523
sg70
(lp52524
sg110
(lp52525
sg63
(lp52526
sg52
(lp52527
sg26
(lp52528
ssS'vestibuloocular'
p52529
(dp52530
g350
(lp52531
I3031
assS'choquet'
p52532
(dp52533
g32
(lp52534
I1265
assS'albrecht'
p52535
(dp52536
g44
(lp52537
I2559
assS'gp'
p52538
(dp52539
g138
(lp52540
I529
asg130
(lp52541
ssS'clulm'
p52542
(dp52543
g110
(lp52544
I1364
assS'alberto'
p52545
(dp52546
g341
(lp52547
I2873
assS'zsef'
p52548
(dp52549
g181
(lp52550
I2311
assS'rsjk'
p52551
(dp52552
g46
(lp52553
I2574
assS'ium'
p52554
(dp52555
g318
(lp52556
I3039
asg145
(lp52557
ssS'straightforward'
p52558
(dp52559
g174
(lp52560
sg22
(lp52561
sg277
(lp52562
sg118
(lp52563
sg34
(lp52564
sg384
(lp52565
sg85
(lp52566
sg344
(lp52567
sg104
(lp52568
sg89
(lp52569
sg130
(lp52570
sg102
(lp52571
sg14
(lp52572
sg16
(lp52573
I345
asg221
(lp52574
sg313
(lp52575
sg46
(lp52576
sg26
(lp52577
ssS'entrop'
p52578
(dp52579
g341
(lp52580
sg8
(lp52581
I522
assS'offlin'
p52582
(dp52583
g83
(lp52584
I253
asg277
(lp52585
ssS'util'
p52586
(dp52587
g74
(lp52588
sg4
(lp52589
sg293
(lp52590
sg34
(lp52591
sg83
(lp52592
sg10
(lp52593
sg344
(lp52594
sg91
(lp52595
sg132
(lp52596
I3064
asg94
(lp52597
sg20
(lp52598
sg63
(lp52599
ssS'icann'
p52600
(dp52601
g59
(lp52602
sg121
(lp52603
I2673
assS'cursiv'
p52604
(dp52605
g76
(lp52606
I3312
assS'hewlett'
p52607
(dp52608
g10
(lp52609
I913
assS'fall'
p52610
(dp52611
g216
(lp52612
sg174
(lp52613
sg18
(lp52614
sg76
(lp52615
sg181
(lp52616
sg329
(lp52617
sg163
(lp52618
sg341
(lp52619
sg42
(lp52620
I3399
asg245
(lp52621
sg48
(lp52622
sg50
(lp52623
sg277
(lp52624
sg114
(lp52625
ssS'tsignific'
p52626
(dp52627
g76
(lp52628
I821
assS'bottleneck'
p52629
(dp52630
g74
(lp52631
I1658
assS'iut'
p52632
(dp52633
g87
(lp52634
I1023
assS'utic'
p52635
(dp52636
g87
(lp52637
I957
assS'kroodsma'
p52638
(dp52639
g116
(lp52640
I186
assS'dampen'
p52641
(dp52642
g102
(lp52643
I2918
assS'nonrigid'
p52644
(dp52645
g138
(lp52646
I43
asg181
(lp52647
ssS'gb'
p52648
(dp52649
g10
(lp52650
I993
assS'neighborhood'
p52651
(dp52652
g230
(lp52653
sg8
(lp52654
sg36
(lp52655
sg68
(lp52656
sg46
(lp52657
sg48
(lp52658
sg110
(lp52659
sg535
(lp52660
sg140
(lp52661
I2564
asg149
(lp52662
ssS'trainabl'
p52663
(dp52664
g30
(lp52665
sg14
(lp52666
sg135
(lp52667
I2580
asg104
(lp52668
ssS'ickl'
p52669
(dp52670
g102
(lp52671
I1109
assS'ritter'
p52672
(dp52673
g59
(lp52674
sg48
(lp52675
I2531
asg176
(lp52676
ssS'zero'
p52677
(dp52678
g124
(lp52679
sg26
(lp52680
sg163
(lp52681
sg283
(lp52682
sg181
(lp52683
sg287
(lp52684
sg74
(lp52685
sg145
(lp52686
sg76
(lp52687
sg118
(lp52688
sg295
(lp52689
sg183
(lp52690
sg80
(lp52691
sg38
(lp52692
sg303
(lp52693
sg306
(lp52694
sg91
(lp52695
sg245
(lp52696
sg94
(lp52697
sg20
(lp52698
sg221
(lp52699
sg535
(lp52700
sg223
(lp52701
sg149
(lp52702
sg230
(lp52703
sg329
(lp52704
sg293
(lp52705
sg32
(lp52706
sg350
(lp52707
sg429
(lp52708
sg68
(lp52709
sg46
(lp52710
sg102
(lp52711
sg178
(lp52712
sg106
(lp52713
sg63
(lp52714
sg22
(lp52715
sg216
(lp52716
sg438
(lp52717
I888
asg440
(lp52718
sg121
(lp52719
sg4
(lp52720
sg6
(lp52721
sg8
(lp52722
sg36
(lp52723
sg235
(lp52724
sg126
(lp52725
sg341
(lp52726
sg40
(lp52727
sg344
(lp52728
sg128
(lp52729
sg130
(lp52730
sg14
(lp52731
sg135
(lp52732
sg50
(lp52733
sg138
(lp52734
sg140
(lp52735
sg354
(lp52736
ssS'autoencod'
p52737
(dp52738
g72
(lp52739
I350
assS'wyeth'
p52740
(dp52741
g6
(lp52742
I8
assS'further'
p52743
(dp52744
g329
(lp52745
sg78
(lp52746
sg277
(lp52747
sg163
(lp52748
sg72
(lp52749
sg303
(lp52750
sg256
(lp52751
sg76
(lp52752
sg118
(lp52753
sg295
(lp52754
sg183
(lp52755
sg59
(lp52756
sg38
(lp52757
sg83
(lp52758
sg85
(lp52759
sg124
(lp52760
sg42
(lp52761
I814
asg306
(lp52762
sg87
(lp52763
sg91
(lp52764
sg245
(lp52765
sg94
(lp52766
sg18
(lp52767
sg223
(lp52768
sg230
(lp52769
sg174
(lp52770
sg293
(lp52771
sg12
(lp52772
sg318
(lp52773
sg102
(lp52774
sg104
(lp52775
sg110
(lp52776
sg63
(lp52777
sg52
(lp52778
sg114
(lp52779
sg216
(lp52780
sg438
(lp52781
sg440
(lp52782
sg332
(lp52783
sg178
(lp52784
sg4
(lp52785
sg181
(lp52786
sg8
(lp52787
sg384
(lp52788
sg235
(lp52789
sg126
(lp52790
sg281
(lp52791
sg10
(lp52792
sg40
(lp52793
sg130
(lp52794
sg135
(lp52795
sg138
(lp52796
sg140
(lp52797
ssS'excit'
p52798
(dp52799
g216
(lp52800
sg174
(lp52801
sg118
(lp52802
sg176
(lp52803
sg70
(lp52804
sg256
(lp52805
sg6
(lp52806
sg262
(lp52807
sg535
(lp52808
sg46
(lp52809
sg14
(lp52810
sg106
(lp52811
I2400
asg50
(lp52812
sg20
(lp52813
sg149
(lp52814
ssS'abe'
p52815
(dp52816
g20
(lp52817
I2634
assS'portland'
p52818
(dp52819
g12
(lp52820
I2863
asg72
(lp52821
ssS'jhli'
p52822
(dp52823
g52
(lp52824
I720
assS'abh'
p52825
(dp52826
g114
(lp52827
I2131
assS'cognit'
p52828
(dp52829
g283
(lp52830
sg26
(lp52831
sg74
(lp52832
sg176
(lp52833
sg76
(lp52834
sg344
(lp52835
sg80
(lp52836
sg18
(lp52837
sg99
(lp52838
sg313
(lp52839
sg116
(lp52840
sg329
(lp52841
sg106
(lp52842
I2482
asg108
(lp52843
sg110
(lp52844
sg230
(lp52845
sg174
(lp52846
sg332
(lp52847
sg4
(lp52848
sg460
(lp52849
sg40
(lp52850
sg128
(lp52851
sg50
(lp52852
ssS'stool'
p52853
(dp52854
g42
(lp52855
I2297
assS'moriet'
p52856
(dp52857
g22
(lp52858
I2436
assS'abt'
p52859
(dp52860
g59
(lp52861
I18
assS'abu'
p52862
(dp52863
g110
(lp52864
sg277
(lp52865
sg223
(lp52866
I3079
assS'munder'
p52867
(dp52868
g72
(lp52869
I2047
assS'fffs'
p52870
(dp52871
g78
(lp52872
I1366
assS'public'
p52873
(dp52874
g440
(lp52875
sg76
(lp52876
sg235
(lp52877
sg124
(lp52878
sg132
(lp52879
sg135
(lp52880
I2616
asg149
(lp52881
ssS'unlabel'
p52882
(dp52883
g30
(lp52884
sg140
(lp52885
I65
asg235
(lp52886
ssS'movement'
p52887
(dp52888
g216
(lp52889
sg174
(lp52890
sg32
(lp52891
sg178
(lp52892
sg118
(lp52893
sg295
(lp52894
sg183
(lp52895
sg460
(lp52896
sg303
(lp52897
sg42
(lp52898
I2136
asg245
(lp52899
sg99
(lp52900
sg350
(lp52901
ssS'compact'
p52902
(dp52903
g230
(lp52904
sg22
(lp52905
sg76
(lp52906
sg42
(lp52907
I376
asg306
(lp52908
sg87
(lp52909
sg130
(lp52910
sg14
(lp52911
sg63
(lp52912
sg256
(lp52913
ssS'variat'
p52914
(dp52915
g163
(lp52916
sg287
(lp52917
sg74
(lp52918
sg256
(lp52919
sg76
(lp52920
sg293
(lp52921
sg245
(lp52922
sg94
(lp52923
sg96
(lp52924
sg535
(lp52925
sg44
(lp52926
sg102
(lp52927
sg104
(lp52928
sg63
(lp52929
sg230
(lp52930
sg118
(lp52931
sg318
(lp52932
sg80
(lp52933
sg6
(lp52934
sg68
(lp52935
sg128
(lp52936
sg130
(lp52937
sg14
(lp52938
sg135
(lp52939
sg138
(lp52940
sg140
(lp52941
sg354
(lp52942
I2377
assS'sophist'
p52943
(dp52944
g74
(lp52945
sg126
(lp52946
sg83
(lp52947
sg85
(lp52948
sg306
(lp52949
sg94
(lp52950
I2800
asg114
(lp52951
ssS'karlsruh'
p52952
(dp52953
g132
(lp52954
I3716
asg36
(lp52955
ssS'ggg'
p52956
(dp52957
g130
(lp52958
I1558
assS'capacitor'
p52959
(dp52960
g22
(lp52961
sg78
(lp52962
sg14
(lp52963
sg20
(lp52964
sg135
(lp52965
I1254
asg256
(lp52966
ssS'ggi'
p52967
(dp52968
g130
(lp52969
I1537
assS'hdm'
p52970
(dp52971
g85
(lp52972
I835
assS'valu'
p52973
(dp52974
g80
(lp52975
sg293
(lp52976
sg344
(lp52977
sg78
(lp52978
sg59
(lp52979
sg484
(lp52980
sg38
(lp52981
sg83
(lp52982
sg85
(lp52983
sg303
(lp52984
sg438
(lp52985
sg116
(lp52986
sg118
(lp52987
sg34
(lp52988
sg460
(lp52989
sg281
(lp52990
sg10
(lp52991
sg40
(lp52992
sg283
(lp52993
sg70
(lp52994
sg26
(lp52995
sg277
(lp52996
sg163
(lp52997
sg89
(lp52998
sg91
(lp52999
sg12
(lp53000
sg94
(lp53001
sg96
(lp53002
sg48
(lp53003
sg99
(lp53004
sg313
(lp53005
sg44
(lp53006
sg149
(lp53007
sg429
(lp53008
sg102
(lp53009
sg104
(lp53010
sg108
(lp53011
sg110
(lp53012
sg114
(lp53013
sg128
(lp53014
sg130
(lp53015
sg132
(lp53016
sg14
(lp53017
sg16
(lp53018
sg135
(lp53019
sg50
(lp53020
sg138
(lp53021
sg140
(lp53022
sg354
(lp53023
sg306
(lp53024
sg245
(lp53025
sg46
(lp53026
sg18
(lp53027
sg221
(lp53028
sg535
(lp53029
sg223
(lp53030
sg350
(lp53031
sg174
(lp53032
sg440
(lp53033
sg332
(lp53034
sg121
(lp53035
sg4
(lp53036
sg6
(lp53037
sg8
(lp53038
sg126
(lp53039
sg341
(lp53040
sg30
(lp53041
sg287
(lp53042
sg74
(lp53043
sg176
(lp53044
sg145
(lp53045
sg256
(lp53046
sg76
(lp53047
sg262
(lp53048
sg295
(lp53049
sg183
(lp53050
sg42
(lp53051
I298
asg230
(lp53052
sg329
(lp53053
sg318
(lp53054
sg178
(lp53055
sg22
(lp53056
sg181
(lp53057
sg235
(lp53058
sg384
(lp53059
sg124
(lp53060
ssS'search'
p53061
(dp53062
g283
(lp53063
sg277
(lp53064
sg72
(lp53065
sg30
(lp53066
sg287
(lp53067
sg145
(lp53068
sg76
(lp53069
sg344
(lp53070
sg59
(lp53071
sg83
(lp53072
sg42
(lp53073
I1633
asg87
(lp53074
sg91
(lp53075
sg20
(lp53076
sg223
(lp53077
sg106
(lp53078
sg63
(lp53079
sg318
(lp53080
sg8
(lp53081
sg34
(lp53082
sg124
(lp53083
sg126
(lp53084
sg281
(lp53085
sg40
(lp53086
sg44
(lp53087
sg130
(lp53088
sg132
(lp53089
sg50
(lp53090
sg138
(lp53091
sg354
(lp53092
ssS'fwo'
p53093
(dp53094
g313
(lp53095
I619
assS'fwf'
p53096
(dp53097
g341
(lp53098
I2744
assS'collater'
p53099
(dp53100
g106
(lp53101
I575
assS'frey'
p53102
(dp53103
g74
(lp53104
I3138
asg70
(lp53105
ssS'hastad'
p53106
(dp53107
g344
(lp53108
sg40
(lp53109
I728
assS'narrow'
p53110
(dp53111
g176
(lp53112
sg22
(lp53113
sg76
(lp53114
sg293
(lp53115
sg295
(lp53116
sg183
(lp53117
sg14
(lp53118
I3212
asg149
(lp53119
ssS'quotient'
p53120
(dp53121
g32
(lp53122
I106
assS'declin'
p53123
(dp53124
g332
(lp53125
I1916
assS'lawrenc'
p53126
(dp53127
g116
(lp53128
sg121
(lp53129
sg80
(lp53130
sg68
(lp53131
sg12
(lp53132
I17
asg114
(lp53133
ssS'transit'
p53134
(dp53135
g440
(lp53136
sg176
(lp53137
sg76
(lp53138
sg293
(lp53139
sg384
(lp53140
sg74
(lp53141
sg306
(lp53142
sg89
(lp53143
sg460
(lp53144
sg104
(lp53145
sg96
(lp53146
I1934
asg18
(lp53147
sg20
(lp53148
ssS'readili'
p53149
(dp53150
g8
(lp53151
sg344
(lp53152
sg68
(lp53153
sg10
(lp53154
sg48
(lp53155
sg138
(lp53156
I1620
asg52
(lp53157
ssS'inappropri'
p53158
(dp53159
g50
(lp53160
I985
asg4
(lp53161
ssS'themin'
p53162
(dp53163
g85
(lp53164
I2535
assS'establish'
p53165
(dp53166
g230
(lp53167
sg287
(lp53168
sg145
(lp53169
sg80
(lp53170
sg6
(lp53171
sg8
(lp53172
sg484
(lp53173
sg42
(lp53174
I561
asg306
(lp53175
sg87
(lp53176
sg89
(lp53177
sg130
(lp53178
sg132
(lp53179
sg14
(lp53180
sg50
(lp53181
sg535
(lp53182
sg223
(lp53183
ssS'memor'
p53184
(dp53185
g42
(lp53186
I1449
asg104
(lp53187
sg116
(lp53188
sg223
(lp53189
sg287
(lp53190
ssS'tesauro'
p53191
(dp53192
g30
(lp53193
sg116
(lp53194
sg440
(lp53195
sg318
(lp53196
sg235
(lp53197
sg295
(lp53198
sg183
(lp53199
sg460
(lp53200
sg59
(lp53201
sg74
(lp53202
sg303
(lp53203
sg306
(lp53204
sg89
(lp53205
sg132
(lp53206
sg106
(lp53207
I68
asg83
(lp53208
sg44
(lp53209
ssS'eye'
p53210
(dp53211
g438
(lp53212
I980
asg32
(lp53213
sg178
(lp53214
sg256
(lp53215
sg118
(lp53216
sg114
(lp53217
sg303
(lp53218
sg350
(lp53219
sg245
(lp53220
sg48
(lp53221
sg99
(lp53222
sg223
(lp53223
sg149
(lp53224
ssS'loxi'
p53225
(dp53226
g178
(lp53227
I327
assS'vingron'
p53228
(dp53229
g130
(lp53230
I3059
assS'regist'
p53231
(dp53232
g216
(lp53233
I1519
asg318
(lp53234
sg83
(lp53235
sg10
(lp53236
ssS'two'
p53237
(dp53238
g80
(lp53239
sg293
(lp53240
sg344
(lp53241
sg78
(lp53242
sg59
(lp53243
sg484
(lp53244
sg38
(lp53245
sg83
(lp53246
sg85
(lp53247
sg303
(lp53248
sg438
(lp53249
sg116
(lp53250
sg118
(lp53251
sg34
(lp53252
sg36
(lp53253
sg460
(lp53254
sg68
(lp53255
sg72
(lp53256
sg281
(lp53257
sg10
(lp53258
sg40
(lp53259
sg283
(lp53260
sg70
(lp53261
sg26
(lp53262
sg277
(lp53263
sg163
(lp53264
sg89
(lp53265
sg91
(lp53266
sg12
(lp53267
sg94
(lp53268
sg96
(lp53269
sg48
(lp53270
sg99
(lp53271
sg313
(lp53272
sg44
(lp53273
sg149
(lp53274
sg429
(lp53275
sg102
(lp53276
sg104
(lp53277
sg106
(lp53278
sg108
(lp53279
sg110
(lp53280
sg63
(lp53281
sg52
(lp53282
sg114
(lp53283
sg128
(lp53284
sg130
(lp53285
sg132
(lp53286
sg14
(lp53287
sg16
(lp53288
sg135
(lp53289
sg50
(lp53290
sg138
(lp53291
sg140
(lp53292
sg354
(lp53293
sg87
(lp53294
sg245
(lp53295
sg46
(lp53296
sg20
(lp53297
sg18
(lp53298
sg221
(lp53299
sg535
(lp53300
sg223
(lp53301
sg350
(lp53302
sg216
(lp53303
sg174
(lp53304
sg440
(lp53305
sg332
(lp53306
sg121
(lp53307
sg4
(lp53308
sg6
(lp53309
sg8
(lp53310
sg126
(lp53311
sg341
(lp53312
sg30
(lp53313
sg287
(lp53314
sg74
(lp53315
sg176
(lp53316
sg145
(lp53317
sg256
(lp53318
sg76
(lp53319
sg262
(lp53320
sg295
(lp53321
sg183
(lp53322
sg42
(lp53323
I1781
asg230
(lp53324
sg329
(lp53325
sg32
(lp53326
sg318
(lp53327
sg178
(lp53328
sg22
(lp53329
sg181
(lp53330
sg384
(lp53331
sg124
(lp53332
ssS'muller'
p53333
(dp53334
g183
(lp53335
I6588
asg80
(lp53336
sg36
(lp53337
ssS'codifi'
p53338
(dp53339
g8
(lp53340
I194
assS'nonvolatil'
p53341
(dp53342
g135
(lp53343
I2467
assS'komp'
p53344
(dp53345
g440
(lp53346
I2546
assS'desir'
p53347
(dp53348
g68
(lp53349
sg72
(lp53350
sg281
(lp53351
sg287
(lp53352
sg74
(lp53353
sg176
(lp53354
sg145
(lp53355
sg76
(lp53356
sg295
(lp53357
sg183
(lp53358
sg59
(lp53359
sg38
(lp53360
sg83
(lp53361
sg42
(lp53362
I563
asg306
(lp53363
sg89
(lp53364
sg91
(lp53365
sg46
(lp53366
sg96
(lp53367
sg99
(lp53368
sg535
(lp53369
sg223
(lp53370
sg429
(lp53371
sg108
(lp53372
sg110
(lp53373
sg114
(lp53374
sg230
(lp53375
sg329
(lp53376
sg440
(lp53377
sg22
(lp53378
sg124
(lp53379
sg126
(lp53380
sg341
(lp53381
sg40
(lp53382
sg130
(lp53383
sg14
(lp53384
sg16
(lp53385
sg140
(lp53386
sg354
(lp53387
ssS'brisk'
p53388
(dp53389
g6
(lp53390
I1894
assS'ital'
p53391
(dp53392
g438
(lp53393
I1290
assS'leonard'
p53394
(dp53395
g80
(lp53396
I2517
assS'expedit'
p53397
(dp53398
g94
(lp53399
I2986
assS'particular'
p53400
(dp53401
g68
(lp53402
sg70
(lp53403
sg78
(lp53404
sg283
(lp53405
sg85
(lp53406
sg30
(lp53407
sg287
(lp53408
sg74
(lp53409
sg176
(lp53410
sg80
(lp53411
sg262
(lp53412
sg344
(lp53413
sg183
(lp53414
sg83
(lp53415
sg114
(lp53416
sg303
(lp53417
sg306
(lp53418
sg87
(lp53419
sg94
(lp53420
sg96
(lp53421
sg48
(lp53422
sg221
(lp53423
sg535
(lp53424
sg223
(lp53425
sg149
(lp53426
sg230
(lp53427
sg329
(lp53428
sg293
(lp53429
sg32
(lp53430
sg429
(lp53431
sg102
(lp53432
sg104
(lp53433
sg106
(lp53434
sg110
(lp53435
sg63
(lp53436
sg52
(lp53437
sg22
(lp53438
sg216
(lp53439
sg438
(lp53440
I1659
asg440
(lp53441
sg332
(lp53442
sg4
(lp53443
sg181
(lp53444
sg235
(lp53445
sg34
(lp53446
sg460
(lp53447
sg124
(lp53448
sg72
(lp53449
sg281
(lp53450
sg10
(lp53451
sg40
(lp53452
sg128
(lp53453
sg130
(lp53454
sg132
(lp53455
sg14
(lp53456
sg16
(lp53457
sg50
(lp53458
sg138
(lp53459
sg140
(lp53460
sg354
(lp53461
ssS'neurosci'
p53462
(dp53463
g216
(lp53464
sg438
(lp53465
I2394
asg70
(lp53466
sg332
(lp53467
sg178
(lp53468
sg4
(lp53469
sg6
(lp53470
sg8
(lp53471
sg50
(lp53472
sg262
(lp53473
sg116
(lp53474
sg350
(lp53475
sg176
(lp53476
sg80
(lp53477
sg174
(lp53478
sg12
(lp53479
sg18
(lp53480
sg106
(lp53481
sg48
(lp53482
sg99
(lp53483
sg149
(lp53484
ssS'yjxj'
p53485
(dp53486
g72
(lp53487
I2294
assS'taub'
p53488
(dp53489
g80
(lp53490
I159
assS'dey'
p53491
(dp53492
g354
(lp53493
I2051
assS'none'
p53494
(dp53495
g74
(lp53496
sg4
(lp53497
sg76
(lp53498
sg83
(lp53499
sg40
(lp53500
sg42
(lp53501
I667
asg89
(lp53502
sg91
(lp53503
sg108
(lp53504
sg221
(lp53505
sg140
(lp53506
sg26
(lp53507
ssS'jdi'
p53508
(dp53509
g384
(lp53510
I1348
assS'jdh'
p53511
(dp53512
g384
(lp53513
I2031
assS'hour'
p53514
(dp53515
g295
(lp53516
sg183
(lp53517
sg124
(lp53518
sg126
(lp53519
sg83
(lp53520
sg87
(lp53521
sg94
(lp53522
sg99
(lp53523
I98
assS'hous'
p53524
(dp53525
g126
(lp53526
sg108
(lp53527
I2545
asg99
(lp53528
ssS'dep'
p53529
(dp53530
g106
(lp53531
I290
asg354
(lp53532
ssS'der'
p53533
(dp53534
g245
(lp53535
sg34
(lp53536
sg429
(lp53537
sg176
(lp53538
sg149
(lp53539
I323
assS'det'
p53540
(dp53541
g102
(lp53542
I644
asg36
(lp53543
ssS'dev'
p53544
(dp53545
g87
(lp53546
sg126
(lp53547
I577
assS'remain'
p53548
(dp53549
g277
(lp53550
sg281
(lp53551
sg287
(lp53552
sg74
(lp53553
sg76
(lp53554
sg262
(lp53555
sg295
(lp53556
sg183
(lp53557
sg484
(lp53558
sg38
(lp53559
sg149
(lp53560
sg85
(lp53561
sg42
(lp53562
I340
asg306
(lp53563
sg91
(lp53564
sg46
(lp53565
sg96
(lp53566
sg18
(lp53567
sg99
(lp53568
sg44
(lp53569
sg350
(lp53570
sg230
(lp53571
sg118
(lp53572
sg460
(lp53573
sg429
(lp53574
sg94
(lp53575
sg110
(lp53576
sg63
(lp53577
sg52
(lp53578
sg114
(lp53579
sg216
(lp53580
sg438
(lp53581
sg440
(lp53582
sg4
(lp53583
sg181
(lp53584
sg235
(lp53585
sg384
(lp53586
sg68
(lp53587
sg72
(lp53588
sg341
(lp53589
sg10
(lp53590
sg14
(lp53591
sg16
(lp53592
sg135
(lp53593
sg50
(lp53594
sg138
(lp53595
ssS'paragraph'
p53596
(dp53597
g48
(lp53598
I1043
assS'del'
p53599
(dp53600
g96
(lp53601
I2262
asg76
(lp53602
ssS'dem'
p53603
(dp53604
g72
(lp53605
I2427
assS'urbia'
p53606
(dp53607
g48
(lp53608
I2016
assS'genicul'
p53609
(dp53610
g106
(lp53611
I2443
asg70
(lp53612
ssS'dec'
p53613
(dp53614
g72
(lp53615
sg8
(lp53616
I2707
assS'dee'
p53617
(dp53618
g96
(lp53619
I2488
assS'deg'
p53620
(dp53621
g42
(lp53622
I2022
assS'stumbl'
p53623
(dp53624
g245
(lp53625
I2619
assS'mackay'
p53626
(dp53627
g30
(lp53628
sg26
(lp53629
sg124
(lp53630
sg126
(lp53631
sg138
(lp53632
sg221
(lp53633
sg313
(lp53634
sg354
(lp53635
I354
assS'share'
p53636
(dp53637
g26
(lp53638
sg174
(lp53639
sg18
(lp53640
sg76
(lp53641
sg181
(lp53642
sg235
(lp53643
sg293
(lp53644
sg59
(lp53645
sg484
(lp53646
sg83
(lp53647
sg303
(lp53648
sg287
(lp53649
sg429
(lp53650
sg104
(lp53651
sg96
(lp53652
sg48
(lp53653
sg110
(lp53654
sg20
(lp53655
sg277
(lp53656
sg149
(lp53657
I1557
assS'sphere'
p53658
(dp53659
g36
(lp53660
sg32
(lp53661
sg283
(lp53662
sg535
(lp53663
I1127
asg181
(lp53664
ssS'minimum'
p53665
(dp53666
g68
(lp53667
sg26
(lp53668
sg163
(lp53669
sg283
(lp53670
sg295
(lp53671
sg183
(lp53672
sg484
(lp53673
sg85
(lp53674
sg89
(lp53675
sg235
(lp53676
sg245
(lp53677
sg94
(lp53678
sg20
(lp53679
sg313
(lp53680
sg44
(lp53681
sg116
(lp53682
sg329
(lp53683
sg429
(lp53684
sg110
(lp53685
sg230
(lp53686
sg174
(lp53687
sg440
(lp53688
sg318
(lp53689
sg121
(lp53690
sg8
(lp53691
sg34
(lp53692
sg36
(lp53693
sg124
(lp53694
sg72
(lp53695
sg341
(lp53696
sg40
(lp53697
sg130
(lp53698
sg138
(lp53699
sg140
(lp53700
I1804
assS'attain'
p53701
(dp53702
g262
(lp53703
sg36
(lp53704
sg42
(lp53705
I2062
asg91
(lp53706
sg14
(lp53707
sg16
(lp53708
ssS'explor'
p53709
(dp53710
g283
(lp53711
sg72
(lp53712
sg256
(lp53713
sg76
(lp53714
sg293
(lp53715
sg344
(lp53716
sg59
(lp53717
sg80
(lp53718
sg83
(lp53719
sg48
(lp53720
sg99
(lp53721
sg313
(lp53722
sg44
(lp53723
sg149
(lp53724
sg116
(lp53725
sg104
(lp53726
sg106
(lp53727
I1595
asg108
(lp53728
sg110
(lp53729
sg114
(lp53730
sg216
(lp53731
sg329
(lp53732
sg332
(lp53733
sg4
(lp53734
sg181
(lp53735
sg235
(lp53736
sg124
(lp53737
sg126
(lp53738
sg40
(lp53739
sg128
(lp53740
sg14
(lp53741
sg16
(lp53742
sg354
(lp53743
ssS'explos'
p53744
(dp53745
g176
(lp53746
sg178
(lp53747
I142
assS'sharp'
p53748
(dp53749
g216
(lp53750
sg329
(lp53751
sg80
(lp53752
sg174
(lp53753
sg295
(lp53754
sg183
(lp53755
sg14
(lp53756
sg138
(lp53757
I491
assS'robinson'
p53758
(dp53759
g245
(lp53760
sg87
(lp53761
sg76
(lp53762
sg128
(lp53763
I637
asg350
(lp53764
ssS'whichev'
p53765
(dp53766
g63
(lp53767
I697
assS'csl'
p53768
(dp53769
g440
(lp53770
I2642
assS'dydw'
p53771
(dp53772
g354
(lp53773
I1120
assS'stim'
p53774
(dp53775
g106
(lp53776
I1979
assS'extrins'
p53777
(dp53778
g99
(lp53779
I454
assS'flawless'
p53780
(dp53781
g145
(lp53782
I2365
assS'cse'
p53783
(dp53784
g8
(lp53785
I2705
assS'comfort'
p53786
(dp53787
g94
(lp53788
I287
asg59
(lp53789
sg277
(lp53790
sg10
(lp53791
ssS'dydx'
p53792
(dp53793
g329
(lp53794
sg354
(lp53795
I757
assS'mapl'
p53796
(dp53797
g181
(lp53798
I1578
assS'narrowli'
p53799
(dp53800
g216
(lp53801
I56
assS'horvitz'
p53802
(dp53803
g91
(lp53804
I2983
assS'ucla'
p53805
(dp53806
g303
(lp53807
I16
assS'dimensionaljeatur'
p53808
(dp53809
g132
(lp53810
I1410
assS'erfc'
p53811
(dp53812
g281
(lp53813
sg262
(lp53814
I1262
assS'ryu'
p53815
(dp53816
g20
(lp53817
I18
assS'regener'
p53818
(dp53819
g70
(lp53820
I2123
assS'redwood'
p53821
(dp53822
g26
(lp53823
I3386
assS'bkc'
p53824
(dp53825
g230
(lp53826
I2374
asg72
(lp53827
ssS'elsag'
p53828
(dp53829
g96
(lp53830
I2707
assS'blood'
p53831
(dp53832
g277
(lp53833
sg99
(lp53834
sg484
(lp53835
sg91
(lp53836
sg135
(lp53837
I410
asg221
(lp53838
ssS'associ'
p53839
(dp53840
g303
(lp53841
sg30
(lp53842
sg287
(lp53843
sg74
(lp53844
sg176
(lp53845
sg80
(lp53846
sg295
(lp53847
sg183
(lp53848
sg484
(lp53849
sg38
(lp53850
sg83
(lp53851
sg63
(lp53852
sg42
(lp53853
I4
asg306
(lp53854
sg91
(lp53855
sg12
(lp53856
sg94
(lp53857
sg20
(lp53858
sg99
(lp53859
sg44
(lp53860
sg116
(lp53861
sg460
(lp53862
sg46
(lp53863
sg102
(lp53864
sg104
(lp53865
sg106
(lp53866
sg108
(lp53867
sg178
(lp53868
sg52
(lp53869
sg114
(lp53870
sg216
(lp53871
sg329
(lp53872
sg440
(lp53873
sg332
(lp53874
sg121
(lp53875
sg4
(lp53876
sg8
(lp53877
sg36
(lp53878
sg384
(lp53879
sg68
(lp53880
sg126
(lp53881
sg281
(lp53882
sg40
(lp53883
sg132
(lp53884
sg14
(lp53885
sg16
(lp53886
sg138
(lp53887
ssS'waea'
p53888
(dp53889
g140
(lp53890
I765
assS'lipread'
p53891
(dp53892
g30
(lp53893
I2613
assS'metbyl'
p53894
(dp53895
g106
(lp53896
I2713
assS'lookahead'
p53897
(dp53898
g89
(lp53899
I1043
asg83
(lp53900
ssS'ulti'
p53901
(dp53902
g72
(lp53903
I2458
assS'alliin'
p53904
(dp53905
g102
(lp53906
I304
assS'helmrel'
p53907
(dp53908
g72
(lp53909
I2426
assS'mislead'
p53910
(dp53911
g223
(lp53912
I2069
assS'bradi'
p53913
(dp53914
g341
(lp53915
I2818
assS'ensembl'
p53916
(dp53917
g484
(lp53918
sg262
(lp53919
sg6
(lp53920
sg235
(lp53921
sg183
(lp53922
sg384
(lp53923
sg124
(lp53924
sg126
(lp53925
sg281
(lp53926
sg102
(lp53927
sg110
(lp53928
sg36
(lp53929
sg12
(lp53930
sg20
(lp53931
sg221
(lp53932
sg460
(lp53933
sg140
(lp53934
I3
assS'edwin'
p53935
(dp53936
g283
(lp53937
I1886
assS'bas'
p53938
(dp53939
g484
(lp53940
I1809
assS'rotat'
p53941
(dp53942
g163
(lp53943
sg30
(lp53944
sg176
(lp53945
sg256
(lp53946
sg76
(lp53947
sg295
(lp53948
sg183
(lp53949
sg303
(lp53950
sg12
(lp53951
sg48
(lp53952
sg99
(lp53953
sg44
(lp53954
sg350
(lp53955
sg429
(lp53956
sg63
(lp53957
sg52
(lp53958
sg32
(lp53959
sg80
(lp53960
sg181
(lp53961
sg460
(lp53962
sg223
(lp53963
sg78
(lp53964
sg138
(lp53965
I738
assS'rand'
p53966
(dp53967
g20
(lp53968
I1384
asg38
(lp53969
ssS'woodland'
p53970
(dp53971
g87
(lp53972
I1757
assS'unimod'
p53973
(dp53974
g216
(lp53975
I1700
assS'radar'
p53976
(dp53977
g114
(lp53978
I1290
assS'through'
p53979
(dp53980
g329
(lp53981
sg70
(lp53982
sg26
(lp53983
sg277
(lp53984
sg163
(lp53985
sg72
(lp53986
sg283
(lp53987
sg85
(lp53988
sg30
(lp53989
sg74
(lp53990
sg256
(lp53991
sg76
(lp53992
sg262
(lp53993
sg460
(lp53994
sg183
(lp53995
sg80
(lp53996
sg38
(lp53997
sg114
(lp53998
sg42
(lp53999
I2313
asg87
(lp54000
sg89
(lp54001
sg91
(lp54002
sg12
(lp54003
sg94
(lp54004
sg20
(lp54005
sg48
(lp54006
sg44
(lp54007
sg149
(lp54008
sg230
(lp54009
sg174
(lp54010
sg293
(lp54011
sg116
(lp54012
sg68
(lp54013
sg46
(lp54014
sg102
(lp54015
sg104
(lp54016
sg108
(lp54017
sg63
(lp54018
sg22
(lp54019
sg216
(lp54020
sg438
(lp54021
sg32
(lp54022
sg318
(lp54023
sg178
(lp54024
sg4
(lp54025
sg6
(lp54026
sg8
(lp54027
sg34
(lp54028
sg36
(lp54029
sg384
(lp54030
sg124
(lp54031
sg126
(lp54032
sg281
(lp54033
sg10
(lp54034
sg118
(lp54035
sg223
(lp54036
sg128
(lp54037
sg132
(lp54038
sg14
(lp54039
sg16
(lp54040
sg350
(lp54041
sg50
(lp54042
sg138
(lp54043
sg140
(lp54044
ssS'infanc'
p54045
(dp54046
g94
(lp54047
I330
assS'suffer'
p54048
(dp54049
g283
(lp54050
sg121
(lp54051
sg4
(lp54052
sg8
(lp54053
sg59
(lp54054
sg484
(lp54055
sg94
(lp54056
sg132
(lp54057
sg14
(lp54058
sg135
(lp54059
I349
assS'schoen'
p54060
(dp54061
g78
(lp54062
I854
assS'psychomotor'
p54063
(dp54064
g99
(lp54065
I3319
assS'enroth'
p54066
(dp54067
g118
(lp54068
I402
assS'late'
p54069
(dp54070
g132
(lp54071
I790
asg277
(lp54072
ssS'gustafsson'
p54073
(dp54074
g106
(lp54075
I2661
assS'pcg'
p54076
(dp54077
g8
(lp54078
I1719
assS'freund'
p54079
(dp54080
g344
(lp54081
sg183
(lp54082
sg140
(lp54083
I3199
assS'good'
p54084
(dp54085
g283
(lp54086
sg70
(lp54087
sg78
(lp54088
sg163
(lp54089
sg72
(lp54090
sg85
(lp54091
sg26
(lp54092
sg30
(lp54093
sg74
(lp54094
sg76
(lp54095
sg262
(lp54096
sg295
(lp54097
sg183
(lp54098
sg59
(lp54099
sg484
(lp54100
sg83
(lp54101
sg114
(lp54102
sg42
(lp54103
I215
asg87
(lp54104
sg89
(lp54105
sg91
(lp54106
sg12
(lp54107
sg94
(lp54108
sg96
(lp54109
sg48
(lp54110
sg221
(lp54111
sg313
(lp54112
sg223
(lp54113
sg350
(lp54114
sg460
(lp54115
sg429
(lp54116
sg318
(lp54117
sg102
(lp54118
sg110
(lp54119
sg63
(lp54120
sg52
(lp54121
sg22
(lp54122
sg329
(lp54123
sg332
(lp54124
sg178
(lp54125
sg4
(lp54126
sg6
(lp54127
sg8
(lp54128
sg34
(lp54129
sg36
(lp54130
sg384
(lp54131
sg124
(lp54132
sg126
(lp54133
sg10
(lp54134
sg344
(lp54135
sg128
(lp54136
sg130
(lp54137
sg132
(lp54138
sg14
(lp54139
sg16
(lp54140
sg138
(lp54141
sg140
(lp54142
ssS'meansquar'
p54143
(dp54144
g68
(lp54145
I2376
assS'inria'
p54146
(dp54147
g287
(lp54148
I3425
assS'nhk'
p54149
(dp54150
g114
(lp54151
I2231
assS'compound'
p54152
(dp54153
g256
(lp54154
I271
assS'polezero'
p54155
(dp54156
g121
(lp54157
I2364
assS'inputjoutputexemplar'
p54158
(dp54159
g94
(lp54160
I1638
assS'porto'
p54161
(dp54162
g18
(lp54163
I2640
assS'subunit'
p54164
(dp54165
g283
(lp54166
I397
assS'rant'
p54167
(dp54168
g59
(lp54169
I651
assS'conic'
p54170
(dp54171
g138
(lp54172
I1508
assS'token'
p54173
(dp54174
g63
(lp54175
I2950
assS'minger'
p54176
(dp54177
g183
(lp54178
I5006
assS'reminisc'
p54179
(dp54180
g440
(lp54181
sg8
(lp54182
I728
assS'clamp'
p54183
(dp54184
g26
(lp54185
sg429
(lp54186
sg341
(lp54187
sg8
(lp54188
I806
assS'asitest'
p54189
(dp54190
g14
(lp54191
I3367
assS'interleav'
p54192
(dp54193
g132
(lp54194
I1765
asg10
(lp54195
sg130
(lp54196
ssS'mental'
p54197
(dp54198
g80
(lp54199
sg181
(lp54200
I350
assS'unequ'
p54201
(dp54202
g306
(lp54203
I2771
assS'hare'
p54204
(dp54205
g126
(lp54206
I1131
assS'hard'
p54207
(dp54208
g287
(lp54209
sg440
(lp54210
sg176
(lp54211
sg277
(lp54212
sg76
(lp54213
sg262
(lp54214
sg183
(lp54215
sg460
(lp54216
sg59
(lp54217
sg341
(lp54218
sg40
(lp54219
sg42
(lp54220
I711
asg63
(lp54221
sg91
(lp54222
sg130
(lp54223
sg132
(lp54224
sg48
(lp54225
sg138
(lp54226
ssS'idea'
p54227
(dp54228
g124
(lp54229
sg26
(lp54230
sg277
(lp54231
sg287
(lp54232
sg145
(lp54233
sg118
(lp54234
sg460
(lp54235
sg183
(lp54236
sg484
(lp54237
sg85
(lp54238
sg89
(lp54239
sg91
(lp54240
sg12
(lp54241
sg20
(lp54242
sg221
(lp54243
sg149
(lp54244
sg329
(lp54245
sg429
(lp54246
sg68
(lp54247
sg63
(lp54248
sg230
(lp54249
sg438
(lp54250
I2107
asg332
(lp54251
sg178
(lp54252
sg8
(lp54253
sg384
(lp54254
sg235
(lp54255
sg126
(lp54256
sg78
(lp54257
sg138
(lp54258
sg140
(lp54259
ssS'fractur'
p54260
(dp54261
g48
(lp54262
I1218
assS'macmillan'
p54263
(dp54264
g341
(lp54265
I2899
assS'oil'
p54266
(dp54267
g78
(lp54268
I2805
assS'connect'
p54269
(dp54270
g329
(lp54271
sg70
(lp54272
sg78
(lp54273
sg283
(lp54274
sg460
(lp54275
sg36
(lp54276
sg26
(lp54277
sg287
(lp54278
sg176
(lp54279
sg145
(lp54280
sg76
(lp54281
sg118
(lp54282
sg295
(lp54283
sg183
(lp54284
sg38
(lp54285
sg303
(lp54286
sg42
(lp54287
I1599
asg68
(lp54288
sg12
(lp54289
sg94
(lp54290
sg18
(lp54291
sg99
(lp54292
sg535
(lp54293
sg44
(lp54294
sg149
(lp54295
sg174
(lp54296
sg32
(lp54297
sg350
(lp54298
sg429
(lp54299
sg318
(lp54300
sg102
(lp54301
sg178
(lp54302
sg108
(lp54303
sg110
(lp54304
sg63
(lp54305
sg114
(lp54306
sg116
(lp54307
sg438
(lp54308
sg440
(lp54309
sg332
(lp54310
sg121
(lp54311
sg4
(lp54312
sg8
(lp54313
sg221
(lp54314
sg384
(lp54315
sg124
(lp54316
sg126
(lp54317
sg341
(lp54318
sg10
(lp54319
sg128
(lp54320
sg130
(lp54321
sg14
(lp54322
sg135
(lp54323
sg50
(lp54324
sg138
(lp54325
ssS'utexa'
p54326
(dp54327
g149
(lp54328
I37
assS'energi'
p54329
(dp54330
g438
(lp54331
I1765
asg74
(lp54332
sg329
(lp54333
sg26
(lp54334
sg181
(lp54335
sg174
(lp54336
sg78
(lp54337
sg384
(lp54338
sg124
(lp54339
sg126
(lp54340
sg223
(lp54341
sg130
(lp54342
sg12
(lp54343
sg14
(lp54344
sg16
(lp54345
sg135
(lp54346
sg138
(lp54347
sg96
(lp54348
ssS'hart'
p54349
(dp54350
g318
(lp54351
I842
asg281
(lp54352
sg63
(lp54353
ssS'prv'
p54354
(dp54355
g344
(lp54356
I1861
assS'orient'
p54357
(dp54358
g283
(lp54359
sg70
(lp54360
sg163
(lp54361
sg30
(lp54362
sg80
(lp54363
sg295
(lp54364
sg183
(lp54365
sg59
(lp54366
sg303
(lp54367
sg12
(lp54368
sg48
(lp54369
sg350
(lp54370
sg118
(lp54371
sg429
(lp54372
sg106
(lp54373
sg438
(lp54374
I965
asg32
(lp54375
sg318
(lp54376
sg178
(lp54377
sg181
(lp54378
sg40
(lp54379
sg149
(lp54380
sg354
(lp54381
ssS'ofdepend'
p54382
(dp54383
g85
(lp54384
I4115
assS'flower'
p54385
(dp54386
g135
(lp54387
I15
assS'admiss'
p54388
(dp54389
g460
(lp54390
I1983
asg83
(lp54391
sg277
(lp54392
ssS'epeat'
p54393
(dp54394
g72
(lp54395
I2806
assS'pearlmutt'
p54396
(dp54397
g46
(lp54398
sg4
(lp54399
I2248
asg262
(lp54400
ssS'htotal'
p54401
(dp54402
g26
(lp54403
I2519
assS'print'
p54404
(dp54405
g42
(lp54406
I2723
asg63
(lp54407
sg138
(lp54408
sg223
(lp54409
sg114
(lp54410
ssS'unaccess'
p54411
(dp54412
g36
(lp54413
I2593
assS'schillen'
p54414
(dp54415
g70
(lp54416
I2459
assS'difficulti'
p54417
(dp54418
g440
(lp54419
sg121
(lp54420
sg277
(lp54421
sg163
(lp54422
sg183
(lp54423
sg460
(lp54424
sg68
(lp54425
sg83
(lp54426
sg40
(lp54427
sg102
(lp54428
I2514
asg110
(lp54429
sg52
(lp54430
ssS'tolat'
p54431
(dp54432
g104
(lp54433
I406
asg114
(lp54434
ssS'mmp'
p54435
(dp54436
g460
(lp54437
I24
assS'mingolla'
p54438
(dp54439
g118
(lp54440
I786
assS'mme'
p54441
(dp54442
g460
(lp54443
I540
assS'workstat'
p54444
(dp54445
g70
(lp54446
sg293
(lp54447
sg124
(lp54448
sg83
(lp54449
sg10
(lp54450
sg87
(lp54451
sg132
(lp54452
sg14
(lp54453
sg16
(lp54454
sg354
(lp54455
I2590
assS'detwwher'
p54456
(dp54457
g102
(lp54458
I2123
assS'zjk'
p54459
(dp54460
g108
(lp54461
I995
assS'gluck'
p54462
(dp54463
g78
(lp54464
sg110
(lp54465
I13
assS'recast'
p54466
(dp54467
g484
(lp54468
I376
assS'zje'
p54469
(dp54470
g221
(lp54471
I39
assS'finland'
p54472
(dp54473
g121
(lp54474
I2677
assS'brainstem'
p54475
(dp54476
g174
(lp54477
I545
assS'vmos'
p54478
(dp54479
g20
(lp54480
I61
assS'lgncortic'
p54481
(dp54482
g438
(lp54483
I1190
assS'biocybernet'
p54484
(dp54485
g114
(lp54486
I2223
assS'anteroventr'
p54487
(dp54488
g174
(lp54489
I2422
assS'drujldt'
p54490
(dp54491
g104
(lp54492
I1425
assS'copper'
p54493
(dp54494
g283
(lp54495
I707
assS'gelbart'
p54496
(dp54497
g32
(lp54498
I3098
assS'perturb'
p54499
(dp54500
g318
(lp54501
sg181
(lp54502
sg110
(lp54503
sg68
(lp54504
sg38
(lp54505
sg535
(lp54506
sg176
(lp54507
sg135
(lp54508
sg99
(lp54509
sg138
(lp54510
I700
assS'exchang'
p54511
(dp54512
g42
(lp54513
I2975
asg132
(lp54514
sg178
(lp54515
ssS'nach'
p54516
(dp54517
g34
(lp54518
I2918
assS'naci'
p54519
(dp54520
g106
(lp54521
I1044
assS'ziuj'
p54522
(dp54523
g128
(lp54524
I249
assS'done'
p54525
(dp54526
g26
(lp54527
sg145
(lp54528
sg256
(lp54529
sg80
(lp54530
sg183
(lp54531
sg59
(lp54532
sg42
(lp54533
I2267
asg306
(lp54534
sg12
(lp54535
sg94
(lp54536
sg44
(lp54537
sg116
(lp54538
sg63
(lp54539
sg114
(lp54540
sg230
(lp54541
sg440
(lp54542
sg178
(lp54543
sg181
(lp54544
sg8
(lp54545
sg36
(lp54546
sg460
(lp54547
sg235
(lp54548
sg126
(lp54549
sg10
(lp54550
sg78
(lp54551
sg132
(lp54552
sg140
(lp54553
ssS'rllmelhart'
p54554
(dp54555
g108
(lp54556
I147
assS'dong'
p54557
(dp54558
g12
(lp54559
I16
assS'stabl'
p54560
(dp54561
g163
(lp54562
sg176
(lp54563
sg80
(lp54564
sg295
(lp54565
sg183
(lp54566
sg306
(lp54567
sg89
(lp54568
sg245
(lp54569
sg46
(lp54570
sg20
(lp54571
sg18
(lp54572
sg221
(lp54573
sg535
(lp54574
sg149
(lp54575
sg116
(lp54576
sg12
(lp54577
sg110
(lp54578
sg230
(lp54579
sg438
(lp54580
I775
asg181
(lp54581
sg34
(lp54582
sg384
(lp54583
sg68
(lp54584
sg281
(lp54585
ssS'carter'
p54586
(dp54587
g183
(lp54588
I6345
assS'neurocomput'
p54589
(dp54590
g36
(lp54591
sg178
(lp54592
I2510
asg10
(lp54593
ssS'timothi'
p54594
(dp54595
g22
(lp54596
I10
asg114
(lp54597
ssS'smoother'
p54598
(dp54599
g329
(lp54600
sg121
(lp54601
I1185
asg178
(lp54602
sg223
(lp54603
ssS'construct'
p54604
(dp54605
g68
(lp54606
sg70
(lp54607
sg78
(lp54608
sg283
(lp54609
sg26
(lp54610
sg30
(lp54611
sg287
(lp54612
sg76
(lp54613
sg295
(lp54614
sg183
(lp54615
sg59
(lp54616
sg484
(lp54617
sg42
(lp54618
I991
asg89
(lp54619
sg91
(lp54620
sg46
(lp54621
sg20
(lp54622
sg221
(lp54623
sg313
(lp54624
sg223
(lp54625
sg350
(lp54626
sg116
(lp54627
sg118
(lp54628
sg429
(lp54629
sg104
(lp54630
sg108
(lp54631
sg63
(lp54632
sg52
(lp54633
sg230
(lp54634
sg329
(lp54635
sg32
(lp54636
sg318
(lp54637
sg121
(lp54638
sg181
(lp54639
sg6
(lp54640
sg235
(lp54641
sg34
(lp54642
sg36
(lp54643
sg124
(lp54644
sg281
(lp54645
sg10
(lp54646
sg40
(lp54647
sg344
(lp54648
sg44
(lp54649
sg130
(lp54650
sg132
(lp54651
sg14
(lp54652
sg16
(lp54653
sg138
(lp54654
sg354
(lp54655
ssS'hicss'
p54656
(dp54657
g26
(lp54658
I3284
assS'statement'
p54659
(dp54660
g230
(lp54661
sg121
(lp54662
I1463
asg34
(lp54663
sg460
(lp54664
sg68
(lp54665
sg341
(lp54666
sg46
(lp54667
ssS'ze'
p54668
(dp54669
g145
(lp54670
I698
assS'twenti'
p54671
(dp54672
g484
(lp54673
sg78
(lp54674
sg18
(lp54675
sg138
(lp54676
I2040
asg124
(lp54677
ssS'booster'
p54678
(dp54679
g344
(lp54680
I2160
assS'dichotom'
p54681
(dp54682
g110
(lp54683
I1045
assS'stirl'
p54684
(dp54685
g174
(lp54686
I15
assS'pare'
p54687
(dp54688
g332
(lp54689
I2712
assS'akal'
p54690
(dp54691
g235
(lp54692
I1689
assS'eqn'
p54693
(dp54694
g102
(lp54695
I623
asg281
(lp54696
sg6
(lp54697
ssS'park'
p54698
(dp54699
g87
(lp54700
sg44
(lp54701
sg128
(lp54702
I188
assS'pari'
p54703
(dp54704
g174
(lp54705
I2860
assS'selector'
p54706
(dp54707
g460
(lp54708
I1265
assS'sangiovanni'
p54709
(dp54710
g34
(lp54711
I2868
assS'part'
p54712
(dp54713
g124
(lp54714
sg26
(lp54715
sg281
(lp54716
sg283
(lp54717
sg303
(lp54718
sg80
(lp54719
sg287
(lp54720
sg176
(lp54721
sg145
(lp54722
sg256
(lp54723
sg76
(lp54724
sg262
(lp54725
sg295
(lp54726
sg183
(lp54727
sg59
(lp54728
sg484
(lp54729
sg83
(lp54730
sg85
(lp54731
sg63
(lp54732
sg42
(lp54733
I51
asg87
(lp54734
sg91
(lp54735
sg12
(lp54736
sg94
(lp54737
sg20
(lp54738
sg535
(lp54739
sg68
(lp54740
sg149
(lp54741
sg118
(lp54742
sg116
(lp54743
sg329
(lp54744
sg293
(lp54745
sg460
(lp54746
sg178
(lp54747
sg429
(lp54748
sg318
(lp54749
sg104
(lp54750
sg110
(lp54751
sg96
(lp54752
sg52
(lp54753
sg230
(lp54754
sg174
(lp54755
sg32
(lp54756
sg332
(lp54757
sg121
(lp54758
sg22
(lp54759
sg181
(lp54760
sg8
(lp54761
sg384
(lp54762
sg235
(lp54763
sg72
(lp54764
sg341
(lp54765
sg10
(lp54766
sg40
(lp54767
sg78
(lp54768
sg14
(lp54769
sg16
(lp54770
sg135
(lp54771
sg138
(lp54772
sg354
(lp54773
ssS'pars'
p54774
(dp54775
g94
(lp54776
I1239
assS'lltu'
p54777
(dp54778
g104
(lp54779
I2471
assS'qld'
p54780
(dp54781
g121
(lp54782
I34
assS'zc'
p54783
(dp54784
g283
(lp54785
I440
assS'exposur'
p54786
(dp54787
g216
(lp54788
sg70
(lp54789
sg99
(lp54790
I1867
assS'b'
p54791
(dp54792
g80
(lp54793
sg293
(lp54794
sg344
(lp54795
sg78
(lp54796
sg59
(lp54797
sg484
(lp54798
sg38
(lp54799
sg83
(lp54800
sg85
(lp54801
sg303
(lp54802
sg118
(lp54803
sg34
(lp54804
sg36
(lp54805
sg460
(lp54806
sg68
(lp54807
sg72
(lp54808
sg281
(lp54809
sg10
(lp54810
sg70
(lp54811
sg26
(lp54812
sg277
(lp54813
sg163
(lp54814
sg89
(lp54815
sg91
(lp54816
sg12
(lp54817
sg94
(lp54818
sg96
(lp54819
sg48
(lp54820
sg99
(lp54821
sg313
(lp54822
sg44
(lp54823
sg149
(lp54824
sg429
(lp54825
sg102
(lp54826
sg104
(lp54827
sg106
(lp54828
sg108
(lp54829
sg110
(lp54830
sg63
(lp54831
sg52
(lp54832
sg114
(lp54833
sg128
(lp54834
sg130
(lp54835
sg132
(lp54836
sg14
(lp54837
sg16
(lp54838
sg135
(lp54839
sg50
(lp54840
sg138
(lp54841
sg354
(lp54842
sg306
(lp54843
sg245
(lp54844
sg46
(lp54845
sg20
(lp54846
sg18
(lp54847
sg221
(lp54848
sg535
(lp54849
sg223
(lp54850
sg350
(lp54851
sg174
(lp54852
sg440
(lp54853
sg332
(lp54854
sg121
(lp54855
sg4
(lp54856
sg6
(lp54857
sg8
(lp54858
sg126
(lp54859
sg341
(lp54860
sg30
(lp54861
sg287
(lp54862
sg74
(lp54863
sg176
(lp54864
sg145
(lp54865
sg256
(lp54866
sg76
(lp54867
sg262
(lp54868
sg295
(lp54869
sg183
(lp54870
sg42
(lp54871
I1843
asg230
(lp54872
sg329
(lp54873
sg32
(lp54874
sg318
(lp54875
sg178
(lp54876
sg22
(lp54877
sg181
(lp54878
sg384
(lp54879
sg124
(lp54880
ssS'contrari'
p54881
(dp54882
g484
(lp54883
sg318
(lp54884
I2154
assS'horizont'
p54885
(dp54886
g350
(lp54887
sg32
(lp54888
sg48
(lp54889
sg256
(lp54890
sg42
(lp54891
I2894
asg245
(lp54892
sg429
(lp54893
sg318
(lp54894
sg12
(lp54895
sg18
(lp54896
sg63
(lp54897
sg149
(lp54898
ssS'kitjkl'
p54899
(dp54900
g149
(lp54901
I938
assS'beij'
p54902
(dp54903
g72
(lp54904
I28
assS'fixat'
p54905
(dp54906
g32
(lp54907
sg178
(lp54908
sg6
(lp54909
sg303
(lp54910
sg12
(lp54911
I1393
asg350
(lp54912
ssS'theoretwalframework'
p54913
(dp54914
g8
(lp54915
I434
assS'incub'
p54916
(dp54917
g106
(lp54918
I1031
assS'sensorimotor'
p54919
(dp54920
g303
(lp54921
I85
assS'rrt'
p54922
(dp54923
g306
(lp54924
I2302
assS'built'
p54925
(dp54926
g287
(lp54927
sg145
(lp54928
sg183
(lp54929
sg40
(lp54930
sg87
(lp54931
sg91
(lp54932
sg78
(lp54933
sg149
(lp54934
I298
assS'onsec'
p54935
(dp54936
g20
(lp54937
I2168
assS'ation'
p54938
(dp54939
g72
(lp54940
sg48
(lp54941
I722
asg89
(lp54942
ssS'cfs'
p54943
(dp54944
g262
(lp54945
I22
assS'onset'
p54946
(dp54947
g174
(lp54948
sg332
(lp54949
sg38
(lp54950
sg6
(lp54951
I1313
assS'wishart'
p54952
(dp54953
g221
(lp54954
I957
assS'build'
p54955
(dp54956
g26
(lp54957
sg176
(lp54958
sg293
(lp54959
sg183
(lp54960
sg59
(lp54961
sg484
(lp54962
sg83
(lp54963
sg42
(lp54964
I937
asg87
(lp54965
sg91
(lp54966
sg94
(lp54967
sg20
(lp54968
sg106
(lp54969
sg118
(lp54970
sg440
(lp54971
sg4
(lp54972
sg384
(lp54973
sg68
(lp54974
sg10
(lp54975
sg40
(lp54976
sg78
(lp54977
sg132
(lp54978
sg14
(lp54979
sg16
(lp54980
sg135
(lp54981
sg140
(lp54982
ssS'minm'
p54983
(dp54984
g72
(lp54985
I1168
assS'kendal'
p54986
(dp54987
g163
(lp54988
I824
assS'mose'
p54989
(dp54990
g223
(lp54991
I3317
assS'compat'
p54992
(dp54993
g80
(lp54994
sg10
(lp54995
sg350
(lp54996
I2574
assS'patterson'
p54997
(dp54998
g174
(lp54999
I449
assS'dayton'
p55000
(dp55001
g344
(lp55002
I32
assS'flute'
p55003
(dp55004
g174
(lp55005
I2228
assS'mosi'
p55006
(dp55007
g256
(lp55008
I2131
assS'compo'
p55009
(dp55010
g176
(lp55011
sg26
(lp55012
sg52
(lp55013
I2572
asg293
(lp55014
ssS'usedepend'
p55015
(dp55016
g106
(lp55017
I2653
assS'ronni'
p55018
(dp55019
g140
(lp55020
I3150
assS'essa'
p55021
(dp55022
g293
(lp55023
I3129
assS'most'
p55024
(dp55025
g329
(lp55026
sg70
(lp55027
sg78
(lp55028
sg277
(lp55029
sg303
(lp55030
sg281
(lp55031
sg181
(lp55032
sg40
(lp55033
sg26
(lp55034
sg30
(lp55035
sg287
(lp55036
sg74
(lp55037
sg145
(lp55038
sg76
(lp55039
sg344
(lp55040
sg183
(lp55041
sg80
(lp55042
sg83
(lp55043
sg85
(lp55044
sg124
(lp55045
sg42
(lp55046
I449
asg306
(lp55047
sg87
(lp55048
sg89
(lp55049
sg68
(lp55050
sg12
(lp55051
sg94
(lp55052
sg48
(lp55053
sg99
(lp55054
sg313
(lp55055
sg44
(lp55056
sg149
(lp55057
sg174
(lp55058
sg32
(lp55059
sg350
(lp55060
sg429
(lp55061
sg318
(lp55062
sg104
(lp55063
sg108
(lp55064
sg110
(lp55065
sg178
(lp55066
sg52
(lp55067
sg114
(lp55068
sg116
(lp55069
sg438
(lp55070
sg440
(lp55071
sg18
(lp55072
sg121
(lp55073
sg4
(lp55074
sg6
(lp55075
sg8
(lp55076
sg460
(lp55077
sg235
(lp55078
sg341
(lp55079
sg10
(lp55080
sg535
(lp55081
sg63
(lp55082
sg223
(lp55083
sg128
(lp55084
sg130
(lp55085
sg132
(lp55086
sg14
(lp55087
sg16
(lp55088
sg135
(lp55089
sg50
(lp55090
sg138
(lp55091
sg140
(lp55092
sg354
(lp55093
ssS'prolifer'
p55094
(dp55095
g438
(lp55096
I1846
assS'assoc'
p55097
(dp55098
g438
(lp55099
I2428
asg106
(lp55100
ssS'charl'
p55101
(dp55102
g106
(lp55103
I2506
asg262
(lp55104
sg163
(lp55105
sg22
(lp55106
sg149
(lp55107
ssS'assoa'
p55108
(dp55109
g106
(lp55110
I946
assS'compu'
p55111
(dp55112
g42
(lp55113
I2777
assS'prentic'
p55114
(dp55115
g230
(lp55116
sg32
(lp55117
sg283
(lp55118
sg10
(lp55119
sg102
(lp55120
sg46
(lp55121
sg108
(lp55122
I2468
assS'charg'
p55123
(dp55124
g438
(lp55125
I1881
asg283
(lp55126
sg256
(lp55127
sg344
(lp55128
sg68
(lp55129
sg245
(lp55130
sg14
(lp55131
sg20
(lp55132
sg135
(lp55133
ssS'kb'
p55134
(dp55135
g72
(lp55136
I2179
asg10
(lp55137
ssS'kg'
p55138
(dp55139
g42
(lp55140
I2230
asg68
(lp55141
ssS'ke'
p55142
(dp55143
g68
(lp55144
sg130
(lp55145
I2441
assS'kj'
p55146
(dp55147
g96
(lp55148
I800
asg221
(lp55149
ssS'ki'
p55150
(dp55151
g221
(lp55152
sg72
(lp55153
sg121
(lp55154
sg22
(lp55155
sg130
(lp55156
I1478
assS'kh'
p55157
(dp55158
g116
(lp55159
sg287
(lp55160
sg6
(lp55161
I2302
assS'ko'
p55162
(dp55163
g14
(lp55164
I3176
assS'kn'
p55165
(dp55166
g96
(lp55167
I933
assS'km'
p55168
(dp55169
g42
(lp55170
I2262
assS'kl'
p55171
(dp55172
g329
(lp55173
sg74
(lp55174
sg262
(lp55175
sg72
(lp55176
sg130
(lp55177
I1212
asg104
(lp55178
sg221
(lp55179
sg149
(lp55180
ssS'ks'
p55181
(dp55182
g102
(lp55183
I880
asg344
(lp55184
ssS'kr'
p55185
(dp55186
g344
(lp55187
I1912
assS'kw'
p55188
(dp55189
g40
(lp55190
I1680
assS'kv'
p55191
(dp55192
g130
(lp55193
I1067
assS'ku'
p55194
(dp55195
g36
(lp55196
I29
assS'kt'
p55197
(dp55198
g124
(lp55199
sg85
(lp55200
sg44
(lp55201
I1555
assS'weigh'
p55202
(dp55203
g50
(lp55204
sg140
(lp55205
I586
assS'touretzki'
p55206
(dp55207
g26
(lp55208
sg30
(lp55209
sg74
(lp55210
sg176
(lp55211
sg256
(lp55212
sg80
(lp55213
sg344
(lp55214
sg83
(lp55215
sg303
(lp55216
sg306
(lp55217
sg12
(lp55218
sg313
(lp55219
sg350
(lp55220
sg116
(lp55221
sg440
(lp55222
sg318
(lp55223
sg178
(lp55224
sg4
(lp55225
sg34
(lp55226
sg36
(lp55227
sg460
(lp55228
sg124
(lp55229
sg126
(lp55230
sg132
(lp55231
sg138
(lp55232
I3540
assS'commerc'
p55233
(dp55234
g40
(lp55235
I2455
assS'iparadis'
p55236
(dp55237
g40
(lp55238
I24
assS'sparrow'
p55239
(dp55240
g116
(lp55241
I39
assS'guhk'
p55242
(dp55243
g72
(lp55244
I3349
assS'inaet'
p55245
(dp55246
g429
(lp55247
I703
assS'deboor'
p55248
(dp55249
g295
(lp55250
I1625
asg183
(lp55251
ssS'fink'
p55252
(dp55253
g36
(lp55254
I33
assS'malach'
p55255
(dp55256
g149
(lp55257
I173
assS'fine'
p55258
(dp55259
g30
(lp55260
sg176
(lp55261
sg70
(lp55262
sg277
(lp55263
sg460
(lp55264
sg68
(lp55265
sg59
(lp55266
sg245
(lp55267
I2205
asg63
(lp55268
ssS'find'
p55269
(dp55270
g329
(lp55271
sg26
(lp55272
sg277
(lp55273
sg163
(lp55274
sg281
(lp55275
sg40
(lp55276
sg30
(lp55277
sg74
(lp55278
sg145
(lp55279
sg80
(lp55280
sg76
(lp55281
sg293
(lp55282
sg295
(lp55283
sg183
(lp55284
sg59
(lp55285
sg484
(lp55286
sg38
(lp55287
sg42
(lp55288
I213
asg306
(lp55289
sg87
(lp55290
sg91
(lp55291
sg245
(lp55292
sg94
(lp55293
sg20
(lp55294
sg48
(lp55295
sg221
(lp55296
sg313
(lp55297
sg44
(lp55298
sg350
(lp55299
sg174
(lp55300
sg460
(lp55301
sg429
(lp55302
sg68
(lp55303
sg46
(lp55304
sg102
(lp55305
sg178
(lp55306
sg106
(lp55307
sg108
(lp55308
sg110
(lp55309
sg63
(lp55310
sg52
(lp55311
sg230
(lp55312
sg438
(lp55313
sg440
(lp55314
sg318
(lp55315
sg121
(lp55316
sg4
(lp55317
sg235
(lp55318
sg34
(lp55319
sg36
(lp55320
sg384
(lp55321
sg124
(lp55322
sg126
(lp55323
sg341
(lp55324
sg535
(lp55325
sg344
(lp55326
sg223
(lp55327
sg128
(lp55328
sg130
(lp55329
sg132
(lp55330
sg14
(lp55331
sg16
(lp55332
sg138
(lp55333
sg140
(lp55334
ssS'multichannel'
p55335
(dp55336
g332
(lp55337
I690
assS'giant'
p55338
(dp55339
g63
(lp55340
I1198
assS'nervous'
p55341
(dp55342
g438
(lp55343
I2416
asg70
(lp55344
sg4
(lp55345
sg245
(lp55346
sg99
(lp55347
sg350
(lp55348
ssS'beat'
p55349
(dp55350
g132
(lp55351
sg295
(lp55352
sg135
(lp55353
I313
asg183
(lp55354
ssS'microelectron'
p55355
(dp55356
g10
(lp55357
I2814
assS'watrous'
p55358
(dp55359
g96
(lp55360
sg341
(lp55361
sg128
(lp55362
I2959
assS'oxfordshir'
p55363
(dp55364
g14
(lp55365
sg16
(lp55366
I56
assS'express'
p55367
(dp55368
g70
(lp55369
sg26
(lp55370
sg163
(lp55371
sg176
(lp55372
sg145
(lp55373
sg262
(lp55374
sg344
(lp55375
sg38
(lp55376
sg85
(lp55377
sg42
(lp55378
I191
asg87
(lp55379
sg245
(lp55380
sg96
(lp55381
sg313
(lp55382
sg223
(lp55383
sg350
(lp55384
sg293
(lp55385
sg32
(lp55386
sg429
(lp55387
sg102
(lp55388
sg110
(lp55389
sg63
(lp55390
sg230
(lp55391
sg440
(lp55392
sg318
(lp55393
sg460
(lp55394
sg124
(lp55395
sg281
(lp55396
sg130
(lp55397
sg132
(lp55398
sg140
(lp55399
ssS'cheaper'
p55400
(dp55401
g283
(lp55402
sg89
(lp55403
I2100
assS'breast'
p55404
(dp55405
g484
(lp55406
I118
assS'meir'
p55407
(dp55408
g140
(lp55409
I3151
asg235
(lp55410
ssS'pmcess'
p55411
(dp55412
g174
(lp55413
I2499
assS'restart'
p55414
(dp55415
g132
(lp55416
I2196
asg34
(lp55417
ssS'ioduclcn'
p55418
(dp55419
g116
(lp55420
I309
assS'huff'
p55421
(dp55422
g83
(lp55423
I2104
assS'jlog'
p55424
(dp55425
g85
(lp55426
I2606
assS'particul'
p55427
(dp55428
g283
(lp55429
I238
assS'common'
p55430
(dp55431
g68
(lp55432
sg181
(lp55433
sg287
(lp55434
sg74
(lp55435
sg344
(lp55436
sg484
(lp55437
sg83
(lp55438
sg303
(lp55439
sg306
(lp55440
sg91
(lp55441
sg94
(lp55442
sg20
(lp55443
sg313
(lp55444
sg223
(lp55445
sg149
(lp55446
sg429
(lp55447
sg332
(lp55448
sg102
(lp55449
sg104
(lp55450
sg110
(lp55451
sg63
(lp55452
sg230
(lp55453
sg32
(lp55454
sg318
(lp55455
sg121
(lp55456
sg4
(lp55457
sg6
(lp55458
sg8
(lp55459
sg36
(lp55460
sg460
(lp55461
sg235
(lp55462
sg126
(lp55463
sg341
(lp55464
sg132
(lp55465
sg135
(lp55466
sg138
(lp55467
sg140
(lp55468
I896
assS'warneboldt'
p55469
(dp55470
g130
(lp55471
I3083
assS'decompos'
p55472
(dp55473
g440
(lp55474
sg332
(lp55475
sg460
(lp55476
sg42
(lp55477
I3252
asg318
(lp55478
sg46
(lp55479
ssS'geoff'
p55480
(dp55481
g89
(lp55482
I2322
assS'lioo'
p55483
(dp55484
g306
(lp55485
I1030
assS'autoassoci'
p55486
(dp55487
g78
(lp55488
I4
assS'expert'
p55489
(dp55490
g30
(lp55491
sg329
(lp55492
sg59
(lp55493
sg277
(lp55494
sg295
(lp55495
sg183
(lp55496
sg460
(lp55497
sg68
(lp55498
sg83
(lp55499
sg63
(lp55500
sg42
(lp55501
I3398
asg87
(lp55502
sg91
(lp55503
sg78
(lp55504
sg132
(lp55505
sg94
(lp55506
sg138
(lp55507
sg52
(lp55508
sg114
(lp55509
ssS'fanh'
p55510
(dp55511
g163
(lp55512
I614
assS'someth'
p55513
(dp55514
g85
(lp55515
sg140
(lp55516
I1690
assS'tjkl'
p55517
(dp55518
g149
(lp55519
I975
assS'additionallowpass'
p55520
(dp55521
g22
(lp55522
I764
assS'infinitessim'
p55523
(dp55524
g44
(lp55525
I902
assS'subscript'
p55526
(dp55527
g293
(lp55528
sg108
(lp55529
sg99
(lp55530
sg130
(lp55531
I1180
assS'experi'
p55532
(dp55533
g329
(lp55534
sg78
(lp55535
sg277
(lp55536
sg163
(lp55537
sg72
(lp55538
sg80
(lp55539
sg283
(lp55540
sg26
(lp55541
sg30
(lp55542
sg176
(lp55543
sg145
(lp55544
sg256
(lp55545
sg76
(lp55546
sg118
(lp55547
sg295
(lp55548
sg183
(lp55549
sg59
(lp55550
sg484
(lp55551
sg83
(lp55552
sg85
(lp55553
sg303
(lp55554
sg42
(lp55555
I1687
asg87
(lp55556
sg89
(lp55557
sg91
(lp55558
sg12
(lp55559
sg94
(lp55560
sg96
(lp55561
sg48
(lp55562
sg99
(lp55563
sg313
(lp55564
sg223
(lp55565
sg149
(lp55566
sg174
(lp55567
sg293
(lp55568
sg32
(lp55569
sg350
(lp55570
sg429
(lp55571
sg104
(lp55572
sg106
(lp55573
sg110
(lp55574
sg178
(lp55575
sg116
(lp55576
sg438
(lp55577
sg440
(lp55578
sg332
(lp55579
sg121
(lp55580
sg181
(lp55581
sg6
(lp55582
sg8
(lp55583
sg34
(lp55584
sg221
(lp55585
sg384
(lp55586
sg124
(lp55587
sg126
(lp55588
sg281
(lp55589
sg344
(lp55590
sg128
(lp55591
sg130
(lp55592
sg132
(lp55593
sg14
(lp55594
sg16
(lp55595
sg135
(lp55596
sg138
(lp55597
sg140
(lp55598
sg354
(lp55599
ssS'expero'
p55600
(dp55601
g20
(lp55602
I1121
assS'altern'
p55603
(dp55604
g78
(lp55605
sg277
(lp55606
sg163
(lp55607
sg36
(lp55608
sg26
(lp55609
sg30
(lp55610
sg74
(lp55611
sg256
(lp55612
sg80
(lp55613
sg295
(lp55614
sg183
(lp55615
sg85
(lp55616
sg42
(lp55617
I334
asg91
(lp55618
sg245
(lp55619
sg94
(lp55620
sg96
(lp55621
sg221
(lp55622
sg313
(lp55623
sg223
(lp55624
sg149
(lp55625
sg104
(lp55626
sg110
(lp55627
sg329
(lp55628
sg440
(lp55629
sg332
(lp55630
sg178
(lp55631
sg22
(lp55632
sg6
(lp55633
sg8
(lp55634
sg99
(lp55635
sg68
(lp55636
sg72
(lp55637
sg128
(lp55638
sg130
(lp55639
sg132
(lp55640
sg50
(lp55641
sg138
(lp55642
ssS'hippocampus'
p55643
(dp55644
g78
(lp55645
sg106
(lp55646
I16
assS'complement'
p55647
(dp55648
g72
(lp55649
sg74
(lp55650
sg50
(lp55651
I164
asg85
(lp55652
sg128
(lp55653
ssS'unrol'
p55654
(dp55655
g10
(lp55656
I2425
assS'moyal'
p55657
(dp55658
g384
(lp55659
I845
assS'pami'
p55660
(dp55661
g42
(lp55662
I3439
asg429
(lp55663
sg63
(lp55664
sg181
(lp55665
sg8
(lp55666
ssS'allard'
p55667
(dp55668
g176
(lp55669
I2530
assS'popul'
p55670
(dp55671
g216
(lp55672
sg438
(lp55673
I419
asg18
(lp55674
sg4
(lp55675
sg6
(lp55676
sg174
(lp55677
sg484
(lp55678
sg130
(lp55679
sg12
(lp55680
sg106
(lp55681
sg108
(lp55682
sg277
(lp55683
ssS'informatik'
p55684
(dp55685
g30
(lp55686
sg34
(lp55687
sg59
(lp55688
sg130
(lp55689
sg132
(lp55690
I25
asg221
(lp55691
sg313
(lp55692
sg223
(lp55693
ssS'hiroyuki'
p55694
(dp55695
g18
(lp55696
I10
assS'annual'
p55697
(dp55698
g145
(lp55699
sg80
(lp55700
sg344
(lp55701
sg78
(lp55702
sg85
(lp55703
sg149
(lp55704
I2839
asg110
(lp55705
sg114
(lp55706
ssS'alon'
p55707
(dp55708
g26
(lp55709
sg174
(lp55710
sg440
(lp55711
sg48
(lp55712
sg277
(lp55713
sg80
(lp55714
sg235
(lp55715
sg262
(lp55716
sg10
(lp55717
sg303
(lp55718
sg128
(lp55719
sg132
(lp55720
sg106
(lp55721
I367
asg135
(lp55722
sg44
(lp55723
sg114
(lp55724
ssS'tempor'
p55725
(dp55726
g70
(lp55727
sg256
(lp55728
sg262
(lp55729
sg295
(lp55730
sg183
(lp55731
sg83
(lp55732
sg114
(lp55733
sg306
(lp55734
sg89
(lp55735
sg245
(lp55736
sg20
(lp55737
sg18
(lp55738
sg99
(lp55739
sg535
(lp55740
sg149
(lp55741
sg329
(lp55742
sg102
(lp55743
sg178
(lp55744
sg106
(lp55745
I1191
asg96
(lp55746
sg22
(lp55747
sg116
(lp55748
sg174
(lp55749
sg440
(lp55750
sg332
(lp55751
sg121
(lp55752
sg4
(lp55753
sg6
(lp55754
sg8
(lp55755
sg460
(lp55756
sg293
(lp55757
sg72
(lp55758
sg128
(lp55759
sg132
(lp55760
ssS'ehud'
p55761
(dp55762
g6
(lp55763
I25
assS'basford'
p55764
(dp55765
g295
(lp55766
sg183
(lp55767
sg91
(lp55768
I726
assS'simpli'
p55769
(dp55770
g124
(lp55771
sg163
(lp55772
sg176
(lp55773
sg145
(lp55774
sg76
(lp55775
sg293
(lp55776
sg85
(lp55777
sg303
(lp55778
sg245
(lp55779
sg94
(lp55780
sg99
(lp55781
sg223
(lp55782
sg440
(lp55783
sg429
(lp55784
sg104
(lp55785
sg63
(lp55786
sg32
(lp55787
sg332
(lp55788
sg8
(lp55789
sg235
(lp55790
sg126
(lp55791
sg341
(lp55792
sg128
(lp55793
sg138
(lp55794
sg140
(lp55795
I577
assS'point'
p55796
(dp55797
g124
(lp55798
sg70
(lp55799
sg78
(lp55800
sg163
(lp55801
sg116
(lp55802
sg68
(lp55803
sg293
(lp55804
sg36
(lp55805
sg26
(lp55806
sg30
(lp55807
sg287
(lp55808
sg74
(lp55809
sg176
(lp55810
sg145
(lp55811
sg256
(lp55812
sg76
(lp55813
sg262
(lp55814
sg295
(lp55815
sg183
(lp55816
sg59
(lp55817
sg80
(lp55818
sg38
(lp55819
sg85
(lp55820
sg303
(lp55821
sg42
(lp55822
I1750
asg306
(lp55823
sg89
(lp55824
sg91
(lp55825
sg12
(lp55826
sg94
(lp55827
sg48
(lp55828
sg99
(lp55829
sg313
(lp55830
sg44
(lp55831
sg230
(lp55832
sg329
(lp55833
sg18
(lp55834
sg32
(lp55835
sg429
(lp55836
sg318
(lp55837
sg46
(lp55838
sg102
(lp55839
sg104
(lp55840
sg108
(lp55841
sg110
(lp55842
sg63
(lp55843
sg22
(lp55844
sg216
(lp55845
sg438
(lp55846
sg440
(lp55847
sg332
(lp55848
sg178
(lp55849
sg4
(lp55850
sg6
(lp55851
sg8
(lp55852
sg34
(lp55853
sg221
(lp55854
sg460
(lp55855
sg235
(lp55856
sg126
(lp55857
sg281
(lp55858
sg10
(lp55859
sg535
(lp55860
sg344
(lp55861
sg223
(lp55862
sg128
(lp55863
sg130
(lp55864
sg132
(lp55865
sg14
(lp55866
sg16
(lp55867
sg135
(lp55868
sg50
(lp55869
sg138
(lp55870
sg140
(lp55871
sg354
(lp55872
ssS'instanti'
p55873
(dp55874
g118
(lp55875
sg74
(lp55876
sg138
(lp55877
I7
asg4
(lp55878
ssS'strabism'
p55879
(dp55880
g149
(lp55881
I117
assS'stanfil'
p55882
(dp55883
g223
(lp55884
I3479
assS'suppli'
p55885
(dp55886
g74
(lp55887
sg145
(lp55888
sg256
(lp55889
sg183
(lp55890
sg78
(lp55891
sg14
(lp55892
sg106
(lp55893
I2257
asg135
(lp55894
sg20
(lp55895
ssS'throughout'
p55896
(dp55897
g30
(lp55898
sg116
(lp55899
sg176
(lp55900
sg121
(lp55901
sg22
(lp55902
sg76
(lp55903
sg235
(lp55904
sg344
(lp55905
sg460
(lp55906
sg68
(lp55907
sg303
(lp55908
sg132
(lp55909
I86
asg145
(lp55910
sg149
(lp55911
ssS'platinum'
p55912
(dp55913
g106
(lp55914
I1059
assS'chol'
p55915
(dp55916
g91
(lp55917
I2090
assS'instrument'
p55918
(dp55919
g174
(lp55920
sg283
(lp55921
sg145
(lp55922
sg4
(lp55923
I583
asg63
(lp55924
ssS'intracellular'
p55925
(dp55926
g438
(lp55927
I1121
asg106
(lp55928
ssS'laplacian'
p55929
(dp55930
g245
(lp55931
I1299
assS'compulalion'
p55932
(dp55933
g85
(lp55934
I4089
assS'addison'
p55935
(dp55936
g26
(lp55937
sg295
(lp55938
sg183
(lp55939
sg130
(lp55940
sg36
(lp55941
sg12
(lp55942
sg50
(lp55943
I1614
assS'appiolclmatlor'
p55944
(dp55945
g163
(lp55946
I609
assS'kalman'
p55947
(dp55948
g230
(lp55949
sg108
(lp55950
I7
asg70
(lp55951
ssS'gas'
p55952
(dp55953
g42
(lp55954
I2295
asg10
(lp55955
ssS'unnecessarili'
p55956
(dp55957
g102
(lp55958
I1695
assS'gap'
p55959
(dp55960
g30
(lp55961
sg287
(lp55962
sg6
(lp55963
sg116
(lp55964
sg72
(lp55965
sg40
(lp55966
sg306
(lp55967
sg50
(lp55968
I1031
asg38
(lp55969
ssS'acti'
p55970
(dp55971
g295
(lp55972
I2066
asg183
(lp55973
ssS'gam'
p55974
(dp55975
g121
(lp55976
I1739
assS'gal'
p55977
(dp55978
g121
(lp55979
I2002
assS'understand'
p55980
(dp55981
g32
(lp55982
sg4
(lp55983
sg181
(lp55984
sg262
(lp55985
sg344
(lp55986
sg59
(lp55987
sg72
(lp55988
sg281
(lp55989
sg10
(lp55990
sg42
(lp55991
I239
asg102
(lp55992
sg94
(lp55993
sg12
(lp55994
sg14
(lp55995
sg106
(lp55996
sg22
(lp55997
sg99
(lp55998
sg535
(lp55999
sg44
(lp56000
sg350
(lp56001
ssS'vang'
p56002
(dp56003
g46
(lp56004
I3578
assS'negleg'
p56005
(dp56006
g36
(lp56007
I1637
assS'repetit'
p56008
(dp56009
g94
(lp56010
sg106
(lp56011
I1778
asg178
(lp56012
sg163
(lp56013
ssS'gab'
p56014
(dp56015
g178
(lp56016
I1421
assS'fut'
p56017
(dp56018
g277
(lp56019
I1997
assS'neuronindex'
p56020
(dp56021
g121
(lp56022
I751
assS'izabl'
p56023
(dp56024
g85
(lp56025
I1913
assS'solid'
p56026
(dp56027
g163
(lp56028
sg262
(lp56029
sg295
(lp56030
sg183
(lp56031
sg303
(lp56032
sg12
(lp56033
sg46
(lp56034
sg20
(lp56035
sg313
(lp56036
sg44
(lp56037
sg350
(lp56038
sg429
(lp56039
sg94
(lp56040
sg102
(lp56041
sg110
(lp56042
sg63
(lp56043
sg116
(lp56044
sg329
(lp56045
sg22
(lp56046
sg235
(lp56047
sg384
(lp56048
sg14
(lp56049
sg16
(lp56050
sg140
(lp56051
I1140
assS'wiedner'
p56052
(dp56053
g68
(lp56054
I29
assS'bill'
p56055
(dp56056
g46
(lp56057
sg70
(lp56058
sg128
(lp56059
I8
asg118
(lp56060
ssS'insens'
p56061
(dp56062
g163
(lp56063
I1224
assS'unifi'
p56064
(dp56065
g118
(lp56066
sg32
(lp56067
sg70
(lp56068
sg124
(lp56069
sg72
(lp56070
sg12
(lp56071
sg132
(lp56072
I3608
asg96
(lp56073
sg44
(lp56074
ssS'rigour'
p56075
(dp56076
g85
(lp56077
I4073
assS'flocculus'
p56078
(dp56079
g350
(lp56080
I1205
assS'subsect'
p56081
(dp56082
g135
(lp56083
I749
assS'propag'
p56084
(dp56085
g283
(lp56086
sg70
(lp56087
sg30
(lp56088
sg76
(lp56089
sg344
(lp56090
sg78
(lp56091
sg42
(lp56092
I3380
asg87
(lp56093
sg89
(lp56094
sg94
(lp56095
sg223
(lp56096
sg108
(lp56097
sg52
(lp56098
sg114
(lp56099
sg116
(lp56100
sg440
(lp56101
sg178
(lp56102
sg34
(lp56103
sg460
(lp56104
sg68
(lp56105
sg72
(lp56106
sg341
(lp56107
sg10
(lp56108
sg128
(lp56109
sg132
(lp56110
sg140
(lp56111
ssS'itselr'
p56112
(dp56113
g106
(lp56114
I201
assS'centr'
p56115
(dp56116
g174
(lp56117
sg176
(lp56118
sg283
(lp56119
sg26
(lp56120
sg235
(lp56121
sg332
(lp56122
sg138
(lp56123
I3234
asg52
(lp56124
ssS'itself'
p56125
(dp56126
g70
(lp56127
sg78
(lp56128
sg26
(lp56129
sg30
(lp56130
sg76
(lp56131
sg262
(lp56132
sg295
(lp56133
sg183
(lp56134
sg80
(lp56135
sg42
(lp56136
I474
asg91
(lp56137
sg12
(lp56138
sg20
(lp56139
sg18
(lp56140
sg149
(lp56141
sg116
(lp56142
sg118
(lp56143
sg429
(lp56144
sg102
(lp56145
sg104
(lp56146
sg106
(lp56147
sg63
(lp56148
sg216
(lp56149
sg174
(lp56150
sg32
(lp56151
sg4
(lp56152
sg181
(lp56153
sg8
(lp56154
sg34
(lp56155
sg36
(lp56156
sg68
(lp56157
sg10
(lp56158
sg130
(lp56159
sg132
(lp56160
sg14
(lp56161
sg16
(lp56162
sg50
(lp56163
ssS'virtu'
p56164
(dp56165
g30
(lp56166
I1547
asg34
(lp56167
sg59
(lp56168
ssS'bernardo'
p56169
(dp56170
g149
(lp56171
I2757
assS'righthand'
p56172
(dp56173
g350
(lp56174
I660
assS'stmequat'
p56175
(dp56176
g535
(lp56177
I1485
assS'ogii'
p56178
(dp56179
g14
(lp56180
I3986
assS'ldot'
p56181
(dp56182
g230
(lp56183
I1196
assS'benign'
p56184
(dp56185
g484
(lp56186
sg89
(lp56187
I101
asg91
(lp56188
ssS'itali'
p56189
(dp56190
g14
(lp56191
sg16
(lp56192
I2601
asg96
(lp56193
sg44
(lp56194
ssS'hammerstrom'
p56195
(dp56196
g10
(lp56197
I1636
assS'monophon'
p56198
(dp56199
g87
(lp56200
I999
assS'development'
p56201
(dp56202
g332
(lp56203
sg48
(lp56204
sg149
(lp56205
I2680
assS'timit'
p56206
(dp56207
g440
(lp56208
sg121
(lp56209
I1245
assS'syn'
p56210
(dp56211
g70
(lp56212
I696
assS'sys'
p56213
(dp56214
g460
(lp56215
sg99
(lp56216
I3256
asg181
(lp56217
ssS'kreiter'
p56218
(dp56219
g70
(lp56220
I2457
assS'moment'
p56221
(dp56222
g20
(lp56223
sg332
(lp56224
I358
asg163
(lp56225
sg460
(lp56226
sg63
(lp56227
ssS'stripe'
p56228
(dp56229
g118
(lp56230
sg277
(lp56231
sg295
(lp56232
sg183
(lp56233
sg10
(lp56234
sg149
(lp56235
I2000
assS'koch'
p56236
(dp56237
g70
(lp56238
sg6
(lp56239
sg181
(lp56240
sg262
(lp56241
sg8
(lp56242
sg12
(lp56243
I2805
asg18
(lp56244
ssS'hyperparamet'
p56245
(dp56246
g124
(lp56247
sg126
(lp56248
I317
assS'recreat'
p56249
(dp56250
g70
(lp56251
I2121
assS'hows'
p56252
(dp56253
g46
(lp56254
I13
assS'travers'
p56255
(dp56256
g438
(lp56257
I281
asg283
(lp56258
sg145
(lp56259
sg26
(lp56260
sg183
(lp56261
sg68
(lp56262
sg91
(lp56263
sg46
(lp56264
ssS'task'
p56265
(dp56266
g124
(lp56267
sg277
(lp56268
sg163
(lp56269
sg72
(lp56270
sg303
(lp56271
sg30
(lp56272
sg287
(lp56273
sg176
(lp56274
sg80
(lp56275
sg76
(lp56276
sg293
(lp56277
sg295
(lp56278
sg183
(lp56279
sg59
(lp56280
sg484
(lp56281
sg38
(lp56282
sg63
(lp56283
sg42
(lp56284
I1803
asg87
(lp56285
sg89
(lp56286
sg94
(lp56287
sg18
(lp56288
sg99
(lp56289
sg223
(lp56290
sg116
(lp56291
sg429
(lp56292
sg68
(lp56293
sg104
(lp56294
sg110
(lp56295
sg178
(lp56296
sg52
(lp56297
sg114
(lp56298
sg230
(lp56299
sg329
(lp56300
sg332
(lp56301
sg121
(lp56302
sg4
(lp56303
sg8
(lp56304
sg221
(lp56305
sg460
(lp56306
sg235
(lp56307
sg126
(lp56308
sg341
(lp56309
sg10
(lp56310
sg40
(lp56311
sg344
(lp56312
sg44
(lp56313
sg78
(lp56314
sg135
(lp56315
sg50
(lp56316
sg138
(lp56317
sg140
(lp56318
ssS'entri'
p56319
(dp56320
g277
(lp56321
sg80
(lp56322
sg83
(lp56323
sg40
(lp56324
sg87
(lp56325
sg104
(lp56326
sg96
(lp56327
I2361
assS'parenthet'
p56328
(dp56329
g44
(lp56330
I2217
assS'noncommut'
p56331
(dp56332
g32
(lp56333
I1205
assS'parametris'
p56334
(dp56335
g126
(lp56336
I312
assS'parenthes'
p56337
(dp56338
g104
(lp56339
I776
asg293
(lp56340
sg329
(lp56341
ssS'rakic'
p56342
(dp56343
g4
(lp56344
sg149
(lp56345
I3209
assS'gilchrist'
p56346
(dp56347
g118
(lp56348
I492
assS'y'
p56349
(dp56350
g283
(lp56351
sg70
(lp56352
sg26
(lp56353
sg277
(lp56354
sg163
(lp56355
sg72
(lp56356
sg293
(lp56357
sg281
(lp56358
sg85
(lp56359
sg460
(lp56360
sg36
(lp56361
sg181
(lp56362
sg30
(lp56363
sg287
(lp56364
sg74
(lp56365
sg176
(lp56366
sg145
(lp56367
sg80
(lp56368
sg262
(lp56369
sg295
(lp56370
sg183
(lp56371
sg59
(lp56372
sg38
(lp56373
sg83
(lp56374
sg114
(lp56375
sg63
(lp56376
sg42
(lp56377
I101
asg306
(lp56378
sg87
(lp56379
sg89
(lp56380
sg91
(lp56381
sg245
(lp56382
sg94
(lp56383
sg96
(lp56384
sg48
(lp56385
sg99
(lp56386
sg313
(lp56387
sg44
(lp56388
sg149
(lp56389
sg118
(lp56390
sg329
(lp56391
sg18
(lp56392
sg32
(lp56393
sg178
(lp56394
sg429
(lp56395
sg318
(lp56396
sg46
(lp56397
sg102
(lp56398
sg104
(lp56399
sg106
(lp56400
sg108
(lp56401
sg110
(lp56402
sg20
(lp56403
sg52
(lp56404
sg22
(lp56405
sg230
(lp56406
sg174
(lp56407
sg440
(lp56408
sg332
(lp56409
sg121
(lp56410
sg4
(lp56411
sg6
(lp56412
sg235
(lp56413
sg34
(lp56414
sg221
(lp56415
sg384
(lp56416
sg124
(lp56417
sg126
(lp56418
sg341
(lp56419
sg535
(lp56420
sg344
(lp56421
sg223
(lp56422
sg128
(lp56423
sg130
(lp56424
sg132
(lp56425
sg50
(lp56426
sg138
(lp56427
sg140
(lp56428
sg354
(lp56429
ssS'nolt'
p56430
(dp56431
g138
(lp56432
I3401
assS'spend'
p56433
(dp56434
g132
(lp56435
I1754
asg94
(lp56436
sg59
(lp56437
ssS'explan'
p56438
(dp56439
g216
(lp56440
sg118
(lp56441
sg4
(lp56442
sg295
(lp56443
sg183
(lp56444
sg116
(lp56445
sg12
(lp56446
sg91
(lp56447
sg132
(lp56448
I70
asg99
(lp56449
sg223
(lp56450
ssS'gelfand'
p56451
(dp56452
g354
(lp56453
I2049
assS'iflearn'
p56454
(dp56455
g223
(lp56456
I872
assS'ldk'
p56457
(dp56458
g114
(lp56459
I2130
assS'shape'
p56460
(dp56461
g283
(lp56462
sg287
(lp56463
sg145
(lp56464
sg76
(lp56465
sg262
(lp56466
sg295
(lp56467
sg183
(lp56468
sg59
(lp56469
sg484
(lp56470
sg91
(lp56471
sg12
(lp56472
sg46
(lp56473
sg48
(lp56474
sg99
(lp56475
sg313
(lp56476
sg223
(lp56477
sg149
(lp56478
sg429
(lp56479
sg63
(lp56480
sg52
(lp56481
sg114
(lp56482
sg318
(lp56483
sg22
(lp56484
sg181
(lp56485
sg460
(lp56486
sg126
(lp56487
sg132
(lp56488
sg14
(lp56489
sg16
(lp56490
sg138
(lp56491
I642
assS'cirrhosi'
p56492
(dp56493
g484
(lp56494
I1580
assS'vixeqj'
p56495
(dp56496
g460
(lp56497
I939
assS'depriv'
p56498
(dp56499
g438
(lp56500
I1007
asg460
(lp56501
sg48
(lp56502
ssS'smwave'
p56503
(dp56504
g484
(lp56505
I1886
assS'levitt'
p56506
(dp56507
g183
(lp56508
I6783
assS'cut'
p56509
(dp56510
g329
(lp56511
sg4
(lp56512
sg163
(lp56513
sg36
(lp56514
sg59
(lp56515
sg126
(lp56516
sg429
(lp56517
sg132
(lp56518
sg14
(lp56519
I3057
asg20
(lp56520
ssS'guyon'
p56521
(dp56522
g183
(lp56523
sg50
(lp56524
I1205
asg63
(lp56525
sg44
(lp56526
ssS'danger'
p56527
(dp56528
g295
(lp56529
sg183
(lp56530
sg42
(lp56531
I622
asg91
(lp56532
sg132
(lp56533
sg135
(lp56534
sg138
(lp56535
ssS'morristown'
p56536
(dp56537
g108
(lp56538
I18
assS'iiiiiiiiiiii'
p56539
(dp56540
g6
(lp56541
I1725
assS'apr'
p56542
(dp56543
g384
(lp56544
I1355
asg114
(lp56545
ssS'cue'
p56546
(dp56547
g118
(lp56548
sg178
(lp56549
sg4
(lp56550
I892
asg80
(lp56551
sg181
(lp56552
sg18
(lp56553
ssS'dixon'
p56554
(dp56555
g283
(lp56556
I39
assS'ag'
p56557
(dp56558
g59
(lp56559
sg221
(lp56560
I32
asg22
(lp56561
sg293
(lp56562
ssS'rgg'
p56563
(dp56564
g72
(lp56565
I3346
asg85
(lp56566
ssS'cun'
p56567
(dp56568
g178
(lp56569
sg26
(lp56570
sg181
(lp56571
sg34
(lp56572
sg344
(lp56573
sg50
(lp56574
sg138
(lp56575
I3250
asg44
(lp56576
ssS'cum'
p56577
(dp56578
g245
(lp56579
I10
asg350
(lp56580
ssS'heidelberg'
p56581
(dp56582
g106
(lp56583
I2747
asg59
(lp56584
sg354
(lp56585
ssS'bin'
p56586
(dp56587
g116
(lp56588
sg38
(lp56589
sg138
(lp56590
I1533
asg22
(lp56591
ssS'bij'
p56592
(dp56593
g104
(lp56594
I1714
assS'bii'
p56595
(dp56596
g230
(lp56597
sg221
(lp56598
I1107
assS'big'
p56599
(dp56600
g329
(lp56601
sg126
(lp56602
sg40
(lp56603
sg89
(lp56604
sg12
(lp56605
sg149
(lp56606
I1649
assS'maxm'
p56607
(dp56608
g72
(lp56609
I1184
assS'itili'
p56610
(dp56611
g52
(lp56612
I1032
assS'bia'
p56613
(dp56614
g8
(lp56615
I896
assS'judgement'
p56616
(dp56617
g332
(lp56618
I1484
assS'lagin'
p56619
(dp56620
g14
(lp56621
sg16
(lp56622
I1074
assS'bit'
p56623
(dp56624
g287
(lp56625
sg283
(lp56626
sg22
(lp56627
sg293
(lp56628
sg78
(lp56629
sg72
(lp56630
sg10
(lp56631
sg40
(lp56632
sg63
(lp56633
sg20
(lp56634
sg85
(lp56635
sg104
(lp56636
sg14
(lp56637
sg16
(lp56638
sg135
(lp56639
sg50
(lp56640
sg138
(lp56641
I1862
asg52
(lp56642
sg26
(lp56643
ssS'peripher'
p56644
(dp56645
g174
(lp56646
I2629
asg20
(lp56647
sg332
(lp56648
sg176
(lp56649
ssS'jxj'
p56650
(dp56651
g78
(lp56652
I1790
assS'semi'
p56653
(dp56654
g283
(lp56655
sg99
(lp56656
I3194
asg341
(lp56657
ssS'logpml'
p56658
(dp56659
g72
(lp56660
I2213
assS'massachussett'
p56661
(dp56662
g460
(lp56663
I18
assS'logpmi'
p56664
(dp56665
g72
(lp56666
I2853
assS'flux'
p56667
(dp56668
g14
(lp56669
sg16
(lp56670
I1258
assS'princip'
p56671
(dp56672
g30
(lp56673
sg32
(lp56674
sg332
(lp56675
sg121
(lp56676
sg8
(lp56677
sg163
(lp56678
sg318
(lp56679
sg128
(lp56680
sg14
(lp56681
sg16
(lp56682
I95
asg96
(lp56683
ssS'boolean'
p56684
(dp56685
g287
(lp56686
sg145
(lp56687
sg293
(lp56688
sg344
(lp56689
sg341
(lp56690
sg85
(lp56691
sg40
(lp56692
sg130
(lp56693
I938
asg110
(lp56694
ssS'wis'
p56695
(dp56696
g40
(lp56697
I2034
assS'bijml'
p56698
(dp56699
g70
(lp56700
I1961
assS'often'
p56701
(dp56702
g70
(lp56703
sg26
(lp56704
sg277
(lp56705
sg281
(lp56706
sg30
(lp56707
sg287
(lp56708
sg74
(lp56709
sg145
(lp56710
sg262
(lp56711
sg344
(lp56712
sg85
(lp56713
sg306
(lp56714
sg89
(lp56715
sg91
(lp56716
sg94
(lp56717
sg48
(lp56718
sg223
(lp56719
sg116
(lp56720
sg293
(lp56721
sg429
(lp56722
sg102
(lp56723
sg104
(lp56724
sg110
(lp56725
sg52
(lp56726
sg114
(lp56727
sg230
(lp56728
sg329
(lp56729
sg32
(lp56730
sg318
(lp56731
sg4
(lp56732
sg181
(lp56733
sg235
(lp56734
sg36
(lp56735
sg460
(lp56736
sg68
(lp56737
sg126
(lp56738
sg341
(lp56739
sg130
(lp56740
sg132
(lp56741
sg50
(lp56742
sg138
(lp56743
sg140
(lp56744
sg354
(lp56745
I201
assS'back'
p56746
(dp56747
g329
(lp56748
sg26
(lp56749
sg283
(lp56750
sg30
(lp56751
sg74
(lp56752
sg256
(lp56753
sg76
(lp56754
sg293
(lp56755
sg295
(lp56756
sg183
(lp56757
sg42
(lp56758
I1522
asg87
(lp56759
sg89
(lp56760
sg245
(lp56761
sg18
(lp56762
sg223
(lp56763
sg174
(lp56764
sg429
(lp56765
sg178
(lp56766
sg108
(lp56767
sg52
(lp56768
sg116
(lp56769
sg438
(lp56770
sg440
(lp56771
sg121
(lp56772
sg34
(lp56773
sg68
(lp56774
sg72
(lp56775
sg341
(lp56776
sg10
(lp56777
sg128
(lp56778
sg132
(lp56779
sg135
(lp56780
sg140
(lp56781
ssS'bach'
p56782
(dp56783
g22
(lp56784
I31
assS'ventricl'
p56785
(dp56786
g116
(lp56787
I388
assS'martial'
p56788
(dp56789
g94
(lp56790
I3473
assS'dawei'
p56791
(dp56792
g12
(lp56793
I14
assS'mirror'
p56794
(dp56795
g14
(lp56796
sg135
(lp56797
I1088
assS'scale'
p56798
(dp56799
g124
(lp56800
sg70
(lp56801
sg78
(lp56802
sg277
(lp56803
sg283
(lp56804
sg181
(lp56805
sg26
(lp56806
sg30
(lp56807
sg74
(lp56808
sg176
(lp56809
sg76
(lp56810
sg293
(lp56811
sg295
(lp56812
sg183
(lp56813
sg59
(lp56814
sg38
(lp56815
sg83
(lp56816
sg63
(lp56817
sg306
(lp56818
sg87
(lp56819
sg460
(lp56820
sg94
(lp56821
sg96
(lp56822
sg313
(lp56823
sg223
(lp56824
sg149
(lp56825
sg329
(lp56826
sg68
(lp56827
sg102
(lp56828
sg104
(lp56829
sg108
(lp56830
sg20
(lp56831
sg52
(lp56832
sg22
(lp56833
sg438
(lp56834
I1461
asg332
(lp56835
sg121
(lp56836
sg4
(lp56837
sg6
(lp56838
sg8
(lp56839
sg36
(lp56840
sg384
(lp56841
sg235
(lp56842
sg126
(lp56843
sg281
(lp56844
sg10
(lp56845
sg535
(lp56846
sg44
(lp56847
sg130
(lp56848
sg14
(lp56849
sg16
(lp56850
sg350
(lp56851
sg50
(lp56852
sg138
(lp56853
ssS'euclidian'
p56854
(dp56855
g59
(lp56856
sg223
(lp56857
sg130
(lp56858
I59
assS'pet'
p56859
(dp56860
g176
(lp56861
sg99
(lp56862
I3222
asg4
(lp56863
sg262
(lp56864
ssS'bohossian'
p56865
(dp56866
g40
(lp56867
I11
assS'pew'
p56868
(dp56869
g350
(lp56870
sg50
(lp56871
sg313
(lp56872
sg181
(lp56873
sg354
(lp56874
I2540
assS'peq'
p56875
(dp56876
g384
(lp56877
I481
assS'per'
p56878
(dp56879
g283
(lp56880
sg70
(lp56881
sg287
(lp56882
sg74
(lp56883
sg76
(lp56884
sg293
(lp56885
sg295
(lp56886
sg183
(lp56887
sg59
(lp56888
sg83
(lp56889
sg42
(lp56890
I3215
asg89
(lp56891
sg91
(lp56892
sg245
(lp56893
sg94
(lp56894
sg96
(lp56895
sg44
(lp56896
sg52
(lp56897
sg114
(lp56898
sg174
(lp56899
sg440
(lp56900
sg121
(lp56901
sg22
(lp56902
sg181
(lp56903
sg8
(lp56904
sg34
(lp56905
sg384
(lp56906
sg341
(lp56907
sg10
(lp56908
sg40
(lp56909
sg78
(lp56910
sg132
(lp56911
sg135
(lp56912
ssS'pes'
p56913
(dp56914
g10
(lp56915
I1663
assS'piscataway'
p56916
(dp56917
g149
(lp56918
I3097
assS'pen'
p56919
(dp56920
g108
(lp56921
I1479
asg63
(lp56922
ssS'mathemat'
p56923
(dp56924
g287
(lp56925
sg74
(lp56926
sg344
(lp56927
sg42
(lp56928
I2164
asg89
(lp56929
sg245
(lp56930
sg46
(lp56931
sg99
(lp56932
sg535
(lp56933
sg223
(lp56934
sg350
(lp56935
sg102
(lp56936
sg108
(lp56937
sg110
(lp56938
sg216
(lp56939
sg174
(lp56940
sg32
(lp56941
sg318
(lp56942
sg22
(lp56943
sg36
(lp56944
sg384
(lp56945
sg124
(lp56946
sg72
(lp56947
sg40
(lp56948
sg149
(lp56949
sg50
(lp56950
sg138
(lp56951
ssS'centroid'
p56952
(dp56953
g293
(lp56954
sg130
(lp56955
I706
assS'autoasoci'
p56956
(dp56957
g78
(lp56958
I1904
assS'scofield'
p56959
(dp56960
g438
(lp56961
I20
assS'carpent'
p56962
(dp56963
g12
(lp56964
I2749
asg104
(lp56965
ssS'saad'
p56966
(dp56967
g46
(lp56968
sg36
(lp56969
sg38
(lp56970
sg354
(lp56971
I357
assS'reproduc'
p56972
(dp56973
g116
(lp56974
sg332
(lp56975
sg80
(lp56976
sg384
(lp56977
sg114
(lp56978
sg303
(lp56979
sg42
(lp56980
I2193
asg132
(lp56981
sg48
(lp56982
sg63
(lp56983
sg350
(lp56984
ssS'intial'
p56985
(dp56986
g74
(lp56987
I1806
assS'impol'
p56988
(dp56989
g110
(lp56990
I2966
assS'plexiform'
p56991
(dp56992
g256
(lp56993
I122
assS'patient'
p56994
(dp56995
g4
(lp56996
sg277
(lp56997
sg484
(lp56998
sg303
(lp56999
sg91
(lp57000
sg135
(lp57001
I156
asg221
(lp57002
ssS'ajxj'
p57003
(dp57004
g535
(lp57005
I546
assS'use'
p57006
(dp57007
g80
(lp57008
sg293
(lp57009
sg344
(lp57010
sg78
(lp57011
sg59
(lp57012
sg484
(lp57013
sg38
(lp57014
sg83
(lp57015
sg85
(lp57016
sg303
(lp57017
sg438
(lp57018
sg116
(lp57019
sg118
(lp57020
sg34
(lp57021
sg36
(lp57022
sg460
(lp57023
sg68
(lp57024
sg72
(lp57025
sg281
(lp57026
sg10
(lp57027
sg40
(lp57028
sg283
(lp57029
sg70
(lp57030
sg26
(lp57031
sg277
(lp57032
sg163
(lp57033
sg89
(lp57034
sg91
(lp57035
sg12
(lp57036
sg94
(lp57037
sg96
(lp57038
sg48
(lp57039
sg99
(lp57040
sg313
(lp57041
sg44
(lp57042
sg149
(lp57043
sg429
(lp57044
sg102
(lp57045
sg104
(lp57046
sg106
(lp57047
sg108
(lp57048
sg110
(lp57049
sg63
(lp57050
sg52
(lp57051
sg114
(lp57052
sg128
(lp57053
sg130
(lp57054
sg132
(lp57055
sg14
(lp57056
sg16
(lp57057
sg135
(lp57058
sg50
(lp57059
sg138
(lp57060
sg140
(lp57061
sg354
(lp57062
sg306
(lp57063
sg87
(lp57064
sg245
(lp57065
sg46
(lp57066
sg20
(lp57067
sg18
(lp57068
sg221
(lp57069
sg223
(lp57070
sg350
(lp57071
sg216
(lp57072
sg174
(lp57073
sg440
(lp57074
sg332
(lp57075
sg121
(lp57076
sg4
(lp57077
sg6
(lp57078
sg8
(lp57079
sg126
(lp57080
sg341
(lp57081
sg30
(lp57082
sg287
(lp57083
sg74
(lp57084
sg176
(lp57085
sg145
(lp57086
sg256
(lp57087
sg76
(lp57088
sg262
(lp57089
sg295
(lp57090
sg183
(lp57091
sg42
(lp57092
I1700
asg230
(lp57093
sg329
(lp57094
sg32
(lp57095
sg318
(lp57096
sg178
(lp57097
sg22
(lp57098
sg181
(lp57099
sg235
(lp57100
sg384
(lp57101
sg124
(lp57102
ssS'oet'
p57103
(dp57104
g59
(lp57105
I2500
assS'idealis'
p57106
(dp57107
g176
(lp57108
I794
assS'fee'
p57109
(dp57110
g344
(lp57111
I2780
assS'sqale'
p57112
(dp57113
g87
(lp57114
I117
assS'from'
p57115
(dp57116
g80
(lp57117
sg293
(lp57118
sg344
(lp57119
sg78
(lp57120
sg59
(lp57121
sg484
(lp57122
sg38
(lp57123
sg83
(lp57124
sg85
(lp57125
sg303
(lp57126
sg438
(lp57127
sg116
(lp57128
sg118
(lp57129
sg34
(lp57130
sg36
(lp57131
sg460
(lp57132
sg68
(lp57133
sg72
(lp57134
sg281
(lp57135
sg10
(lp57136
sg40
(lp57137
sg283
(lp57138
sg70
(lp57139
sg26
(lp57140
sg277
(lp57141
sg163
(lp57142
sg89
(lp57143
sg91
(lp57144
sg12
(lp57145
sg94
(lp57146
sg96
(lp57147
sg48
(lp57148
sg99
(lp57149
sg313
(lp57150
sg44
(lp57151
sg149
(lp57152
sg429
(lp57153
sg102
(lp57154
sg104
(lp57155
sg106
(lp57156
sg108
(lp57157
sg110
(lp57158
sg63
(lp57159
sg52
(lp57160
sg114
(lp57161
sg128
(lp57162
sg130
(lp57163
sg132
(lp57164
sg14
(lp57165
sg16
(lp57166
sg135
(lp57167
sg50
(lp57168
sg138
(lp57169
sg140
(lp57170
sg354
(lp57171
sg306
(lp57172
sg87
(lp57173
sg245
(lp57174
sg46
(lp57175
sg20
(lp57176
sg18
(lp57177
sg221
(lp57178
sg535
(lp57179
sg223
(lp57180
sg350
(lp57181
sg216
(lp57182
sg174
(lp57183
sg440
(lp57184
sg332
(lp57185
sg121
(lp57186
sg4
(lp57187
sg6
(lp57188
sg8
(lp57189
sg126
(lp57190
sg341
(lp57191
sg30
(lp57192
sg287
(lp57193
sg74
(lp57194
sg176
(lp57195
sg145
(lp57196
sg256
(lp57197
sg76
(lp57198
sg262
(lp57199
sg295
(lp57200
sg183
(lp57201
sg42
(lp57202
I61
asg230
(lp57203
sg329
(lp57204
sg32
(lp57205
sg318
(lp57206
sg178
(lp57207
sg22
(lp57208
sg181
(lp57209
sg235
(lp57210
sg384
(lp57211
sg124
(lp57212
ssS'impos'
p57213
(dp57214
g176
(lp57215
sg70
(lp57216
sg183
(lp57217
sg460
(lp57218
sg42
(lp57219
I3313
asg429
(lp57220
sg110
(lp57221
sg102
(lp57222
sg14
(lp57223
sg99
(lp57224
sg350
(lp57225
ssS'elucid'
p57226
(dp57227
g132
(lp57228
I492
asg36
(lp57229
ssS'constraint'
p57230
(dp57231
g26
(lp57232
sg30
(lp57233
sg78
(lp57234
sg59
(lp57235
sg83
(lp57236
sg303
(lp57237
sg96
(lp57238
sg221
(lp57239
sg535
(lp57240
sg44
(lp57241
sg350
(lp57242
sg118
(lp57243
sg429
(lp57244
sg102
(lp57245
sg106
(lp57246
sg110
(lp57247
sg4
(lp57248
sg230
(lp57249
sg438
(lp57250
I1813
asg440
(lp57251
sg318
(lp57252
sg22
(lp57253
sg8
(lp57254
sg460
(lp57255
sg341
(lp57256
sg40
(lp57257
sg130
(lp57258
sg132
(lp57259
sg14
(lp57260
sg50
(lp57261
sg140
(lp57262
sg354
(lp57263
ssS'morri'
p57264
(dp57265
g106
(lp57266
I2510
assS'preclud'
p57267
(dp57268
g429
(lp57269
I1889
assS'electrophysiolog'
p57270
(dp57271
g135
(lp57272
I1547
asg262
(lp57273
ssS'alpaydm'
p57274
(dp57275
g178
(lp57276
I8
assS'pertin'
p57277
(dp57278
g230
(lp57279
I497
asg22
(lp57280
ssS'predat'
p57281
(dp57282
g293
(lp57283
I163
assS'nsm'
p57284
(dp57285
g293
(lp57286
I2188
assS'qecis'
p57287
(dp57288
g63
(lp57289
I985
assS'nsf'
p57290
(dp57291
g277
(lp57292
sg344
(lp57293
sg124
(lp57294
sg281
(lp57295
sg10
(lp57296
sg341
(lp57297
sg306
(lp57298
sg89
(lp57299
sg40
(lp57300
sg313
(lp57301
I2034
assS'martinez'
p57302
(dp57303
g12
(lp57304
I2664
assS'keystrok'
p57305
(dp57306
g94
(lp57307
I102
assS'steven'
p57308
(dp57309
g106
(lp57310
I2507
asg145
(lp57311
sg59
(lp57312
sg262
(lp57313
ssS'alienc'
p57314
(dp57315
g178
(lp57316
I617
assS'martinet'
p57317
(dp57318
g22
(lp57319
I2433
assS'framet'
p57320
(dp57321
g76
(lp57322
I1996
assS'inclus'
p57323
(dp57324
g74
(lp57325
sg121
(lp57326
sg8
(lp57327
sg183
(lp57328
sg59
(lp57329
sg14
(lp57330
I4464
assS'ontario'
p57331
(dp57332
g126
(lp57333
sg138
(lp57334
I29
assS'fictiti'
p57335
(dp57336
g124
(lp57337
sg126
(lp57338
I948
assS'mka'
p57339
(dp57340
g130
(lp57341
I1379
assS'pseudoinvers'
p57342
(dp57343
g96
(lp57344
I574
assS'mkl'
p57345
(dp57346
g130
(lp57347
I1161
assS'weighbd'
p57348
(dp57349
g295
(lp57350
I1214
asg183
(lp57351
ssS'mkv'
p57352
(dp57353
g130
(lp57354
I1205
assS'handbook'
p57355
(dp57356
g74
(lp57357
sg332
(lp57358
sg4
(lp57359
sg128
(lp57360
I2878
assS'includ'
p57361
(dp57362
g283
(lp57363
sg78
(lp57364
sg277
(lp57365
sg163
(lp57366
sg72
(lp57367
sg85
(lp57368
sg40
(lp57369
sg26
(lp57370
sg30
(lp57371
sg287
(lp57372
sg74
(lp57373
sg145
(lp57374
sg80
(lp57375
sg76
(lp57376
sg293
(lp57377
sg344
(lp57378
sg183
(lp57379
sg484
(lp57380
sg83
(lp57381
sg114
(lp57382
sg63
(lp57383
sg42
(lp57384
I568
asg87
(lp57385
sg89
(lp57386
sg460
(lp57387
sg245
(lp57388
sg94
(lp57389
sg96
(lp57390
sg48
(lp57391
sg99
(lp57392
sg313
(lp57393
sg44
(lp57394
sg149
(lp57395
sg350
(lp57396
sg429
(lp57397
sg318
(lp57398
sg102
(lp57399
sg108
(lp57400
sg110
(lp57401
sg20
(lp57402
sg52
(lp57403
sg22
(lp57404
sg230
(lp57405
sg118
(lp57406
sg32
(lp57407
sg332
(lp57408
sg181
(lp57409
sg6
(lp57410
sg8
(lp57411
sg34
(lp57412
sg221
(lp57413
sg384
(lp57414
sg124
(lp57415
sg126
(lp57416
sg281
(lp57417
sg10
(lp57418
sg535
(lp57419
sg223
(lp57420
sg128
(lp57421
sg36
(lp57422
sg14
(lp57423
sg16
(lp57424
sg135
(lp57425
sg138
(lp57426
sg140
(lp57427
ssS'forward'
p57428
(dp57429
g283
(lp57430
sg70
(lp57431
sg277
(lp57432
sg163
(lp57433
sg76
(lp57434
sg484
(lp57435
sg87
(lp57436
sg12
(lp57437
sg96
(lp57438
sg18
(lp57439
sg350
(lp57440
sg108
(lp57441
sg110
(lp57442
sg114
(lp57443
sg230
(lp57444
sg440
(lp57445
sg36
(lp57446
sg384
(lp57447
sg68
(lp57448
sg126
(lp57449
sg10
(lp57450
sg128
(lp57451
sg14
(lp57452
sg16
(lp57453
sg135
(lp57454
sg460
(lp57455
sg140
(lp57456
I1386
assS'implaus'
p57457
(dp57458
g68
(lp57459
I2043
assS'quiescenc'
p57460
(dp57461
g132
(lp57462
I1824
assS'tonotop'
p57463
(dp57464
g174
(lp57465
I1196
asg332
(lp57466
ssS'reorgan'
p57467
(dp57468
g176
(lp57469
sg149
(lp57470
I2706
assS'boyd'
p57471
(dp57472
g114
(lp57473
I2359
assS'gauss'
p57474
(dp57475
g230
(lp57476
I1837
asg163
(lp57477
ssS'muscl'
p57478
(dp57479
g245
(lp57480
sg135
(lp57481
I338
asg99
(lp57482
sg303
(lp57483
ssS'quiescent'
p57484
(dp57485
g132
(lp57486
I1901
assS'translat'
p57487
(dp57488
g329
(lp57489
sg68
(lp57490
sg178
(lp57491
sg22
(lp57492
sg78
(lp57493
sg145
(lp57494
sg63
(lp57495
sg429
(lp57496
sg44
(lp57497
sg94
(lp57498
sg96
(lp57499
sg138
(lp57500
I737
asg223
(lp57501
sg350
(lp57502
ssS'isra'
p57503
(dp57504
g145
(lp57505
I2991
assS'concaten'
p57506
(dp57507
g104
(lp57508
sg440
(lp57509
I1483
asg178
(lp57510
ssS'jonathan'
p57511
(dp57512
g4
(lp57513
I19
assS'vermi'
p57514
(dp57515
g350
(lp57516
I1207
assS'rumelharl'
p57517
(dp57518
g108
(lp57519
I2552
assS'rnay'
p57520
(dp57521
g350
(lp57522
I781
assS'fey'
p57523
(dp57524
g163
(lp57525
I979
assS'rumelhart'
p57526
(dp57527
g230
(lp57528
sg283
(lp57529
sg76
(lp57530
sg181
(lp57531
sg344
(lp57532
sg36
(lp57533
sg40
(lp57534
sg42
(lp57535
I3371
asg87
(lp57536
sg78
(lp57537
sg108
(lp57538
sg110
(lp57539
sg223
(lp57540
sg114
(lp57541
ssS'curv'
p57542
(dp57543
g283
(lp57544
sg70
(lp57545
sg145
(lp57546
sg256
(lp57547
sg262
(lp57548
sg183
(lp57549
sg59
(lp57550
sg484
(lp57551
sg38
(lp57552
sg85
(lp57553
sg235
(lp57554
sg12
(lp57555
sg46
(lp57556
sg18
(lp57557
sg313
(lp57558
sg223
(lp57559
sg102
(lp57560
sg110
(lp57561
sg63
(lp57562
sg114
(lp57563
sg32
(lp57564
sg22
(lp57565
sg181
(lp57566
sg8
(lp57567
sg36
(lp57568
sg68
(lp57569
sg78
(lp57570
sg14
(lp57571
sg16
(lp57572
sg140
(lp57573
I1142
assS'flop'
p57574
(dp57575
g20
(lp57576
I1789
assS'constant'
p57577
(dp57578
g329
(lp57579
sg163
(lp57580
sg281
(lp57581
sg40
(lp57582
sg287
(lp57583
sg74
(lp57584
sg176
(lp57585
sg145
(lp57586
sg256
(lp57587
sg80
(lp57588
sg262
(lp57589
sg295
(lp57590
sg183
(lp57591
sg59
(lp57592
sg38
(lp57593
sg83
(lp57594
sg85
(lp57595
sg87
(lp57596
sg245
(lp57597
sg94
(lp57598
sg18
(lp57599
sg221
(lp57600
sg313
(lp57601
sg223
(lp57602
sg149
(lp57603
sg174
(lp57604
sg12
(lp57605
sg46
(lp57606
sg102
(lp57607
sg106
(lp57608
sg108
(lp57609
sg22
(lp57610
sg230
(lp57611
sg438
(lp57612
I367
asg118
(lp57613
sg332
(lp57614
sg121
(lp57615
sg4
(lp57616
sg181
(lp57617
sg235
(lp57618
sg34
(lp57619
sg36
(lp57620
sg460
(lp57621
sg68
(lp57622
sg126
(lp57623
sg341
(lp57624
sg535
(lp57625
sg344
(lp57626
sg128
(lp57627
sg78
(lp57628
sg132
(lp57629
sg14
(lp57630
sg16
(lp57631
sg350
(lp57632
sg50
(lp57633
ssS'curs'
p57634
(dp57635
g121
(lp57636
sg306
(lp57637
sg89
(lp57638
I39
asg341
(lp57639
ssS'metal'
p57640
(dp57641
g10
(lp57642
I929
assS'utilis'
p57643
(dp57644
g14
(lp57645
I3266
asg283
(lp57646
ssS'constanc'
p57647
(dp57648
g102
(lp57649
I3238
asg216
(lp57650
sg118
(lp57651
ssS'wgen'
p57652
(dp57653
g295
(lp57654
I2045
asg183
(lp57655
ssS'lyl'
p57656
(dp57657
g344
(lp57658
sg535
(lp57659
I608
assS'curl'
p57660
(dp57661
g48
(lp57662
I1390
assS'prevail'
p57663
(dp57664
g80
(lp57665
I2231
assS'lyi'
p57666
(dp57667
g287
(lp57668
I1685
assS'heinemann'
p57669
(dp57670
g106
(lp57671
I2842
assS'constitut'
p57672
(dp57673
g32
(lp57674
sg145
(lp57675
sg130
(lp57676
I1262
asg46
(lp57677
sg108
(lp57678
sg99
(lp57679
ssS'sequenti'
p57680
(dp57681
g332
(lp57682
sg178
(lp57683
sg277
(lp57684
sg76
(lp57685
sg460
(lp57686
sg183
(lp57687
sg384
(lp57688
sg306
(lp57689
sg89
(lp57690
sg91
(lp57691
sg128
(lp57692
sg104
(lp57693
sg132
(lp57694
sg14
(lp57695
sg16
(lp57696
sg20
(lp57697
sg99
(lp57698
sg313
(lp57699
sg354
(lp57700
I51
assS'confinn'
p57701
(dp57702
g149
(lp57703
I2210
assS'priori'
p57704
(dp57705
g74
(lp57706
sg145
(lp57707
sg76
(lp57708
sg293
(lp57709
sg91
(lp57710
sg130
(lp57711
I2179
asg104
(lp57712
sg221
(lp57713
ssS'kqk'
p57714
(dp57715
g102
(lp57716
I2139
assS'asymmetr'
p57717
(dp57718
g245
(lp57719
I2426
asg384
(lp57720
ssS'outp'
p57721
(dp57722
g20
(lp57723
I1562
assS'varadarajan'
p57724
(dp57725
g32
(lp57726
I1327
assS'lend'
p57727
(dp57728
g384
(lp57729
I120
asg114
(lp57730
ssS'mismatch'
p57731
(dp57732
g104
(lp57733
I1103
asg70
(lp57734
sg38
(lp57735
sg181
(lp57736
ssS'binar'
p57737
(dp57738
g63
(lp57739
I584
assS'polynomiali'
p57740
(dp57741
g40
(lp57742
I765
assS'lent'
p57743
(dp57744
g72
(lp57745
I2448
assS'seniolthesi'
p57746
(dp57747
g76
(lp57748
I3336
assS'fisher'
p57749
(dp57750
g36
(lp57751
sg91
(lp57752
I2928
asg26
(lp57753
ssS'papp'
p57754
(dp57755
g163
(lp57756
I848
assS'unaffect'
p57757
(dp57758
g245
(lp57759
sg438
(lp57760
I1159
assS'deserv'
p57761
(dp57762
g72
(lp57763
sg138
(lp57764
I3153
assS'gaussianshap'
p57765
(dp57766
g22
(lp57767
I1137
assS'hspice'
p57768
(dp57769
g20
(lp57770
I1270
assS'wkfikfjk'
p57771
(dp57772
g74
(lp57773
I465
assS'tradeoff'
p57774
(dp57775
g68
(lp57776
sg126
(lp57777
sg85
(lp57778
sg102
(lp57779
sg63
(lp57780
sg140
(lp57781
I396
assS'logadd'
p57782
(dp57783
g10
(lp57784
I2219
assS'qian'
p57785
(dp57786
g26
(lp57787
I323
assS'treue'
p57788
(dp57789
g216
(lp57790
I2017
assS'queri'
p57791
(dp57792
g145
(lp57793
sg295
(lp57794
sg183
(lp57795
sg344
(lp57796
sg223
(lp57797
sg91
(lp57798
sg130
(lp57799
sg104
(lp57800
sg313
(lp57801
sg140
(lp57802
sg354
(lp57803
I2
assS'ellips'
p57804
(dp57805
g14
(lp57806
sg16
(lp57807
I588
asg124
(lp57808
ssS'ellipt'
p57809
(dp57810
g96
(lp57811
I142
asg48
(lp57812
sg8
(lp57813
ssS'antisymmetr'
p57814
(dp57815
g32
(lp57816
I1005
assS'denham'
p57817
(dp57818
g332
(lp57819
I11
assS'sensit'
p57820
(dp57821
g283
(lp57822
sg70
(lp57823
sg256
(lp57824
sg295
(lp57825
sg183
(lp57826
sg59
(lp57827
sg85
(lp57828
sg91
(lp57829
sg245
(lp57830
sg18
(lp57831
sg221
(lp57832
sg313
(lp57833
sg223
(lp57834
sg149
(lp57835
sg118
(lp57836
sg318
(lp57837
sg102
(lp57838
sg63
(lp57839
sg116
(lp57840
sg174
(lp57841
sg332
(lp57842
sg6
(lp57843
sg181
(lp57844
sg384
(lp57845
sg14
(lp57846
sg16
(lp57847
I1990
assS'elsewher'
p57848
(dp57849
g438
(lp57850
I664
asg176
(lp57851
sg76
(lp57852
sg181
(lp57853
sg8
(lp57854
sg110
(lp57855
sg384
(lp57856
sg303
(lp57857
sg78
(lp57858
sg46
(lp57859
sg106
(lp57860
sg99
(lp57861
sg63
(lp57862
ssS'conspicu'
p57863
(dp57864
g8
(lp57865
I1117
assS'mower'
p57866
(dp57867
g438
(lp57868
I2459
assS'conspici'
p57869
(dp57870
g178
(lp57871
I2119
assS'alto'
p57872
(dp57873
g283
(lp57874
I51
assS'becam'
p57875
(dp57876
g78
(lp57877
sg332
(lp57878
sg8
(lp57879
I205
assS'fovea'
p57880
(dp57881
g12
(lp57882
I2887
asg245
(lp57883
sg178
(lp57884
ssS'atlas'
p57885
(dp57886
g313
(lp57887
I2078
assS'maryland'
p57888
(dp57889
g128
(lp57890
I186
assS'fatal'
p57891
(dp57892
g135
(lp57893
I329
assS'passiv'
p57894
(dp57895
g245
(lp57896
sg14
(lp57897
sg140
(lp57898
I2430
assS'queen'
p57899
(dp57900
g132
(lp57901
I864
assS'xxxxxxxx'
p57902
(dp57903
g52
(lp57904
I1172
assS'mous'
p57905
(dp57906
g42
(lp57907
I1972
asg63
(lp57908
sg6
(lp57909
ssS'vlx'
p57910
(dp57911
g460
(lp57912
I884
assS'volum'
p57913
(dp57914
g68
(lp57915
sg163
(lp57916
sg283
(lp57917
sg30
(lp57918
sg76
(lp57919
sg344
(lp57920
sg59
(lp57921
sg303
(lp57922
sg46
(lp57923
sg96
(lp57924
sg99
(lp57925
sg313
(lp57926
sg223
(lp57927
sg20
(lp57928
sg440
(lp57929
sg318
(lp57930
sg121
(lp57931
sg124
(lp57932
sg126
(lp57933
sg50
(lp57934
sg138
(lp57935
sg140
(lp57936
sg354
(lp57937
I3202
assS'kann'
p57938
(dp57939
g40
(lp57940
I2467
assS'pftpoiitao'
p57941
(dp57942
g350
(lp57943
I1260
assS'lter'
p57944
(dp57945
g89
(lp57946
I2450
assS'shock'
p57947
(dp57948
g106
(lp57949
I622
assS'geograph'
p57950
(dp57951
g74
(lp57952
I381
assS'fortun'
p57953
(dp57954
g116
(lp57955
sg72
(lp57956
I1262
asg63
(lp57957
sg85
(lp57958
ssS'veli'
p57959
(dp57960
g36
(lp57961
I1883
assS'synergist'
p57962
(dp57963
g350
(lp57964
I2046
assS'leakag'
p57965
(dp57966
g245
(lp57967
I2420
assS'accomod'
p57968
(dp57969
g104
(lp57970
I295
asg74
(lp57971
sg350
(lp57972
ssS'copenhagen'
p57973
(dp57974
g38
(lp57975
sg140
(lp57976
I13
asg235
(lp57977
ssS'carlo'
p57978
(dp57979
g30
(lp57980
sg74
(lp57981
sg124
(lp57982
sg126
(lp57983
sg313
(lp57984
sg354
(lp57985
I81
assS'iwafgr'
p57986
(dp57987
g293
(lp57988
I3180
assS'hypern'
p57989
(dp57990
g283
(lp57991
I323
assS'iimw'
p57992
(dp57993
g306
(lp57994
I1391
assS'iclii'
p57995
(dp57996
g102
(lp57997
I2715
assS'chicago'
p57998
(dp57999
g99
(lp58000
I271
assS'resembl'
p58001
(dp58002
g174
(lp58003
sg26
(lp58004
sg384
(lp58005
sg484
(lp58006
sg245
(lp58007
sg130
(lp58008
I1691
asg102
(lp58009
sg99
(lp58010
ssS'access'
p58011
(dp58012
g283
(lp58013
sg145
(lp58014
sg293
(lp58015
sg36
(lp58016
sg484
(lp58017
sg38
(lp58018
sg83
(lp58019
sg10
(lp58020
sg306
(lp58021
sg128
(lp58022
I453
asg104
(lp58023
ssS'microprocessor'
p58024
(dp58025
g10
(lp58026
I5
assS'impuls'
p58027
(dp58028
g245
(lp58029
sg174
(lp58030
sg121
(lp58031
sg4
(lp58032
I479
asg256
(lp58033
ssS'deduc'
p58034
(dp58035
g132
(lp58036
I2928
assS'perimet'
p58037
(dp58038
g14
(lp58039
sg16
(lp58040
I832
assS'consolid'
p58041
(dp58042
g245
(lp58043
I61
assS'lovejoy'
p58044
(dp58045
g293
(lp58046
I3240
assS'datalimit'
p58047
(dp58048
g126
(lp58049
I67
assS'adiabat'
p58050
(dp58051
g535
(lp58052
I209
assS'sinm'
p58053
(dp58054
g36
(lp58055
I1441
assS'sink'
p58056
(dp58057
g20
(lp58058
I367
asg256
(lp58059
ssS'sing'
p58060
(dp58061
g116
(lp58062
sg48
(lp58063
sg138
(lp58064
I2
assS'sine'
p58065
(dp58066
g116
(lp58067
sg256
(lp58068
sg313
(lp58069
sg128
(lp58070
I2292
asg22
(lp58071
ssS'sinb'
p58072
(dp58073
g14
(lp58074
sg16
(lp58075
I908
assS'sinc'
p58076
(dp58077
g68
(lp58078
sg78
(lp58079
sg277
(lp58080
sg163
(lp58081
sg72
(lp58082
sg281
(lp58083
sg26
(lp58084
sg80
(lp58085
sg287
(lp58086
sg145
(lp58087
sg256
(lp58088
sg76
(lp58089
sg262
(lp58090
sg295
(lp58091
sg183
(lp58092
sg59
(lp58093
sg484
(lp58094
sg83
(lp58095
sg85
(lp58096
sg303
(lp58097
sg42
(lp58098
I196
asg306
(lp58099
sg87
(lp58100
sg89
(lp58101
sg91
(lp58102
sg12
(lp58103
sg94
(lp58104
sg18
(lp58105
sg221
(lp58106
sg44
(lp58107
sg350
(lp58108
sg230
(lp58109
sg293
(lp58110
sg32
(lp58111
sg245
(lp58112
sg318
(lp58113
sg46
(lp58114
sg102
(lp58115
sg104
(lp58116
sg106
(lp58117
sg63
(lp58118
sg52
(lp58119
sg22
(lp58120
sg216
(lp58121
sg118
(lp58122
sg440
(lp58123
sg332
(lp58124
sg4
(lp58125
sg6
(lp58126
sg235
(lp58127
sg34
(lp58128
sg36
(lp58129
sg384
(lp58130
sg124
(lp58131
sg126
(lp58132
sg341
(lp58133
sg40
(lp58134
sg344
(lp58135
sg223
(lp58136
sg128
(lp58137
sg130
(lp58138
sg132
(lp58139
sg14
(lp58140
sg16
(lp58141
sg135
(lp58142
sg50
(lp58143
sg460
(lp58144
sg140
(lp58145
sg354
(lp58146
ssS'remark'
p58147
(dp58148
g287
(lp58149
sg181
(lp58150
sg384
(lp58151
sg281
(lp58152
sg341
(lp58153
I2148
asg535
(lp58154
ssS'cere'
p58155
(dp58156
g350
(lp58157
I1879
assS'cybernet'
p58158
(dp58159
g181
(lp58160
sg460
(lp58161
sg63
(lp58162
sg429
(lp58163
sg46
(lp58164
sg18
(lp58165
sg110
(lp58166
sg535
(lp58167
sg149
(lp58168
I2823
assS'aobaku'
p58169
(dp58170
g20
(lp58171
I34
assS'dilat'
p58172
(dp58173
g138
(lp58174
I739
assS'conceiv'
p58175
(dp58176
g94
(lp58177
sg287
(lp58178
sg68
(lp58179
sg44
(lp58180
I1735
asg149
(lp58181
ssS'djaferi'
p58182
(dp58183
g83
(lp58184
I2822
assS'cybernei'
p58185
(dp58186
g8
(lp58187
I2563
assS'implement'
p58188
(dp58189
g124
(lp58190
sg70
(lp58191
sg78
(lp58192
sg72
(lp58193
sg293
(lp58194
sg283
(lp58195
sg303
(lp58196
sg30
(lp58197
sg145
(lp58198
sg118
(lp58199
sg295
(lp58200
sg183
(lp58201
sg59
(lp58202
sg38
(lp58203
sg83
(lp58204
sg63
(lp58205
sg42
(lp58206
I547
asg87
(lp58207
sg89
(lp58208
sg68
(lp58209
sg245
(lp58210
sg96
(lp58211
sg48
(lp58212
sg99
(lp58213
sg313
(lp58214
sg44
(lp58215
sg149
(lp58216
sg174
(lp58217
sg18
(lp58218
sg350
(lp58219
sg318
(lp58220
sg102
(lp58221
sg104
(lp58222
sg108
(lp58223
sg110
(lp58224
sg20
(lp58225
sg52
(lp58226
sg22
(lp58227
sg116
(lp58228
sg438
(lp58229
sg440
(lp58230
sg332
(lp58231
sg178
(lp58232
sg4
(lp58233
sg181
(lp58234
sg8
(lp58235
sg34
(lp58236
sg221
(lp58237
sg460
(lp58238
sg235
(lp58239
sg126
(lp58240
sg10
(lp58241
sg40
(lp58242
sg344
(lp58243
sg223
(lp58244
sg128
(lp58245
sg130
(lp58246
sg132
(lp58247
sg14
(lp58248
sg16
(lp58249
sg135
(lp58250
sg50
(lp58251
sg138
(lp58252
sg140
(lp58253
ssS'brauer'
p58254
(dp58255
g163
(lp58256
I127
assS'seeeagl'
p58257
(dp58258
g59
(lp58259
I60
assS'limx'
p58260
(dp58261
g287
(lp58262
I2662
assS'highestfrequ'
p58263
(dp58264
g22
(lp58265
I2096
assS'foundat'
p58266
(dp58267
g230
(lp58268
sg74
(lp58269
sg176
(lp58270
sg145
(lp58271
sg181
(lp58272
sg6
(lp58273
sg163
(lp58274
sg344
(lp58275
sg40
(lp58276
sg341
(lp58277
sg114
(lp58278
sg281
(lp58279
sg429
(lp58280
sg91
(lp58281
sg354
(lp58282
sg26
(lp58283
sg106
(lp58284
I2494
asg108
(lp58285
sg110
(lp58286
sg313
(lp58287
sg149
(lp58288
ssS'limw'
p58289
(dp58290
g341
(lp58291
I1889
assS'limt'
p58292
(dp58293
g34
(lp58294
sg46
(lp58295
I2353
assS'toherent'
p58296
(dp58297
g332
(lp58298
I2251
assS'limh'
p58299
(dp58300
g72
(lp58301
I857
assS'limo'
p58302
(dp58303
g20
(lp58304
I2376
assS'oval'
p58305
(dp58306
g174
(lp58307
sg429
(lp58308
sg89
(lp58309
I1364
asg91
(lp58310
ssS'exploitautotypist'
p58311
(dp58312
g94
(lp58313
I3043
assS'fachbereich'
p58314
(dp58315
g34
(lp58316
I8
assS'chi'
p58317
(dp58318
g59
(lp58319
I3453
assS'anticorrel'
p58320
(dp58321
g70
(lp58322
sg99
(lp58323
I132
asg149
(lp58324
ssS'loizou'
p58325
(dp58326
g281
(lp58327
I933
assS'advoc'
p58328
(dp58329
g68
(lp58330
sg178
(lp58331
sg354
(lp58332
I346
assS'butler'
p58333
(dp58334
g76
(lp58335
I1985
assS'normalizati'
p58336
(dp58337
g96
(lp58338
I737
assS'schmechel'
p58339
(dp58340
g438
(lp58341
I2452
assS'neurotransmitt'
p58342
(dp58343
g106
(lp58344
I136
asg4
(lp58345
ssS'mvjl'
p58346
(dp58347
g130
(lp58348
I2771
assS'iip'
p58349
(dp58350
g130
(lp58351
I1073
assS'iir'
p58352
(dp58353
g121
(lp58354
sg128
(lp58355
I2758
assS'iid'
p58356
(dp58357
g221
(lp58358
I401
assS'transient'
p58359
(dp58360
g70
(lp58361
sg256
(lp58362
sg6
(lp58363
sg384
(lp58364
sg68
(lp58365
sg135
(lp58366
I2295
asg350
(lp58367
ssS'iia'
p58368
(dp58369
g256
(lp58370
I827
assS'iim'
p58371
(dp58372
g138
(lp58373
I364
assS'iii'
p58374
(dp58375
g68
(lp58376
sg70
(lp58377
sg26
(lp58378
sg303
(lp58379
sg42
(lp58380
I610
asg306
(lp58381
sg46
(lp58382
sg48
(lp58383
sg99
(lp58384
sg313
(lp58385
sg223
(lp58386
sg102
(lp58387
sg104
(lp58388
sg108
(lp58389
sg4
(lp58390
sg174
(lp58391
sg32
(lp58392
sg318
(lp58393
sg22
(lp58394
sg6
(lp58395
sg124
(lp58396
sg72
(lp58397
sg281
(lp58398
sg535
(lp58399
sg130
(lp58400
sg132
(lp58401
sg138
(lp58402
sg354
(lp58403
ssS'iij'
p58404
(dp58405
g12
(lp58406
sg118
(lp58407
sg149
(lp58408
I943
assS'tentat'
p58409
(dp58410
g130
(lp58411
I801
assS'account'
p58412
(dp58413
g181
(lp58414
sg30
(lp58415
sg74
(lp58416
sg176
(lp58417
sg76
(lp58418
sg295
(lp58419
sg183
(lp58420
sg80
(lp58421
sg85
(lp58422
sg303
(lp58423
sg460
(lp58424
sg245
(lp58425
sg96
(lp58426
sg48
(lp58427
sg223
(lp58428
sg350
(lp58429
sg12
(lp58430
sg332
(lp58431
sg102
(lp58432
sg106
(lp58433
I1494
asg110
(lp58434
sg22
(lp58435
sg216
(lp58436
sg118
(lp58437
sg440
(lp58438
sg318
(lp58439
sg121
(lp58440
sg4
(lp58441
sg6
(lp58442
sg8
(lp58443
sg384
(lp58444
sg124
(lp58445
sg130
(lp58446
sg149
(lp58447
sg50
(lp58448
sg138
(lp58449
sg354
(lp58450
ssS'alik'
p58451
(dp58452
g116
(lp58453
I1289
assS'f'
p58454
(dp58455
g329
(lp58456
sg78
(lp58457
sg277
(lp58458
sg163
(lp58459
sg72
(lp58460
sg293
(lp58461
sg281
(lp58462
sg85
(lp58463
sg36
(lp58464
sg181
(lp58465
sg40
(lp58466
sg26
(lp58467
sg30
(lp58468
sg287
(lp58469
sg74
(lp58470
sg176
(lp58471
sg145
(lp58472
sg256
(lp58473
sg76
(lp58474
sg262
(lp58475
sg295
(lp58476
sg183
(lp58477
sg59
(lp58478
sg484
(lp58479
sg38
(lp58480
sg83
(lp58481
sg114
(lp58482
sg124
(lp58483
sg42
(lp58484
I99
asg306
(lp58485
sg87
(lp58486
sg80
(lp58487
sg68
(lp58488
sg12
(lp58489
sg46
(lp58490
sg96
(lp58491
sg48
(lp58492
sg99
(lp58493
sg313
(lp58494
sg44
(lp58495
sg350
(lp58496
sg118
(lp58497
sg230
(lp58498
sg174
(lp58499
sg18
(lp58500
sg116
(lp58501
sg178
(lp58502
sg245
(lp58503
sg429
(lp58504
sg318
(lp58505
sg102
(lp58506
sg104
(lp58507
sg108
(lp58508
sg20
(lp58509
sg52
(lp58510
sg22
(lp58511
sg216
(lp58512
sg438
(lp58513
sg440
(lp58514
sg332
(lp58515
sg121
(lp58516
sg4
(lp58517
sg6
(lp58518
sg8
(lp58519
sg34
(lp58520
sg221
(lp58521
sg384
(lp58522
sg235
(lp58523
sg126
(lp58524
sg341
(lp58525
sg535
(lp58526
sg344
(lp58527
sg63
(lp58528
sg223
(lp58529
sg128
(lp58530
sg130
(lp58531
sg132
(lp58532
sg14
(lp58533
sg50
(lp58534
sg460
(lp58535
sg140
(lp58536
sg354
(lp58537
ssS'zeitschrijt'
p58538
(dp58539
g116
(lp58540
I2450
assS'prager'
p58541
(dp58542
g108
(lp58543
I215
assS'erwin'
p58544
(dp58545
g48
(lp58546
I17
assS'aliz'
p58547
(dp58548
g38
(lp58549
I2771
assS'unreal'
p58550
(dp58551
g85
(lp58552
I1912
assS'obvious'
p58553
(dp58554
g230
(lp58555
sg287
(lp58556
sg70
(lp58557
sg163
(lp58558
sg121
(lp58559
sg8
(lp58560
sg295
(lp58561
sg183
(lp58562
sg460
(lp58563
sg68
(lp58564
sg341
(lp58565
sg85
(lp58566
sg223
(lp58567
sg140
(lp58568
I1043
asg78
(lp58569
sg132
(lp58570
sg145
(lp58571
sg221
(lp58572
sg59
(lp58573
sg44
(lp58574
ssS'parkway'
p58575
(dp58576
g106
(lp58577
I303
assS'fetch'
p58578
(dp58579
g10
(lp58580
I661
assS'aliv'
p58581
(dp58582
g293
(lp58583
I501
assS'digitis'
p58584
(dp58585
g52
(lp58586
I1909
assS'blasdel'
p58587
(dp58588
g48
(lp58589
sg149
(lp58590
I2984
assS'delbruck'
p58591
(dp58592
g245
(lp58593
I1433
assS'downhil'
p58594
(dp58595
g46
(lp58596
I1000
assS'vehrencamp'
p58597
(dp58598
g116
(lp58599
I2344
assS'onlin'
p58600
(dp58601
g135
(lp58602
I1678
asg83
(lp58603
sg114
(lp58604
ssS'tan'
p58605
(dp58606
g32
(lp58607
sg332
(lp58608
I899
asg350
(lp58609
ssS'everywher'
p58610
(dp58611
g329
(lp58612
I1051
asg287
(lp58613
ssS'surfac'
p58614
(dp58615
g30
(lp58616
sg74
(lp58617
sg176
(lp58618
sg80
(lp58619
sg183
(lp58620
sg59
(lp58621
sg46
(lp58622
sg44
(lp58623
sg149
(lp58624
sg118
(lp58625
sg429
(lp58626
sg110
(lp58627
sg438
(lp58628
I1766
asg318
(lp58629
sg121
(lp58630
sg181
(lp58631
sg8
(lp58632
sg34
(lp58633
sg460
(lp58634
sg124
(lp58635
sg341
(lp58636
sg14
(lp58637
sg16
(lp58638
ssS'afo'
p58639
(dp58640
g130
(lp58641
I2545
assS'andersen'
p58642
(dp58643
g216
(lp58644
I2020
asg303
(lp58645
ssS'run'
p58646
(dp58647
g329
(lp58648
sg70
(lp58649
sg26
(lp58650
sg277
(lp58651
sg287
(lp58652
sg74
(lp58653
sg145
(lp58654
sg293
(lp58655
sg344
(lp58656
sg183
(lp58657
sg59
(lp58658
sg42
(lp58659
I3038
asg87
(lp58660
sg89
(lp58661
sg318
(lp58662
sg20
(lp58663
sg48
(lp58664
sg313
(lp58665
sg149
(lp58666
sg135
(lp58667
sg32
(lp58668
sg332
(lp58669
sg104
(lp58670
sg52
(lp58671
sg114
(lp58672
sg174
(lp58673
sg440
(lp58674
sg18
(lp58675
sg178
(lp58676
sg4
(lp58677
sg8
(lp58678
sg460
(lp58679
sg124
(lp58680
sg126
(lp58681
sg10
(lp58682
sg128
(lp58683
sg78
(lp58684
sg350
(lp58685
sg50
(lp58686
sg138
(lp58687
sg140
(lp58688
sg354
(lp58689
ssS'likewis'
p58690
(dp58691
g46
(lp58692
sg114
(lp58693
sg70
(lp58694
sg341
(lp58695
I556
asg329
(lp58696
ssS'compuat'
p58697
(dp58698
g78
(lp58699
I3068
assS'xxxx'
p58700
(dp58701
g52
(lp58702
I1173
assS'physio'
p58703
(dp58704
g99
(lp58705
I42
assS'inst'
p58706
(dp58707
g102
(lp58708
I3642
asg174
(lp58709
sg460
(lp58710
sg6
(lp58711
sg36
(lp58712
ssS'redund'
p58713
(dp58714
g145
(lp58715
sg256
(lp58716
sg163
(lp58717
sg102
(lp58718
sg94
(lp58719
sg50
(lp58720
sg354
(lp58721
I203
assS'philosophi'
p58722
(dp58723
g72
(lp58724
I800
assS'physic'
p58725
(dp58726
g163
(lp58727
sg30
(lp58728
sg74
(lp58729
sg176
(lp58730
sg256
(lp58731
sg262
(lp58732
sg78
(lp58733
sg484
(lp58734
sg85
(lp58735
sg303
(lp58736
sg42
(lp58737
I551
asg12
(lp58738
sg46
(lp58739
sg99
(lp58740
sg149
(lp58741
sg174
(lp58742
sg429
(lp58743
sg102
(lp58744
sg104
(lp58745
sg438
(lp58746
sg32
(lp58747
sg318
(lp58748
sg235
(lp58749
sg36
(lp58750
sg384
(lp58751
sg124
(lp58752
sg126
(lp58753
sg341
(lp58754
sg40
(lp58755
sg130
(lp58756
sg14
(lp58757
sg16
(lp58758
sg460
(lp58759
sg354
(lp58760
ssS'acx'
p58761
(dp58762
g26
(lp58763
I1376
assS'rotmion'
p58764
(dp58765
g350
(lp58766
I2136
assS'bind'
p58767
(dp58768
g70
(lp58769
sg178
(lp58770
sg26
(lp58771
sg181
(lp58772
sg149
(lp58773
I2480
assS'correspond'
p58774
(dp58775
g124
(lp58776
sg70
(lp58777
sg78
(lp58778
sg163
(lp58779
sg72
(lp58780
sg68
(lp58781
sg281
(lp58782
sg283
(lp58783
sg460
(lp58784
sg303
(lp58785
sg26
(lp58786
sg30
(lp58787
sg287
(lp58788
sg74
(lp58789
sg145
(lp58790
sg256
(lp58791
sg76
(lp58792
sg118
(lp58793
sg295
(lp58794
sg183
(lp58795
sg59
(lp58796
sg80
(lp58797
sg38
(lp58798
sg83
(lp58799
sg85
(lp58800
sg63
(lp58801
sg42
(lp58802
I3201
asg87
(lp58803
sg89
(lp58804
sg91
(lp58805
sg245
(lp58806
sg94
(lp58807
sg96
(lp58808
sg48
(lp58809
sg221
(lp58810
sg313
(lp58811
sg44
(lp58812
sg149
(lp58813
sg230
(lp58814
sg329
(lp58815
sg32
(lp58816
sg350
(lp58817
sg429
(lp58818
sg318
(lp58819
sg102
(lp58820
sg104
(lp58821
sg108
(lp58822
sg110
(lp58823
sg178
(lp58824
sg22
(lp58825
sg216
(lp58826
sg438
(lp58827
sg440
(lp58828
sg332
(lp58829
sg121
(lp58830
sg4
(lp58831
sg8
(lp58832
sg34
(lp58833
sg36
(lp58834
sg384
(lp58835
sg235
(lp58836
sg126
(lp58837
sg341
(lp58838
sg40
(lp58839
sg344
(lp58840
sg223
(lp58841
sg130
(lp58842
sg132
(lp58843
sg14
(lp58844
sg16
(lp58845
sg135
(lp58846
sg138
(lp58847
sg140
(lp58848
sg354
(lp58849
ssS'fjmax'
p58850
(dp58851
g38
(lp58852
I2714
assS'prepositus'
p58853
(dp58854
g350
(lp58855
I990
assS'atnr'
p58856
(dp58857
g48
(lp58858
I603
assS'linea'
p58859
(dp58860
g163
(lp58861
I2001
assS'cyij'
p58862
(dp58863
g80
(lp58864
I613
assS'symbola'
p58865
(dp58866
g72
(lp58867
I976
assS'elesvi'
p58868
(dp58869
g281
(lp58870
I2448
assS'chrx'
p58871
(dp58872
g38
(lp58873
I3304
assS'bunch'
p58874
(dp58875
g181
(lp58876
I515
assS'lf'
p58877
(dp58878
g354
(lp58879
I1908
assS'lg'
p58880
(dp58881
g354
(lp58882
I1491
assS'ld'
p58883
(dp58884
g102
(lp58885
I2754
asg230
(lp58886
sg221
(lp58887
sg183
(lp58888
ssS'le'
p58889
(dp58890
g26
(lp58891
sg283
(lp58892
sg121
(lp58893
sg76
(lp58894
sg181
(lp58895
sg344
(lp58896
sg221
(lp58897
sg484
(lp58898
sg281
(lp58899
sg178
(lp58900
sg40
(lp58901
sg306
(lp58902
sg52
(lp58903
sg46
(lp58904
sg102
(lp58905
sg94
(lp58906
sg20
(lp58907
sg50
(lp58908
sg138
(lp58909
I3249
asg44
(lp58910
sg36
(lp58911
ssS'lb'
p58912
(dp58913
g306
(lp58914
sg429
(lp58915
sg128
(lp58916
sg354
(lp58917
I1497
assS'lc'
p58918
(dp58919
g174
(lp58920
sg20
(lp58921
sg178
(lp58922
sg128
(lp58923
I690
assS'la'
p58924
(dp58925
g145
(lp58926
sg262
(lp58927
sg295
(lp58928
sg183
(lp58929
sg38
(lp58930
sg303
(lp58931
sg42
(lp58932
I1663
asg91
(lp58933
sg96
(lp58934
sg18
(lp58935
sg221
(lp58936
sg535
(lp58937
sg350
(lp58938
sg116
(lp58939
sg429
(lp58940
sg230
(lp58941
sg174
(lp58942
sg318
(lp58943
sg121
(lp58944
sg4
(lp58945
sg8
(lp58946
sg36
(lp58947
sg124
(lp58948
sg72
(lp58949
sg128
(lp58950
sg50
(lp58951
sg138
(lp58952
sg140
(lp58953
ssS'ln'
p58954
(dp58955
g8
(lp58956
sg295
(lp58957
sg183
(lp58958
sg163
(lp58959
sg38
(lp58960
sg341
(lp58961
sg344
(lp58962
sg106
(lp58963
I2520
assS'lo'
p58964
(dp58965
g230
(lp58966
sg438
(lp58967
sg121
(lp58968
sg8
(lp58969
sg344
(lp58970
sg42
(lp58971
I1666
asg18
(lp58972
ssS'll'
p58973
(dp58974
g283
(lp58975
sg287
(lp58976
sg293
(lp58977
sg484
(lp58978
sg306
(lp58979
sg46
(lp58980
sg20
(lp58981
sg18
(lp58982
sg99
(lp58983
sg535
(lp58984
sg223
(lp58985
sg104
(lp58986
sg106
(lp58987
I1254
asg108
(lp58988
sg110
(lp58989
sg96
(lp58990
sg114
(lp58991
sg121
(lp58992
sg8
(lp58993
sg44
(lp58994
sg128
(lp58995
sg130
(lp58996
ssS'lm'
p58997
(dp58998
g438
(lp58999
I1401
asg283
(lp59000
sg178
(lp59001
sg22
(lp59002
sg72
(lp59003
sg245
(lp59004
ssS'lj'
p59005
(dp59006
g256
(lp59007
sg34
(lp59008
sg36
(lp59009
sg102
(lp59010
sg130
(lp59011
I949
asg12
(lp59012
sg22
(lp59013
ssS'lk'
p59014
(dp59015
g178
(lp59016
sg484
(lp59017
sg221
(lp59018
I1789
assS'lh'
p59019
(dp59020
g183
(lp59021
sg128
(lp59022
I223
asg8
(lp59023
ssS'labor'
p59024
(dp59025
g91
(lp59026
I137
asg85
(lp59027
ssS'lv'
p59028
(dp59029
g20
(lp59030
sg8
(lp59031
sg80
(lp59032
sg149
(lp59033
I3060
assS'lw'
p59034
(dp59035
g36
(lp59036
sg306
(lp59037
I2023
asg74
(lp59038
ssS'lt'
p59039
(dp59040
g230
(lp59041
sg440
(lp59042
sg318
(lp59043
sg262
(lp59044
sg124
(lp59045
sg40
(lp59046
sg130
(lp59047
I2469
asg102
(lp59048
sg18
(lp59049
sg350
(lp59050
ssS'lu'
p59051
(dp59052
g22
(lp59053
sg80
(lp59054
sg8
(lp59055
sg460
(lp59056
sg87
(lp59057
sg128
(lp59058
I1602
assS'lr'
p59059
(dp59060
g221
(lp59061
sg306
(lp59062
sg99
(lp59063
I3445
asg32
(lp59064
sg114
(lp59065
ssS'ls'
p59066
(dp59067
g230
(lp59068
sg106
(lp59069
I1989
asg176
(lp59070
sg384
(lp59071
ssS'lp'
p59072
(dp59073
g145
(lp59074
sg68
(lp59075
sg221
(lp59076
sg91
(lp59077
I831
asg281
(lp59078
ssS'lq'
p59079
(dp59080
g440
(lp59081
I465
asg460
(lp59082
ssS'tsw'
p59083
(dp59084
g114
(lp59085
I2129
assS'tst'
p59086
(dp59087
g6
(lp59088
I974
assS'dq'
p59089
(dp59090
g163
(lp59091
I618
assS'lz'
p59092
(dp59093
g32
(lp59094
I1902
assS'tsp'
p59095
(dp59096
g8
(lp59097
I2542
assS'ly'
p59098
(dp59099
g32
(lp59100
sg318
(lp59101
sg72
(lp59102
sg106
(lp59103
I956
asg18
(lp59104
sg313
(lp59105
ssS'junction'
p59106
(dp59107
g344
(lp59108
sg118
(lp59109
sg306
(lp59110
I263
asg429
(lp59111
ssS'greater'
p59112
(dp59113
g116
(lp59114
sg329
(lp59115
sg74
(lp59116
sg121
(lp59117
sg80
(lp59118
sg262
(lp59119
sg78
(lp59120
sg68
(lp59121
sg10
(lp59122
sg118
(lp59123
sg429
(lp59124
sg70
(lp59125
sg145
(lp59126
sg135
(lp59127
I1042
asg221
(lp59128
sg63
(lp59129
sg114
(lp59130
ssS'dac'
p59131
(dp59132
g14
(lp59133
sg16
(lp59134
I1914
assS'dan'
p59135
(dp59136
g460
(lp59137
sg87
(lp59138
I13
asg181
(lp59139
ssS'spell'
p59140
(dp59141
g42
(lp59142
I2125
asg94
(lp59143
sg114
(lp59144
ssS'dat'
p59145
(dp59146
g48
(lp59147
I1818
assS'mention'
p59148
(dp59149
g287
(lp59150
sg32
(lp59151
sg178
(lp59152
sg4
(lp59153
I1392
asg235
(lp59154
sg295
(lp59155
sg183
(lp59156
sg126
(lp59157
sg341
(lp59158
sg72
(lp59159
sg535
(lp59160
ssS'furst'
p59161
(dp59162
g145
(lp59163
I3026
assS'wowi'
p59164
(dp59165
g341
(lp59166
I2636
assS'sij'
p59167
(dp59168
g74
(lp59169
sg178
(lp59170
I704
assS'day'
p59171
(dp59172
g116
(lp59173
sg72
(lp59174
sg83
(lp59175
sg94
(lp59176
sg132
(lp59177
sg14
(lp59178
sg106
(lp59179
I122
asg99
(lp59180
sg16
(lp59181
sg96
(lp59182
sg114
(lp59183
ssS'loliiin'
p59184
(dp59185
g106
(lp59186
I2002
assS'miikkulainen'
p59187
(dp59188
g149
(lp59189
I21
assS'februari'
p59190
(dp59191
g341
(lp59192
I2814
assS'disregard'
p59193
(dp59194
g32
(lp59195
I2757
asg22
(lp59196
ssS'feldman'
p59197
(dp59198
g429
(lp59199
sg10
(lp59200
I2622
assS'artola'
p59201
(dp59202
g149
(lp59203
I3041
assS'lff'
p59204
(dp59205
g22
(lp59206
I306
assS'thwart'
p59207
(dp59208
g44
(lp59209
I1344
assS'lfc'
p59210
(dp59211
g332
(lp59212
I876
assS'hinen'
p59213
(dp59214
g38
(lp59215
I2945
assS'intellig'
p59216
(dp59217
g277
(lp59218
sg281
(lp59219
sg145
(lp59220
sg293
(lp59221
sg295
(lp59222
sg183
(lp59223
sg91
(lp59224
sg94
(lp59225
sg20
(lp59226
sg221
(lp59227
sg429
(lp59228
sg63
(lp59229
sg174
(lp59230
sg332
(lp59231
sg126
(lp59232
sg341
(lp59233
sg344
(lp59234
sg128
(lp59235
sg78
(lp59236
sg132
(lp59237
sg140
(lp59238
sg354
(lp59239
I3113
assS'strip'
p59240
(dp59241
g12
(lp59242
I351
asg256
(lp59243
sg76
(lp59244
ssS'marler'
p59245
(dp59246
g116
(lp59247
I223
assS'fluctuat'
p59248
(dp59249
g121
(lp59250
sg235
(lp59251
sg354
(lp59252
I1743
asg18
(lp59253
sg140
(lp59254
sg350
(lp59255
ssS'waaa'
p59256
(dp59257
g140
(lp59258
I778
assS'grajski'
p59259
(dp59260
g176
(lp59261
I411
assS'res'
p59262
(dp59263
g438
(lp59264
sg4
(lp59265
sg6
(lp59266
sg42
(lp59267
I3414
asg12
(lp59268
sg181
(lp59269
sg106
(lp59270
sg99
(lp59271
sg63
(lp59272
ssS'rep'
p59273
(dp59274
g32
(lp59275
sg91
(lp59276
I3117
asg235
(lp59277
ssS'orientat'
p59278
(dp59279
g48
(lp59280
I734
assS'cwt'
p59281
(dp59282
g22
(lp59283
I454
assS'ret'
p59284
(dp59285
g22
(lp59286
I696
asg262
(lp59287
ssS'posteriori'
p59288
(dp59289
g440
(lp59290
I8
asg87
(lp59291
sg221
(lp59292
sg76
(lp59293
ssS'mate'
p59294
(dp59295
g132
(lp59296
I2729
asg30
(lp59297
ssS'ren'
p59298
(dp59299
g26
(lp59300
I23
assS'reo'
p59301
(dp59302
g36
(lp59303
I1440
assS'istituto'
p59304
(dp59305
g96
(lp59306
I11
assS'rec'
p59307
(dp59308
g181
(lp59309
I2490
assS'rea'
p59310
(dp59311
g4
(lp59312
I1563
assS'ref'
p59313
(dp59314
g20
(lp59315
I455
assS'math'
p59316
(dp59317
g174
(lp59318
sg32
(lp59319
sg176
(lp59320
sg145
(lp59321
sg36
(lp59322
sg38
(lp59323
sg281
(lp59324
sg10
(lp59325
sg40
(lp59326
sg70
(lp59327
sg104
(lp59328
sg106
(lp59329
I2942
asg114
(lp59330
ssS'clarifi'
p59331
(dp59332
g42
(lp59333
I219
asg135
(lp59334
sg72
(lp59335
ssS'insid'
p59336
(dp59337
g174
(lp59338
sg318
(lp59339
sg121
(lp59340
sg42
(lp59341
I2275
asg130
(lp59342
sg104
(lp59343
sg18
(lp59344
sg350
(lp59345
ssS'frank'
p59346
(dp59347
g354
(lp59348
I2918
assS'franl'
p59349
(dp59350
g429
(lp59351
I1142
assS'mjpl'
p59352
(dp59353
g230
(lp59354
I1804
assS'andlor'
p59355
(dp59356
g68
(lp59357
I2112
assS'releas'
p59358
(dp59359
g106
(lp59360
I134
asg59
(lp59361
ssS'salama'
p59362
(dp59363
g48
(lp59364
I2179
assS'dmax'
p59365
(dp59366
g85
(lp59367
I2404
assS'likelihood'
p59368
(dp59369
g26
(lp59370
sg72
(lp59371
sg30
(lp59372
sg74
(lp59373
sg145
(lp59374
sg76
(lp59375
sg293
(lp59376
sg42
(lp59377
I858
asg87
(lp59378
sg91
(lp59379
sg12
(lp59380
sg94
(lp59381
sg221
(lp59382
sg313
(lp59383
sg329
(lp59384
sg440
(lp59385
sg318
(lp59386
sg36
(lp59387
sg460
(lp59388
sg124
(lp59389
sg126
(lp59390
sg130
(lp59391
sg138
(lp59392
ssS'afterward'
p59393
(dp59394
g295
(lp59395
I3517
asg183
(lp59396
ssS'complianc'
p59397
(dp59398
g460
(lp59399
I305
assS'dirichlet'
p59400
(dp59401
g221
(lp59402
I975
assS'yare'
p59403
(dp59404
g118
(lp59405
I1110
asg38
(lp59406
sg163
(lp59407
ssS'neckti'
p59408
(dp59409
g181
(lp59410
I513
assS'llsed'
p59411
(dp59412
g108
(lp59413
I2388
assS'accumulatiun'
p59414
(dp59415
g245
(lp59416
I2214
assS'mortal'
p59417
(dp59418
g277
(lp59419
I373
assS'minken'
p59420
(dp59421
g32
(lp59422
I414
assS'retain'
p59423
(dp59424
g230
(lp59425
sg438
(lp59426
I556
asg121
(lp59427
sg80
(lp59428
sg8
(lp59429
sg344
(lp59430
sg14
(lp59431
sg99
(lp59432
sg149
(lp59433
ssS'south'
p59434
(dp59435
g106
(lp59436
I304
assS'attenu'
p59437
(dp59438
g102
(lp59439
I2316
asg118
(lp59440
ssS'sach'
p59441
(dp59442
g174
(lp59443
I563
assS'advisor'
p59444
(dp59445
g59
(lp59446
I287
assS'hey'
p59447
(dp59448
g318
(lp59449
I764
assS'facil'
p59450
(dp59451
g295
(lp59452
sg183
(lp59453
sg135
(lp59454
I849
assS'suffic'
p59455
(dp59456
g277
(lp59457
sg163
(lp59458
sg341
(lp59459
sg42
(lp59460
I2754
asg132
(lp59461
sg135
(lp59462
sg63
(lp59463
ssS'liitmann'
p59464
(dp59465
g59
(lp59466
I2453
assS'ixd'
p59467
(dp59468
g76
(lp59469
I663
assS'monkey'
p59470
(dp59471
g216
(lp59472
sg176
(lp59473
sg6
(lp59474
sg181
(lp59475
sg99
(lp59476
I3236
asg350
(lp59477
ssS'pott'
p59478
(dp59479
g26
(lp59480
I2328
assS'messag'
p59481
(dp59482
g6
(lp59483
I2332
assS'tierpsychogi'
p59484
(dp59485
g116
(lp59486
I2452
assS'oysmooth'
p59487
(dp59488
g89
(lp59489
I1234
assS'basilar'
p59490
(dp59491
g174
(lp59492
I250
assS'biochem'
p59493
(dp59494
g130
(lp59495
I1730
assS'oftoday'
p59496
(dp59497
g132
(lp59498
I108
assS'ogi'
p59499
(dp59500
g440
(lp59501
I2467
assS'spiegel'
p59502
(dp59503
g245
(lp59504
I14
assS'hen'
p59505
(dp59506
g10
(lp59507
I606
assS'milleret'
p59508
(dp59509
g149
(lp59510
I2282
assS'prune'
p59511
(dp59512
g295
(lp59513
sg183
(lp59514
sg344
(lp59515
sg87
(lp59516
sg44
(lp59517
I2368
asg149
(lp59518
ssS'embodi'
p59519
(dp59520
g132
(lp59521
sg438
(lp59522
I1202
asg74
(lp59523
sg332
(lp59524
sg181
(lp59525
ssS'ballard'
p59526
(dp59527
g429
(lp59528
sg138
(lp59529
I1510
asg181
(lp59530
ssS'structur'
p59531
(dp59532
g283
(lp59533
sg70
(lp59534
sg26
(lp59535
sg163
(lp59536
sg72
(lp59537
sg181
(lp59538
sg30
(lp59539
sg74
(lp59540
sg80
(lp59541
sg293
(lp59542
sg295
(lp59543
sg183
(lp59544
sg59
(lp59545
sg38
(lp59546
sg85
(lp59547
sg42
(lp59548
I270
asg87
(lp59549
sg245
(lp59550
sg94
(lp59551
sg96
(lp59552
sg48
(lp59553
sg221
(lp59554
sg44
(lp59555
sg350
(lp59556
sg116
(lp59557
sg12
(lp59558
sg429
(lp59559
sg104
(lp59560
sg106
(lp59561
sg108
(lp59562
sg110
(lp59563
sg52
(lp59564
sg114
(lp59565
sg230
(lp59566
sg32
(lp59567
sg18
(lp59568
sg178
(lp59569
sg4
(lp59570
sg6
(lp59571
sg8
(lp59572
sg68
(lp59573
sg126
(lp59574
sg344
(lp59575
sg128
(lp59576
sg130
(lp59577
sg14
(lp59578
sg16
(lp59579
sg149
(lp59580
ssS'charact'
p59581
(dp59582
g30
(lp59583
sg283
(lp59584
sg178
(lp59585
sg4
(lp59586
sg181
(lp59587
sg76
(lp59588
sg183
(lp59589
sg63
(lp59590
sg42
(lp59591
I1638
asg52
(lp59592
sg102
(lp59593
sg94
(lp59594
sg138
(lp59595
sg44
(lp59596
sg114
(lp59597
ssS'lover'
p59598
(dp59599
g344
(lp59600
I1817
assS'trento'
p59601
(dp59602
g96
(lp59603
I19
assS'iftj'
p59604
(dp59605
g104
(lp59606
I1640
assS'idav'
p59607
(dp59608
g223
(lp59609
I713
assS'thereaft'
p59610
(dp59611
g104
(lp59612
I2199
assS'mfld'
p59613
(dp59614
g72
(lp59615
I1321
assS'have'
p59616
(dp59617
g80
(lp59618
sg293
(lp59619
sg344
(lp59620
sg78
(lp59621
sg59
(lp59622
sg484
(lp59623
sg38
(lp59624
sg83
(lp59625
sg85
(lp59626
sg303
(lp59627
sg438
(lp59628
sg116
(lp59629
sg118
(lp59630
sg34
(lp59631
sg36
(lp59632
sg460
(lp59633
sg68
(lp59634
sg72
(lp59635
sg281
(lp59636
sg10
(lp59637
sg40
(lp59638
sg283
(lp59639
sg70
(lp59640
sg26
(lp59641
sg277
(lp59642
sg163
(lp59643
sg89
(lp59644
sg91
(lp59645
sg12
(lp59646
sg94
(lp59647
sg96
(lp59648
sg48
(lp59649
sg99
(lp59650
sg313
(lp59651
sg44
(lp59652
sg149
(lp59653
sg429
(lp59654
sg102
(lp59655
sg104
(lp59656
sg106
(lp59657
sg108
(lp59658
sg110
(lp59659
sg63
(lp59660
sg52
(lp59661
sg114
(lp59662
sg128
(lp59663
sg130
(lp59664
sg132
(lp59665
sg14
(lp59666
sg16
(lp59667
sg135
(lp59668
sg50
(lp59669
sg138
(lp59670
sg140
(lp59671
sg354
(lp59672
sg306
(lp59673
sg87
(lp59674
sg46
(lp59675
sg20
(lp59676
sg18
(lp59677
sg221
(lp59678
sg535
(lp59679
sg223
(lp59680
sg350
(lp59681
sg216
(lp59682
sg174
(lp59683
sg440
(lp59684
sg332
(lp59685
sg121
(lp59686
sg4
(lp59687
sg6
(lp59688
sg8
(lp59689
sg126
(lp59690
sg341
(lp59691
sg30
(lp59692
sg287
(lp59693
sg74
(lp59694
sg176
(lp59695
sg145
(lp59696
sg256
(lp59697
sg76
(lp59698
sg262
(lp59699
sg295
(lp59700
sg183
(lp59701
sg42
(lp59702
I590
asg230
(lp59703
sg329
(lp59704
sg32
(lp59705
sg318
(lp59706
sg178
(lp59707
sg22
(lp59708
sg181
(lp59709
sg235
(lp59710
sg384
(lp59711
sg124
(lp59712
ssS'victorrio'
p59713
(dp59714
g44
(lp59715
I2623
assS'mik'
p59716
(dp59717
g74
(lp59718
I1869
assS'mij'
p59719
(dp59720
g306
(lp59721
sg535
(lp59722
I1464
assS'miyak'
p59723
(dp59724
g108
(lp59725
I220
asg181
(lp59726
sg114
(lp59727
ssS'min'
p59728
(dp59729
g318
(lp59730
sg34
(lp59731
sg72
(lp59732
sg126
(lp59733
sg85
(lp59734
sg42
(lp59735
I1227
asg245
(lp59736
sg306
(lp59737
sg283
(lp59738
sg130
(lp59739
sg132
(lp59740
sg14
(lp59741
sg106
(lp59742
ssS'mia'
p59743
(dp59744
g130
(lp59745
I1357
assS'mid'
p59746
(dp59747
g116
(lp59748
sg10
(lp59749
I2344
assS'olcillalinl'
p59750
(dp59751
g20
(lp59752
I1242
assS'mix'
p59753
(dp59754
g440
(lp59755
sg22
(lp59756
sg8
(lp59757
sg68
(lp59758
sg72
(lp59759
sg303
(lp59760
sg46
(lp59761
sg135
(lp59762
I1753
asg313
(lp59763
ssS'yishay'
p59764
(dp59765
g145
(lp59766
I8
asg85
(lp59767
ssS'lwlckl'
p59768
(dp59769
g235
(lp59770
I1606
assS'mip'
p59771
(dp59772
g126
(lp59773
I1994
asg10
(lp59774
sg52
(lp59775
sg83
(lp59776
ssS'mis'
p59777
(dp59778
g245
(lp59779
I1946
assS'uninterpret'
p59780
(dp59781
g74
(lp59782
I728
assS'mit'
p59783
(dp59784
g283
(lp59785
sg70
(lp59786
sg303
(lp59787
sg30
(lp59788
sg74
(lp59789
sg262
(lp59790
sg295
(lp59791
sg183
(lp59792
sg83
(lp59793
sg63
(lp59794
sg306
(lp59795
sg12
(lp59796
sg20
(lp59797
sg18
(lp59798
sg99
(lp59799
sg313
(lp59800
sg223
(lp59801
sg350
(lp59802
sg116
(lp59803
sg293
(lp59804
sg429
(lp59805
sg332
(lp59806
sg108
(lp59807
sg96
(lp59808
sg114
(lp59809
sg230
(lp59810
sg329
(lp59811
sg440
(lp59812
sg318
(lp59813
sg181
(lp59814
sg235
(lp59815
sg36
(lp59816
sg460
(lp59817
sg124
(lp59818
sg40
(lp59819
sg344
(lp59820
sg78
(lp59821
sg132
(lp59822
sg138
(lp59823
I3298
assS'miw'
p59824
(dp59825
g306
(lp59826
I1333
assS'miv'
p59827
(dp59828
g130
(lp59829
I940
assS'expertis'
p59830
(dp59831
g295
(lp59832
I242
asg183
(lp59833
sg34
(lp59834
ssS'unless'
p59835
(dp59836
g287
(lp59837
sg293
(lp59838
sg484
(lp59839
sg83
(lp59840
sg429
(lp59841
sg91
(lp59842
sg108
(lp59843
I1087
assS'preliminari'
p59844
(dp59845
g329
(lp59846
sg74
(lp59847
sg4
(lp59848
sg277
(lp59849
sg183
(lp59850
sg341
(lp59851
sg63
(lp59852
sg429
(lp59853
sg94
(lp59854
sg96
(lp59855
sg110
(lp59856
sg138
(lp59857
I3092
asg26
(lp59858
ssS'mimick'
p59859
(dp59860
g63
(lp59861
I215
assS'clog'
p59862
(dp59863
g78
(lp59864
I2843
assS'eight'
p59865
(dp59866
g116
(lp59867
sg74
(lp59868
sg283
(lp59869
sg78
(lp59870
sg68
(lp59871
sg104
(lp59872
sg14
(lp59873
sg135
(lp59874
sg110
(lp59875
sg138
(lp59876
I660
asg52
(lp59877
ssS'sedal'
p59878
(dp59879
g135
(lp59880
I36
assS'transcript'
p59881
(dp59882
g96
(lp59883
I1554
asg91
(lp59884
sg76
(lp59885
ssS'complementari'
p59886
(dp59887
g102
(lp59888
I1468
asg460
(lp59889
ssS'homogen'
p59890
(dp59891
g74
(lp59892
sg48
(lp59893
I986
asg83
(lp59894
sg318
(lp59895
ssS'bregman'
p59896
(dp59897
g332
(lp59898
I209
assS'gather'
p59899
(dp59900
g4
(lp59901
I1608
asg223
(lp59902
sg114
(lp59903
ssS'request'
p59904
(dp59905
g96
(lp59906
I989
asg91
(lp59907
sg10
(lp59908
sg74
(lp59909
ssS'oelec'
p59910
(dp59911
g121
(lp59912
I19
assS'takeuchi'
p59913
(dp59914
g176
(lp59915
I190
assS'llina'
p59916
(dp59917
g332
(lp59918
I168
assS'occasion'
p59919
(dp59920
g30
(lp59921
sg174
(lp59922
sg68
(lp59923
sg34
(lp59924
sg6
(lp59925
I1904
assS'text'
p59926
(dp59927
g30
(lp59928
sg174
(lp59929
sg70
(lp59930
sg318
(lp59931
sg178
(lp59932
sg6
(lp59933
sg163
(lp59934
sg295
(lp59935
sg183
(lp59936
sg140
(lp59937
I1532
asg102
(lp59938
sg94
(lp59939
sg96
(lp59940
sg48
(lp59941
sg44
(lp59942
sg114
(lp59943
ssS'gurney'
p59944
(dp59945
g283
(lp59946
I346
assS'ljcnn'
p59947
(dp59948
g59
(lp59949
I3335
assS'empir'
p59950
(dp59951
g163
(lp59952
sg287
(lp59953
sg176
(lp59954
sg145
(lp59955
sg80
(lp59956
sg295
(lp59957
sg183
(lp59958
sg85
(lp59959
sg89
(lp59960
sg313
(lp59961
sg223
(lp59962
sg110
(lp59963
sg329
(lp59964
sg318
(lp59965
sg181
(lp59966
sg36
(lp59967
sg72
(lp59968
sg281
(lp59969
sg344
(lp59970
sg130
(lp59971
sg132
(lp59972
sg50
(lp59973
I656
assS'gori'
p59974
(dp59975
g341
(lp59976
sg128
(lp59977
I2802
assS'viewiag'
p59978
(dp59979
g181
(lp59980
I59
assS'texa'
p59981
(dp59982
g281
(lp59983
sg149
(lp59984
I28
assS'categor'
p59985
(dp59986
g116
(lp59987
sg74
(lp59988
sg344
(lp59989
sg484
(lp59990
sg128
(lp59991
I59
asg110
(lp59992
sg44
(lp59993
ssS'jnt'
p59994
(dp59995
g306
(lp59996
I38
assS'nonsequenti'
p59997
(dp59998
g354
(lp59999
I3142
assS'bernard'
p60000
(dp60001
g132
(lp60002
I3756
assS'controli'
p60003
(dp60004
g230
(lp60005
I632
assS'conclus'
p60006
(dp60007
g68
(lp60008
sg26
(lp60009
sg283
(lp60010
sg85
(lp60011
sg74
(lp60012
sg176
(lp60013
sg145
(lp60014
sg256
(lp60015
sg76
(lp60016
sg118
(lp60017
sg183
(lp60018
sg83
(lp60019
sg114
(lp60020
sg63
(lp60021
sg42
(lp60022
I3233
asg306
(lp60023
sg87
(lp60024
sg89
(lp60025
sg91
(lp60026
sg245
(lp60027
sg46
(lp60028
sg96
(lp60029
sg48
(lp60030
sg221
(lp60031
sg535
(lp60032
sg44
(lp60033
sg149
(lp60034
sg329
(lp60035
sg32
(lp60036
sg350
(lp60037
sg429
(lp60038
sg104
(lp60039
sg106
(lp60040
sg108
(lp60041
sg110
(lp60042
sg20
(lp60043
sg52
(lp60044
sg22
(lp60045
sg174
(lp60046
sg440
(lp60047
sg318
(lp60048
sg121
(lp60049
sg4
(lp60050
sg6
(lp60051
sg8
(lp60052
sg34
(lp60053
sg36
(lp60054
sg235
(lp60055
sg126
(lp60056
sg281
(lp60057
sg40
(lp60058
sg128
(lp60059
sg130
(lp60060
sg14
(lp60061
sg135
(lp60062
sg50
(lp60063
sg140
(lp60064
ssS'crosscorrel'
p60065
(dp60066
g6
(lp60067
I1439
assS'communist'
p60068
(dp60069
g74
(lp60070
I397
assS'marsden'
p60071
(dp60072
g163
(lp60073
I1404
assS'inferior'
p60074
(dp60075
g174
(lp60076
sg221
(lp60077
sg223
(lp60078
sg6
(lp60079
I2336
asg303
(lp60080
ssS'equilibrium'
p60081
(dp60082
g438
(lp60083
I579
asg74
(lp60084
sg124
(lp60085
sg384
(lp60086
sg14
(lp60087
sg176
(lp60088
sg46
(lp60089
sg104
(lp60090
sg16
(lp60091
sg535
(lp60092
sg149
(lp60093
ssS'buntin'
p60094
(dp60095
g221
(lp60096
I2542
assS'ludicr'
p60097
(dp60098
g145
(lp60099
I1511
assS'lobe'
p60100
(dp60101
g4
(lp60102
I3531
assS'geman'
p60103
(dp60104
g484
(lp60105
sg176
(lp60106
sg140
(lp60107
I3134
assS'xdl'
p60108
(dp60109
g78
(lp60110
I1866
assS'disagre'
p60111
(dp60112
g48
(lp60113
sg221
(lp60114
sg85
(lp60115
sg140
(lp60116
I305
assS'arrhythmia'
p60117
(dp60118
g135
(lp60119
I153
assS'bear'
p60120
(dp60121
g438
(lp60122
I2449
asg74
(lp60123
sg80
(lp60124
sg181
(lp60125
sg174
(lp60126
sg78
(lp60127
sg460
(lp60128
ssS'beam'
p60129
(dp60130
g283
(lp60131
sg8
(lp60132
I1231
assS'bean'
p60133
(dp60134
g14
(lp60135
sg16
(lp60136
I591
assS'tlve'
p60137
(dp60138
g230
(lp60139
I2701
assS'cressi'
p60140
(dp60141
g124
(lp60142
I1253
assS'hebb'
p60143
(dp60144
g70
(lp60145
sg535
(lp60146
sg149
(lp60147
I1063
assS'bead'
p60148
(dp60149
g138
(lp60150
I1177
assS'aken'
p60151
(dp60152
g535
(lp60153
I1477
assS'reluct'
p60154
(dp60155
g94
(lp60156
I457
assS'organ'
p60157
(dp60158
g104
(lp60159
sg145
(lp60160
sg76
(lp60161
sg262
(lp60162
sg59
(lp60163
sg303
(lp60164
sg42
(lp60165
I2
asg245
(lp60166
sg20
(lp60167
sg48
(lp60168
sg535
(lp60169
sg44
(lp60170
sg350
(lp60171
sg174
(lp60172
sg293
(lp60173
sg12
(lp60174
sg429
(lp60175
sg318
(lp60176
sg102
(lp60177
sg178
(lp60178
sg106
(lp60179
sg52
(lp60180
sg116
(lp60181
sg438
(lp60182
sg118
(lp60183
sg332
(lp60184
sg121
(lp60185
sg80
(lp60186
sg68
(lp60187
sg72
(lp60188
sg10
(lp60189
sg40
(lp60190
sg149
(lp60191
ssS'reweight'
p60192
(dp60193
g354
(lp60194
I1451
assS'meila'
p60195
(dp60196
g460
(lp60197
I1500
assS'integr'
p60198
(dp60199
g70
(lp60200
sg303
(lp60201
sg80
(lp60202
sg262
(lp60203
sg295
(lp60204
sg183
(lp60205
sg59
(lp60206
sg38
(lp60207
sg83
(lp60208
sg85
(lp60209
sg63
(lp60210
sg87
(lp60211
sg245
(lp60212
sg46
(lp60213
sg20
(lp60214
sg313
(lp60215
sg350
(lp60216
sg174
(lp60217
sg429
(lp60218
sg318
(lp60219
sg94
(lp60220
sg104
(lp60221
sg96
(lp60222
sg22
(lp60223
sg216
(lp60224
sg438
(lp60225
I2041
asg332
(lp60226
sg178
(lp60227
sg4
(lp60228
sg384
(lp60229
sg124
(lp60230
sg126
(lp60231
sg281
(lp60232
sg10
(lp60233
sg130
(lp60234
sg132
(lp60235
sg14
(lp60236
sg16
(lp60237
sg135
(lp60238
sg138
(lp60239
sg354
(lp60240
ssS'carbondal'
p60241
(dp60242
g245
(lp60243
I22
assS'gibson'
p60244
(dp60245
g12
(lp60246
I2729
assS'conform'
p60247
(dp60248
g306
(lp60249
sg26
(lp60250
sg52
(lp60251
sg8
(lp60252
I1148
assS'crossvalid'
p60253
(dp60254
g36
(lp60255
sg140
(lp60256
sg354
(lp60257
I2119
assS'ocular'
p60258
(dp60259
g438
(lp60260
I52
asg350
(lp60261
sg48
(lp60262
sg149
(lp60263
ssS'och'
p60264
(dp60265
g176
(lp60266
I2527
assS'jwkt'
p60267
(dp60268
g22
(lp60269
I389
assS'diversif'
p60270
(dp60271
g235
(lp60272
I2416
assS'pattern'
p60273
(dp60274
g68
(lp60275
sg70
(lp60276
sg26
(lp60277
sg277
(lp60278
sg293
(lp60279
sg281
(lp60280
sg283
(lp60281
sg36
(lp60282
sg181
(lp60283
sg303
(lp60284
sg287
(lp60285
sg76
(lp60286
sg118
(lp60287
sg183
(lp60288
sg38
(lp60289
sg83
(lp60290
sg114
(lp60291
sg63
(lp60292
sg42
(lp60293
I3435
asg306
(lp60294
sg91
(lp60295
sg12
(lp60296
sg94
(lp60297
sg48
(lp60298
sg99
(lp60299
sg535
(lp60300
sg44
(lp60301
sg149
(lp60302
sg116
(lp60303
sg174
(lp60304
sg18
(lp60305
sg429
(lp60306
sg318
(lp60307
sg102
(lp60308
sg104
(lp60309
sg106
(lp60310
sg110
(lp60311
sg178
(lp60312
sg52
(lp60313
sg22
(lp60314
sg230
(lp60315
sg438
(lp60316
sg440
(lp60317
sg332
(lp60318
sg121
(lp60319
sg4
(lp60320
sg6
(lp60321
sg8
(lp60322
sg221
(lp60323
sg384
(lp60324
sg235
(lp60325
sg72
(lp60326
sg341
(lp60327
sg10
(lp60328
sg40
(lp60329
sg223
(lp60330
sg130
(lp60331
sg132
(lp60332
sg135
(lp60333
sg50
(lp60334
sg138
(lp60335
sg140
(lp60336
ssS'boundari'
p60337
(dp60338
g30
(lp60339
sg174
(lp60340
sg440
(lp60341
sg176
(lp60342
sg70
(lp60343
sg76
(lp60344
sg262
(lp60345
sg34
(lp60346
sg384
(lp60347
sg484
(lp60348
sg341
(lp60349
sg118
(lp60350
sg42
(lp60351
I1778
asg63
(lp60352
sg102
(lp60353
sg14
(lp60354
sg16
(lp60355
sg48
(lp60356
sg535
(lp60357
ssS'ralph'
p60358
(dp60359
g102
(lp60360
I13
asg245
(lp60361
ssS'distractor'
p60362
(dp60363
g293
(lp60364
sg332
(lp60365
sg4
(lp60366
I1163
asg303
(lp60367
ssS'jwke'
p60368
(dp60369
g22
(lp60370
I376
assS'progress'
p60371
(dp60372
g118
(lp60373
sg74
(lp60374
sg332
(lp60375
sg4
(lp60376
sg8
(lp60377
sg34
(lp60378
sg183
(lp60379
sg384
(lp60380
sg281
(lp60381
sg10
(lp60382
sg89
(lp60383
sg85
(lp60384
sg78
(lp60385
sg102
(lp60386
sg59
(lp60387
sg44
(lp60388
I762
asg149
(lp60389
ssS'karnath'
p60390
(dp60391
g303
(lp60392
I451
assS'patholog'
p60393
(dp60394
g91
(lp60395
I482
assS'letchworth'
p60396
(dp60397
g52
(lp60398
I2616
assS'presynapt'
p60399
(dp60400
g106
(lp60401
I138
asg70
(lp60402
sg149
(lp60403
ssS'furlan'
p60404
(dp60405
g96
(lp60406
I21
assS'vrd'
p60407
(dp60408
g130
(lp60409
I1155
assS'vri'
p60410
(dp60411
g121
(lp60412
sg128
(lp60413
I2773
assS'vhg'
p60414
(dp60415
g178
(lp60416
I1571
assS'demodul'
p60417
(dp60418
g22
(lp60419
I319
assS'instant'
p60420
(dp60421
g262
(lp60422
sg76
(lp60423
sg8
(lp60424
I847
assS'buster'
p60425
(dp60426
g256
(lp60427
I28
assS'noranda'
p60428
(dp60429
g138
(lp60430
I3258
assS'mkvmlv'
p60431
(dp60432
g130
(lp60433
I966
assS'equal'
p60434
(dp60435
g329
(lp60436
sg26
(lp60437
sg281
(lp60438
sg283
(lp60439
sg30
(lp60440
sg287
(lp60441
sg74
(lp60442
sg145
(lp60443
sg76
(lp60444
sg118
(lp60445
sg295
(lp60446
sg183
(lp60447
sg59
(lp60448
sg484
(lp60449
sg83
(lp60450
sg85
(lp60451
sg42
(lp60452
I764
asg306
(lp60453
sg20
(lp60454
sg48
(lp60455
sg68
(lp60456
sg44
(lp60457
sg149
(lp60458
sg230
(lp60459
sg135
(lp60460
sg293
(lp60461
sg318
(lp60462
sg102
(lp60463
sg18
(lp60464
sg108
(lp60465
sg110
(lp60466
sg114
(lp60467
sg216
(lp60468
sg174
(lp60469
sg440
(lp60470
sg332
(lp60471
sg80
(lp60472
sg6
(lp60473
sg235
(lp60474
sg384
(lp60475
sg124
(lp60476
sg126
(lp60477
sg341
(lp60478
sg10
(lp60479
sg40
(lp60480
sg128
(lp60481
sg350
(lp60482
sg50
(lp60483
sg460
(lp60484
sg140
(lp60485
sg354
(lp60486
ssS'instanc'
p60487
(dp60488
g70
(lp60489
sg26
(lp60490
sg277
(lp60491
sg145
(lp60492
sg293
(lp60493
sg295
(lp60494
sg183
(lp60495
sg85
(lp60496
sg303
(lp60497
sg306
(lp60498
sg235
(lp60499
sg94
(lp60500
sg20
(lp60501
sg44
(lp60502
sg429
(lp60503
sg46
(lp60504
sg104
(lp60505
sg110
(lp60506
sg114
(lp60507
sg440
(lp60508
sg181
(lp60509
sg8
(lp60510
sg384
(lp60511
sg68
(lp60512
sg126
(lp60513
sg344
(lp60514
sg140
(lp60515
I188
assS'equat'
p60516
(dp60517
g68
(lp60518
sg163
(lp60519
sg116
(lp60520
sg281
(lp60521
sg283
(lp60522
sg40
(lp60523
sg176
(lp60524
sg262
(lp60525
sg295
(lp60526
sg183
(lp60527
sg484
(lp60528
sg38
(lp60529
sg85
(lp60530
sg42
(lp60531
I257
asg306
(lp60532
sg91
(lp60533
sg12
(lp60534
sg46
(lp60535
sg48
(lp60536
sg99
(lp60537
sg313
(lp60538
sg149
(lp60539
sg118
(lp60540
sg230
(lp60541
sg329
(lp60542
sg32
(lp60543
sg245
(lp60544
sg429
(lp60545
sg318
(lp60546
sg178
(lp60547
sg108
(lp60548
sg110
(lp60549
sg114
(lp60550
sg216
(lp60551
sg438
(lp60552
sg440
(lp60553
sg18
(lp60554
sg121
(lp60555
sg4
(lp60556
sg235
(lp60557
sg34
(lp60558
sg221
(lp60559
sg384
(lp60560
sg124
(lp60561
sg126
(lp60562
sg341
(lp60563
sg535
(lp60564
sg128
(lp60565
sg130
(lp60566
sg132
(lp60567
sg14
(lp60568
sg16
(lp60569
sg350
(lp60570
sg50
(lp60571
sg138
(lp60572
sg140
(lp60573
sg354
(lp60574
ssS'freeli'
p60575
(dp60576
g118
(lp60577
I1020
asg59
(lp60578
sg80
(lp60579
ssS'assurn'
p60580
(dp60581
g350
(lp60582
I874
assS'comment'
p60583
(dp60584
g116
(lp60585
sg178
(lp60586
sg256
(lp60587
sg262
(lp60588
sg124
(lp60589
sg91
(lp60590
sg18
(lp60591
sg110
(lp60592
sg140
(lp60593
I3077
asg26
(lp60594
ssS'unfold'
p60595
(dp60596
g178
(lp60597
I1639
assS'gone'
p60598
(dp60599
g121
(lp60600
I2769
assS'guidelin'
p60601
(dp60602
g114
(lp60603
I778
assS'carver'
p60604
(dp60605
g256
(lp60606
I2080
assS'commenc'
p60607
(dp60608
g145
(lp60609
sg89
(lp60610
I175
assS'fukushima'
p60611
(dp60612
g178
(lp60613
sg181
(lp60614
I462
assS'krogh'
p60615
(dp60616
g235
(lp60617
sg26
(lp60618
sg140
(lp60619
I10
asg130
(lp60620
ssS'movemen'
p60621
(dp60622
g99
(lp60623
I1820
assS'anomali'
p60624
(dp60625
g78
(lp60626
I60
assS'gaze'
p60627
(dp60628
g245
(lp60629
sg94
(lp60630
I310
asg32
(lp60631
sg293
(lp60632
sg350
(lp60633
ssS'ah'
p60634
(dp60635
g121
(lp60636
sg99
(lp60637
I3311
assS'deepli'
p60638
(dp60639
g72
(lp60640
I147
assS'columnar'
p60641
(dp60642
g12
(lp60643
sg149
(lp60644
I1467
assS'upcom'
p60645
(dp60646
g306
(lp60647
I1758
assS'accentu'
p60648
(dp60649
g256
(lp60650
I571
assS'ridder'
p60651
(dp60652
g6
(lp60653
I2000
assS'polzer'
p60654
(dp60655
g130
(lp60656
I3079
assS'bulk'
p60657
(dp60658
g181
(lp60659
I1448
assS'inrorm'
p60660
(dp60661
g106
(lp60662
I58
assS'bull'
p60663
(dp60664
g176
(lp60665
I2495
assS'reinhold'
p60666
(dp60667
g128
(lp60668
I2885
assS'fellow'
p60669
(dp60670
g74
(lp60671
sg138
(lp60672
I3259
assS'homer'
p60673
(dp60674
g70
(lp60675
I18
assS'determinist'
p60676
(dp60677
g163
(lp60678
sg121
(lp60679
sg4
(lp60680
sg8
(lp60681
sg110
(lp60682
sg384
(lp60683
sg262
(lp60684
sg38
(lp60685
sg535
(lp60686
sg36
(lp60687
sg89
(lp60688
sg460
(lp60689
sg130
(lp60690
sg293
(lp60691
sg221
(lp60692
sg313
(lp60693
sg354
(lp60694
I527
assS'multi'
p60695
(dp60696
g287
(lp60697
sg440
(lp60698
sg318
(lp60699
sg121
(lp60700
sg76
(lp60701
sg163
(lp60702
sg344
(lp60703
sg72
(lp60704
sg32
(lp60705
sg10
(lp60706
sg63
(lp60707
sg87
(lp60708
sg245
(lp60709
sg94
(lp60710
sg96
(lp60711
sg135
(lp60712
I99
asg110
(lp60713
sg178
(lp60714
sg52
(lp60715
ssS'viiiina'
p60716
(dp60717
g59
(lp60718
I3384
assS'plain'
p60719
(dp60720
g223
(lp60721
I1064
assS'anuradha'
p60722
(dp60723
g230
(lp60724
I24
assS'nd'
p60725
(dp60726
g174
(lp60727
sg68
(lp60728
sg4
(lp60729
sg8
(lp60730
sg344
(lp60731
sg163
(lp60732
sg262
(lp60733
sg72
(lp60734
sg10
(lp60735
sg20
(lp60736
sg99
(lp60737
I3275
asg313
(lp60738
sg114
(lp60739
ssS'yse'
p60740
(dp60741
g318
(lp60742
I1485
assS'rectangular'
p60743
(dp60744
g181
(lp60745
sg8
(lp60746
I894
assS'defin'
p60747
(dp60748
g124
(lp60749
sg26
(lp60750
sg277
(lp60751
sg163
(lp60752
sg72
(lp60753
sg281
(lp60754
sg283
(lp60755
sg40
(lp60756
sg30
(lp60757
sg287
(lp60758
sg74
(lp60759
sg176
(lp60760
sg145
(lp60761
sg80
(lp60762
sg118
(lp60763
sg344
(lp60764
sg59
(lp60765
sg38
(lp60766
sg83
(lp60767
sg85
(lp60768
sg303
(lp60769
sg42
(lp60770
I277
asg306
(lp60771
sg89
(lp60772
sg91
(lp60773
sg245
(lp60774
sg94
(lp60775
sg96
(lp60776
sg221
(lp60777
sg313
(lp60778
sg223
(lp60779
sg230
(lp60780
sg329
(lp60781
sg293
(lp60782
sg116
(lp60783
sg32
(lp60784
sg429
(lp60785
sg68
(lp60786
sg46
(lp60787
sg102
(lp60788
sg104
(lp60789
sg108
(lp60790
sg110
(lp60791
sg178
(lp60792
sg52
(lp60793
sg22
(lp60794
sg216
(lp60795
sg438
(lp60796
sg440
(lp60797
sg318
(lp60798
sg121
(lp60799
sg181
(lp60800
sg6
(lp60801
sg8
(lp60802
sg36
(lp60803
sg460
(lp60804
sg235
(lp60805
sg126
(lp60806
sg341
(lp60807
sg535
(lp60808
sg130
(lp60809
sg14
(lp60810
sg16
(lp60811
sg135
(lp60812
sg138
(lp60813
sg140
(lp60814
sg354
(lp60815
ssS'cox'
p60816
(dp60817
g14
(lp60818
sg16
(lp60819
I2438
assS'au'
p60820
(dp60821
g230
(lp60822
sg135
(lp60823
I39
asg121
(lp60824
sg176
(lp60825
ssS'ill'
p60826
(dp60827
g438
(lp60828
I1921
asg121
(lp60829
sg277
(lp60830
sg40
(lp60831
sg130
(lp60832
sg108
(lp60833
ssS'yokohama'
p60834
(dp60835
g440
(lp60836
I2653
assS'ilm'
p60837
(dp60838
g283
(lp60839
I709
assS'lanska'
p60840
(dp60841
g350
(lp60842
I2906
assS'helper'
p60843
(dp60844
g70
(lp60845
I694
assS'almost'
p60846
(dp60847
g230
(lp60848
sg174
(lp60849
sg74
(lp60850
sg332
(lp60851
sg283
(lp60852
sg256
(lp60853
sg329
(lp60854
sg34
(lp60855
sg36
(lp60856
sg384
(lp60857
sg293
(lp60858
sg287
(lp60859
sg318
(lp60860
sg12
(lp60861
sg18
(lp60862
sg48
(lp60863
sg138
(lp60864
sg140
(lp60865
I1624
asg350
(lp60866
ssS'unction'
p60867
(dp60868
g85
(lp60869
I2341
assS'cultur'
p60870
(dp60871
g20
(lp60872
I2476
asg74
(lp60873
ssS'narendra'
p60874
(dp60875
g230
(lp60876
sg46
(lp60877
sg128
(lp60878
I259
assS'substanti'
p60879
(dp60880
g74
(lp60881
sg332
(lp60882
sg121
(lp60883
sg80
(lp60884
sg6
(lp60885
sg235
(lp60886
sg183
(lp60887
sg163
(lp60888
sg89
(lp60889
sg14
(lp60890
sg102
(lp60891
sg94
(lp60892
sg16
(lp60893
sg354
(lp60894
I2441
asg149
(lp60895
ssS'ax'
p60896
(dp60897
g329
(lp60898
sg96
(lp60899
sg108
(lp60900
I647
asg4
(lp60901
sg287
(lp60902
ssS'prose'
p60903
(dp60904
g94
(lp60905
I152
assS'partner'
p60906
(dp60907
g216
(lp60908
I558
assS'tllter'
p60909
(dp60910
g108
(lp60911
I2338
assS'squir'
p60912
(dp60913
g99
(lp60914
I2876
assS'prosc'
p60915
(dp60916
g174
(lp60917
I1814
assS'portray'
p60918
(dp60919
g145
(lp60920
I1824
asg63
(lp60921
sg76
(lp60922
ssS'facet'
p60923
(dp60924
g256
(lp60925
I2144
assS'cjm'
p60926
(dp60927
g281
(lp60928
I1111
assS'oint'
p60929
(dp60930
g91
(lp60931
I65
assS'auto'
p60932
(dp60933
g30
(lp60934
sg176
(lp60935
sg72
(lp60936
sg126
(lp60937
sg94
(lp60938
I118
asg20
(lp60939
ssS'infer'
p60940
(dp60941
g74
(lp60942
sg4
(lp60943
sg235
(lp60944
sg91
(lp60945
sg128
(lp60946
sg130
(lp60947
sg104
(lp60948
sg63
(lp60949
sg354
(lp60950
I3074
assS'stimulustun'
p60951
(dp60952
g70
(lp60953
I792
assS'coronari'
p60954
(dp60955
g91
(lp60956
I2873
assS'lazzaro'
p60957
(dp60958
g174
(lp60959
I461
asg20
(lp60960
sg10
(lp60961
ssS'lazzari'
p60962
(dp60963
g96
(lp60964
I2727
assS'denot'
p60965
(dp60966
g283
(lp60967
sg281
(lp60968
sg36
(lp60969
sg176
(lp60970
sg145
(lp60971
sg76
(lp60972
sg262
(lp60973
sg295
(lp60974
sg183
(lp60975
sg484
(lp60976
sg83
(lp60977
sg85
(lp60978
sg42
(lp60979
I1160
asg306
(lp60980
sg91
(lp60981
sg12
(lp60982
sg46
(lp60983
sg18
(lp60984
sg99
(lp60985
sg313
(lp60986
sg223
(lp60987
sg230
(lp60988
sg118
(lp60989
sg293
(lp60990
sg429
(lp60991
sg102
(lp60992
sg108
(lp60993
sg216
(lp60994
sg438
(lp60995
sg32
(lp60996
sg48
(lp60997
sg178
(lp60998
sg80
(lp60999
sg235
(lp61000
sg34
(lp61001
sg221
(lp61002
sg384
(lp61003
sg124
(lp61004
sg72
(lp61005
sg341
(lp61006
sg344
(lp61007
sg44
(lp61008
sg128
(lp61009
sg130
(lp61010
sg132
(lp61011
sg14
(lp61012
sg16
(lp61013
sg135
(lp61014
sg460
(lp61015
sg140
(lp61016
ssS'saund'
p61017
(dp61018
g74
(lp61019
I867
assS'iot'
p61020
(dp61021
g130
(lp61022
I1388
assS'ioo'
p61023
(dp61024
g245
(lp61025
I1360
assS'ion'
p61026
(dp61027
g48
(lp61028
sg181
(lp61029
sg59
(lp61030
sg42
(lp61031
I2586
asg318
(lp61032
sg130
(lp61033
sg18
(lp61034
sg354
(lp61035
ssS'iog'
p61036
(dp61037
g85
(lp61038
I3024
assS'giuliani'
p61039
(dp61040
g96
(lp61041
I25
assS'judgment'
p61042
(dp61043
g74
(lp61044
I142
assS'tighter'
p61045
(dp61046
g318
(lp61047
I2510
assS'ststam'
p61048
(dp61049
g174
(lp61050
I1861
assS'center'
p61051
(dp61052
g70
(lp61053
sg78
(lp61054
sg163
(lp61055
sg40
(lp61056
sg26
(lp61057
sg30
(lp61058
sg256
(lp61059
sg76
(lp61060
sg295
(lp61061
sg183
(lp61062
sg59
(lp61063
sg80
(lp61064
sg303
(lp61065
sg42
(lp61066
I2896
asg89
(lp61067
sg12
(lp61068
sg48
(lp61069
sg221
(lp61070
sg313
(lp61071
sg44
(lp61072
sg149
(lp61073
sg116
(lp61074
sg174
(lp61075
sg32
(lp61076
sg318
(lp61077
sg102
(lp61078
sg108
(lp61079
sg63
(lp61080
sg52
(lp61081
sg114
(lp61082
sg230
(lp61083
sg438
(lp61084
sg440
(lp61085
sg18
(lp61086
sg178
(lp61087
sg22
(lp61088
sg6
(lp61089
sg8
(lp61090
sg36
(lp61091
sg384
(lp61092
sg10
(lp61093
sg535
(lp61094
sg130
(lp61095
sg132
(lp61096
sg14
(lp61097
sg16
(lp61098
sg50
(lp61099
sg354
(lp61100
ssS'kotkin'
p61101
(dp61102
g89
(lp61103
I2636
assS'neural'
p61104
(dp61105
g80
(lp61106
sg293
(lp61107
sg344
(lp61108
sg78
(lp61109
sg59
(lp61110
sg484
(lp61111
sg38
(lp61112
sg83
(lp61113
sg85
(lp61114
sg303
(lp61115
sg438
(lp61116
sg116
(lp61117
sg118
(lp61118
sg34
(lp61119
sg36
(lp61120
sg460
(lp61121
sg68
(lp61122
sg72
(lp61123
sg281
(lp61124
sg10
(lp61125
sg40
(lp61126
sg283
(lp61127
sg70
(lp61128
sg26
(lp61129
sg277
(lp61130
sg163
(lp61131
sg89
(lp61132
sg91
(lp61133
sg12
(lp61134
sg94
(lp61135
sg96
(lp61136
sg48
(lp61137
sg99
(lp61138
sg313
(lp61139
sg44
(lp61140
sg149
(lp61141
sg429
(lp61142
sg102
(lp61143
sg104
(lp61144
sg106
(lp61145
sg108
(lp61146
sg110
(lp61147
sg63
(lp61148
sg52
(lp61149
sg114
(lp61150
sg128
(lp61151
sg130
(lp61152
sg132
(lp61153
sg14
(lp61154
sg16
(lp61155
sg135
(lp61156
sg50
(lp61157
sg138
(lp61158
sg140
(lp61159
sg354
(lp61160
sg306
(lp61161
sg87
(lp61162
sg245
(lp61163
sg46
(lp61164
sg20
(lp61165
sg18
(lp61166
sg221
(lp61167
sg535
(lp61168
sg223
(lp61169
sg350
(lp61170
sg216
(lp61171
sg174
(lp61172
sg440
(lp61173
sg332
(lp61174
sg121
(lp61175
sg4
(lp61176
sg6
(lp61177
sg8
(lp61178
sg126
(lp61179
sg341
(lp61180
sg30
(lp61181
sg287
(lp61182
sg74
(lp61183
sg176
(lp61184
sg145
(lp61185
sg256
(lp61186
sg76
(lp61187
sg262
(lp61188
sg295
(lp61189
sg183
(lp61190
sg42
(lp61191
I72
asg230
(lp61192
sg329
(lp61193
sg32
(lp61194
sg318
(lp61195
sg178
(lp61196
sg22
(lp61197
sg181
(lp61198
sg235
(lp61199
sg384
(lp61200
sg124
(lp61201
ssS'nevertheless'
p61202
(dp61203
g332
(lp61204
sg76
(lp61205
sg262
(lp61206
sg36
(lp61207
sg281
(lp61208
sg85
(lp61209
sg42
(lp61210
I614
asg89
(lp61211
sg99
(lp61212
sg44
(lp61213
ssS'com'
p61214
(dp61215
g30
(lp61216
sg121
(lp61217
sg163
(lp61218
sg78
(lp61219
sg128
(lp61220
I27
asg44
(lp61221
ssS'viterbi'
p61222
(dp61223
g440
(lp61224
I951
asg87
(lp61225
sg76
(lp61226
ssS'versatil'
p61227
(dp61228
g63
(lp61229
I492
assS'col'
p61230
(dp61231
g91
(lp61232
I2142
assS'thought'
p61233
(dp61234
g30
(lp61235
sg329
(lp61236
sg74
(lp61237
sg332
(lp61238
sg4
(lp61239
sg235
(lp61240
sg116
(lp61241
sg262
(lp61242
sg126
(lp61243
sg306
(lp61244
sg128
(lp61245
I668
asg52
(lp61246
ssS'interpol'
p61247
(dp61248
g30
(lp61249
sg287
(lp61250
sg440
(lp61251
sg293
(lp61252
sg429
(lp61253
sg10
(lp61254
sg306
(lp61255
sg85
(lp61256
sg46
(lp61257
sg96
(lp61258
sg135
(lp61259
sg99
(lp61260
sg138
(lp61261
I2800
asg223
(lp61262
ssS'rush'
p61263
(dp61264
g83
(lp61265
I520
assS'latest'
p61266
(dp61267
g14
(lp61268
I4054
asg63
(lp61269
sg350
(lp61270
ssS'unari'
p61271
(dp61272
g50
(lp61273
I270
assS'nualber'
p61274
(dp61275
g42
(lp61276
I1681
assS'biometrika'
p61277
(dp61278
g130
(lp61279
I3181
assS'clinic'
p61280
(dp61281
g303
(lp61282
sg135
(lp61283
I2506
asg91
(lp61284
sg484
(lp61285
sg4
(lp61286
ssS'surpass'
p61287
(dp61288
g34
(lp61289
I742
asg63
(lp61290
sg83
(lp61291
ssS'tone'
p61292
(dp61293
g174
(lp61294
sg332
(lp61295
sg4
(lp61296
I172
assS'systemat'
p61297
(dp61298
g332
(lp61299
sg235
(lp61300
sg59
(lp61301
sg72
(lp61302
sg306
(lp61303
sg110
(lp61304
sg63
(lp61305
sg223
(lp61306
sg354
(lp61307
I184
assS'cogsci'
p61308
(dp61309
g116
(lp61310
sg18
(lp61311
I289
assS'adv'
p61312
(dp61313
g99
(lp61314
I3166
assS'manukian'
p61315
(dp61316
g44
(lp61317
I516
assS'adt'
p61318
(dp61319
g12
(lp61320
I868
assS'imilar'
p61321
(dp61322
g72
(lp61323
I2818
assS'adp'
p61324
(dp61325
g68
(lp61326
I2927
assS'exampletl'
p61327
(dp61328
g223
(lp61329
I2621
assS'prism'
p61330
(dp61331
g303
(lp61332
I2736
assS'gmd'
p61333
(dp61334
g36
(lp61335
sg354
(lp61336
I19
assS'add'
p61337
(dp61338
g174
(lp61339
sg145
(lp61340
sg26
(lp61341
sg6
(lp61342
sg329
(lp61343
sg295
(lp61344
sg183
(lp61345
sg262
(lp61346
sg341
(lp61347
sg10
(lp61348
sg42
(lp61349
I1235
asg132
(lp61350
sg89
(lp61351
sg91
(lp61352
sg102
(lp61353
sg14
(lp61354
sg85
(lp61355
sg344
(lp61356
ssS'adb'
p61357
(dp61358
g121
(lp61359
I2612
assS'adc'
p61360
(dp61361
g135
(lp61362
I2541
assS'match'
p61363
(dp61364
g283
(lp61365
sg70
(lp61366
sg277
(lp61367
sg163
(lp61368
sg287
(lp61369
sg76
(lp61370
sg293
(lp61371
sg344
(lp61372
sg80
(lp61373
sg303
(lp61374
sg42
(lp61375
I1618
asg245
(lp61376
sg94
(lp61377
sg48
(lp61378
sg44
(lp61379
sg149
(lp61380
sg118
(lp61381
sg429
(lp61382
sg104
(lp61383
sg63
(lp61384
sg52
(lp61385
sg4
(lp61386
sg116
(lp61387
sg329
(lp61388
sg318
(lp61389
sg22
(lp61390
sg181
(lp61391
sg68
(lp61392
sg72
(lp61393
sg341
(lp61394
sg128
(lp61395
sg135
(lp61396
sg138
(lp61397
ssS'raven'
p61398
(dp61399
g4
(lp61400
I3489
assS'paillard'
p61401
(dp61402
g80
(lp61403
I2565
assS'immens'
p61404
(dp61405
g78
(lp61406
sg83
(lp61407
I688
assS'icii'
p61408
(dp61409
g102
(lp61410
I2685
assS'miiller'
p61411
(dp61412
g36
(lp61413
sg354
(lp61414
I3154
assS'testb'
p61415
(dp61416
g132
(lp61417
I98
asg223
(lp61418
ssS'sondik'
p61419
(dp61420
g293
(lp61421
I3277
assS'realis'
p61422
(dp61423
g384
(lp61424
sg283
(lp61425
sg52
(lp61426
I2
assS'testl'
p61427
(dp61428
g14
(lp61429
I3968
assS'insert'
p61430
(dp61431
g163
(lp61432
sg295
(lp61433
sg183
(lp61434
sg10
(lp61435
sg89
(lp61436
sg130
(lp61437
I2797
asg104
(lp61438
sg96
(lp61439
sg20
(lp61440
sg44
(lp61441
ssS'like'
p61442
(dp61443
g283
(lp61444
sg70
(lp61445
sg78
(lp61446
sg277
(lp61447
sg303
(lp61448
sg26
(lp61449
sg30
(lp61450
sg287
(lp61451
sg74
(lp61452
sg176
(lp61453
sg145
(lp61454
sg256
(lp61455
sg76
(lp61456
sg262
(lp61457
sg295
(lp61458
sg183
(lp61459
sg484
(lp61460
sg83
(lp61461
sg85
(lp61462
sg63
(lp61463
sg42
(lp61464
I1139
asg87
(lp61465
sg91
(lp61466
sg12
(lp61467
sg94
(lp61468
sg96
(lp61469
sg48
(lp61470
sg223
(lp61471
sg149
(lp61472
sg118
(lp61473
sg116
(lp61474
sg329
(lp61475
sg18
(lp61476
sg32
(lp61477
sg318
(lp61478
sg46
(lp61479
sg104
(lp61480
sg106
(lp61481
sg110
(lp61482
sg20
(lp61483
sg52
(lp61484
sg216
(lp61485
sg438
(lp61486
sg440
(lp61487
sg332
(lp61488
sg178
(lp61489
sg6
(lp61490
sg235
(lp61491
sg34
(lp61492
sg36
(lp61493
sg460
(lp61494
sg68
(lp61495
sg10
(lp61496
sg40
(lp61497
sg128
(lp61498
sg130
(lp61499
sg132
(lp61500
sg14
(lp61501
sg16
(lp61502
sg350
(lp61503
sg138
(lp61504
sg140
(lp61505
sg354
(lp61506
ssS'success'
p61507
(dp61508
g283
(lp61509
sg70
(lp61510
sg26
(lp61511
sg30
(lp61512
sg74
(lp61513
sg176
(lp61514
sg145
(lp61515
sg76
(lp61516
sg293
(lp61517
sg78
(lp61518
sg59
(lp61519
sg38
(lp61520
sg306
(lp61521
sg87
(lp61522
sg89
(lp61523
sg245
(lp61524
sg20
(lp61525
sg114
(lp61526
sg99
(lp61527
sg313
(lp61528
sg44
(lp61529
sg318
(lp61530
sg52
(lp61531
sg4
(lp61532
sg329
(lp61533
sg332
(lp61534
sg178
(lp61535
sg22
(lp61536
sg8
(lp61537
sg34
(lp61538
sg460
(lp61539
sg68
(lp61540
sg128
(lp61541
sg132
(lp61542
sg14
(lp61543
sg16
(lp61544
sg135
(lp61545
I151
assS'lsec'
p61546
(dp61547
g14
(lp61548
sg16
(lp61549
I799
assS'ikeuchi'
p61550
(dp61551
g80
(lp61552
sg223
(lp61553
I3374
assS'solmd'
p61554
(dp61555
g174
(lp61556
I951
assS'cijyj'
p61557
(dp61558
g18
(lp61559
I974
assS'pyaq'
p61560
(dp61561
g384
(lp61562
I1388
assS'soft'
p61563
(dp61564
g329
(lp61565
sg318
(lp61566
sg121
(lp61567
sg26
(lp61568
sg76
(lp61569
sg68
(lp61570
sg38
(lp61571
sg10
(lp61572
sg87
(lp61573
sg91
(lp61574
sg221
(lp61575
sg313
(lp61576
I2228
assS'espolls'
p61577
(dp61578
g174
(lp61579
I2387
assS'italian'
p61580
(dp61581
g96
(lp61582
I1542
assS'dispatch'
p61583
(dp61584
g83
(lp61585
I53
assS'iiiut'
p61586
(dp61587
g230
(lp61588
I1372
assS'njo'
p61589
(dp61590
g106
(lp61591
I968
assS'hair'
p61592
(dp61593
g174
(lp61594
I265
assS'convez'
p61595
(dp61596
g484
(lp61597
I2613
assS'convey'
p61598
(dp61599
g102
(lp61600
sg50
(lp61601
I1217
asg181
(lp61602
ssS'convex'
p61603
(dp61604
g230
(lp61605
sg30
(lp61606
sg121
(lp61607
sg8
(lp61608
sg34
(lp61609
sg341
(lp61610
sg130
(lp61611
I309
assS'proper'
p61612
(dp61613
g216
(lp61614
sg74
(lp61615
sg22
(lp61616
sg277
(lp61617
sg221
(lp61618
sg68
(lp61619
sg40
(lp61620
sg42
(lp61621
I355
asg78
(lp61622
sg303
(lp61623
sg18
(lp61624
sg99
(lp61625
sg44
(lp61626
sg350
(lp61627
ssS'genuin'
p61628
(dp61629
g114
(lp61630
I2077
assS'mihir'
p61631
(dp61632
g145
(lp61633
I2998
assS'est'
p61634
(dp61635
g30
(lp61636
I1020
asg344
(lp61637
sg145
(lp61638
ssS'garrett'
p61639
(dp61640
g429
(lp61641
sg8
(lp61642
I2603
assS'sdsc'
p61643
(dp61644
g26
(lp61645
I3390
assS'slide'
p61646
(dp61647
g116
(lp61648
sg460
(lp61649
sg18
(lp61650
I1027
asg26
(lp61651
ssS'joachim'
p61652
(dp61653
g130
(lp61654
I8
assS'invas'
p61655
(dp61656
g484
(lp61657
I1562
assS'esa'
p61658
(dp61659
g83
(lp61660
I2161
assS'nonempti'
p61661
(dp61662
g287
(lp61663
I3346
assS'tmj'
p61664
(dp61665
g104
(lp61666
I1766
assS'tmm'
p61667
(dp61668
g38
(lp61669
I755
assS'esi'
p61670
(dp61671
g341
(lp61672
I1531
assS'softmax'
p61673
(dp61674
g329
(lp61675
sg178
(lp61676
sg76
(lp61677
sg36
(lp61678
sg460
(lp61679
sg124
(lp61680
sg138
(lp61681
I1252
assS'dcn'
p61682
(dp61683
g174
(lp61684
sg354
(lp61685
I1438
assS'caqfjtij'
p61686
(dp61687
g26
(lp61688
I2112
assS'slight'
p61689
(dp61690
g216
(lp61691
sg484
(lp61692
sg176
(lp61693
sg70
(lp61694
sg4
(lp61695
sg181
(lp61696
sg235
(lp61697
sg36
(lp61698
sg262
(lp61699
sg85
(lp61700
sg63
(lp61701
sg42
(lp61702
I2789
asg68
(lp61703
sg89
(lp61704
sg130
(lp61705
sg245
(lp61706
sg76
(lp61707
sg138
(lp61708
sg44
(lp61709
ssS'noisi'
p61710
(dp61711
g329
(lp61712
sg70
(lp61713
sg6
(lp61714
sg235
(lp61715
sg295
(lp61716
sg183
(lp61717
sg460
(lp61718
sg262
(lp61719
sg83
(lp61720
sg63
(lp61721
sg344
(lp61722
sg104
(lp61723
sg102
(lp61724
sg14
(lp61725
I4333
asg313
(lp61726
ssS'host'
p61727
(dp61728
g283
(lp61729
sg10
(lp61730
I467
assS'kristin'
p61731
(dp61732
g440
(lp61733
I2451
assS'somat'
p61734
(dp61735
g106
(lp61736
I659
assS'although'
p61737
(dp61738
g68
(lp61739
sg70
(lp61740
sg26
(lp61741
sg277
(lp61742
sg72
(lp61743
sg145
(lp61744
sg262
(lp61745
sg344
(lp61746
sg183
(lp61747
sg83
(lp61748
sg85
(lp61749
sg63
(lp61750
sg306
(lp61751
sg87
(lp61752
sg89
(lp61753
sg91
(lp61754
sg245
(lp61755
sg20
(lp61756
sg48
(lp61757
sg99
(lp61758
sg44
(lp61759
sg329
(lp61760
sg18
(lp61761
sg102
(lp61762
sg104
(lp61763
sg108
(lp61764
sg96
(lp61765
sg52
(lp61766
sg116
(lp61767
sg174
(lp61768
sg440
(lp61769
sg332
(lp61770
sg4
(lp61771
sg6
(lp61772
sg8
(lp61773
sg34
(lp61774
sg36
(lp61775
sg460
(lp61776
sg124
(lp61777
sg126
(lp61778
sg281
(lp61779
sg10
(lp61780
sg40
(lp61781
sg128
(lp61782
sg135
(lp61783
sg50
(lp61784
sg138
(lp61785
sg140
(lp61786
I1595
assS'dcs'
p61787
(dp61788
g429
(lp61789
sg8
(lp61790
I2633
assS'simpler'
p61791
(dp61792
g30
(lp61793
sg178
(lp61794
sg4
(lp61795
sg277
(lp61796
sg384
(lp61797
sg102
(lp61798
sg14
(lp61799
I3499
asg96
(lp61800
sg63
(lp61801
sg44
(lp61802
ssS'about'
p61803
(dp61804
g70
(lp61805
sg78
(lp61806
sg277
(lp61807
sg163
(lp61808
sg293
(lp61809
sg26
(lp61810
sg30
(lp61811
sg287
(lp61812
sg176
(lp61813
sg145
(lp61814
sg256
(lp61815
sg80
(lp61816
sg262
(lp61817
sg295
(lp61818
sg183
(lp61819
sg59
(lp61820
sg38
(lp61821
sg83
(lp61822
sg42
(lp61823
I2041
asg91
(lp61824
sg12
(lp61825
sg94
(lp61826
sg20
(lp61827
sg48
(lp61828
sg99
(lp61829
sg313
(lp61830
sg44
(lp61831
sg149
(lp61832
sg116
(lp61833
sg18
(lp61834
sg429
(lp61835
sg318
(lp61836
sg102
(lp61837
sg104
(lp61838
sg106
(lp61839
sg108
(lp61840
sg63
(lp61841
sg52
(lp61842
sg22
(lp61843
sg216
(lp61844
sg329
(lp61845
sg32
(lp61846
sg332
(lp61847
sg4
(lp61848
sg6
(lp61849
sg8
(lp61850
sg34
(lp61851
sg36
(lp61852
sg460
(lp61853
sg68
(lp61854
sg126
(lp61855
sg344
(lp61856
sg223
(lp61857
sg130
(lp61858
sg132
(lp61859
sg14
(lp61860
sg16
(lp61861
sg350
(lp61862
sg50
(lp61863
sg138
(lp61864
sg140
(lp61865
sg354
(lp61866
ssS'actual'
p61867
(dp61868
g68
(lp61869
sg70
(lp61870
sg26
(lp61871
sg40
(lp61872
sg287
(lp61873
sg145
(lp61874
sg76
(lp61875
sg262
(lp61876
sg295
(lp61877
sg183
(lp61878
sg59
(lp61879
sg80
(lp61880
sg83
(lp61881
sg85
(lp61882
sg42
(lp61883
I2776
asg306
(lp61884
sg89
(lp61885
sg12
(lp61886
sg46
(lp61887
sg48
(lp61888
sg221
(lp61889
sg535
(lp61890
sg44
(lp61891
sg32
(lp61892
sg429
(lp61893
sg318
(lp61894
sg94
(lp61895
sg102
(lp61896
sg104
(lp61897
sg106
(lp61898
sg110
(lp61899
sg63
(lp61900
sg52
(lp61901
sg114
(lp61902
sg230
(lp61903
sg329
(lp61904
sg440
(lp61905
sg18
(lp61906
sg178
(lp61907
sg22
(lp61908
sg8
(lp61909
sg99
(lp61910
sg460
(lp61911
sg235
(lp61912
sg72
(lp61913
sg313
(lp61914
sg344
(lp61915
sg78
(lp61916
sg132
(lp61917
sg135
(lp61918
sg354
(lp61919
ssS'justin'
p61920
(dp61921
g132
(lp61922
I3535
asg89
(lp61923
ssS'chervonenki'
p61924
(dp61925
g287
(lp61926
I457
asg85
(lp61927
ssS'rrn'
p61928
(dp61929
g384
(lp61930
I1121
assS'justif'
p61931
(dp61932
g174
(lp61933
I832
assS'flake'
p61934
(dp61935
g283
(lp61936
I708
assS'transaclion'
p61937
(dp61938
g85
(lp61939
I4043
assS'gretter'
p61940
(dp61941
g96
(lp61942
I2762
assS'ander'
p61943
(dp61944
g26
(lp61945
sg140
(lp61946
I9
asg235
(lp61947
ssS'rule'
p61948
(dp61949
g70
(lp61950
sg287
(lp61951
sg76
(lp61952
sg293
(lp61953
sg295
(lp61954
sg183
(lp61955
sg484
(lp61956
sg38
(lp61957
sg42
(lp61958
I920
asg91
(lp61959
sg12
(lp61960
sg46
(lp61961
sg20
(lp61962
sg48
(lp61963
sg221
(lp61964
sg535
(lp61965
sg223
(lp61966
sg149
(lp61967
sg329
(lp61968
sg429
(lp61969
sg104
(lp61970
sg108
(lp61971
sg110
(lp61972
sg116
(lp61973
sg438
(lp61974
sg318
(lp61975
sg8
(lp61976
sg34
(lp61977
sg36
(lp61978
sg384
(lp61979
sg124
(lp61980
sg126
(lp61981
sg10
(lp61982
sg44
(lp61983
sg130
(lp61984
sg132
(lp61985
sg350
(lp61986
sg50
(lp61987
sg354
(lp61988
ssS'fals'
p61989
(dp61990
g121
(lp61991
sg26
(lp61992
sg78
(lp61993
sg132
(lp61994
sg135
(lp61995
I331
asg114
(lp61996
ssS'discard'
p61997
(dp61998
g116
(lp61999
sg124
(lp62000
sg126
(lp62001
I1574
asg76
(lp62002
ssS'certainti'
p62003
(dp62004
g178
(lp62005
sg91
(lp62006
I2660
assS'predictor'
p62007
(dp62008
g230
(lp62009
sg484
(lp62010
sg124
(lp62011
sg6
(lp62012
sg8
(lp62013
sg50
(lp62014
sg235
(lp62015
sg183
(lp62016
sg245
(lp62017
sg221
(lp62018
sg140
(lp62019
I167
assS'zexp'
p62020
(dp62021
g460
(lp62022
I2189
assS'devegvar'
p62023
(dp62024
g63
(lp62025
I460
assS'pergamon'
p62026
(dp62027
g118
(lp62028
I2586
assS'rpresent'
p62029
(dp62030
g80
(lp62031
I2526
assS'dataset'
p62032
(dp62033
g460
(lp62034
sg124
(lp62035
sg63
(lp62036
sg89
(lp62037
sg14
(lp62038
sg16
(lp62039
sg138
(lp62040
I2022
assS'chronic'
p62041
(dp62042
g106
(lp62043
I2415
assS'incremcll'
p62044
(dp62045
g245
(lp62046
I2532
assS'tsip'
p62047
(dp62048
g10
(lp62049
I982
assS'leverag'
p62050
(dp62051
g10
(lp62052
I312
assS'alspector'
p62053
(dp62054
g30
(lp62055
sg295
(lp62056
sg183
(lp62057
sg59
(lp62058
sg89
(lp62059
sg44
(lp62060
I2609
assS'iogist'
p62061
(dp62062
g341
(lp62063
I846
assS'legendr'
p62064
(dp62065
g89
(lp62066
I240
assS'pictur'
p62067
(dp62068
g36
(lp62069
sg429
(lp62070
sg38
(lp62071
sg535
(lp62072
I1889
asg114
(lp62073
ssS'rcr'
p62074
(dp62075
g38
(lp62076
I1439
assS'inasmuch'
p62077
(dp62078
g460
(lp62079
I2709
assS'kelso'
p62080
(dp62081
g106
(lp62082
I144
assS'rcd'
p62083
(dp62084
g256
(lp62085
I979
assS'schraudolph'
p62086
(dp62087
g132
(lp62088
sg89
(lp62089
sg318
(lp62090
sg50
(lp62091
I13
assS'maximumentropi'
p62092
(dp62093
g130
(lp62094
I3210
assS'unexpect'
p62095
(dp62096
g78
(lp62097
sg74
(lp62098
sg18
(lp62099
sg128
(lp62100
I1808
asg293
(lp62101
ssS'bus'
p62102
(dp62103
g135
(lp62104
I1653
asg10
(lp62105
ssS'coke'
p62106
(dp62107
g223
(lp62108
I2176
assS'but'
p62109
(dp62110
g344
(lp62111
sg329
(lp62112
sg70
(lp62113
sg26
(lp62114
sg277
(lp62115
sg163
(lp62116
sg72
(lp62117
sg68
(lp62118
sg293
(lp62119
sg460
(lp62120
sg181
(lp62121
sg303
(lp62122
sg30
(lp62123
sg350
(lp62124
sg74
(lp62125
sg176
(lp62126
sg145
(lp62127
sg256
(lp62128
sg76
(lp62129
sg262
(lp62130
sg295
(lp62131
sg183
(lp62132
sg59
(lp62133
sg80
(lp62134
sg38
(lp62135
sg83
(lp62136
sg85
(lp62137
sg124
(lp62138
sg42
(lp62139
I968
asg306
(lp62140
sg89
(lp62141
sg91
(lp62142
sg12
(lp62143
sg94
(lp62144
sg20
(lp62145
sg48
(lp62146
sg99
(lp62147
sg44
(lp62148
sg149
(lp62149
sg118
(lp62150
sg230
(lp62151
sg174
(lp62152
sg18
(lp62153
sg116
(lp62154
sg32
(lp62155
sg245
(lp62156
sg429
(lp62157
sg318
(lp62158
sg102
(lp62159
sg104
(lp62160
sg106
(lp62161
sg108
(lp62162
sg110
(lp62163
sg178
(lp62164
sg52
(lp62165
sg114
(lp62166
sg216
(lp62167
sg438
(lp62168
sg440
(lp62169
sg332
(lp62170
sg121
(lp62171
sg4
(lp62172
sg6
(lp62173
sg8
(lp62174
sg34
(lp62175
sg36
(lp62176
sg384
(lp62177
sg235
(lp62178
sg126
(lp62179
sg10
(lp62180
sg40
(lp62181
sg287
(lp62182
sg63
(lp62183
sg223
(lp62184
sg128
(lp62185
sg78
(lp62186
sg132
(lp62187
sg14
(lp62188
sg16
(lp62189
sg135
(lp62190
sg50
(lp62191
sg138
(lp62192
sg140
(lp62193
sg354
(lp62194
ssS'buj'
p62195
(dp62196
g104
(lp62197
I1249
assS'davidj'
p62198
(dp62199
g10
(lp62200
I58
assS'bun'
p62201
(dp62202
g221
(lp62203
I962
assS'sandi'
p62204
(dp62205
g293
(lp62206
I24
assS'bub'
p62207
(dp62208
g303
(lp62209
I856
assS'mead'
p62210
(dp62211
g174
(lp62212
I463
asg20
(lp62213
sg22
(lp62214
sg256
(lp62215
ssS'bug'
p62216
(dp62217
g18
(lp62218
I126
assS'wisc'
p62219
(dp62220
g344
(lp62221
I38
assS'mcps'
p62222
(dp62223
g10
(lp62224
I2766
assS'wise'
p62225
(dp62226
g70
(lp62227
sg114
(lp62228
I953
assS'levin'
p62229
(dp62230
g230
(lp62231
sg4
(lp62232
sg341
(lp62233
sg130
(lp62234
I3204
assS'murphi'
p62235
(dp62236
g484
(lp62237
I1789
assS'j'
p62238
(dp62239
g80
(lp62240
sg293
(lp62241
sg344
(lp62242
sg78
(lp62243
sg59
(lp62244
sg484
(lp62245
sg38
(lp62246
sg83
(lp62247
sg85
(lp62248
sg303
(lp62249
sg438
(lp62250
sg116
(lp62251
sg118
(lp62252
sg34
(lp62253
sg36
(lp62254
sg460
(lp62255
sg68
(lp62256
sg72
(lp62257
sg281
(lp62258
sg10
(lp62259
sg40
(lp62260
sg283
(lp62261
sg70
(lp62262
sg26
(lp62263
sg277
(lp62264
sg163
(lp62265
sg89
(lp62266
sg91
(lp62267
sg12
(lp62268
sg96
(lp62269
sg48
(lp62270
sg99
(lp62271
sg313
(lp62272
sg44
(lp62273
sg149
(lp62274
sg429
(lp62275
sg102
(lp62276
sg104
(lp62277
sg106
(lp62278
sg108
(lp62279
sg110
(lp62280
sg63
(lp62281
sg52
(lp62282
sg114
(lp62283
sg128
(lp62284
sg130
(lp62285
sg132
(lp62286
sg14
(lp62287
sg16
(lp62288
sg135
(lp62289
sg50
(lp62290
sg138
(lp62291
sg140
(lp62292
sg354
(lp62293
sg306
(lp62294
sg87
(lp62295
sg245
(lp62296
sg46
(lp62297
sg20
(lp62298
sg18
(lp62299
sg221
(lp62300
sg535
(lp62301
sg223
(lp62302
sg350
(lp62303
sg216
(lp62304
sg174
(lp62305
sg440
(lp62306
sg332
(lp62307
sg121
(lp62308
sg4
(lp62309
sg6
(lp62310
sg8
(lp62311
sg126
(lp62312
sg341
(lp62313
sg30
(lp62314
sg287
(lp62315
sg74
(lp62316
sg176
(lp62317
sg145
(lp62318
sg256
(lp62319
sg76
(lp62320
sg262
(lp62321
sg295
(lp62322
sg183
(lp62323
sg42
(lp62324
I152
asg230
(lp62325
sg329
(lp62326
sg32
(lp62327
sg318
(lp62328
sg178
(lp62329
sg22
(lp62330
sg181
(lp62331
sg235
(lp62332
sg384
(lp62333
sg124
(lp62334
ssS'breimann'
p62335
(dp62336
g235
(lp62337
I3042
assS'flip'
p62338
(dp62339
g20
(lp62340
I1788
asg80
(lp62341
ssS'bandpass'
p62342
(dp62343
g174
(lp62344
sg48
(lp62345
I861
assS'pii'
p62346
(dp62347
g46
(lp62348
I2977
assS'pij'
p62349
(dp62350
g306
(lp62351
I763
assS'pik'
p62352
(dp62353
g74
(lp62354
I1586
assS'jlilil'
p62355
(dp62356
g221
(lp62357
I993
assS'pin'
p62358
(dp62359
g38
(lp62360
sg140
(lp62361
I1268
assS'pio'
p62362
(dp62363
g46
(lp62364
I2825
assS'povo'
p62365
(dp62366
g96
(lp62367
I18
assS'pic'
p62368
(dp62369
g178
(lp62370
I1528
assS'pid'
p62371
(dp62372
g295
(lp62373
I3383
asg183
(lp62374
ssS'pie'
p62375
(dp62376
g48
(lp62377
I795
assS'pig'
p62378
(dp62379
g106
(lp62380
I2997
assS'circul'
p62381
(dp62382
g78
(lp62383
I2840
assS'probabl'
p62384
(dp62385
g283
(lp62386
sg78
(lp62387
sg277
(lp62388
sg163
(lp62389
sg72
(lp62390
sg281
(lp62391
sg26
(lp62392
sg30
(lp62393
sg287
(lp62394
sg74
(lp62395
sg176
(lp62396
sg145
(lp62397
sg80
(lp62398
sg76
(lp62399
sg262
(lp62400
sg295
(lp62401
sg183
(lp62402
sg59
(lp62403
sg484
(lp62404
sg38
(lp62405
sg85
(lp62406
sg303
(lp62407
sg42
(lp62408
I3321
asg306
(lp62409
sg87
(lp62410
sg91
(lp62411
sg94
(lp62412
sg96
(lp62413
sg18
(lp62414
sg221
(lp62415
sg313
(lp62416
sg223
(lp62417
sg293
(lp62418
sg460
(lp62419
sg318
(lp62420
sg102
(lp62421
sg178
(lp62422
sg110
(lp62423
sg63
(lp62424
sg52
(lp62425
sg114
(lp62426
sg329
(lp62427
sg440
(lp62428
sg332
(lp62429
sg121
(lp62430
sg4
(lp62431
sg6
(lp62432
sg34
(lp62433
sg36
(lp62434
sg384
(lp62435
sg124
(lp62436
sg126
(lp62437
sg341
(lp62438
sg344
(lp62439
sg130
(lp62440
sg132
(lp62441
sg50
(lp62442
sg138
(lp62443
sg354
(lp62444
ssS'boser'
p62445
(dp62446
g94
(lp62447
I3551
asg181
(lp62448
ssS'guidanc'
p62449
(dp62450
g132
(lp62451
I3424
asg59
(lp62452
ssS'oal'
p62453
(dp62454
g20
(lp62455
I1556
assS'detail'
p62456
(dp62457
g277
(lp62458
sg287
(lp62459
sg176
(lp62460
sg145
(lp62461
sg256
(lp62462
sg76
(lp62463
sg118
(lp62464
sg344
(lp62465
sg183
(lp62466
sg59
(lp62467
sg83
(lp62468
sg85
(lp62469
sg63
(lp62470
sg87
(lp62471
sg91
(lp62472
sg96
(lp62473
sg48
(lp62474
sg313
(lp62475
sg223
(lp62476
sg350
(lp62477
sg329
(lp62478
sg293
(lp62479
sg32
(lp62480
sg102
(lp62481
sg104
(lp62482
sg110
(lp62483
sg178
(lp62484
sg52
(lp62485
sg114
(lp62486
sg230
(lp62487
sg438
(lp62488
I617
asg440
(lp62489
sg121
(lp62490
sg4
(lp62491
sg181
(lp62492
sg235
(lp62493
sg36
(lp62494
sg384
(lp62495
sg124
(lp62496
sg72
(lp62497
sg10
(lp62498
sg40
(lp62499
sg44
(lp62500
sg14
(lp62501
sg16
(lp62502
sg140
(lp62503
sg354
(lp62504
ssS'virtual'
p62505
(dp62506
g438
(lp62507
I2069
asg32
(lp62508
sg145
(lp62509
sg174
(lp62510
sg34
(lp62511
sg36
(lp62512
sg59
(lp62513
sg293
(lp62514
sg429
(lp62515
sg128
(lp62516
sg99
(lp62517
sg223
(lp62518
ssS'laplac'
p62519
(dp62520
g22
(lp62521
sg350
(lp62522
I1528
assS'semiparametr'
p62523
(dp62524
g354
(lp62525
I3225
assS'thorp'
p62526
(dp62527
g106
(lp62528
I2627
assS'oay'
p62529
(dp62530
g223
(lp62531
I1395
assS'nelson'
p62532
(dp62533
g440
(lp62534
I22
asg176
(lp62535
sg10
(lp62536
ssS'verstraten'
p62537
(dp62538
g216
(lp62539
I616
assS'symplect'
p62540
(dp62541
g163
(lp62542
I1
assS'kldiverg'
p62543
(dp62544
g130
(lp62545
I1091
assS'ababab'
p62546
(dp62547
g332
(lp62548
I1715
assS'baker'
p62549
(dp62550
g32
(lp62551
I1234
assS'oram'
p62552
(dp62553
g181
(lp62554
I283
assS'experhnent'
p62555
(dp62556
g429
(lp62557
I2179
assS'domani'
p62558
(dp62559
g124
(lp62560
I3097
assS'rbin'
p62561
(dp62562
g38
(lp62563
I2641
assS'vliw'
p62564
(dp62565
g10
(lp62566
I507
assS'lllm'
p62567
(dp62568
g283
(lp62569
I714
assS'nwu'
p62570
(dp62571
g99
(lp62572
I43
assS'ullman'
p62573
(dp62574
g18
(lp62575
sg70
(lp62576
sg223
(lp62577
sg8
(lp62578
I749
assS'idcn'
p62579
(dp62580
g354
(lp62581
I1485
assS'krzyzak'
p62582
(dp62583
g96
(lp62584
I970
assS'sleep'
p62585
(dp62586
g74
(lp62587
I3146
asg70
(lp62588
ssS'climb'
p62589
(dp62590
g350
(lp62591
I2647
assS'nadaraya'
p62592
(dp62593
g295
(lp62594
I791
asg183
(lp62595
ssS'consider'
p62596
(dp62597
g283
(lp62598
sg163
(lp62599
sg30
(lp62600
sg176
(lp62601
sg293
(lp62602
sg344
(lp62603
sg59
(lp62604
sg83
(lp62605
sg85
(lp62606
sg42
(lp62607
I199
asg91
(lp62608
sg46
(lp62609
sg18
(lp62610
sg221
(lp62611
sg535
(lp62612
sg350
(lp62613
sg108
(lp62614
sg110
(lp62615
sg63
(lp62616
sg118
(lp62617
sg32
(lp62618
sg121
(lp62619
sg34
(lp62620
sg126
(lp62621
sg10
(lp62622
sg14
(lp62623
sg138
(lp62624
ssS'introd'
p62625
(dp62626
g344
(lp62627
sg174
(lp62628
I2748
asg484
(lp62629
ssS'kawato'
p62630
(dp62631
g99
(lp62632
I3415
assS'patcher'
p62633
(dp62634
g83
(lp62635
I2832
assS'troller'
p62636
(dp62637
g96
(lp62638
I2815
assS'dmaz'
p62639
(dp62640
g85
(lp62641
I850
assS'tween'
p62642
(dp62643
g4
(lp62644
sg149
(lp62645
I2332
assS'illustr'
p62646
(dp62647
g70
(lp62648
sg116
(lp62649
sg74
(lp62650
sg176
(lp62651
sg256
(lp62652
sg295
(lp62653
sg183
(lp62654
sg38
(lp62655
sg303
(lp62656
sg89
(lp62657
sg12
(lp62658
sg46
(lp62659
sg48
(lp62660
sg221
(lp62661
sg313
(lp62662
sg223
(lp62663
sg149
(lp62664
sg230
(lp62665
sg429
(lp62666
sg94
(lp62667
sg102
(lp62668
sg106
(lp62669
I539
asg52
(lp62670
sg22
(lp62671
sg216
(lp62672
sg332
(lp62673
sg4
(lp62674
sg124
(lp62675
sg281
(lp62676
sg535
(lp62677
sg44
(lp62678
sg78
(lp62679
sg132
(lp62680
sg14
(lp62681
sg16
(lp62682
sg135
(lp62683
sg50
(lp62684
sg140
(lp62685
ssS'concret'
p62686
(dp62687
g116
(lp62688
sg94
(lp62689
I2183
assS'crite'
p62690
(dp62691
g83
(lp62692
I9
assS'under'
p62693
(dp62694
g124
(lp62695
sg78
(lp62696
sg277
(lp62697
sg72
(lp62698
sg281
(lp62699
sg460
(lp62700
sg181
(lp62701
sg30
(lp62702
sg350
(lp62703
sg74
(lp62704
sg176
(lp62705
sg145
(lp62706
sg76
(lp62707
sg262
(lp62708
sg295
(lp62709
sg183
(lp62710
sg38
(lp62711
sg83
(lp62712
sg85
(lp62713
sg42
(lp62714
I2180
asg306
(lp62715
sg91
(lp62716
sg12
(lp62717
sg94
(lp62718
sg20
(lp62719
sg48
(lp62720
sg221
(lp62721
sg535
(lp62722
sg44
(lp62723
sg149
(lp62724
sg230
(lp62725
sg116
(lp62726
sg32
(lp62727
sg245
(lp62728
sg318
(lp62729
sg46
(lp62730
sg102
(lp62731
sg106
(lp62732
sg108
(lp62733
sg63
(lp62734
sg52
(lp62735
sg22
(lp62736
sg216
(lp62737
sg438
(lp62738
sg440
(lp62739
sg18
(lp62740
sg121
(lp62741
sg4
(lp62742
sg6
(lp62743
sg8
(lp62744
sg34
(lp62745
sg384
(lp62746
sg235
(lp62747
sg126
(lp62748
sg341
(lp62749
sg223
(lp62750
sg130
(lp62751
sg14
(lp62752
sg16
(lp62753
sg135
(lp62754
sg138
(lp62755
sg140
(lp62756
ssS'tweed'
p62757
(dp62758
g32
(lp62759
sg350
(lp62760
I2451
assS'testabl'
p62761
(dp62762
g176
(lp62763
sg4
(lp62764
I156
asg303
(lp62765
ssS'vili'
p62766
(dp62767
g32
(lp62768
sg350
(lp62769
I1009
assS'everi'
p62770
(dp62771
g283
(lp62772
sg70
(lp62773
sg287
(lp62774
sg74
(lp62775
sg76
(lp62776
sg295
(lp62777
sg183
(lp62778
sg80
(lp62779
sg83
(lp62780
sg42
(lp62781
I1202
asg306
(lp62782
sg87
(lp62783
sg89
(lp62784
sg91
(lp62785
sg94
(lp62786
sg96
(lp62787
sg221
(lp62788
sg329
(lp62789
sg32
(lp62790
sg46
(lp62791
sg104
(lp62792
sg52
(lp62793
sg114
(lp62794
sg230
(lp62795
sg438
(lp62796
sg440
(lp62797
sg121
(lp62798
sg4
(lp62799
sg181
(lp62800
sg34
(lp62801
sg36
(lp62802
sg460
(lp62803
sg124
(lp62804
sg10
(lp62805
sg344
(lp62806
sg128
(lp62807
sg130
(lp62808
ssS'risk'
p62809
(dp62810
g277
(lp62811
sg235
(lp62812
sg295
(lp62813
sg183
(lp62814
sg281
(lp62815
sg36
(lp62816
sg354
(lp62817
I723
assS'rise'
p62818
(dp62819
g174
(lp62820
I943
asg283
(lp62821
sg145
(lp62822
sg26
(lp62823
sg181
(lp62824
sg262
(lp62825
sg124
(lp62826
sg341
(lp62827
sg10
(lp62828
sg18
(lp62829
ssS'risc'
p62830
(dp62831
g10
(lp62832
I879
assS'quantif'
p62833
(dp62834
g99
(lp62835
I1370
assS'lyaea'
p62836
(dp62837
g318
(lp62838
I1110
assS'quantiz'
p62839
(dp62840
g70
(lp62841
sg72
(lp62842
sg89
(lp62843
I881
asg20
(lp62844
sg63
(lp62845
sg52
(lp62846
ssS'multiquadr'
p62847
(dp62848
g96
(lp62849
I895
assS'quantis'
p62850
(dp62851
g283
(lp62852
sg52
(lp62853
I535
assS'quantit'
p62854
(dp62855
g438
(lp62856
I102
asg4
(lp62857
sg181
(lp62858
sg262
(lp62859
sg36
(lp62860
sg59
(lp62861
sg80
(lp62862
sg38
(lp62863
sg85
(lp62864
sg128
(lp62865
sg12
(lp62866
sg94
(lp62867
sg48
(lp62868
sg277
(lp62869
sg149
(lp62870
ssS'neur'
p62871
(dp62872
g318
(lp62873
I2861
assS'school'
p62874
(dp62875
g332
(lp62876
sg277
(lp62877
sg80
(lp62878
sg34
(lp62879
sg36
(lp62880
sg281
(lp62881
sg83
(lp62882
sg245
(lp62883
sg176
(lp62884
sg89
(lp62885
sg91
(lp62886
sg132
(lp62887
I3495
asg94
(lp62888
sg221
(lp62889
sg313
(lp62890
sg223
(lp62891
ssS'oxlx'
p62892
(dp62893
g32
(lp62894
I1092
assS'qnix'
p62895
(dp62896
g440
(lp62897
I377
assS'loess'
p62898
(dp62899
g313
(lp62900
I1281
assS'nostrand'
p62901
(dp62902
g128
(lp62903
I2884
assS'preprogram'
p62904
(dp62905
g14
(lp62906
sg16
(lp62907
I2194
assS'kinzelbach'
p62908
(dp62909
g384
(lp62910
I2396
assS'pauml'
p62911
(dp62912
g183
(lp62913
I5194
assS'transduct'
p62914
(dp62915
g174
(lp62916
I2700
assS'enjoy'
p62917
(dp62918
g216
(lp62919
I381
assS'sirnplifi'
p62920
(dp62921
g350
(lp62922
I783
assS'naiv'
p62923
(dp62924
g94
(lp62925
sg99
(lp62926
I1442
asg91
(lp62927
sg76
(lp62928
ssS'direct'
p62929
(dp62930
g283
(lp62931
sg70
(lp62932
sg26
(lp62933
sg277
(lp62934
sg163
(lp62935
sg293
(lp62936
sg30
(lp62937
sg287
(lp62938
sg176
(lp62939
sg256
(lp62940
sg80
(lp62941
sg118
(lp62942
sg460
(lp62943
sg59
(lp62944
sg83
(lp62945
sg63
(lp62946
sg42
(lp62947
I1733
asg89
(lp62948
sg91
(lp62949
sg12
(lp62950
sg94
(lp62951
sg20
(lp62952
sg48
(lp62953
sg99
(lp62954
sg44
(lp62955
sg350
(lp62956
sg230
(lp62957
sg329
(lp62958
sg18
(lp62959
sg32
(lp62960
sg245
(lp62961
sg318
(lp62962
sg102
(lp62963
sg104
(lp62964
sg110
(lp62965
sg178
(lp62966
sg52
(lp62967
sg22
(lp62968
sg216
(lp62969
sg174
(lp62970
sg440
(lp62971
sg332
(lp62972
sg121
(lp62973
sg4
(lp62974
sg6
(lp62975
sg8
(lp62976
sg34
(lp62977
sg36
(lp62978
sg384
(lp62979
sg124
(lp62980
sg126
(lp62981
sg10
(lp62982
sg40
(lp62983
sg223
(lp62984
sg128
(lp62985
sg130
(lp62986
sg14
(lp62987
sg135
(lp62988
sg50
(lp62989
sg138
(lp62990
ssS'street'
p62991
(dp62992
g440
(lp62993
I35
asg22
(lp62994
sg76
(lp62995
sg293
(lp62996
sg10
(lp62997
sg87
(lp62998
ssS'inhibitorili'
p62999
(dp63000
g174
(lp63001
I853
assS'chader'
p63002
(dp63003
g118
(lp63004
I2578
assS'blue'
p63005
(dp63006
g114
(lp63007
I770
assS'rubel'
p63008
(dp63009
g48
(lp63010
I2210
assS'ogram'
p63011
(dp63012
g174
(lp63013
I2427
assS'blum'
p63014
(dp63015
g287
(lp63016
sg145
(lp63017
sg341
(lp63018
I2796
assS'conduct'
p63019
(dp63020
g329
(lp63021
sg484
(lp63022
sg344
(lp63023
sg124
(lp63024
sg72
(lp63025
sg85
(lp63026
sg94
(lp63027
sg14
(lp63028
sg20
(lp63029
sg135
(lp63030
sg138
(lp63031
I2454
assS'kambhatla'
p63032
(dp63033
g221
(lp63034
I2602
assS'symmetr'
p63035
(dp63036
g118
(lp63037
sg176
(lp63038
sg178
(lp63039
sg80
(lp63040
sg6
(lp63041
sg163
(lp63042
sg34
(lp63043
sg384
(lp63044
sg68
(lp63045
sg38
(lp63046
sg341
(lp63047
sg85
(lp63048
sg91
(lp63049
sg460
(lp63050
sg130
(lp63051
sg59
(lp63052
sg46
(lp63053
sg106
(lp63054
I641
asg18
(lp63055
sg535
(lp63056
sg44
(lp63057
ssS'asymmetri'
p63058
(dp63059
g38
(lp63060
sg76
(lp63061
I2144
assS'manipul'
p63062
(dp63063
g318
(lp63064
sg4
(lp63065
sg80
(lp63066
sg460
(lp63067
sg303
(lp63068
sg332
(lp63069
sg91
(lp63070
sg99
(lp63071
I3252
asg59
(lp63072
ssS'patt'
p63073
(dp63074
g384
(lp63075
sg48
(lp63076
I1731
asg181
(lp63077
ssS'casdag'
p63078
(dp63079
g295
(lp63080
sg183
(lp63081
sg121
(lp63082
I816
assS'iiuii'
p63083
(dp63084
g6
(lp63085
I1744
assS'lode'
p63086
(dp63087
g108
(lp63088
I1124
assS'snowbird'
p63089
(dp63090
g14
(lp63091
I4659
asg44
(lp63092
ssS'beck'
p63093
(dp63094
g10
(lp63095
I43
assS'pratt'
p63096
(dp63097
g30
(lp63098
I456
assS'debugg'
p63099
(dp63100
g10
(lp63101
I580
assS'path'
p63102
(dp63103
g440
(lp63104
sg80
(lp63105
sg262
(lp63106
sg89
(lp63107
sg96
(lp63108
sg14
(lp63109
sg106
(lp63110
I2585
asg135
(lp63111
sg99
(lp63112
sg16
(lp63113
sg46
(lp63114
ssS'xft'
p63115
(dp63116
g245
(lp63117
I2511
assS'lemmer'
p63118
(dp63119
g183
(lp63120
I6789
assS'precis'
p63121
(dp63122
g70
(lp63123
sg72
(lp63124
sg74
(lp63125
sg295
(lp63126
sg183
(lp63127
sg59
(lp63128
sg85
(lp63129
sg303
(lp63130
sg42
(lp63131
I630
asg20
(lp63132
sg350
(lp63133
sg230
(lp63134
sg104
(lp63135
sg63
(lp63136
sg216
(lp63137
sg32
(lp63138
sg318
(lp63139
sg22
(lp63140
sg6
(lp63141
sg68
(lp63142
sg126
(lp63143
sg281
(lp63144
sg10
(lp63145
sg344
(lp63146
sg130
(lp63147
sg14
(lp63148
sg16
(lp63149
sg354
(lp63150
ssS'wdx'
p63151
(dp63152
g295
(lp63153
I2455
asg183
(lp63154
ssS'raab'
p63155
(dp63156
g10
(lp63157
I2857
assS'fourier'
p63158
(dp63159
g116
(lp63160
sg174
(lp63161
sg145
(lp63162
sg68
(lp63163
sg85
(lp63164
sg102
(lp63165
sg48
(lp63166
I349
assS'isomorph'
p63167
(dp63168
g118
(lp63169
sg32
(lp63170
I968
assS'tremend'
p63171
(dp63172
g94
(lp63173
I48
assS'stray'
p63174
(dp63175
g63
(lp63176
I578
assS'blumer'
p63177
(dp63178
g287
(lp63179
I441
assS'printz'
p63180
(dp63181
g4
(lp63182
I2799
assS'blockag'
p63183
(dp63184
g438
(lp63185
I2289
assS'santini'
p63186
(dp63187
g76
(lp63188
I2238
assS'amherst'
p63189
(dp63190
g329
(lp63191
I3025
asg83
(lp63192
ssS'offi'
p63193
(dp63194
g104
(lp63195
I1776
assS'ymin'
p63196
(dp63197
g121
(lp63198
I1532
assS'describ'
p63199
(dp63200
g329
(lp63201
sg70
(lp63202
sg26
(lp63203
sg72
(lp63204
sg68
(lp63205
sg283
(lp63206
sg40
(lp63207
sg30
(lp63208
sg74
(lp63209
sg176
(lp63210
sg145
(lp63211
sg256
(lp63212
sg76
(lp63213
sg262
(lp63214
sg460
(lp63215
sg78
(lp63216
sg59
(lp63217
sg80
(lp63218
sg38
(lp63219
sg83
(lp63220
sg85
(lp63221
sg306
(lp63222
sg87
(lp63223
sg89
(lp63224
sg91
(lp63225
sg12
(lp63226
sg94
(lp63227
sg96
(lp63228
sg48
(lp63229
sg99
(lp63230
sg313
(lp63231
sg223
(lp63232
sg350
(lp63233
sg118
(lp63234
sg116
(lp63235
sg174
(lp63236
sg293
(lp63237
sg32
(lp63238
sg245
(lp63239
sg429
(lp63240
sg318
(lp63241
sg46
(lp63242
sg102
(lp63243
sg104
(lp63244
sg108
(lp63245
sg110
(lp63246
sg63
(lp63247
sg52
(lp63248
sg230
(lp63249
sg438
(lp63250
I275
asg440
(lp63251
sg332
(lp63252
sg121
(lp63253
sg4
(lp63254
sg181
(lp63255
sg8
(lp63256
sg384
(lp63257
sg124
(lp63258
sg126
(lp63259
sg281
(lp63260
sg535
(lp63261
sg344
(lp63262
sg128
(lp63263
sg130
(lp63264
sg132
(lp63265
sg14
(lp63266
sg16
(lp63267
sg135
(lp63268
sg138
(lp63269
sg140
(lp63270
sg354
(lp63271
ssS'would'
p63272
(dp63273
g283
(lp63274
sg70
(lp63275
sg26
(lp63276
sg277
(lp63277
sg303
(lp63278
sg30
(lp63279
sg287
(lp63280
sg74
(lp63281
sg176
(lp63282
sg145
(lp63283
sg80
(lp63284
sg76
(lp63285
sg262
(lp63286
sg295
(lp63287
sg183
(lp63288
sg484
(lp63289
sg83
(lp63290
sg85
(lp63291
sg63
(lp63292
sg306
(lp63293
sg87
(lp63294
sg89
(lp63295
sg91
(lp63296
sg245
(lp63297
sg94
(lp63298
sg96
(lp63299
sg18
(lp63300
sg99
(lp63301
sg313
(lp63302
sg44
(lp63303
sg350
(lp63304
sg118
(lp63305
sg230
(lp63306
sg329
(lp63307
sg293
(lp63308
sg32
(lp63309
sg318
(lp63310
sg46
(lp63311
sg102
(lp63312
sg110
(lp63313
sg20
(lp63314
sg52
(lp63315
sg114
(lp63316
sg216
(lp63317
sg438
(lp63318
I2295
asg440
(lp63319
sg48
(lp63320
sg178
(lp63321
sg22
(lp63322
sg181
(lp63323
sg235
(lp63324
sg34
(lp63325
sg36
(lp63326
sg68
(lp63327
sg126
(lp63328
sg10
(lp63329
sg40
(lp63330
sg344
(lp63331
sg223
(lp63332
sg128
(lp63333
sg78
(lp63334
sg132
(lp63335
sg14
(lp63336
sg16
(lp63337
sg138
(lp63338
sg140
(lp63339
sg354
(lp63340
ssS'subtleti'
p63341
(dp63342
g85
(lp63343
I3211
assS'tl'
p63344
(dp63345
g230
(lp63346
sg18
(lp63347
sg384
(lp63348
sg108
(lp63349
I852
asg8
(lp63350
ssS'logmerg'
p63351
(dp63352
g87
(lp63353
I1971
assS'apac'
p63354
(dp63355
g72
(lp63356
I974
assS'asses'
p63357
(dp63358
g181
(lp63359
I1261
assS'spike'
p63360
(dp63361
g174
(lp63362
sg329
(lp63363
sg70
(lp63364
sg6
(lp63365
sg262
(lp63366
sg106
(lp63367
I804
assS'decentr'
p63368
(dp63369
g83
(lp63370
I1614
assS'jiaec'
p63371
(dp63372
g20
(lp63373
I1094
assS'griev'
p63374
(dp63375
g70
(lp63376
I1532
assS'autogen'
p63377
(dp63378
g116
(lp63379
I2509
assS'suddarth'
p63380
(dp63381
g277
(lp63382
sg223
(lp63383
I3494
assS'unrestrain'
p63384
(dp63385
g80
(lp63386
I2530
assS'phone'
p63387
(dp63388
g96
(lp63389
I119
asg87
(lp63390
sg178
(lp63391
sg440
(lp63392
sg76
(lp63393
ssS'hofmann'
p63394
(dp63395
g130
(lp63396
I7
assS'pulscwldlh'
p63397
(dp63398
g14
(lp63399
I4317
assS'tg'
p63400
(dp63401
g36
(lp63402
sg262
(lp63403
I1590
assS'vpl'
p63404
(dp63405
g10
(lp63406
I1082
assS'vpo'
p63407
(dp63408
g10
(lp63409
I1076
assS'adijaj'
p63410
(dp63411
g384
(lp63412
I493
assS'must'
p63413
(dp63414
g68
(lp63415
sg70
(lp63416
sg26
(lp63417
sg277
(lp63418
sg283
(lp63419
sg287
(lp63420
sg176
(lp63421
sg76
(lp63422
sg262
(lp63423
sg295
(lp63424
sg183
(lp63425
sg59
(lp63426
sg80
(lp63427
sg83
(lp63428
sg85
(lp63429
sg303
(lp63430
sg42
(lp63431
I2122
asg306
(lp63432
sg87
(lp63433
sg89
(lp63434
sg245
(lp63435
sg46
(lp63436
sg20
(lp63437
sg48
(lp63438
sg535
(lp63439
sg44
(lp63440
sg350
(lp63441
sg293
(lp63442
sg429
(lp63443
sg94
(lp63444
sg102
(lp63445
sg104
(lp63446
sg110
(lp63447
sg63
(lp63448
sg52
(lp63449
sg114
(lp63450
sg230
(lp63451
sg329
(lp63452
sg440
(lp63453
sg4
(lp63454
sg181
(lp63455
sg34
(lp63456
sg460
(lp63457
sg124
(lp63458
sg10
(lp63459
sg344
(lp63460
sg78
(lp63461
sg14
(lp63462
sg16
(lp63463
sg135
(lp63464
sg50
(lp63465
sg138
(lp63466
sg354
(lp63467
ssS'me'
p63468
(dp63469
g460
(lp63470
sg72
(lp63471
sg87
(lp63472
sg132
(lp63473
sg14
(lp63474
sg16
(lp63475
I2569
asg18
(lp63476
sg20
(lp63477
ssS'md'
p63478
(dp63479
g438
(lp63480
I1024
asg70
(lp63481
sg4
(lp63482
sg295
(lp63483
sg183
(lp63484
sg59
(lp63485
sg484
(lp63486
sg281
(lp63487
sg128
(lp63488
sg106
(lp63489
sg99
(lp63490
sg22
(lp63491
ssS'mg'
p63492
(dp63493
g230
(lp63494
sg91
(lp63495
I2160
assS'mf'
p63496
(dp63497
g332
(lp63498
I1273
asg176
(lp63499
ssS'ma'
p63500
(dp63501
g30
(lp63502
sg74
(lp63503
sg145
(lp63504
sg262
(lp63505
sg83
(lp63506
sg303
(lp63507
sg306
(lp63508
sg12
(lp63509
sg20
(lp63510
sg99
(lp63511
sg313
(lp63512
sg223
(lp63513
sg350
(lp63514
sg116
(lp63515
sg118
(lp63516
sg114
(lp63517
sg230
(lp63518
sg329
(lp63519
sg440
(lp63520
sg318
(lp63521
sg22
(lp63522
sg181
(lp63523
sg460
(lp63524
sg293
(lp63525
sg10
(lp63526
sg138
(lp63527
I3327
assS'multinomi'
p63528
(dp63529
g329
(lp63530
sg91
(lp63531
I1022
assS'mc'
p63532
(dp63533
g70
(lp63534
I1964
assS'cheapest'
p63535
(dp63536
g89
(lp63537
I661
assS'mm'
p63538
(dp63539
g283
(lp63540
sg22
(lp63541
sg59
(lp63542
sg72
(lp63543
sg10
(lp63544
sg20
(lp63545
sg135
(lp63546
I196
asg256
(lp63547
ssS'ml'
p63548
(dp63549
g30
(lp63550
sg438
(lp63551
I1322
asg145
(lp63552
sg256
(lp63553
sg72
(lp63554
sg126
(lp63555
sg12
(lp63556
sg135
(lp63557
sg223
(lp63558
sg354
(lp63559
ssS'mo'
p63560
(dp63561
g14
(lp63562
sg106
(lp63563
I1884
asg135
(lp63564
ssS'mn'
p63565
(dp63566
g438
(lp63567
I235
asg74
(lp63568
sg6
(lp63569
sg295
(lp63570
sg183
(lp63571
sg102
(lp63572
sg104
(lp63573
sg149
(lp63574
ssS'mi'
p63575
(dp63576
g216
(lp63577
sg438
(lp63578
I209
asg440
(lp63579
sg256
(lp63580
sg344
(lp63581
sg78
(lp63582
sg72
(lp63583
sg281
(lp63584
sg306
(lp63585
sg130
(lp63586
sg102
(lp63587
sg104
(lp63588
sg99
(lp63589
sg354
(lp63590
ssS'mk'
p63591
(dp63592
g438
(lp63593
I313
asg221
(lp63594
sg130
(lp63595
ssS'mj'
p63596
(dp63597
g230
(lp63598
sg438
(lp63599
I308
asg440
(lp63600
sg70
(lp63601
sg72
(lp63602
sg130
(lp63603
sg102
(lp63604
ssS'mt'
p63605
(dp63606
g216
(lp63607
sg6
(lp63608
sg306
(lp63609
sg245
(lp63610
sg221
(lp63611
sg354
(lp63612
I2139
assS'mw'
p63613
(dp63614
g306
(lp63615
sg135
(lp63616
I2538
asg72
(lp63617
sg22
(lp63618
sg256
(lp63619
ssS'mv'
p63620
(dp63621
g245
(lp63622
sg106
(lp63623
I2125
asg306
(lp63624
sg256
(lp63625
ssS'introduc'
p63626
(dp63627
g124
(lp63628
sg78
(lp63629
sg163
(lp63630
sg283
(lp63631
sg303
(lp63632
sg26
(lp63633
sg30
(lp63634
sg145
(lp63635
sg80
(lp63636
sg295
(lp63637
sg183
(lp63638
sg484
(lp63639
sg38
(lp63640
sg85
(lp63641
sg63
(lp63642
sg87
(lp63643
sg89
(lp63644
sg91
(lp63645
sg96
(lp63646
sg99
(lp63647
sg44
(lp63648
sg429
(lp63649
sg178
(lp63650
sg110
(lp63651
sg20
(lp63652
sg52
(lp63653
sg230
(lp63654
sg438
(lp63655
I1206
asg440
(lp63656
sg121
(lp63657
sg4
(lp63658
sg8
(lp63659
sg221
(lp63660
sg384
(lp63661
sg235
(lp63662
sg126
(lp63663
sg341
(lp63664
sg40
(lp63665
sg344
(lp63666
sg130
(lp63667
sg135
(lp63668
sg50
(lp63669
sg460
(lp63670
ssS'ms'
p63671
(dp63672
g174
(lp63673
sg440
(lp63674
sg121
(lp63675
sg181
(lp63676
sg8
(lp63677
sg87
(lp63678
sg96
(lp63679
I1720
asg52
(lp63680
sg350
(lp63681
ssS'mr'
p63682
(dp63683
g72
(lp63684
sg484
(lp63685
sg145
(lp63686
I2747
assS'ta'
p63687
(dp63688
g4
(lp63689
sg6
(lp63690
sg344
(lp63691
sg20
(lp63692
sg135
(lp63693
I608
asg26
(lp63694
ssS'my'
p63695
(dp63696
g106
(lp63697
I1882
asg130
(lp63698
ssS'mx'
p63699
(dp63700
g178
(lp63701
I2299
assS'loosen'
p63702
(dp63703
g42
(lp63704
I3164
assS'oool'
p63705
(dp63706
g429
(lp63707
I311
assS'tsypkin'
p63708
(dp63709
g306
(lp63710
I2238
assS'hemoglobin'
p63711
(dp63712
g130
(lp63713
I1744
assS'importantto'
p63714
(dp63715
g85
(lp63716
I3028
assS'kaelbl'
p63717
(dp63718
g293
(lp63719
I3054
assS'chaouki'
p63720
(dp63721
g46
(lp63722
I14
assS'lhml'
p63723
(dp63724
g72
(lp63725
I2350
assS'gridpoint'
p63726
(dp63727
g70
(lp63728
I892
assS'mohan'
p63729
(dp63730
g181
(lp63731
I2306
assS'attract'
p63732
(dp63733
g318
(lp63734
sg429
(lp63735
sg50
(lp63736
sg68
(lp63737
sg83
(lp63738
sg36
(lp63739
sg87
(lp63740
sg183
(lp63741
sg221
(lp63742
sg138
(lp63743
I39
asg114
(lp63744
ssS'enc'
p63745
(dp63746
g72
(lp63747
sg8
(lp63748
I2389
assS'end'
p63749
(dp63750
g70
(lp63751
sg145
(lp63752
sg76
(lp63753
sg295
(lp63754
sg183
(lp63755
sg42
(lp63756
I1774
asg94
(lp63757
sg96
(lp63758
sg18
(lp63759
sg99
(lp63760
sg44
(lp63761
sg32
(lp63762
sg318
(lp63763
sg102
(lp63764
sg63
(lp63765
sg230
(lp63766
sg174
(lp63767
sg440
(lp63768
sg332
(lp63769
sg121
(lp63770
sg22
(lp63771
sg8
(lp63772
sg34
(lp63773
sg460
(lp63774
sg68
(lp63775
sg10
(lp63776
sg344
(lp63777
sg78
(lp63778
sg132
(lp63779
ssS'trill'
p63780
(dp63781
g116
(lp63782
sg332
(lp63783
I1733
assS'eng'
p63784
(dp63785
g283
(lp63786
sg76
(lp63787
sg460
(lp63788
sg87
(lp63789
sg108
(lp63790
I2516
asg99
(lp63791
sg114
(lp63792
ssS'midsagitt'
p63793
(dp63794
g32
(lp63795
sg350
(lp63796
I2225
assS'iiii'
p63797
(dp63798
g106
(lp63799
I975
asg108
(lp63800
sg70
(lp63801
sg6
(lp63802
ssS'concis'
p63803
(dp63804
g74
(lp63805
sg130
(lp63806
sg48
(lp63807
sg313
(lp63808
sg354
(lp63809
I3124
assS'ent'
p63810
(dp63811
g106
(lp63812
I291
asg48
(lp63813
ssS'rudich'
p63814
(dp63815
g145
(lp63816
I3035
assS'ofthi'
p63817
(dp63818
g46
(lp63819
I1577
assS'gate'
p63820
(dp63821
g329
(lp63822
sg118
(lp63823
sg256
(lp63824
sg287
(lp63825
sg295
(lp63826
sg183
(lp63827
sg460
(lp63828
sg68
(lp63829
sg40
(lp63830
sg42
(lp63831
I1570
asg34
(lp63832
sg429
(lp63833
sg87
(lp63834
sg245
(lp63835
sg14
(lp63836
sg20
(lp63837
sg135
(lp63838
sg344
(lp63839
ssS'widespread'
p63840
(dp63841
g216
(lp63842
I382
asg83
(lp63843
sg22
(lp63844
sg114
(lp63845
ssS'tcurrent'
p63846
(dp63847
g99
(lp63848
I262
assS'erlenbaum'
p63849
(dp63850
g36
(lp63851
I3184
assS'goodhilli'
p63852
(dp63853
g149
(lp63854
I1942
assS'llogo'
p63855
(dp63856
g72
(lp63857
I2219
assS'befor'
p63858
(dp63859
g70
(lp63860
sg78
(lp63861
sg277
(lp63862
sg287
(lp63863
sg76
(lp63864
sg293
(lp63865
sg295
(lp63866
sg183
(lp63867
sg80
(lp63868
sg89
(lp63869
sg12
(lp63870
sg20
(lp63871
sg99
(lp63872
sg149
(lp63873
sg329
(lp63874
sg102
(lp63875
sg104
(lp63876
sg106
(lp63877
sg108
(lp63878
sg63
(lp63879
sg174
(lp63880
sg216
(lp63881
sg438
(lp63882
I286
asg318
(lp63883
sg178
(lp63884
sg22
(lp63885
sg8
(lp63886
sg126
(lp63887
sg281
(lp63888
sg130
(lp63889
sg14
(lp63890
ssS'mesh'
p63891
(dp63892
g108
(lp63893
I1958
assS'circumscrib'
p63894
(dp63895
g429
(lp63896
sg68
(lp63897
I3083
assS'parallel'
p63898
(dp63899
g283
(lp63900
sg26
(lp63901
sg277
(lp63902
sg163
(lp63903
sg76
(lp63904
sg295
(lp63905
sg183
(lp63906
sg484
(lp63907
sg83
(lp63908
sg114
(lp63909
sg42
(lp63910
I600
asg87
(lp63911
sg91
(lp63912
sg20
(lp63913
sg48
(lp63914
sg535
(lp63915
sg223
(lp63916
sg429
(lp63917
sg104
(lp63918
sg106
(lp63919
sg108
(lp63920
sg63
(lp63921
sg52
(lp63922
sg22
(lp63923
sg216
(lp63924
sg32
(lp63925
sg178
(lp63926
sg4
(lp63927
sg181
(lp63928
sg34
(lp63929
sg36
(lp63930
sg341
(lp63931
sg10
(lp63932
sg40
(lp63933
sg344
(lp63934
sg44
(lp63935
sg78
(lp63936
sg14
(lp63937
sg16
(lp63938
sg50
(lp63939
ssS'moticn'
p63940
(dp63941
g245
(lp63942
I402
assS'bormann'
p63943
(dp63944
g384
(lp63945
I2392
assS'bootstrap'
p63946
(dp63947
g96
(lp63948
I1846
asg484
(lp63949
sg76
(lp63950
ssS'exclud'
p63951
(dp63952
g332
(lp63953
sg121
(lp63954
I672
asg26
(lp63955
sg181
(lp63956
sg295
(lp63957
sg183
(lp63958
sg59
(lp63959
sg484
(lp63960
sg78
(lp63961
ssS'patent'
p63962
(dp63963
g174
(lp63964
I617
assS'environ'
p63965
(dp63966
g70
(lp63967
sg80
(lp63968
sg293
(lp63969
sg59
(lp63970
sg484
(lp63971
sg83
(lp63972
sg303
(lp63973
sg42
(lp63974
I2575
asg89
(lp63975
sg12
(lp63976
sg94
(lp63977
sg18
(lp63978
sg99
(lp63979
sg313
(lp63980
sg329
(lp63981
sg102
(lp63982
sg104
(lp63983
sg63
(lp63984
sg230
(lp63985
sg438
(lp63986
sg332
(lp63987
sg221
(lp63988
sg460
(lp63989
sg10
(lp63990
sg135
(lp63991
sg50
(lp63992
ssS'sollichg'
p63993
(dp63994
g235
(lp63995
I21
assS'incorpor'
p63996
(dp63997
g70
(lp63998
sg26
(lp63999
sg74
(lp64000
sg256
(lp64001
sg295
(lp64002
sg183
(lp64003
sg87
(lp64004
sg89
(lp64005
sg245
(lp64006
sg313
(lp64007
sg223
(lp64008
sg429
(lp64009
sg52
(lp64010
sg4
(lp64011
sg8
(lp64012
sg34
(lp64013
sg126
(lp64014
sg281
(lp64015
sg10
(lp64016
sg44
(lp64017
sg14
(lp64018
sg16
(lp64019
I1361
assS'enter'
p64020
(dp64021
g80
(lp64022
sg36
(lp64023
sg124
(lp64024
sg126
(lp64025
sg83
(lp64026
sg91
(lp64027
sg130
(lp64028
I1979
asg102
(lp64029
sg94
(lp64030
sg114
(lp64031
ssS'exclus'
p64032
(dp64033
g438
(lp64034
sg287
(lp64035
sg484
(lp64036
sg42
(lp64037
I2455
asg306
(lp64038
sg110
(lp64039
sg223
(lp64040
sg149
(lp64041
ssS'amit'
p64042
(dp64043
g262
(lp64044
I545
assS'amir'
p64045
(dp64046
g32
(lp64047
sg68
(lp64048
sg149
(lp64049
I2896
assS'composit'
p64050
(dp64051
g460
(lp64052
sg118
(lp64053
sg96
(lp64054
I1103
asg341
(lp64055
sg429
(lp64056
ssS'deform'
p64057
(dp64058
g138
(lp64059
I9
asg181
(lp64060
ssS'pulscwidth'
p64061
(dp64062
g14
(lp64063
I4298
assS'over'
p64064
(dp64065
g329
(lp64066
sg70
(lp64067
sg78
(lp64068
sg277
(lp64069
sg163
(lp64070
sg68
(lp64071
sg80
(lp64072
sg281
(lp64073
sg283
(lp64074
sg460
(lp64075
sg181
(lp64076
sg303
(lp64077
sg30
(lp64078
sg287
(lp64079
sg74
(lp64080
sg176
(lp64081
sg145
(lp64082
sg256
(lp64083
sg76
(lp64084
sg293
(lp64085
sg295
(lp64086
sg183
(lp64087
sg59
(lp64088
sg484
(lp64089
sg38
(lp64090
sg83
(lp64091
sg85
(lp64092
sg63
(lp64093
sg306
(lp64094
sg87
(lp64095
sg89
(lp64096
sg91
(lp64097
sg12
(lp64098
sg94
(lp64099
sg96
(lp64100
sg48
(lp64101
sg221
(lp64102
sg313
(lp64103
sg44
(lp64104
sg149
(lp64105
sg116
(lp64106
sg174
(lp64107
sg32
(lp64108
sg245
(lp64109
sg429
(lp64110
sg318
(lp64111
sg46
(lp64112
sg102
(lp64113
sg18
(lp64114
sg108
(lp64115
sg110
(lp64116
sg178
(lp64117
sg52
(lp64118
sg22
(lp64119
sg230
(lp64120
sg438
(lp64121
I702
asg440
(lp64122
sg332
(lp64123
sg121
(lp64124
sg4
(lp64125
sg6
(lp64126
sg235
(lp64127
sg34
(lp64128
sg36
(lp64129
sg384
(lp64130
sg124
(lp64131
sg126
(lp64132
sg341
(lp64133
sg10
(lp64134
sg344
(lp64135
sg223
(lp64136
sg128
(lp64137
sg130
(lp64138
sg132
(lp64139
sg14
(lp64140
sg16
(lp64141
sg135
(lp64142
sg50
(lp64143
sg138
(lp64144
sg140
(lp64145
sg354
(lp64146
ssS'becaus'
p64147
(dp64148
g68
(lp64149
sg70
(lp64150
sg26
(lp64151
sg277
(lp64152
sg163
(lp64153
sg30
(lp64154
sg287
(lp64155
sg74
(lp64156
sg256
(lp64157
sg76
(lp64158
sg262
(lp64159
sg344
(lp64160
sg183
(lp64161
sg80
(lp64162
sg83
(lp64163
sg85
(lp64164
sg303
(lp64165
sg42
(lp64166
I2072
asg306
(lp64167
sg87
(lp64168
sg89
(lp64169
sg460
(lp64170
sg12
(lp64171
sg46
(lp64172
sg20
(lp64173
sg48
(lp64174
sg99
(lp64175
sg223
(lp64176
sg149
(lp64177
sg230
(lp64178
sg293
(lp64179
sg350
(lp64180
sg429
(lp64181
sg318
(lp64182
sg94
(lp64183
sg104
(lp64184
sg110
(lp64185
sg63
(lp64186
sg114
(lp64187
sg216
(lp64188
sg174
(lp64189
sg32
(lp64190
sg18
(lp64191
sg178
(lp64192
sg22
(lp64193
sg235
(lp64194
sg34
(lp64195
sg36
(lp64196
sg384
(lp64197
sg124
(lp64198
sg126
(lp64199
sg281
(lp64200
sg10
(lp64201
sg40
(lp64202
sg44
(lp64203
sg128
(lp64204
sg132
(lp64205
sg135
(lp64206
sg138
(lp64207
sg140
(lp64208
sg354
(lp64209
ssS'london'
p64210
(dp64211
g174
(lp64212
sg176
(lp64213
sg163
(lp64214
sg295
(lp64215
sg183
(lp64216
sg384
(lp64217
sg245
(lp64218
sg12
(lp64219
I2590
assS'unfeas'
p64220
(dp64221
g96
(lp64222
I290
assS'cortic'
p64223
(dp64224
g438
(lp64225
I202
asg332
(lp64226
sg70
(lp64227
sg80
(lp64228
sg6
(lp64229
sg262
(lp64230
sg176
(lp64231
sg429
(lp64232
sg318
(lp64233
sg12
(lp64234
sg106
(lp64235
sg48
(lp64236
sg535
(lp64237
sg149
(lp64238
ssS'digest'
p64239
(dp64240
g135
(lp64241
I2543
assS'compensatori'
p64242
(dp64243
g350
(lp64244
I461
assS'shinghal'
p64245
(dp64246
g63
(lp64247
I3197
assS'clk'
p64248
(dp64249
g22
(lp64250
I683
assS'iixi'
p64251
(dp64252
g78
(lp64253
I1876
assS'cla'
p64254
(dp64255
g318
(lp64256
I2869
assS'clb'
p64257
(dp64258
g132
(lp64259
I2356
assS'complex'
p64260
(dp64261
g68
(lp64262
sg277
(lp64263
sg163
(lp64264
sg30
(lp64265
sg287
(lp64266
sg74
(lp64267
sg118
(lp64268
sg295
(lp64269
sg183
(lp64270
sg59
(lp64271
sg85
(lp64272
sg63
(lp64273
sg42
(lp64274
I694
asg306
(lp64275
sg91
(lp64276
sg46
(lp64277
sg20
(lp64278
sg48
(lp64279
sg221
(lp64280
sg535
(lp64281
sg223
(lp64282
sg350
(lp64283
sg329
(lp64284
sg293
(lp64285
sg429
(lp64286
sg318
(lp64287
sg102
(lp64288
sg104
(lp64289
sg108
(lp64290
sg110
(lp64291
sg178
(lp64292
sg230
(lp64293
sg438
(lp64294
sg332
(lp64295
sg121
(lp64296
sg22
(lp64297
sg181
(lp64298
sg34
(lp64299
sg36
(lp64300
sg460
(lp64301
sg124
(lp64302
sg126
(lp64303
sg341
(lp64304
sg10
(lp64305
sg40
(lp64306
sg344
(lp64307
sg44
(lp64308
sg130
(lp64309
sg132
(lp64310
sg14
(lp64311
sg135
(lp64312
sg138
(lp64313
sg354
(lp64314
ssS'fade'
p64315
(dp64316
g245
(lp64317
I732
asg295
(lp64318
sg26
(lp64319
sg183
(lp64320
ssS'comprehens'
p64321
(dp64322
g344
(lp64323
sg118
(lp64324
sg145
(lp64325
sg341
(lp64326
I2897
assS'bryson'
p64327
(dp64328
g59
(lp64329
I3442
assS'qualiti'
p64330
(dp64331
g118
(lp64332
sg74
(lp64333
sg22
(lp64334
sg181
(lp64335
sg295
(lp64336
sg183
(lp64337
sg245
(lp64338
sg91
(lp64339
sg130
(lp64340
sg132
(lp64341
sg46
(lp64342
sg34
(lp64343
sg138
(lp64344
I234
assS'misorient'
p64345
(dp64346
g181
(lp64347
I924
assS'hitherto'
p64348
(dp64349
g14
(lp64350
I2779
asg384
(lp64351
ssS'choic'
p64352
(dp64353
g283
(lp64354
sg72
(lp64355
sg30
(lp64356
sg287
(lp64357
sg145
(lp64358
sg80
(lp64359
sg344
(lp64360
sg83
(lp64361
sg85
(lp64362
sg306
(lp64363
sg91
(lp64364
sg245
(lp64365
sg46
(lp64366
sg221
(lp64367
sg223
(lp64368
sg429
(lp64369
sg94
(lp64370
sg102
(lp64371
sg230
(lp64372
sg32
(lp64373
sg318
(lp64374
sg4
(lp64375
sg8
(lp64376
sg34
(lp64377
sg384
(lp64378
sg124
(lp64379
sg126
(lp64380
sg44
(lp64381
sg132
(lp64382
sg135
(lp64383
sg140
(lp64384
sg354
(lp64385
I2071
assS'alex'
p64386
(dp64387
g293
(lp64388
sg303
(lp64389
I20
assS'persever'
p64390
(dp64391
g4
(lp64392
I3535
assS'handoptim'
p64393
(dp64394
g10
(lp64395
I122
assS'each'
p64396
(dp64397
g80
(lp64398
sg293
(lp64399
sg344
(lp64400
sg78
(lp64401
sg59
(lp64402
sg484
(lp64403
sg38
(lp64404
sg83
(lp64405
sg85
(lp64406
sg303
(lp64407
sg438
(lp64408
sg116
(lp64409
sg118
(lp64410
sg34
(lp64411
sg36
(lp64412
sg460
(lp64413
sg68
(lp64414
sg72
(lp64415
sg281
(lp64416
sg10
(lp64417
sg40
(lp64418
sg283
(lp64419
sg70
(lp64420
sg26
(lp64421
sg277
(lp64422
sg163
(lp64423
sg89
(lp64424
sg91
(lp64425
sg12
(lp64426
sg94
(lp64427
sg96
(lp64428
sg48
(lp64429
sg99
(lp64430
sg313
(lp64431
sg44
(lp64432
sg149
(lp64433
sg429
(lp64434
sg102
(lp64435
sg104
(lp64436
sg106
(lp64437
sg108
(lp64438
sg110
(lp64439
sg63
(lp64440
sg52
(lp64441
sg114
(lp64442
sg128
(lp64443
sg132
(lp64444
sg14
(lp64445
sg16
(lp64446
sg135
(lp64447
sg50
(lp64448
sg138
(lp64449
sg140
(lp64450
sg354
(lp64451
sg306
(lp64452
sg87
(lp64453
sg245
(lp64454
sg46
(lp64455
sg20
(lp64456
sg18
(lp64457
sg221
(lp64458
sg535
(lp64459
sg223
(lp64460
sg350
(lp64461
sg216
(lp64462
sg174
(lp64463
sg440
(lp64464
sg332
(lp64465
sg121
(lp64466
sg4
(lp64467
sg6
(lp64468
sg8
(lp64469
sg126
(lp64470
sg341
(lp64471
sg30
(lp64472
sg287
(lp64473
sg74
(lp64474
sg145
(lp64475
sg256
(lp64476
sg76
(lp64477
sg262
(lp64478
sg295
(lp64479
sg183
(lp64480
sg42
(lp64481
I1859
asg329
(lp64482
sg32
(lp64483
sg318
(lp64484
sg178
(lp64485
sg22
(lp64486
sg181
(lp64487
sg235
(lp64488
sg384
(lp64489
sg124
(lp64490
ssS'electrostat'
p64491
(dp64492
g438
(lp64493
I1904
assS'loughborough'
p64494
(dp64495
g59
(lp64496
I3315
assS'prohibit'
p64497
(dp64498
g135
(lp64499
I1032
asg48
(lp64500
sg22
(lp64501
ssS'hermanski'
p64502
(dp64503
g440
(lp64504
I2787
assS'uncorrupt'
p64505
(dp64506
g318
(lp64507
I2442
assS'storck'
p64508
(dp64509
g313
(lp64510
I297
assS'rerer'
p64511
(dp64512
g106
(lp64513
I2516
assS'depress'
p64514
(dp64515
g106
(lp64516
I10
asg50
(lp64517
ssS'evid'
p64518
(dp64519
g440
(lp64520
sg176
(lp64521
sg145
(lp64522
sg4
(lp64523
sg34
(lp64524
sg183
(lp64525
sg124
(lp64526
sg126
(lp64527
sg281
(lp64528
sg303
(lp64529
sg344
(lp64530
sg87
(lp64531
sg12
(lp64532
sg106
(lp64533
I444
asg350
(lp64534
sg99
(lp64535
sg138
(lp64536
sg223
(lp64537
sg354
(lp64538
ssS'dinosaur'
p64539
(dp64540
g181
(lp64541
I1803
assS'goe'
p64542
(dp64543
g36
(lp64544
sg18
(lp64545
I922
asg176
(lp64546
sg262
(lp64547
ssS'newli'
p64548
(dp64549
g30
(lp64550
sg91
(lp64551
I1596
asg26
(lp64552
ssS'ajw'
p64553
(dp64554
g295
(lp64555
I1694
asg183
(lp64556
ssS'aji'
p64557
(dp64558
g460
(lp64559
I950
assS'matan'
p64560
(dp64561
g181
(lp64562
I2458
assS'pattemi'
p64563
(dp64564
g110
(lp64565
I1361
assS'got'
p64566
(dp64567
g329
(lp64568
sg18
(lp64569
I1596
assS'gov'
p64570
(dp64571
g70
(lp64572
I21
assS'resistor'
p64573
(dp64574
g14
(lp64575
I2822
asg256
(lp64576
ssS'inou'
p64577
(dp64578
g96
(lp64579
I389
assS'free'
p64580
(dp64581
g329
(lp64582
sg74
(lp64583
sg178
(lp64584
sg4
(lp64585
sg262
(lp64586
sg460
(lp64587
sg484
(lp64588
sg72
(lp64589
sg85
(lp64590
sg42
(lp64591
I925
asg130
(lp64592
sg102
(lp64593
sg14
(lp64594
sg16
(lp64595
sg163
(lp64596
sg26
(lp64597
ssS'fred'
p64598
(dp64599
g118
(lp64600
sg40
(lp64601
I2213
assS'motiondetect'
p64602
(dp64603
g70
(lp64604
I1357
assS'fieldof'
p64605
(dp64606
g293
(lp64607
I655
assS'acedem'
p64608
(dp64609
g87
(lp64610
I2448
assS'puzzl'
p64611
(dp64612
g118
(lp64613
I1794
assS'errslil'
p64614
(dp64615
g295
(lp64616
I2102
asg183
(lp64617
ssS'longterm'
p64618
(dp64619
g106
(lp64620
I1275
asg99
(lp64621
ssS'filter'
p64622
(dp64623
g283
(lp64624
sg70
(lp64625
sg277
(lp64626
sg104
(lp64627
sg256
(lp64628
sg262
(lp64629
sg295
(lp64630
sg183
(lp64631
sg59
(lp64632
sg42
(lp64633
I2352
asg20
(lp64634
sg48
(lp64635
sg313
(lp64636
sg44
(lp64637
sg350
(lp64638
sg118
(lp64639
sg429
(lp64640
sg102
(lp64641
sg178
(lp64642
sg108
(lp64643
sg96
(lp64644
sg52
(lp64645
sg230
(lp64646
sg174
(lp64647
sg332
(lp64648
sg121
(lp64649
sg22
(lp64650
sg181
(lp64651
sg344
(lp64652
sg128
(lp64653
sg78
(lp64654
ssS'lozano'
p64655
(dp64656
g460
(lp64657
I637
assS'nazional'
p64658
(dp64659
g96
(lp64660
I2690
assS'soda'
p64661
(dp64662
g128
(lp64663
I2805
assS'app'
p64664
(dp64665
g176
(lp64666
I2501
asg38
(lp64667
sg163
(lp64668
ssS'onto'
p64669
(dp64670
g30
(lp64671
sg438
(lp64672
sg32
(lp64673
sg283
(lp64674
sg22
(lp64675
sg34
(lp64676
sg38
(lp64677
sg83
(lp64678
sg42
(lp64679
I1861
asg306
(lp64680
sg102
(lp64681
sg106
(lp64682
sg99
(lp64683
sg44
(lp64684
ssS'erfi'
p64685
(dp64686
g262
(lp64687
I1653
assS'littmann'
p64688
(dp64689
g59
(lp64690
I12
assS'edward'
p64691
(dp64692
g124
(lp64693
sg126
(lp64694
I10
asg22
(lp64695
sg293
(lp64696
ssS'rang'
p64697
(dp64698
g329
(lp64699
sg281
(lp64700
sg283
(lp64701
sg85
(lp64702
sg287
(lp64703
sg176
(lp64704
sg256
(lp64705
sg80
(lp64706
sg344
(lp64707
sg183
(lp64708
sg59
(lp64709
sg484
(lp64710
sg38
(lp64711
sg83
(lp64712
sg114
(lp64713
sg303
(lp64714
sg306
(lp64715
sg245
(lp64716
sg94
(lp64717
sg48
(lp64718
sg68
(lp64719
sg44
(lp64720
sg149
(lp64721
sg116
(lp64722
sg135
(lp64723
sg32
(lp64724
sg12
(lp64725
sg318
(lp64726
sg46
(lp64727
sg102
(lp64728
sg104
(lp64729
sg63
(lp64730
sg52
(lp64731
sg22
(lp64732
sg230
(lp64733
sg174
(lp64734
sg440
(lp64735
sg332
(lp64736
sg181
(lp64737
sg6
(lp64738
sg235
(lp64739
sg36
(lp64740
sg460
(lp64741
sg124
(lp64742
sg341
(lp64743
sg10
(lp64744
sg128
(lp64745
sg14
(lp64746
sg16
(lp64747
sg350
(lp64748
sg354
(lp64749
I1376
assS'demitri'
p64750
(dp64751
g429
(lp64752
I2579
assS'rank'
p64753
(dp64754
g74
(lp64755
sg277
(lp64756
sg295
(lp64757
sg183
(lp64758
sg59
(lp64759
sg124
(lp64760
sg40
(lp64761
sg306
(lp64762
sg130
(lp64763
sg132
(lp64764
I592
assS'necess'
p64765
(dp64766
g76
(lp64767
I2086
assS'restrict'
p64768
(dp64769
g283
(lp64770
sg70
(lp64771
sg163
(lp64772
sg59
(lp64773
sg306
(lp64774
sg89
(lp64775
sg46
(lp64776
sg221
(lp64777
sg535
(lp64778
sg329
(lp64779
sg429
(lp64780
sg318
(lp64781
sg102
(lp64782
sg104
(lp64783
sg438
(lp64784
I1972
asg32
(lp64785
sg332
(lp64786
sg8
(lp64787
sg34
(lp64788
sg384
(lp64789
sg68
(lp64790
sg281
(lp64791
sg128
(lp64792
sg130
(lp64793
sg14
(lp64794
sg16
(lp64795
sg460
(lp64796
sg140
(lp64797
ssS'will'
p64798
(dp64799
g329
(lp64800
sg70
(lp64801
sg78
(lp64802
sg277
(lp64803
sg72
(lp64804
sg68
(lp64805
sg281
(lp64806
sg460
(lp64807
sg36
(lp64808
sg40
(lp64809
sg26
(lp64810
sg30
(lp64811
sg287
(lp64812
sg176
(lp64813
sg145
(lp64814
sg256
(lp64815
sg76
(lp64816
sg262
(lp64817
sg295
(lp64818
sg183
(lp64819
sg59
(lp64820
sg80
(lp64821
sg38
(lp64822
sg83
(lp64823
sg85
(lp64824
sg124
(lp64825
sg306
(lp64826
sg89
(lp64827
sg91
(lp64828
sg12
(lp64829
sg94
(lp64830
sg20
(lp64831
sg48
(lp64832
sg99
(lp64833
sg313
(lp64834
sg44
(lp64835
sg149
(lp64836
sg118
(lp64837
sg230
(lp64838
sg174
(lp64839
sg293
(lp64840
sg116
(lp64841
sg32
(lp64842
sg245
(lp64843
sg429
(lp64844
sg318
(lp64845
sg46
(lp64846
sg102
(lp64847
sg104
(lp64848
sg108
(lp64849
sg110
(lp64850
sg63
(lp64851
sg52
(lp64852
sg114
(lp64853
sg216
(lp64854
sg438
(lp64855
I642
asg440
(lp64856
sg332
(lp64857
sg121
(lp64858
sg4
(lp64859
sg181
(lp64860
sg8
(lp64861
sg34
(lp64862
sg221
(lp64863
sg384
(lp64864
sg235
(lp64865
sg126
(lp64866
sg341
(lp64867
sg10
(lp64868
sg535
(lp64869
sg344
(lp64870
sg223
(lp64871
sg128
(lp64872
sg130
(lp64873
sg132
(lp64874
sg14
(lp64875
sg16
(lp64876
sg350
(lp64877
sg50
(lp64878
sg138
(lp64879
sg140
(lp64880
sg354
(lp64881
ssS'alreadi'
p64882
(dp64883
g287
(lp64884
sg59
(lp64885
sg283
(lp64886
sg277
(lp64887
sg80
(lp64888
sg235
(lp64889
sg384
(lp64890
sg68
(lp64891
sg72
(lp64892
sg83
(lp64893
sg10
(lp64894
sg42
(lp64895
I966
asg484
(lp64896
sg91
(lp64897
sg104
(lp64898
sg94
(lp64899
sg114
(lp64900
sg63
(lp64901
sg44
(lp64902
sg354
(lp64903
ssS'primari'
p64904
(dp64905
g30
(lp64906
sg438
(lp64907
I953
asg32
(lp64908
sg176
(lp64909
sg70
(lp64910
sg4
(lp64911
sg277
(lp64912
sg295
(lp64913
sg183
(lp64914
sg26
(lp64915
sg10
(lp64916
sg78
(lp64917
sg12
(lp64918
sg94
(lp64919
sg22
(lp64920
sg313
(lp64921
sg44
(lp64922
sg149
(lp64923
ssS'withheld'
p64924
(dp64925
g85
(lp64926
I265
assS'toy'
p64927
(dp64928
g221
(lp64929
sg126
(lp64930
I1788
assS'rewritten'
p64931
(dp64932
g132
(lp64933
sg535
(lp64934
sg140
(lp64935
I2023
assS'top'
p64936
(dp64937
g70
(lp64938
sg30
(lp64939
sg74
(lp64940
sg262
(lp64941
sg183
(lp64942
sg484
(lp64943
sg83
(lp64944
sg85
(lp64945
sg42
(lp64946
I2363
asg12
(lp64947
sg20
(lp64948
sg48
(lp64949
sg44
(lp64950
sg350
(lp64951
sg318
(lp64952
sg178
(lp64953
sg63
(lp64954
sg52
(lp64955
sg22
(lp64956
sg116
(lp64957
sg118
(lp64958
sg18
(lp64959
sg121
(lp64960
sg4
(lp64961
sg235
(lp64962
sg384
(lp64963
sg126
(lp64964
sg14
(lp64965
sg135
(lp64966
sg138
(lp64967
sg140
(lp64968
ssS'inverteriv'
p64969
(dp64970
g20
(lp64971
I759
assS'tot'
p64972
(dp64973
g138
(lp64974
I569
assS'toj'
p64975
(dp64976
g350
(lp64977
I531
assS'ton'
p64978
(dp64979
g38
(lp64980
I346
assS'too'
p64981
(dp64982
g277
(lp64983
sg287
(lp64984
sg74
(lp64985
sg176
(lp64986
sg145
(lp64987
sg295
(lp64988
sg183
(lp64989
sg85
(lp64990
sg42
(lp64991
I693
asg91
(lp64992
sg20
(lp64993
sg221
(lp64994
sg44
(lp64995
sg110
(lp64996
sg52
(lp64997
sg174
(lp64998
sg178
(lp64999
sg235
(lp65000
sg34
(lp65001
sg384
(lp65002
sg132
(lp65003
sg135
(lp65004
sg50
(lp65005
ssS'tom'
p65006
(dp65007
g277
(lp65008
sg46
(lp65009
sg132
(lp65010
sg14
(lp65011
sg16
(lp65012
I42
asg99
(lp65013
ssS'decorrel'
p65014
(dp65015
g12
(lp65016
sg96
(lp65017
sg484
(lp65018
sg50
(lp65019
I72
asg163
(lp65020
ssS'corpus'
p65021
(dp65022
g96
(lp65023
I127
asg87
(lp65024
sg440
(lp65025
ssS'toe'
p65026
(dp65027
g89
(lp65028
I835
assS'consol'
p65029
(dp65030
g42
(lp65031
I1957
assS'servo'
p65032
(dp65033
g126
(lp65034
I2505
assS'tool'
p65035
(dp65036
g32
(lp65037
sg145
(lp65038
sg22
(lp65039
sg38
(lp65040
sg384
(lp65041
sg126
(lp65042
sg10
(lp65043
sg40
(lp65044
sg85
(lp65045
sg132
(lp65046
I374
asg104
(lp65047
sg99
(lp65048
sg535
(lp65049
sg350
(lp65050
ssS'pavel'
p65051
(dp65052
g110
(lp65053
I10
assS'took'
p65054
(dp65055
g126
(lp65056
sg83
(lp65057
sg87
(lp65058
sg89
(lp65059
sg18
(lp65060
sg138
(lp65061
sg52
(lp65062
sg354
(lp65063
I2584
assS'ooannlon'
p65064
(dp65065
g72
(lp65066
I985
assS'incur'
p65067
(dp65068
g306
(lp65069
I277
asg283
(lp65070
sg145
(lp65071
sg281
(lp65072
ssS'conserv'
p65073
(dp65074
g245
(lp65075
sg94
(lp65076
I3404
asg163
(lp65077
sg83
(lp65078
sg329
(lp65079
ssS'stator'
p65080
(dp65081
g78
(lp65082
I907
assS'simula'
p65083
(dp65084
g121
(lp65085
I2664
assS'popyack'
p65086
(dp65087
g14
(lp65088
sg16
(lp65089
I2618
assS'neurophysiolog'
p65090
(dp65091
g174
(lp65092
sg32
(lp65093
sg4
(lp65094
sg80
(lp65095
sg8
(lp65096
sg118
(lp65097
sg12
(lp65098
I625
asg303
(lp65099
sg350
(lp65100
ssS'tooz'
p65101
(dp65102
g26
(lp65103
I3229
assS'pancak'
p65104
(dp65105
g30
(lp65106
I1430
assS'dopaminerg'
p65107
(dp65108
g4
(lp65109
I171
assS'qmi'
p65110
(dp65111
g72
(lp65112
I2946
assS'pyramidal'
p65113
(dp65114
g106
(lp65115
I1009
assS'erickson'
p65116
(dp65117
g216
(lp65118
I2018
assS'gabor'
p65119
(dp65120
g59
(lp65121
sg181
(lp65122
I1148
assS'modulatori'
p65123
(dp65124
g4
(lp65125
I2910
assS'edmondo'
p65126
(dp65127
g96
(lp65128
I39
assS'eai'
p65129
(dp65130
g429
(lp65131
I1099
assS'fashion'
p65132
(dp65133
g287
(lp65134
sg6
(lp65135
sg59
(lp65136
sg68
(lp65137
sg87
(lp65138
sg130
(lp65139
sg12
(lp65140
sg18
(lp65141
sg99
(lp65142
sg63
(lp65143
sg140
(lp65144
I1723
assS'fleisher'
p65145
(dp65146
g341
(lp65147
I2911
assS'ran'
p65148
(dp65149
g293
(lp65150
sg303
(lp65151
I1853
assS'programmin'
p65152
(dp65153
g89
(lp65154
I451
assS'ram'
p65155
(dp65156
g135
(lp65157
I1196
asg283
(lp65158
sg6
(lp65159
ssS'hypernet'
p65160
(dp65161
g283
(lp65162
I105
assS'raw'
p65163
(dp65164
g70
(lp65165
sg6
(lp65166
sg76
(lp65167
sg132
(lp65168
I1907
asg121
(lp65169
sg63
(lp65170
ssS'rat'
p65171
(dp65172
g116
(lp65173
sg106
(lp65174
I2618
asg80
(lp65175
ssS'rar'
p65176
(dp65177
g384
(lp65178
I1349
assS'pvcn'
p65179
(dp65180
g174
(lp65181
I2578
assS'furlanello'
p65182
(dp65183
g96
(lp65184
I10
assS'lbl'
p65185
(dp65186
g12
(lp65187
I2845
assS'gurewitz'
p65188
(dp65189
g74
(lp65190
I3206
assS'ray'
p65191
(dp65192
g36
(lp65193
I1078
assS'hisashi'
p65194
(dp65195
g42
(lp65196
I9
assS'maxq'
p65197
(dp65198
g293
(lp65199
I2382
assS'thorough'
p65200
(dp65201
g126
(lp65202
sg149
(lp65203
I1928
assS'fuzzi'
p65204
(dp65205
g42
(lp65206
I1631
asg68
(lp65207
ssS'thoma'
p65208
(dp65209
g132
(lp65210
I3476
asg78
(lp65211
sg130
(lp65212
ssS'juli'
p65213
(dp65214
g68
(lp65215
sg145
(lp65216
sg22
(lp65217
I2531
asg40
(lp65218
ssS'stiil'
p65219
(dp65220
g223
(lp65221
I2747
assS'transistor'
p65222
(dp65223
g256
(lp65224
sg10
(lp65225
sg245
(lp65226
sg14
(lp65227
sg16
(lp65228
sg135
(lp65229
I1150
asg20
(lp65230
ssS'insur'
p65231
(dp65232
g277
(lp65233
sg44
(lp65234
I232
assS'aldrighetti'
p65235
(dp65236
g484
(lp65237
I2571
assS'though'
p65238
(dp65239
g283
(lp65240
sg277
(lp65241
sg163
(lp65242
sg287
(lp65243
sg145
(lp65244
sg76
(lp65245
sg183
(lp65246
sg85
(lp65247
sg303
(lp65248
sg42
(lp65249
I407
asg306
(lp65250
sg87
(lp65251
sg89
(lp65252
sg91
(lp65253
sg48
(lp65254
sg44
(lp65255
sg350
(lp65256
sg174
(lp65257
sg52
(lp65258
sg114
(lp65259
sg438
(lp65260
sg440
(lp65261
sg18
(lp65262
sg121
(lp65263
sg34
(lp65264
sg126
(lp65265
sg341
(lp65266
sg128
(lp65267
sg130
(lp65268
sg135
(lp65269
sg50
(lp65270
sg354
(lp65271
ssS'vestibu'
p65272
(dp65273
g350
(lp65274
I2911
assS'posner'
p65275
(dp65276
g18
(lp65277
I166
asg178
(lp65278
ssS'bsj'
p65279
(dp65280
g535
(lp65281
I1691
assS'coil'
p65282
(dp65283
g14
(lp65284
sg16
(lp65285
I496
asg26
(lp65286
ssS'insul'
p65287
(dp65288
g78
(lp65289
sg106
(lp65290
I1058
assS'coin'
p65291
(dp65292
g89
(lp65293
I2623
assS'oton'
p65294
(dp65295
g178
(lp65296
I234
assS'nontarget'
p65297
(dp65298
g4
(lp65299
I1169
assS'flow'
p65300
(dp65301
g116
(lp65302
sg59
(lp65303
sg70
(lp65304
sg4
(lp65305
sg384
(lp65306
sg68
(lp65307
sg535
(lp65308
sg429
(lp65309
sg46
(lp65310
sg245
(lp65311
sg14
(lp65312
sg16
(lp65313
I698
asg99
(lp65314
sg20
(lp65315
sg44
(lp65316
sg256
(lp65317
ssS'metam'
p65318
(dp65319
g216
(lp65320
I2321
assS'lllodel'
p65321
(dp65322
g48
(lp65323
I472
assS'declar'
p65324
(dp65325
g99
(lp65326
I2775
assS'tha'
p65327
(dp65328
g221
(lp65329
I346
assS'diag'
p65330
(dp65331
g318
(lp65332
I1448
assS'yann'
p65333
(dp65334
g132
(lp65335
sg138
(lp65336
I3248
assS'synapt'
p65337
(dp65338
g438
(lp65339
I437
asg70
(lp65340
sg176
(lp65341
sg121
(lp65342
sg4
(lp65343
sg6
(lp65344
sg384
(lp65345
sg68
(lp65346
sg535
(lp65347
sg429
(lp65348
sg12
(lp65349
sg14
(lp65350
sg106
(lp65351
sg40
(lp65352
sg99
(lp65353
sg16
(lp65354
sg52
(lp65355
sg149
(lp65356
ssS'bair'
p65357
(dp65358
g6
(lp65359
I9
assS'synaps'
p65360
(dp65361
g438
(lp65362
sg70
(lp65363
sg176
(lp65364
sg121
(lp65365
sg256
(lp65366
sg384
(lp65367
sg10
(lp65368
sg42
(lp65369
I1574
asg128
(lp65370
sg12
(lp65371
sg14
(lp65372
sg106
(lp65373
sg135
(lp65374
sg535
(lp65375
sg149
(lp65376
ssS'abl'
p65377
(dp65378
g277
(lp65379
sg287
(lp65380
sg74
(lp65381
sg176
(lp65382
sg145
(lp65383
sg256
(lp65384
sg76
(lp65385
sg118
(lp65386
sg295
(lp65387
sg183
(lp65388
sg59
(lp65389
sg83
(lp65390
sg303
(lp65391
sg89
(lp65392
sg91
(lp65393
sg94
(lp65394
sg99
(lp65395
sg535
(lp65396
sg223
(lp65397
sg329
(lp65398
sg108
(lp65399
sg63
(lp65400
sg116
(lp65401
sg174
(lp65402
sg332
(lp65403
sg178
(lp65404
sg293
(lp65405
sg344
(lp65406
sg128
(lp65407
sg78
(lp65408
sg14
(lp65409
sg50
(lp65410
sg138
(lp65411
sg140
(lp65412
I1203
assS'mjols'
p65413
(dp65414
g429
(lp65415
sg8
(lp65416
I22
assS'random'
p65417
(dp65418
g68
(lp65419
sg78
(lp65420
sg277
(lp65421
sg163
(lp65422
sg283
(lp65423
sg26
(lp65424
sg74
(lp65425
sg145
(lp65426
sg80
(lp65427
sg76
(lp65428
sg262
(lp65429
sg344
(lp65430
sg183
(lp65431
sg59
(lp65432
sg484
(lp65433
sg38
(lp65434
sg83
(lp65435
sg85
(lp65436
sg42
(lp65437
I131
asg306
(lp65438
sg89
(lp65439
sg94
(lp65440
sg18
(lp65441
sg221
(lp65442
sg313
(lp65443
sg223
(lp65444
sg149
(lp65445
sg116
(lp65446
sg429
(lp65447
sg102
(lp65448
sg108
(lp65449
sg110
(lp65450
sg52
(lp65451
sg216
(lp65452
sg318
(lp65453
sg121
(lp65454
sg4
(lp65455
sg181
(lp65456
sg235
(lp65457
sg34
(lp65458
sg36
(lp65459
sg384
(lp65460
sg124
(lp65461
sg126
(lp65462
sg281
(lp65463
sg128
(lp65464
sg130
(lp65465
sg132
(lp65466
sg135
(lp65467
sg50
(lp65468
sg460
(lp65469
sg140
(lp65470
sg354
(lp65471
ssS'popl'
p65472
(dp65473
g91
(lp65474
I3035
assS'connectionsof'
p65475
(dp65476
g149
(lp65477
I823
assS'coordinat'
p65478
(dp65479
g48
(lp65480
I811
assS'diminish'
p65481
(dp65482
g306
(lp65483
I1516
asg145
(lp65484
ssS'radic'
p65485
(dp65486
g63
(lp65487
I721
assS'rapaport'
p65488
(dp65489
g176
(lp65490
I2615
assS'munber'
p65491
(dp65492
g40
(lp65493
I2371
assS'absolut'
p65494
(dp65495
g329
(lp65496
sg118
(lp65497
sg256
(lp65498
sg277
(lp65499
sg8
(lp65500
sg59
(lp65501
sg293
(lp65502
sg341
(lp65503
sg40
(lp65504
sg63
(lp65505
sg130
(lp65506
sg94
(lp65507
sg303
(lp65508
sg535
(lp65509
sg354
(lp65510
I3011
assS'seung'
p65511
(dp65512
g36
(lp65513
sg85
(lp65514
sg140
(lp65515
I3173
assS'spite'
p65516
(dp65517
g438
(lp65518
I1103
asg83
(lp65519
ssS'herself'
p65520
(dp65521
g293
(lp65522
I580
assS'configur'
p65523
(dp65524
g70
(lp65525
sg26
(lp65526
sg74
(lp65527
sg293
(lp65528
sg295
(lp65529
sg183
(lp65530
sg59
(lp65531
sg83
(lp65532
sg42
(lp65533
I2558
asg306
(lp65534
sg94
(lp65535
sg20
(lp65536
sg108
(lp65537
sg110
(lp65538
sg63
(lp65539
sg118
(lp65540
sg22
(lp65541
sg181
(lp65542
sg8
(lp65543
sg460
(lp65544
sg68
(lp65545
sg128
(lp65546
sg130
(lp65547
sg14
(lp65548
sg16
(lp65549
ssS'baluja'
p65550
(dp65551
g94
(lp65552
I312
asg277
(lp65553
ssS'dikxixk'
p65554
(dp65555
g130
(lp65556
I449
assS'ocr'
p65557
(dp65558
g30
(lp65559
sg183
(lp65560
sg178
(lp65561
I536
assS'oct'
p65562
(dp65563
g20
(lp65564
sg72
(lp65565
sg10
(lp65566
sg140
(lp65567
I3096
asg114
(lp65568
ssS'waltz'
p65569
(dp65570
g223
(lp65571
I3482
assS'shadmehr'
p65572
(dp65573
g99
(lp65574
I12
assS'watch'
p65575
(dp65576
g59
(lp65577
I750
assS'fluid'
p65578
(dp65579
g535
(lp65580
I688
assS'neuropros'
p65581
(dp65582
g140
(lp65583
I3167
assS'hypogloss'
p65584
(dp65585
g116
(lp65586
I353
assS'report'
p65587
(dp65588
g283
(lp65589
sg78
(lp65590
sg281
(lp65591
sg30
(lp65592
sg74
(lp65593
sg176
(lp65594
sg80
(lp65595
sg76
(lp65596
sg118
(lp65597
sg295
(lp65598
sg183
(lp65599
sg59
(lp65600
sg484
(lp65601
sg83
(lp65602
sg303
(lp65603
sg42
(lp65604
I685
asg306
(lp65605
sg87
(lp65606
sg89
(lp65607
sg91
(lp65608
sg12
(lp65609
sg94
(lp65610
sg96
(lp65611
sg48
(lp65612
sg99
(lp65613
sg313
(lp65614
sg44
(lp65615
sg350
(lp65616
sg230
(lp65617
sg329
(lp65618
sg18
(lp65619
sg245
(lp65620
sg429
(lp65621
sg104
(lp65622
sg106
(lp65623
sg108
(lp65624
sg110
(lp65625
sg63
(lp65626
sg114
(lp65627
sg216
(lp65628
sg174
(lp65629
sg440
(lp65630
sg332
(lp65631
sg121
(lp65632
sg181
(lp65633
sg6
(lp65634
sg221
(lp65635
sg460
(lp65636
sg124
(lp65637
sg126
(lp65638
sg341
(lp65639
sg10
(lp65640
sg40
(lp65641
sg344
(lp65642
sg223
(lp65643
sg36
(lp65644
sg132
(lp65645
sg354
(lp65646
ssS'reconstruct'
p65647
(dp65648
g30
(lp65649
sg74
(lp65650
sg22
(lp65651
sg80
(lp65652
sg295
(lp65653
sg183
(lp65654
sg59
(lp65655
sg78
(lp65656
sg102
(lp65657
sg14
(lp65658
sg16
(lp65659
I2253
asg99
(lp65660
sg52
(lp65661
ssS'impic'
p65662
(dp65663
g535
(lp65664
I1797
assS'apl'
p65665
(dp65666
g110
(lp65667
I3204
assS'tsotso'
p65668
(dp65669
g178
(lp65670
I158
assS'tigat'
p65671
(dp65672
g63
(lp65673
I2161
assS'individua'
p65674
(dp65675
g114
(lp65676
I2172
assS'twice'
p65677
(dp65678
g230
(lp65679
sg174
(lp65680
sg74
(lp65681
sg287
(lp65682
sg116
(lp65683
sg83
(lp65684
sg85
(lp65685
sg303
(lp65686
sg87
(lp65687
sg78
(lp65688
sg46
(lp65689
sg48
(lp65690
I1645
asg52
(lp65691
ssS'malsburg'
p65692
(dp65693
g429
(lp65694
sg176
(lp65695
sg149
(lp65696
I324
assS'lobul'
p65697
(dp65698
g303
(lp65699
I2788
assS'rfwr'
p65700
(dp65701
g295
(lp65702
I883
asg183
(lp65703
ssS'llthe'
p65704
(dp65705
g121
(lp65706
I2763
assS'pfeiffer'
p65707
(dp65708
g59
(lp65709
I1077
assS'swept'
p65710
(dp65711
g14
(lp65712
I4368
asg429
(lp65713
ssS'habit'
p65714
(dp65715
g99
(lp65716
I3434
assS'rjakt'
p65717
(dp65718
g68
(lp65719
I2331
assS'resist'
p65720
(dp65721
g438
(lp65722
I1132
asg256
(lp65723
sg118
(lp65724
sg245
(lp65725
sg14
(lp65726
sg106
(lp65727
sg135
(lp65728
sg99
(lp65729
sg20
(lp65730
ssS'keenan'
p65731
(dp65732
g138
(lp65733
I3304
assS'num'
p65734
(dp65735
g91
(lp65736
I2100
assS'corrupt'
p65737
(dp65738
g30
(lp65739
sg318
(lp65740
sg235
(lp65741
sg124
(lp65742
sg85
(lp65743
sg102
(lp65744
I2077
asg163
(lp65745
sg52
(lp65746
ssS'percuss'
p65747
(dp65748
g174
(lp65749
I1582
assS'nuc'
p65750
(dp65751
g350
(lp65752
I1768
assS'flammia'
p65753
(dp65754
g440
(lp65755
I2544
assS'tensor'
p65756
(dp65757
g34
(lp65758
I864
assS'databas'
p65759
(dp65760
g277
(lp65761
sg76
(lp65762
sg344
(lp65763
sg183
(lp65764
sg484
(lp65765
sg42
(lp65766
I5
asg91
(lp65767
sg94
(lp65768
sg96
(lp65769
sg223
(lp65770
sg429
(lp65771
sg178
(lp65772
sg63
(lp65773
sg440
(lp65774
sg121
(lp65775
sg181
(lp65776
sg6
(lp65777
sg44
(lp65778
sg132
(lp65779
sg14
(lp65780
sg16
(lp65781
sg138
(lp65782
ssS'mux'
p65783
(dp65784
g22
(lp65785
I717
assS'discoveri'
p65786
(dp65787
g30
(lp65788
sg183
(lp65789
sg74
(lp65790
sg50
(lp65791
I1687
asg344
(lp65792
ssS'brownian'
p65793
(dp65794
g262
(lp65795
I2179
assS'outstand'
p65796
(dp65797
g63
(lp65798
I2670
assS'finger'
p65799
(dp65800
g94
(lp65801
sg59
(lp65802
sg176
(lp65803
sg99
(lp65804
I3090
assS'intrat'
p65805
(dp65806
g318
(lp65807
I2044
assS'approach'
p65808
(dp65809
g329
(lp65810
sg70
(lp65811
sg78
(lp65812
sg277
(lp65813
sg163
(lp65814
sg68
(lp65815
sg281
(lp65816
sg283
(lp65817
sg36
(lp65818
sg26
(lp65819
sg30
(lp65820
sg287
(lp65821
sg74
(lp65822
sg145
(lp65823
sg76
(lp65824
sg293
(lp65825
sg295
(lp65826
sg183
(lp65827
sg59
(lp65828
sg484
(lp65829
sg83
(lp65830
sg85
(lp65831
sg124
(lp65832
sg42
(lp65833
I324
asg306
(lp65834
sg87
(lp65835
sg89
(lp65836
sg91
(lp65837
sg12
(lp65838
sg94
(lp65839
sg96
(lp65840
sg48
(lp65841
sg99
(lp65842
sg313
(lp65843
sg44
(lp65844
sg149
(lp65845
sg174
(lp65846
sg460
(lp65847
sg245
(lp65848
sg429
(lp65849
sg318
(lp65850
sg46
(lp65851
sg104
(lp65852
sg108
(lp65853
sg110
(lp65854
sg52
(lp65855
sg114
(lp65856
sg230
(lp65857
sg438
(lp65858
sg440
(lp65859
sg332
(lp65860
sg178
(lp65861
sg4
(lp65862
sg181
(lp65863
sg8
(lp65864
sg34
(lp65865
sg221
(lp65866
sg384
(lp65867
sg235
(lp65868
sg126
(lp65869
sg341
(lp65870
sg40
(lp65871
sg344
(lp65872
sg223
(lp65873
sg130
(lp65874
sg132
(lp65875
sg14
(lp65876
sg16
(lp65877
sg135
(lp65878
sg50
(lp65879
sg138
(lp65880
sg140
(lp65881
sg354
(lp65882
ssS'sornng'
p65883
(dp65884
g20
(lp65885
I2035
assS'handwritten'
p65886
(dp65887
g30
(lp65888
sg178
(lp65889
sg76
(lp65890
sg181
(lp65891
sg183
(lp65892
sg42
(lp65893
I82
asg94
(lp65894
sg50
(lp65895
sg138
(lp65896
sg44
(lp65897
sg114
(lp65898
ssS'positron'
p65899
(dp65900
g318
(lp65901
I2308
assS'nistir'
p65902
(dp65903
g178
(lp65904
I2545
assS'weak'
p65905
(dp65906
g216
(lp65907
sg287
(lp65908
sg74
(lp65909
sg145
(lp65910
sg26
(lp65911
sg344
(lp65912
sg183
(lp65913
sg68
(lp65914
sg38
(lp65915
sg281
(lp65916
sg85
(lp65917
sg132
(lp65918
sg70
(lp65919
sg106
(lp65920
I343
asg63
(lp65921
sg149
(lp65922
ssS'nsec'
p65923
(dp65924
g20
(lp65925
I671
assS'southeast'
p65926
(dp65927
g80
(lp65928
I346
assS'eli'
p65929
(dp65930
g429
(lp65931
I2598
assS'wear'
p65932
(dp65933
g59
(lp65934
sg303
(lp65935
I2734
assS'bes'
p65936
(dp65937
g277
(lp65938
I3032
assS'news'
p65939
(dp65940
g277
(lp65941
I3129
assS'brnpbmk'
p65942
(dp65943
g230
(lp65944
I1050
assS'vergenc'
p65945
(dp65946
g32
(lp65947
sg350
(lp65948
I117
assS'protect'
p65949
(dp65950
g132
(lp65951
sg14
(lp65952
I3060
asg78
(lp65953
sg114
(lp65954
ssS'acoust'
p65955
(dp65956
g116
(lp65957
sg174
(lp65958
sg440
(lp65959
sg332
(lp65960
sg22
(lp65961
sg87
(lp65962
sg94
(lp65963
I3601
asg96
(lp65964
ssS'rightward'
p65965
(dp65966
g245
(lp65967
I2102
asg303
(lp65968
ssS'irregular'
p65969
(dp65970
g223
(lp65971
I3425
asg262
(lp65972
ssS'terrenc'
p65973
(dp65974
g318
(lp65975
sg303
(lp65976
sg91
(lp65977
sg132
(lp65978
sg106
(lp65979
I21
asg50
(lp65980
sg350
(lp65981
ssS'fault'
p65982
(dp65983
g78
(lp65984
sg332
(lp65985
sg91
(lp65986
I45
assS'myer'
p65987
(dp65988
g78
(lp65989
sg91
(lp65990
I3038
assS'inpol'
p65991
(dp65992
g18
(lp65993
I1112
assS'ventricular'
p65994
(dp65995
g135
(lp65996
I209
assS'elayo'
p65997
(dp65998
g87
(lp65999
I1397
assS'monograph'
p66000
(dp66001
g216
(lp66002
sg96
(lp66003
I2854
assS'ansatz'
p66004
(dp66005
g102
(lp66006
I1049
asg384
(lp66007
ssS'pariet'
p66008
(dp66009
g303
(lp66010
I7
assS'sullivan'
p66011
(dp66012
g99
(lp66013
I3286
asg223
(lp66014
ssS'fogelman'
p66015
(dp66016
g178
(lp66017
I2512
assS'jxi'
p66018
(dp66019
g72
(lp66020
sg145
(lp66021
I1371
assS'somatosensori'
p66022
(dp66023
g176
(lp66024
I3
assS'trust'
p66025
(dp66026
g183
(lp66027
sg8
(lp66028
I135
assS'bingo'
p66029
(dp66030
g318
(lp66031
sg50
(lp66032
I447
assS'evalul'
p66033
(dp66034
g36
(lp66035
I1109
assS'been'
p66036
(dp66037
g80
(lp66038
sg293
(lp66039
sg344
(lp66040
sg78
(lp66041
sg59
(lp66042
sg484
(lp66043
sg38
(lp66044
sg83
(lp66045
sg303
(lp66046
sg438
(lp66047
sg34
(lp66048
sg36
(lp66049
sg460
(lp66050
sg68
(lp66051
sg72
(lp66052
sg281
(lp66053
sg10
(lp66054
sg40
(lp66055
sg283
(lp66056
sg70
(lp66057
sg26
(lp66058
sg277
(lp66059
sg163
(lp66060
sg89
(lp66061
sg91
(lp66062
sg12
(lp66063
sg94
(lp66064
sg96
(lp66065
sg48
(lp66066
sg99
(lp66067
sg313
(lp66068
sg44
(lp66069
sg149
(lp66070
sg429
(lp66071
sg102
(lp66072
sg104
(lp66073
sg106
(lp66074
sg110
(lp66075
sg63
(lp66076
sg52
(lp66077
sg114
(lp66078
sg128
(lp66079
sg130
(lp66080
sg132
(lp66081
sg14
(lp66082
sg16
(lp66083
sg135
(lp66084
sg138
(lp66085
sg140
(lp66086
sg354
(lp66087
sg306
(lp66088
sg87
(lp66089
sg245
(lp66090
sg46
(lp66091
sg20
(lp66092
sg221
(lp66093
sg223
(lp66094
sg350
(lp66095
sg216
(lp66096
sg174
(lp66097
sg440
(lp66098
sg332
(lp66099
sg121
(lp66100
sg4
(lp66101
sg6
(lp66102
sg8
(lp66103
sg126
(lp66104
sg341
(lp66105
sg287
(lp66106
sg74
(lp66107
sg176
(lp66108
sg145
(lp66109
sg256
(lp66110
sg76
(lp66111
sg295
(lp66112
sg183
(lp66113
sg42
(lp66114
I591
asg230
(lp66115
sg329
(lp66116
sg32
(lp66117
sg318
(lp66118
sg178
(lp66119
sg22
(lp66120
sg181
(lp66121
sg235
(lp66122
sg384
(lp66123
sg124
(lp66124
ssS'alistair'
p66125
(dp66126
g283
(lp66127
I10
assS'accumul'
p66128
(dp66129
g174
(lp66130
sg283
(lp66131
sg178
(lp66132
sg4
(lp66133
sg8
(lp66134
sg295
(lp66135
sg183
(lp66136
sg68
(lp66137
sg10
(lp66138
sg245
(lp66139
sg138
(lp66140
I1571
asg44
(lp66141
ssS'leerink'
p66142
(dp66143
g135
(lp66144
I2601
assS'beet'
p66145
(dp66146
g174
(lp66147
I2379
assS'mult'
p66148
(dp66149
g22
(lp66150
I711
assS'cortico'
p66151
(dp66152
g438
(lp66153
I223
assS'loft'
p66154
(dp66155
g149
(lp66156
I810
assS'hepard'
p66157
(dp66158
g223
(lp66159
I2603
assS'xxx'
p66160
(dp66161
g52
(lp66162
I1206
assS'uncommon'
p66163
(dp66164
g78
(lp66165
I203
assS'craft'
p66166
(dp66167
g59
(lp66168
I3010
assS'llll'
p66169
(dp66170
g116
(lp66171
I1814
assS'morioton'
p66172
(dp66173
g341
(lp66174
I2566
assS'xiiyi'
p66175
(dp66176
g72
(lp66177
I2905
assS'catch'
p66178
(dp66179
g42
(lp66180
I1755
asg354
(lp66181
ssS'xdyi'
p66182
(dp66183
g72
(lp66184
I2876
assS'lessen'
p66185
(dp66186
g102
(lp66187
sg438
(lp66188
I1558
assS'n'
p66189
(dp66190
g80
(lp66191
sg344
(lp66192
sg78
(lp66193
sg59
(lp66194
sg484
(lp66195
sg38
(lp66196
sg83
(lp66197
sg85
(lp66198
sg303
(lp66199
sg438
(lp66200
sg116
(lp66201
sg118
(lp66202
sg34
(lp66203
sg36
(lp66204
sg460
(lp66205
sg68
(lp66206
sg72
(lp66207
sg281
(lp66208
sg10
(lp66209
sg40
(lp66210
sg283
(lp66211
sg26
(lp66212
sg163
(lp66213
sg89
(lp66214
sg91
(lp66215
sg94
(lp66216
sg96
(lp66217
sg48
(lp66218
sg99
(lp66219
sg313
(lp66220
sg44
(lp66221
sg149
(lp66222
sg429
(lp66223
sg102
(lp66224
sg104
(lp66225
sg106
(lp66226
sg108
(lp66227
sg110
(lp66228
sg63
(lp66229
sg52
(lp66230
sg128
(lp66231
sg130
(lp66232
sg14
(lp66233
sg16
(lp66234
sg135
(lp66235
sg50
(lp66236
sg140
(lp66237
sg354
(lp66238
sg306
(lp66239
sg87
(lp66240
sg245
(lp66241
sg46
(lp66242
sg20
(lp66243
sg18
(lp66244
sg221
(lp66245
sg535
(lp66246
sg223
(lp66247
sg350
(lp66248
sg216
(lp66249
sg174
(lp66250
sg440
(lp66251
sg332
(lp66252
sg121
(lp66253
sg4
(lp66254
sg6
(lp66255
sg8
(lp66256
sg126
(lp66257
sg341
(lp66258
sg30
(lp66259
sg287
(lp66260
sg74
(lp66261
sg176
(lp66262
sg145
(lp66263
sg256
(lp66264
sg76
(lp66265
sg262
(lp66266
sg295
(lp66267
sg183
(lp66268
sg42
(lp66269
I1252
asg230
(lp66270
sg329
(lp66271
sg32
(lp66272
sg318
(lp66273
sg178
(lp66274
sg22
(lp66275
sg235
(lp66276
sg384
(lp66277
sg124
(lp66278
ssS'lesser'
p66279
(dp66280
g83
(lp66281
I2779
assS'dept'
p66282
(dp66283
g283
(lp66284
sg72
(lp66285
sg74
(lp66286
sg145
(lp66287
sg262
(lp66288
sg344
(lp66289
sg484
(lp66290
sg38
(lp66291
sg303
(lp66292
sg18
(lp66293
sg99
(lp66294
sg110
(lp66295
sg114
(lp66296
sg329
(lp66297
sg4
(lp66298
sg6
(lp66299
sg8
(lp66300
sg36
(lp66301
sg384
(lp66302
sg124
(lp66303
sg126
(lp66304
sg138
(lp66305
I3511
asg14
(lp66306
sg460
(lp66307
ssS'riclulrd'
p66308
(dp66309
g135
(lp66310
I2297
assS'lperform'
p66311
(dp66312
g277
(lp66313
I1367
assS'omohundro'
p66314
(dp66315
g30
(lp66316
I5
assS'della'
p66317
(dp66318
g8
(lp66319
I2449
assS'pyramid'
p66320
(dp66321
g438
(lp66322
I1140
asg106
(lp66323
sg68
(lp66324
sg429
(lp66325
sg6
(lp66326
ssS'kingsburi'
p66327
(dp66328
g10
(lp66329
I21
assS'brewer'
p66330
(dp66331
g223
(lp66332
I3096
assS'elogp'
p66333
(dp66334
g329
(lp66335
I1375
assS'tabl'
p66336
(dp66337
g283
(lp66338
sg277
(lp66339
sg163
(lp66340
sg72
(lp66341
sg104
(lp66342
sg74
(lp66343
sg76
(lp66344
sg293
(lp66345
sg344
(lp66346
sg183
(lp66347
sg59
(lp66348
sg484
(lp66349
sg83
(lp66350
sg87
(lp66351
sg89
(lp66352
sg91
(lp66353
sg94
(lp66354
sg96
(lp66355
sg48
(lp66356
sg221
(lp66357
sg32
(lp66358
sg178
(lp66359
sg52
(lp66360
sg329
(lp66361
sg440
(lp66362
sg18
(lp66363
sg121
(lp66364
sg34
(lp66365
sg460
(lp66366
sg124
(lp66367
sg126
(lp66368
sg281
(lp66369
sg10
(lp66370
sg128
(lp66371
sg78
(lp66372
sg132
(lp66373
sg135
(lp66374
sg138
(lp66375
I2538
assS'ganong'
p66376
(dp66377
g106
(lp66378
I2696
assS'tresp'
p66379
(dp66380
g221
(lp66381
sg91
(lp66382
I721
assS'trainneuron'
p66383
(dp66384
g59
(lp66385
I2510
assS'carob'
p66386
(dp66387
g26
(lp66388
I13
assS'uncrit'
p66389
(dp66390
g34
(lp66391
I944
assS'suggest'
p66392
(dp66393
g68
(lp66394
sg70
(lp66395
sg163
(lp66396
sg72
(lp66397
sg30
(lp66398
sg80
(lp66399
sg118
(lp66400
sg295
(lp66401
sg183
(lp66402
sg83
(lp66403
sg85
(lp66404
sg303
(lp66405
sg42
(lp66406
I2505
asg306
(lp66407
sg89
(lp66408
sg91
(lp66409
sg245
(lp66410
sg94
(lp66411
sg96
(lp66412
sg48
(lp66413
sg99
(lp66414
sg44
(lp66415
sg149
(lp66416
sg174
(lp66417
sg350
(lp66418
sg46
(lp66419
sg102
(lp66420
sg18
(lp66421
sg106
(lp66422
sg52
(lp66423
sg216
(lp66424
sg438
(lp66425
sg440
(lp66426
sg332
(lp66427
sg4
(lp66428
sg6
(lp66429
sg235
(lp66430
sg34
(lp66431
sg124
(lp66432
sg126
(lp66433
sg341
(lp66434
sg344
(lp66435
sg223
(lp66436
sg128
(lp66437
sg130
(lp66438
sg132
(lp66439
sg14
(lp66440
sg16
(lp66441
sg135
(lp66442
sg50
(lp66443
sg138
(lp66444
sg140
(lp66445
ssS'spectacular'
p66446
(dp66447
g89
(lp66448
I262
assS'drawback'
p66449
(dp66450
g34
(lp66451
sg14
(lp66452
sg16
(lp66453
I1981
asg145
(lp66454
sg94
(lp66455
ssS'complet'
p66456
(dp66457
g70
(lp66458
sg287
(lp66459
sg74
(lp66460
sg176
(lp66461
sg256
(lp66462
sg80
(lp66463
sg262
(lp66464
sg78
(lp66465
sg59
(lp66466
sg484
(lp66467
sg83
(lp66468
sg306
(lp66469
sg89
(lp66470
sg91
(lp66471
sg12
(lp66472
sg94
(lp66473
sg20
(lp66474
sg221
(lp66475
sg223
(lp66476
sg116
(lp66477
sg293
(lp66478
sg245
(lp66479
sg102
(lp66480
sg104
(lp66481
sg108
(lp66482
sg110
(lp66483
sg63
(lp66484
sg230
(lp66485
sg118
(lp66486
sg440
(lp66487
sg178
(lp66488
sg4
(lp66489
sg8
(lp66490
sg34
(lp66491
sg36
(lp66492
sg384
(lp66493
sg68
(lp66494
sg10
(lp66495
sg40
(lp66496
sg128
(lp66497
sg130
(lp66498
sg132
(lp66499
sg14
(lp66500
sg16
(lp66501
sg460
(lp66502
sg140
(lp66503
I1096
assS'rotor'
p66504
(dp66505
g78
(lp66506
I900
assS'vvt'
p66507
(dp66508
g12
(lp66509
I1074
assS'propagauon'
p66510
(dp66511
g223
(lp66512
I2617
assS'riseman'
p66513
(dp66514
g429
(lp66515
I400
assS'vve'
p66516
(dp66517
g18
(lp66518
sg48
(lp66519
I1932
assS'lcd'
p66520
(dp66521
g135
(lp66522
I879
assS'felleman'
p66523
(dp66524
g176
(lp66525
I2562
assS'greatest'
p66526
(dp66527
g176
(lp66528
I525
assS'westfalen'
p66529
(dp66530
g130
(lp66531
I3105
assS'derstr'
p66532
(dp66533
g295
(lp66534
I3927
asg183
(lp66535
ssS'everyth'
p66536
(dp66537
g26
(lp66538
sg83
(lp66539
sg114
(lp66540
I1373
assS'buchsbaum'
p66541
(dp66542
g245
(lp66543
I238
assS'eeeag'
p66544
(dp66545
g178
(lp66546
I2438
assS'rodent'
p66547
(dp66548
g80
(lp66549
I50
assS'depart'
p66550
(dp66551
g329
(lp66552
sg281
(lp66553
sg283
(lp66554
sg287
(lp66555
sg74
(lp66556
sg80
(lp66557
sg76
(lp66558
sg183
(lp66559
sg484
(lp66560
sg83
(lp66561
sg87
(lp66562
sg89
(lp66563
sg245
(lp66564
sg46
(lp66565
sg20
(lp66566
sg313
(lp66567
sg223
(lp66568
sg149
(lp66569
sg116
(lp66570
sg174
(lp66571
sg12
(lp66572
sg429
(lp66573
sg178
(lp66574
sg106
(lp66575
sg52
(lp66576
sg114
(lp66577
sg230
(lp66578
sg438
(lp66579
I27
asg32
(lp66580
sg135
(lp66581
sg121
(lp66582
sg22
(lp66583
sg181
(lp66584
sg235
(lp66585
sg384
(lp66586
sg124
(lp66587
sg126
(lp66588
sg341
(lp66589
sg10
(lp66590
sg132
(lp66591
sg14
(lp66592
sg16
(lp66593
sg350
(lp66594
sg50
(lp66595
sg138
(lp66596
sg354
(lp66597
ssS'enlighten'
p66598
(dp66599
g85
(lp66600
I2636
assS'elk'
p66601
(dp66602
g22
(lp66603
sg140
(lp66604
I240
assS'igax'
p66605
(dp66606
g287
(lp66607
I2997
assS'elo'
p66608
(dp66609
g72
(lp66610
I2669
assS'ell'
p66611
(dp66612
g384
(lp66613
I1003
asg72
(lp66614
ssS'interfer'
p66615
(dp66616
g295
(lp66617
sg50
(lp66618
I323
asg384
(lp66619
sg183
(lp66620
sg14
(lp66621
sg99
(lp66622
ssS'conjug'
p66623
(dp66624
g8
(lp66625
sg34
(lp66626
sg36
(lp66627
sg124
(lp66628
sg126
(lp66629
sg14
(lp66630
sg16
(lp66631
sg221
(lp66632
sg138
(lp66633
I2117
assS'oculomotor'
p66634
(dp66635
g245
(lp66636
I1202
asg32
(lp66637
sg350
(lp66638
ssS'ele'
p66639
(dp66640
g132
(lp66641
I2278
assS'igan'
p66642
(dp66643
g350
(lp66644
I1766
assS'primit'
p66645
(dp66646
g293
(lp66647
sg535
(lp66648
I315
asg52
(lp66649
sg83
(lp66650
ssS'els'
p66651
(dp66652
g145
(lp66653
I1033
asg26
(lp66654
sg293
(lp66655
ssS'expon'
p66656
(dp66657
g32
(lp66658
I741
assS'explanatori'
p66659
(dp66660
g118
(lp66661
I428
assS'gave'
p66662
(dp66663
g26
(lp66664
sg124
(lp66665
sg63
(lp66666
sg128
(lp66667
sg96
(lp66668
sg14
(lp66669
sg16
(lp66670
sg138
(lp66671
sg140
(lp66672
I2738
assS'stationar'
p66673
(dp66674
g130
(lp66675
I1244
assS'neurophysiol'
p66676
(dp66677
g106
(lp66678
I2867
asg176
(lp66679
sg181
(lp66680
ssS'roach'
p66681
(dp66682
g14
(lp66683
sg16
(lp66684
I2443
assS'detector'
p66685
(dp66686
g216
(lp66687
sg70
(lp66688
sg78
(lp66689
sg245
(lp66690
sg20
(lp66691
sg135
(lp66692
I876
asg63
(lp66693
sg149
(lp66694
ssS'rescal'
p66695
(dp66696
g32
(lp66697
I850
asg124
(lp66698
sg460
(lp66699
ssS'apart'
p66700
(dp66701
g30
(lp66702
sg32
(lp66703
sg318
(lp66704
sg26
(lp66705
sg6
(lp66706
sg36
(lp66707
sg126
(lp66708
sg130
(lp66709
sg102
(lp66710
sg140
(lp66711
I2854
asg149
(lp66712
ssS'cext'
p66713
(dp66714
g20
(lp66715
I1278
assS'arbitrari'
p66716
(dp66717
g283
(lp66718
sg163
(lp66719
sg30
(lp66720
sg287
(lp66721
sg76
(lp66722
sg78
(lp66723
sg38
(lp66724
sg85
(lp66725
sg42
(lp66726
I1274
asg306
(lp66727
sg46
(lp66728
sg96
(lp66729
sg99
(lp66730
sg44
(lp66731
sg350
(lp66732
sg102
(lp66733
sg230
(lp66734
sg121
(lp66735
sg68
(lp66736
sg40
(lp66737
sg132
(lp66738
sg138
(lp66739
sg354
(lp66740
ssS'contradict'
p66741
(dp66742
g42
(lp66743
I419
asg68
(lp66744
sg181
(lp66745
ssS'hunt'
p66746
(dp66747
g138
(lp66748
I1601
assS'orlando'
p66749
(dp66750
g245
(lp66751
I2948
asg256
(lp66752
ssS'ncsa'
p66753
(dp66754
g36
(lp66755
sg48
(lp66756
I1978
assS'sirianni'
p66757
(dp66758
g174
(lp66759
I2569
assS'unstabl'
p66760
(dp66761
g230
(lp66762
sg438
(lp66763
I779
asg34
(lp66764
sg384
(lp66765
sg38
(lp66766
sg306
(lp66767
sg245
(lp66768
sg18
(lp66769
sg221
(lp66770
ssS'schwarz'
p66771
(dp66772
g38
(lp66773
I3318
assS'jes'
p66774
(dp66775
g306
(lp66776
I900
assS'csiszar'
p66777
(dp66778
g72
(lp66779
I262
assS'jyi'
p66780
(dp66781
g72
(lp66782
I2861
assS'acad'
p66783
(dp66784
g438
(lp66785
I2442
asg176
(lp66786
sg174
(lp66787
sg12
(lp66788
sg106
(lp66789
sg48
(lp66790
ssS'terld'
p66791
(dp66792
g145
(lp66793
I1839
assS'occulud'
p66794
(dp66795
g52
(lp66796
I2217
assS'indirect'
p66797
(dp66798
g4
(lp66799
I1872
asg8
(lp66800
sg59
(lp66801
sg126
(lp66802
sg83
(lp66803
sg429
(lp66804
ssS'wtai'
p66805
(dp66806
g135
(lp66807
I615
assS'ich'
p66808
(dp66809
g235
(lp66810
I2683
assS'cooper'
p66811
(dp66812
g438
(lp66813
I63
asg318
(lp66814
sg277
(lp66815
sg235
(lp66816
sg295
(lp66817
sg183
(lp66818
sg59
(lp66819
sg83
(lp66820
sg429
(lp66821
sg106
(lp66822
sg221
(lp66823
sg535
(lp66824
sg140
(lp66825
sg149
(lp66826
ssS'cns'
p66827
(dp66828
g118
(lp66829
sg178
(lp66830
I2660
asg256
(lp66831
ssS'harriman'
p66832
(dp66833
g108
(lp66834
I2546
assS'alamo'
p66835
(dp66836
g295
(lp66837
sg183
(lp66838
sg384
(lp66839
I56
assS'icc'
p66840
(dp66841
g6
(lp66842
I1440
assS'ica'
p66843
(dp66844
g163
(lp66845
I52
assS'icd'
p66846
(dp66847
g135
(lp66848
I74
assS'ucsc'
p66849
(dp66850
g341
(lp66851
I26
assS'ucsd'
p66852
(dp66853
g116
(lp66854
sg18
(lp66855
sg8
(lp66856
I42
assS'oftrain'
p66857
(dp66858
g99
(lp66859
I3440
assS'shamir'
p66860
(dp66861
g140
(lp66862
I3204
assS'cni'
p66863
(dp66864
g102
(lp66865
I673
assS'cord'
p66866
(dp66867
g181
(lp66868
I79
assS'core'
p66869
(dp66870
g94
(lp66871
I2722
asg20
(lp66872
sg10
(lp66873
sg78
(lp66874
ssS'subspac'
p66875
(dp66876
g30
(lp66877
sg32
(lp66878
sg8
(lp66879
sg38
(lp66880
sg72
(lp66881
sg306
(lp66882
sg44
(lp66883
sg130
(lp66884
I1685
asg46
(lp66885
sg52
(lp66886
ssS'ootherwis'
p66887
(dp66888
g94
(lp66889
I1004
assS'cort'
p66890
(dp66891
g183
(lp66892
sg48
(lp66893
I72
assS'discount'
p66894
(dp66895
g132
(lp66896
sg306
(lp66897
sg50
(lp66898
I732
asg83
(lp66899
sg293
(lp66900
ssS'quaternion'
p66901
(dp66902
g32
(lp66903
sg350
(lp66904
I2438
assS'corr'
p66905
(dp66906
g484
(lp66907
I1905
assS'hsu'
p66908
(dp66909
g277
(lp66910
I3192
assS'cugel'
p66911
(dp66912
g118
(lp66913
I403
assS'meyer'
p66914
(dp66915
g174
(lp66916
I165
asg59
(lp66917
sg535
(lp66918
ssS'chapter'
p66919
(dp66920
g42
(lp66921
I1385
asg50
(lp66922
sg80
(lp66923
sg76
(lp66924
ssS'alexand'
p66925
(dp66926
g216
(lp66927
sg30
(lp66928
sg354
(lp66929
I2913
assS'occlus'
p66930
(dp66931
g332
(lp66932
sg70
(lp66933
sg91
(lp66934
I2874
asg181
(lp66935
ssS'eat'
p66936
(dp66937
g18
(lp66938
I127
assS'plum'
p66939
(dp66940
g4
(lp66941
I3508
assS'surround'
p66942
(dp66943
g438
(lp66944
sg121
(lp66945
sg256
(lp66946
sg6
(lp66947
sg118
(lp66948
sg460
(lp66949
sg63
(lp66950
sg42
(lp66951
I2853
asg102
(lp66952
sg12
(lp66953
sg14
(lp66954
sg16
(lp66955
sg108
(lp66956
sg178
(lp66957
sg149
(lp66958
ssS'unfortun'
p66959
(dp66960
g30
(lp66961
sg484
(lp66962
sg440
(lp66963
sg318
(lp66964
sg70
(lp66965
sg277
(lp66966
sg78
(lp66967
sg384
(lp66968
sg68
(lp66969
sg74
(lp66970
sg281
(lp66971
sg306
(lp66972
sg176
(lp66973
sg89
(lp66974
sg91
(lp66975
sg104
(lp66976
sg94
(lp66977
sg50
(lp66978
I309
assS'distinct'
p66979
(dp66980
g70
(lp66981
sg30
(lp66982
sg287
(lp66983
sg74
(lp66984
sg183
(lp66985
sg484
(lp66986
sg303
(lp66987
sg223
(lp66988
sg230
(lp66989
sg130
(lp66990
sg429
(lp66991
sg63
(lp66992
sg52
(lp66993
sg216
(lp66994
sg318
(lp66995
sg4
(lp66996
sg181
(lp66997
sg68
(lp66998
sg341
(lp66999
sg40
(lp67000
sg128
(lp67001
sg78
(lp67002
sg50
(lp67003
I1058
assS'plus'
p67004
(dp67005
g329
(lp67006
sg32
(lp67007
sg121
(lp67008
sg287
(lp67009
sg293
(lp67010
sg262
(lp67011
sg63
(lp67012
sg94
(lp67013
sg106
(lp67014
I907
asg135
(lp67015
sg138
(lp67016
sg44
(lp67017
sg114
(lp67018
ssS'untrain'
p67019
(dp67020
g99
(lp67021
I1801
asg76
(lp67022
ssS'nickol'
p67023
(dp67024
g135
(lp67025
I2435
assS'produc'
p67026
(dp67027
g68
(lp67028
sg70
(lp67029
sg72
(lp67030
sg283
(lp67031
sg80
(lp67032
sg74
(lp67033
sg256
(lp67034
sg76
(lp67035
sg262
(lp67036
sg344
(lp67037
sg78
(lp67038
sg484
(lp67039
sg83
(lp67040
sg114
(lp67041
sg42
(lp67042
I67
asg306
(lp67043
sg87
(lp67044
sg89
(lp67045
sg245
(lp67046
sg94
(lp67047
sg20
(lp67048
sg18
(lp67049
sg99
(lp67050
sg313
(lp67051
sg149
(lp67052
sg174
(lp67053
sg293
(lp67054
sg318
(lp67055
sg46
(lp67056
sg102
(lp67057
sg104
(lp67058
sg106
(lp67059
sg108
(lp67060
sg52
(lp67061
sg22
(lp67062
sg116
(lp67063
sg438
(lp67064
sg440
(lp67065
sg332
(lp67066
sg4
(lp67067
sg34
(lp67068
sg221
(lp67069
sg460
(lp67070
sg124
(lp67071
sg126
(lp67072
sg341
(lp67073
sg10
(lp67074
sg118
(lp67075
sg36
(lp67076
sg132
(lp67077
sg14
(lp67078
sg16
(lp67079
sg350
(lp67080
sg138
(lp67081
ssS'aha'
p67082
(dp67083
g484
(lp67084
I1790
assS'yochai'
p67085
(dp67086
g440
(lp67087
I17
assS'tackl'
p67088
(dp67089
g59
(lp67090
sg318
(lp67091
I390
asg68
(lp67092
sg163
(lp67093
ssS'ppi'
p67094
(dp67095
g72
(lp67096
I3376
assS'encod'
p67097
(dp67098
g26
(lp67099
sg30
(lp67100
sg76
(lp67101
sg344
(lp67102
sg78
(lp67103
sg83
(lp67104
sg303
(lp67105
sg99
(lp67106
sg44
(lp67107
sg149
(lp67108
sg429
(lp67109
sg102
(lp67110
sg110
(lp67111
sg116
(lp67112
sg118
(lp67113
sg178
(lp67114
sg22
(lp67115
sg181
(lp67116
sg34
(lp67117
sg72
(lp67118
sg128
(lp67119
I1653
asg350
(lp67120
ssS'bath'
p67121
(dp67122
g106
(lp67123
I1557
assS'bielefeld'
p67124
(dp67125
g59
(lp67126
sg48
(lp67127
I32
assS'dreyfus'
p67128
(dp67129
g50
(lp67130
I1647
assS'storag'
p67131
(dp67132
g438
(lp67133
I1831
asg181
(lp67134
sg245
(lp67135
sg14
(lp67136
sg106
(lp67137
sg135
(lp67138
sg99
(lp67139
sg535
(lp67140
sg44
(lp67141
ssS'gis'
p67142
(dp67143
g235
(lp67144
I1415
assS'allocentr'
p67145
(dp67146
g80
(lp67147
I592
assS'ajudg'
p67148
(dp67149
g332
(lp67150
I1681
assS'closur'
p67151
(dp67152
g384
(lp67153
I1017
assS'transform'
p67154
(dp67155
g283
(lp67156
sg163
(lp67157
sg287
(lp67158
sg145
(lp67159
sg76
(lp67160
sg295
(lp67161
sg183
(lp67162
sg59
(lp67163
sg85
(lp67164
sg303
(lp67165
sg46
(lp67166
sg96
(lp67167
sg99
(lp67168
sg223
(lp67169
sg350
(lp67170
sg174
(lp67171
sg429
(lp67172
sg102
(lp67173
sg104
(lp67174
sg63
(lp67175
sg22
(lp67176
sg116
(lp67177
sg438
(lp67178
I350
asg32
(lp67179
sg6
(lp67180
sg181
(lp67181
sg34
(lp67182
sg384
(lp67183
sg68
(lp67184
sg44
(lp67185
sg78
(lp67186
sg132
(lp67187
sg14
(lp67188
sg16
(lp67189
sg138
(lp67190
sg354
(lp67191
ssS'virgin'
p67192
(dp67193
g14
(lp67194
I3136
assS'anderson'
p67195
(dp67196
g438
(lp67197
I2424
asg178
(lp67198
sg8
(lp67199
sg341
(lp67200
sg102
(lp67201
sg108
(lp67202
ssS'gil'
p67203
(dp67204
g108
(lp67205
I614
assS'gij'
p67206
(dp67207
g36
(lp67208
I651
asg40
(lp67209
ssS'gii'
p67210
(dp67211
g99
(lp67212
I766
assS'psychol'
p67213
(dp67214
g176
(lp67215
sg99
(lp67216
I3191
assS'head'
p67217
(dp67218
g216
(lp67219
sg32
(lp67220
sg318
(lp67221
sg121
(lp67222
sg80
(lp67223
sg181
(lp67224
sg293
(lp67225
sg83
(lp67226
sg303
(lp67227
sg42
(lp67228
I1769
asg138
(lp67229
sg350
(lp67230
ssS'medium'
p67231
(dp67232
g14
(lp67233
sg50
(lp67234
I388
asg181
(lp67235
sg8
(lp67236
ssS'timescal'
p67237
(dp67238
g14
(lp67239
sg16
(lp67240
I1779
asg70
(lp67241
ssS'fjj'
p67242
(dp67243
g20
(lp67244
I1731
assS'epsilon'
p67245
(dp67246
g14
(lp67247
I3385
assS'heat'
p67248
(dp67249
g14
(lp67250
sg16
(lp67251
I2042
assS'hear'
p67252
(dp67253
g116
(lp67254
sg174
(lp67255
I2556
asg332
(lp67256
ssS'lncs'
p67257
(dp67258
g174
(lp67259
I2515
assS'flavour'
p67260
(dp67261
g138
(lp67262
I834
assS'vtarget'
p67263
(dp67264
g132
(lp67265
I812
assS'photodiod'
p67266
(dp67267
g256
(lp67268
I919
assS'supp'
p67269
(dp67270
g104
(lp67271
I3083
assS'vivj'
p67272
(dp67273
g12
(lp67274
I1030
assS'vivo'
p67275
(dp67276
g262
(lp67277
sg149
(lp67278
I2916
assS'wigstrom'
p67279
(dp67280
g106
(lp67281
I1227
assS'fundament'
p67282
(dp67283
g116
(lp67284
sg118
(lp67285
sg36
(lp67286
sg68
(lp67287
sg38
(lp67288
sg429
(lp67289
sg128
(lp67290
sg132
(lp67291
I3174
asg46
(lp67292
sg44
(lp67293
ssS'latch'
p67294
(dp67295
g20
(lp67296
I1840
asg70
(lp67297
ssS'pelham'
p67298
(dp67299
g106
(lp67300
I302
assS'erdc'
p67301
(dp67302
g283
(lp67303
I12
assS'penal'
p67304
(dp67305
g235
(lp67306
sg295
(lp67307
sg183
(lp67308
sg102
(lp67309
sg221
(lp67310
sg138
(lp67311
I318
assS'meanfield'
p67312
(dp67313
g438
(lp67314
I1465
asg130
(lp67315
ssS'detectipn'
p67316
(dp67317
g484
(lp67318
I2586
assS'inordin'
p67319
(dp67320
g318
(lp67321
I998
asg145
(lp67322
ssS'nk'
p67323
(dp67324
g344
(lp67325
sg287
(lp67326
I2470
assS'arbitrarili'
p67327
(dp67328
g230
(lp67329
sg329
(lp67330
sg74
(lp67331
sg181
(lp67332
sg163
(lp67333
sg59
(lp67334
sg68
(lp67335
sg341
(lp67336
sg85
(lp67337
sg42
(lp67338
I3039
asg128
(lp67339
sg102
(lp67340
sg104
(lp67341
sg63
(lp67342
sg354
(lp67343
ssS'kaufman'
p67344
(dp67345
g12
(lp67346
I2640
asg59
(lp67347
sg221
(lp67348
sg4
(lp67349
ssS'reenter'
p67350
(dp67351
g80
(lp67352
I180
assS'lander'
p67353
(dp67354
g440
(lp67355
I2636
assS'haml'
p67356
(dp67357
g72
(lp67358
I1450
assS'mcallum'
p67359
(dp67360
g293
(lp67361
I2183
assS'check'
p67362
(dp67363
g70
(lp67364
sg460
(lp67365
sg124
(lp67366
sg126
(lp67367
sg42
(lp67368
I3022
asg429
(lp67369
sg63
(lp67370
sg114
(lp67371
ssS'hami'
p67372
(dp67373
g72
(lp67374
I2705
assS'assembl'
p67375
(dp67376
g4
(lp67377
sg460
(lp67378
sg10
(lp67379
sg104
(lp67380
sg59
(lp67381
sg149
(lp67382
I2452
assS'ni'
p67383
(dp67384
g102
(lp67385
sg30
(lp67386
sg18
(lp67387
sg178
(lp67388
sg313
(lp67389
I1019
assS'nj'
p67390
(dp67391
g30
(lp67392
sg287
(lp67393
sg108
(lp67394
sg121
(lp67395
sg80
(lp67396
sg163
(lp67397
sg36
(lp67398
sg38
(lp67399
sg83
(lp67400
sg12
(lp67401
sg91
(lp67402
sg128
(lp67403
sg78
(lp67404
sg132
(lp67405
sg46
(lp67406
sg135
(lp67407
I507
asg149
(lp67408
ssS'depol'
p67409
(dp67410
g106
(lp67411
I1986
assS'nl'
p67412
(dp67413
g287
(lp67414
sg332
(lp67415
sg121
(lp67416
sg8
(lp67417
sg38
(lp67418
sg245
(lp67419
sg102
(lp67420
I761
assS'nm'
p67421
(dp67422
g256
(lp67423
sg38
(lp67424
sg429
(lp67425
sg102
(lp67426
I625
asg46
(lp67427
sg221
(lp67428
sg26
(lp67429
ssS'nn'
p67430
(dp67431
g14
(lp67432
I4667
asg96
(lp67433
sg121
(lp67434
sg281
(lp67435
sg36
(lp67436
ssS'no'
p67437
(dp67438
g329
(lp67439
sg70
(lp67440
sg78
(lp67441
sg277
(lp67442
sg163
(lp67443
sg72
(lp67444
sg68
(lp67445
sg80
(lp67446
sg281
(lp67447
sg283
(lp67448
sg85
(lp67449
sg181
(lp67450
sg26
(lp67451
sg30
(lp67452
sg74
(lp67453
sg176
(lp67454
sg256
(lp67455
sg76
(lp67456
sg293
(lp67457
sg295
(lp67458
sg183
(lp67459
sg59
(lp67460
sg484
(lp67461
sg38
(lp67462
sg83
(lp67463
sg114
(lp67464
sg124
(lp67465
sg42
(lp67466
I751
asg87
(lp67467
sg89
(lp67468
sg91
(lp67469
sg12
(lp67470
sg46
(lp67471
sg96
(lp67472
sg48
(lp67473
sg99
(lp67474
sg313
(lp67475
sg223
(lp67476
sg149
(lp67477
sg230
(lp67478
sg174
(lp67479
sg116
(lp67480
sg32
(lp67481
sg178
(lp67482
sg429
(lp67483
sg318
(lp67484
sg102
(lp67485
sg104
(lp67486
sg106
(lp67487
sg108
(lp67488
sg110
(lp67489
sg20
(lp67490
sg52
(lp67491
sg22
(lp67492
sg216
(lp67493
sg438
(lp67494
sg440
(lp67495
sg18
(lp67496
sg121
(lp67497
sg4
(lp67498
sg6
(lp67499
sg8
(lp67500
sg34
(lp67501
sg36
(lp67502
sg460
(lp67503
sg235
(lp67504
sg126
(lp67505
sg341
(lp67506
sg40
(lp67507
sg344
(lp67508
sg63
(lp67509
sg128
(lp67510
sg130
(lp67511
sg132
(lp67512
sg14
(lp67513
sg16
(lp67514
sg135
(lp67515
sg138
(lp67516
sg140
(lp67517
sg354
(lp67518
ssS'na'
p67519
(dp67520
g106
(lp67521
I2069
assS'nb'
p67522
(dp67523
g318
(lp67524
I1128
assS'nc'
p67525
(dp67526
g245
(lp67527
I2526
assS'tip'
p67528
(dp67529
g106
(lp67530
I1064
asg176
(lp67531
sg72
(lp67532
sg59
(lp67533
ssS'ne'
p67534
(dp67535
g174
(lp67536
sg135
(lp67537
I1217
asg48
(lp67538
ssS'nf'
p67539
(dp67540
g174
(lp67541
sg287
(lp67542
sg130
(lp67543
I490
assS'ng'
p67544
(dp67545
g287
(lp67546
sg121
(lp67547
I1996
asg85
(lp67548
ssS'til'
p67549
(dp67550
g313
(lp67551
I631
assS'ny'
p67552
(dp67553
g230
(lp67554
sg332
(lp67555
sg6
(lp67556
sg460
(lp67557
sg341
(lp67558
sg114
(lp67559
sg12
(lp67560
sg306
(lp67561
sg89
(lp67562
sg91
(lp67563
sg128
(lp67564
sg132
(lp67565
sg106
(lp67566
I306
asg108
(lp67567
sg102
(lp67568
sg149
(lp67569
ssS'tin'
p67570
(dp67571
g104
(lp67572
I27
asg68
(lp67573
sg70
(lp67574
ssS'tio'
p67575
(dp67576
g121
(lp67577
I1608
assS'tih'
p67578
(dp67579
g40
(lp67580
I976
assS'tii'
p67581
(dp67582
g26
(lp67583
I2083
assS'tij'
p67584
(dp67585
g12
(lp67586
I885
asg174
(lp67587
sg30
(lp67588
sg68
(lp67589
ssS'np'
p67590
(dp67591
g287
(lp67592
sg384
(lp67593
sg178
(lp67594
I33
asg110
(lp67595
ssS'tie'
p67596
(dp67597
g181
(lp67598
sg277
(lp67599
sg8
(lp67600
I865
asg281
(lp67601
sg40
(lp67602
sg87
(lp67603
ssS'implant'
p67604
(dp67605
g135
(lp67606
I71
assS'ns'
p67607
(dp67608
g14
(lp67609
I3149
asg283
(lp67610
ssS'nt'
p67611
(dp67612
g384
(lp67613
I1030
assS'nu'
p67614
(dp67615
g89
(lp67616
I2919
assS'nw'
p67617
(dp67618
g135
(lp67619
I187
assS'node'
p67620
(dp67621
g287
(lp67622
sg108
(lp67623
sg121
(lp67624
sg256
(lp67625
sg34
(lp67626
sg18
(lp67627
sg460
(lp67628
sg145
(lp67629
sg38
(lp67630
sg42
(lp67631
I1447
asg283
(lp67632
sg110
(lp67633
sg91
(lp67634
sg128
(lp67635
sg183
(lp67636
sg104
(lp67637
sg20
(lp67638
sg135
(lp67639
sg50
(lp67640
sg59
(lp67641
sg44
(lp67642
ssS'tjif'
p67643
(dp67644
g104
(lp67645
I1539
assS'shikano'
p67646
(dp67647
g94
(lp67648
sg96
(lp67649
sg108
(lp67650
I2596
assS'benefici'
p67651
(dp67652
g484
(lp67653
sg18
(lp67654
I2451
asg145
(lp67655
ssS'tjij'
p67656
(dp67657
g149
(lp67658
I799
assS'eigenvalu'
p67659
(dp67660
g295
(lp67661
sg183
(lp67662
sg38
(lp67663
sg34
(lp67664
sg102
(lp67665
I861
asg46
(lp67666
ssS'consid'
p67667
(dp67668
g329
(lp67669
sg26
(lp67670
sg277
(lp67671
sg163
(lp67672
sg72
(lp67673
sg293
(lp67674
sg281
(lp67675
sg283
(lp67676
sg40
(lp67677
sg30
(lp67678
sg350
(lp67679
sg74
(lp67680
sg176
(lp67681
sg145
(lp67682
sg76
(lp67683
sg262
(lp67684
sg344
(lp67685
sg183
(lp67686
sg484
(lp67687
sg38
(lp67688
sg85
(lp67689
sg303
(lp67690
sg42
(lp67691
I1817
asg306
(lp67692
sg89
(lp67693
sg91
(lp67694
sg12
(lp67695
sg94
(lp67696
sg96
(lp67697
sg48
(lp67698
sg221
(lp67699
sg313
(lp67700
sg44
(lp67701
sg149
(lp67702
sg118
(lp67703
sg116
(lp67704
sg174
(lp67705
sg18
(lp67706
sg32
(lp67707
sg245
(lp67708
sg429
(lp67709
sg68
(lp67710
sg46
(lp67711
sg102
(lp67712
sg104
(lp67713
sg108
(lp67714
sg110
(lp67715
sg63
(lp67716
sg52
(lp67717
sg22
(lp67718
sg230
(lp67719
sg438
(lp67720
sg440
(lp67721
sg332
(lp67722
sg121
(lp67723
sg4
(lp67724
sg181
(lp67725
sg235
(lp67726
sg36
(lp67727
sg384
(lp67728
sg124
(lp67729
sg126
(lp67730
sg341
(lp67731
sg10
(lp67732
sg535
(lp67733
sg287
(lp67734
sg223
(lp67735
sg128
(lp67736
sg130
(lp67737
sg132
(lp67738
sg14
(lp67739
sg16
(lp67740
sg135
(lp67741
sg50
(lp67742
sg460
(lp67743
sg354
(lp67744
ssS'sql'
p67745
(dp67746
g102
(lp67747
I877
assS'equidist'
p67748
(dp67749
g163
(lp67750
I1109
assS'fjp'
p67751
(dp67752
g429
(lp67753
I1757
assS'faster'
p67754
(dp67755
g329
(lp67756
sg283
(lp67757
sg70
(lp67758
sg256
(lp67759
sg76
(lp67760
sg94
(lp67761
sg277
(lp67762
sg303
(lp67763
sg42
(lp67764
I171
asg130
(lp67765
sg14
(lp67766
sg108
(lp67767
sg50
(lp67768
sg138
(lp67769
sg44
(lp67770
ssS'barrionuevo'
p67771
(dp67772
g106
(lp67773
I362
assS'uninfluenc'
p67774
(dp67775
g4
(lp67776
I3147
assS'inexpens'
p67777
(dp67778
g78
(lp67779
sg10
(lp67780
I289
assS'nonexpans'
p67781
(dp67782
g306
(lp67783
I2115
assS'serious'
p67784
(dp67785
g287
(lp67786
sg277
(lp67787
sg34
(lp67788
sg126
(lp67789
sg429
(lp67790
sg132
(lp67791
sg135
(lp67792
I336
assS'nuclei'
p67793
(dp67794
g174
(lp67795
sg332
(lp67796
sg80
(lp67797
sg14
(lp67798
sg16
(lp67799
I230
asg350
(lp67800
ssS'backward'
p67801
(dp67802
g440
(lp67803
sg76
(lp67804
sg460
(lp67805
sg72
(lp67806
sg87
(lp67807
sg44
(lp67808
I1339
assS'orwp'
p67809
(dp67810
g40
(lp67811
I2484
assS'llx'
p67812
(dp67813
g96
(lp67814
I949
asg221
(lp67815
ssS'rom'
p67816
(dp67817
g85
(lp67818
I4342
assS'ron'
p67819
(dp67820
g440
(lp67821
sg135
(lp67822
I102
asg178
(lp67823
sg85
(lp67824
sg10
(lp67825
ssS'roo'
p67826
(dp67827
g281
(lp67828
I808
assS'rob'
p67829
(dp67830
g221
(lp67831
sg138
(lp67832
I3245
assS'celebr'
p67833
(dp67834
g306
(lp67835
I391
assS'llw'
p67836
(dp67837
g96
(lp67838
I951
asg306
(lp67839
ssS'focus'
p67840
(dp67841
g74
(lp67842
sg295
(lp67843
sg183
(lp67844
sg59
(lp67845
sg38
(lp67846
sg83
(lp67847
sg306
(lp67848
sg48
(lp67849
sg12
(lp67850
sg18
(lp67851
sg221
(lp67852
sg350
(lp67853
sg110
(lp67854
sg230
(lp67855
sg438
(lp67856
I569
asg332
(lp67857
sg178
(lp67858
sg4
(lp67859
sg8
(lp67860
sg235
(lp67861
sg72
(lp67862
sg10
(lp67863
sg40
(lp67864
sg14
(lp67865
sg149
(lp67866
ssS'interspeci'
p67867
(dp67868
g48
(lp67869
I544
assS'llt'
p67870
(dp67871
g34
(lp67872
sg262
(lp67873
sg130
(lp67874
I1554
assS'llk'
p67875
(dp67876
g8
(lp67877
I1675
assS'roy'
p67878
(dp67879
g306
(lp67880
sg48
(lp67881
I2216
asg74
(lp67882
ssS'signific'
p67883
(dp67884
g124
(lp67885
sg70
(lp67886
sg78
(lp67887
sg277
(lp67888
sg283
(lp67889
sg181
(lp67890
sg26
(lp67891
sg30
(lp67892
sg74
(lp67893
sg80
(lp67894
sg76
(lp67895
sg344
(lp67896
sg183
(lp67897
sg59
(lp67898
sg484
(lp67899
sg38
(lp67900
sg83
(lp67901
sg85
(lp67902
sg63
(lp67903
sg87
(lp67904
sg12
(lp67905
sg94
(lp67906
sg96
(lp67907
sg99
(lp67908
sg313
(lp67909
sg44
(lp67910
sg149
(lp67911
sg318
(lp67912
sg104
(lp67913
sg106
(lp67914
sg110
(lp67915
sg20
(lp67916
sg52
(lp67917
sg114
(lp67918
sg230
(lp67919
sg438
(lp67920
I124
asg440
(lp67921
sg332
(lp67922
sg121
(lp67923
sg4
(lp67924
sg6
(lp67925
sg8
(lp67926
sg34
(lp67927
sg460
(lp67928
sg235
(lp67929
sg126
(lp67930
sg281
(lp67931
sg10
(lp67932
sg223
(lp67933
sg128
(lp67934
sg130
(lp67935
sg14
(lp67936
sg16
(lp67937
sg50
(lp67938
sg138
(lp67939
sg140
(lp67940
ssS'adjj'
p67941
(dp67942
g130
(lp67943
I2627
assS'llm'
p67944
(dp67945
g59
(lp67946
I940
assS'lll'
p67947
(dp67948
g245
(lp67949
I2506
assS'llc'
p67950
(dp67951
g8
(lp67952
I1758
assS'ror'
p67953
(dp67954
g106
(lp67955
I54
assS'ros'
p67956
(dp67957
g74
(lp67958
I1802
assS'compani'
p67959
(dp67960
g163
(lp67961
I2193
assS'hoquonci'
p67962
(dp67963
g332
(lp67964
I2268
assS'row'
p67965
(dp67966
g116
(lp67967
sg440
(lp67968
sg178
(lp67969
sg256
(lp67970
sg181
(lp67971
sg163
(lp67972
sg110
(lp67973
sg68
(lp67974
sg277
(lp67975
sg40
(lp67976
sg306
(lp67977
sg104
(lp67978
sg94
(lp67979
sg96
(lp67980
sg48
(lp67981
sg221
(lp67982
sg138
(lp67983
sg303
(lp67984
sg354
(lp67985
I3009
assS'proxim'
p67986
(dp67987
g332
(lp67988
sg110
(lp67989
sg130
(lp67990
I232
assS'environment'
p67991
(dp67992
g438
(lp67993
I824
asg176
(lp67994
sg99
(lp67995
sg283
(lp67996
sg104
(lp67997
ssS'quasi'
p67998
(dp67999
g34
(lp68000
sg83
(lp68001
sg181
(lp68002
sg8
(lp68003
I1361
assS'henderson'
p68004
(dp68005
g94
(lp68006
I3555
asg63
(lp68007
sg181
(lp68008
sg114
(lp68009
ssS'highperform'
p68010
(dp68011
g283
(lp68012
I1951
assS'sourc'
p68013
(dp68014
g277
(lp68015
sg96
(lp68016
sg80
(lp68017
sg295
(lp68018
sg183
(lp68019
sg59
(lp68020
sg85
(lp68021
sg42
(lp68022
I1669
asg245
(lp68023
sg20
(lp68024
sg221
(lp68025
sg223
(lp68026
sg350
(lp68027
sg63
(lp68028
sg174
(lp68029
sg440
(lp68030
sg332
(lp68031
sg6
(lp68032
sg126
(lp68033
sg14
(lp68034
sg16
(lp68035
sg135
(lp68036
ssS'jdl'
p68037
(dp68038
g102
(lp68039
I473
asg38
(lp68040
ssS'flmi'
p68041
(dp68042
g281
(lp68043
I1277
assS'niel'
p68044
(dp68045
g38
(lp68046
I33
assS'minski'
p68047
(dp68048
g429
(lp68049
I460
assS'feasibl'
p68050
(dp68051
g440
(lp68052
I837
asg283
(lp68053
sg178
(lp68054
sg183
(lp68055
sg124
(lp68056
sg281
(lp68057
sg40
(lp68058
sg78
(lp68059
ssS'cook'
p68060
(dp68061
g174
(lp68062
I2377
asg87
(lp68063
sg332
(lp68064
ssS'li'
p68065
(dp68066
g30
(lp68067
sg287
(lp68068
sg293
(lp68069
sg295
(lp68070
sg183
(lp68071
sg484
(lp68072
sg12
(lp68073
sg46
(lp68074
sg99
(lp68075
sg313
(lp68076
sg429
(lp68077
sg318
(lp68078
sg102
(lp68079
sg104
(lp68080
sg230
(lp68081
sg438
(lp68082
I1427
asg332
(lp68083
sg121
(lp68084
sg8
(lp68085
sg384
(lp68086
sg126
(lp68087
sg14
(lp68088
sg50
(lp68089
sg354
(lp68090
ssS'ilxi'
p68091
(dp68092
g78
(lp68093
I1865
assS'level'
p68094
(dp68095
g124
(lp68096
sg70
(lp68097
sg283
(lp68098
sg287
(lp68099
sg74
(lp68100
sg145
(lp68101
sg256
(lp68102
sg262
(lp68103
sg344
(lp68104
sg59
(lp68105
sg83
(lp68106
sg303
(lp68107
sg42
(lp68108
I2027
asg87
(lp68109
sg94
(lp68110
sg20
(lp68111
sg18
(lp68112
sg99
(lp68113
sg535
(lp68114
sg223
(lp68115
sg350
(lp68116
sg293
(lp68117
sg32
(lp68118
sg429
(lp68119
sg68
(lp68120
sg46
(lp68121
sg102
(lp68122
sg63
(lp68123
sg52
(lp68124
sg216
(lp68125
sg118
(lp68126
sg440
(lp68127
sg178
(lp68128
sg181
(lp68129
sg6
(lp68130
sg8
(lp68131
sg384
(lp68132
sg235
(lp68133
sg126
(lp68134
sg10
(lp68135
sg128
(lp68136
sg130
(lp68137
sg132
(lp68138
sg14
(lp68139
sg460
(lp68140
ssS'cnj'
p68141
(dp68142
g102
(lp68143
I605
assS'necessit'
p68144
(dp68145
g350
(lp68146
I2697
assS'enol'
p68147
(dp68148
g183
(lp68149
I5213
assS'shortcom'
p68150
(dp68151
g132
(lp68152
I2635
assS'yale'
p68153
(dp68154
g295
(lp68155
sg183
(lp68156
sg429
(lp68157
sg8
(lp68158
I16
assS'quick'
p68159
(dp68160
g30
(lp68161
sg329
(lp68162
sg34
(lp68163
sg124
(lp68164
sg38
(lp68165
sg42
(lp68166
I2710
asg306
(lp68167
sg46
(lp68168
sg18
(lp68169
ssS'lever'
p68170
(dp68171
g99
(lp68172
I2900
assS'spent'
p68173
(dp68174
g80
(lp68175
I203
assS'slower'
p68176
(dp68177
g245
(lp68178
I2586
asg70
(lp68179
sg178
(lp68180
sg40
(lp68181
ssS'trend'
p68182
(dp68183
g38
(lp68184
sg121
(lp68185
sg138
(lp68186
I2541
asg10
(lp68187
sg303
(lp68188
ssS'sidelob'
p68189
(dp68190
g102
(lp68191
I2558
assS'pmx'
p68192
(dp68193
g20
(lp68194
I1590
assS'colin'
p68195
(dp68196
g14
(lp68197
sg16
(lp68198
I49
assS'iettl'
p68199
(dp68200
g34
(lp68201
I2159
assS'magnif'
p68202
(dp68203
g176
(lp68204
I100
asg76
(lp68205
ssS'pml'
p68206
(dp68207
g72
(lp68208
sg22
(lp68209
I1094
assS'condud'
p68210
(dp68211
g76
(lp68212
I3122
assS'oflearn'
p68213
(dp68214
g36
(lp68215
I1130
asg78
(lp68216
ssS'port'
p68217
(dp68218
g99
(lp68219
I1062
asg10
(lp68220
ssS'inconclus'
p68221
(dp68222
g91
(lp68223
I287
asg303
(lp68224
ssS'spectral'
p68225
(dp68226
g145
(lp68227
sg22
(lp68228
sg281
(lp68229
sg96
(lp68230
I67
asg48
(lp68231
sg256
(lp68232
ssS'dej'
p68233
(dp68234
g350
(lp68235
I515
assS'den'
p68236
(dp68237
g32
(lp68238
sg108
(lp68239
sg354
(lp68240
I881
assS'angelini'
p68241
(dp68242
g96
(lp68243
I1558
assS'kamm'
p68244
(dp68245
g440
(lp68246
I2736
assS'water'
p68247
(dp68248
g102
(lp68249
I1354
asg78
(lp68250
sg318
(lp68251
sg26
(lp68252
ssS'naylor'
p68253
(dp68254
g63
(lp68255
I795
assS'intersect'
p68256
(dp68257
g32
(lp68258
sg283
(lp68259
sg145
(lp68260
sg36
(lp68261
sg59
(lp68262
sg85
(lp68263
sg104
(lp68264
sg48
(lp68265
I1541
assS'jorg'
p68266
(dp68267
g354
(lp68268
I10
assS'henkl'
p68269
(dp68270
g110
(lp68271
I15
assS'doria'
p68272
(dp68273
g484
(lp68274
I2569
assS'lernverfahren'
p68275
(dp68276
g34
(lp68277
I2916
assS'thirti'
p68278
(dp68279
g230
(lp68280
I404
assS'healthi'
p68281
(dp68282
g78
(lp68283
I77
assS'automaton'
p68284
(dp68285
g104
(lp68286
I192
assS'uxuy'
p68287
(dp68288
g6
(lp68289
I412
assS'dioxid'
p68290
(dp68291
g283
(lp68292
I705
assS'ard'
p68293
(dp68294
g124
(lp68295
sg126
(lp68296
I846
assS'selfplay'
p68297
(dp68298
g132
(lp68299
I1766
assS'circumv'
p68300
(dp68301
g145
(lp68302
sg22
(lp68303
I534
assS'nrmse'
p68304
(dp68305
g59
(lp68306
I1961
assS'thrun'
p68307
(dp68308
g132
(lp68309
I9
asg89
(lp68310
sg313
(lp68311
sg223
(lp68312
ssS'visibl'
p68313
(dp68314
g174
(lp68315
I1989
asg59
(lp68316
sg18
(lp68317
sg341
(lp68318
sg68
(lp68319
ssS'santoso'
p68320
(dp68321
g78
(lp68322
I24
assS'smallish'
p68323
(dp68324
g74
(lp68325
I1987
assS'salamon'
p68326
(dp68327
g26
(lp68328
sg140
(lp68329
I3044
asg235
(lp68330
ssS'iipw'
p68331
(dp68332
g245
(lp68333
I1795
assS'pll'
p68334
(dp68335
g183
(lp68336
I5317
assS'gustatrson'
p68337
(dp68338
g106
(lp68339
I150
assS'memori'
p68340
(dp68341
g70
(lp68342
sg104
(lp68343
sg287
(lp68344
sg80
(lp68345
sg293
(lp68346
sg295
(lp68347
sg183
(lp68348
sg42
(lp68349
I926
asg89
(lp68350
sg245
(lp68351
sg20
(lp68352
sg18
(lp68353
sg99
(lp68354
sg535
(lp68355
sg223
(lp68356
sg116
(lp68357
sg12
(lp68358
sg178
(lp68359
sg106
(lp68360
sg110
(lp68361
sg63
(lp68362
sg52
(lp68363
sg216
(lp68364
sg438
(lp68365
sg332
(lp68366
sg121
(lp68367
sg4
(lp68368
sg181
(lp68369
sg384
(lp68370
sg10
(lp68371
sg313
(lp68372
sg128
(lp68373
sg14
(lp68374
ssS'autodata'
p68375
(dp68376
g72
(lp68377
I2453
assS'konig'
p68378
(dp68379
g440
(lp68380
sg149
(lp68381
I3039
assS'decatur'
p68382
(dp68383
g344
(lp68384
I2078
assS'tecnologica'
p68385
(dp68386
g96
(lp68387
I17
assS'percept'
p68388
(dp68389
g216
(lp68390
sg174
(lp68391
sg74
(lp68392
sg332
(lp68393
sg178
(lp68394
sg118
(lp68395
sg116
(lp68396
sg59
(lp68397
sg293
(lp68398
sg72
(lp68399
sg10
(lp68400
sg223
(lp68401
sg12
(lp68402
sg18
(lp68403
sg135
(lp68404
I101
asg63
(lp68405
sg44
(lp68406
ssS'prey'
p68407
(dp68408
g59
(lp68409
I1049
assS'criteria'
p68410
(dp68411
g74
(lp68412
sg70
(lp68413
sg293
(lp68414
sg344
(lp68415
sg460
(lp68416
sg484
(lp68417
sg72
(lp68418
sg429
(lp68419
sg306
(lp68420
sg46
(lp68421
sg104
(lp68422
sg108
(lp68423
I1793
asg163
(lp68424
sg313
(lp68425
ssS'australian'
p68426
(dp68427
g30
(lp68428
sg135
(lp68429
I2416
asg121
(lp68430
ssS'today'
p68431
(dp68432
g94
(lp68433
sg108
(lp68434
I139
asg70
(lp68435
ssS'athena'
p68436
(dp68437
g230
(lp68438
sg306
(lp68439
I2927
assS'capit'
p68440
(dp68441
g94
(lp68442
sg114
(lp68443
sg22
(lp68444
sg140
(lp68445
I693
asg130
(lp68446
ssS'registr'
p68447
(dp68448
g318
(lp68449
I3026
asg283
(lp68450
ssS'conductor'
p68451
(dp68452
g14
(lp68453
sg16
(lp68454
I314
assS'emiss'
p68455
(dp68456
g460
(lp68457
sg318
(lp68458
I2309
asg76
(lp68459
ssS'kurtosi'
p68460
(dp68461
g318
(lp68462
I2101
assS'casey'
p68463
(dp68464
g63
(lp68465
I792
assS'prototyp'
p68466
(dp68467
g132
(lp68468
I2733
asg78
(lp68469
sg91
(lp68470
sg44
(lp68471
sg130
(lp68472
ssS'vmax'
p68473
(dp68474
g460
(lp68475
I1992
assS'whittl'
p68476
(dp68477
g118
(lp68478
I321
assS'yji'
p68479
(dp68480
g384
(lp68481
I1838
assS'judg'
p68482
(dp68483
g74
(lp68484
sg332
(lp68485
sg181
(lp68486
sg42
(lp68487
I2501
asg283
(lp68488
sg46
(lp68489
sg138
(lp68490
sg350
(lp68491
ssS'judd'
p68492
(dp68493
g36
(lp68494
sg110
(lp68495
sg223
(lp68496
I3301
assS'compart'
p68497
(dp68498
g118
(lp68499
I770
assS'fjopt'
p68500
(dp68501
g38
(lp68502
I2844
assS'inadequ'
p68503
(dp68504
g89
(lp68505
I1535
asg83
(lp68506
sg76
(lp68507
ssS'purpos'
p68508
(dp68509
g70
(lp68510
sg163
(lp68511
sg72
(lp68512
sg287
(lp68513
sg76
(lp68514
sg63
(lp68515
sg42
(lp68516
I1005
asg87
(lp68517
sg89
(lp68518
sg94
(lp68519
sg20
(lp68520
sg18
(lp68521
sg99
(lp68522
sg96
(lp68523
sg52
(lp68524
sg22
(lp68525
sg329
(lp68526
sg440
(lp68527
sg318
(lp68528
sg121
(lp68529
sg4
(lp68530
sg235
(lp68531
sg36
(lp68532
sg460
(lp68533
sg124
(lp68534
sg126
(lp68535
sg10
(lp68536
sg130
(lp68537
sg14
(lp68538
sg16
(lp68539
sg50
(lp68540
ssS'bayoumi'
p68541
(dp68542
g245
(lp68543
I2894
assS'stream'
p68544
(dp68545
g174
(lp68546
sg332
(lp68547
sg76
(lp68548
sg10
(lp68549
sg283
(lp68550
sg14
(lp68551
sg135
(lp68552
I1903
asg223
(lp68553
ssS'critic'
p68554
(dp68555
g438
(lp68556
I999
asg18
(lp68557
sg4
(lp68558
sg80
(lp68559
sg78
(lp68560
sg245
(lp68561
sg318
(lp68562
sg132
(lp68563
sg46
(lp68564
sg20
(lp68565
sg48
(lp68566
sg110
(lp68567
sg63
(lp68568
sg277
(lp68569
sg354
(lp68570
ssS'intracort'
p68571
(dp68572
g12
(lp68573
sg438
(lp68574
I511
assS'angelaki'
p68575
(dp68576
g350
(lp68577
I2579
assS'contamin'
p68578
(dp68579
g70
(lp68580
I2233
assS'hydrogen'
p68581
(dp68582
g14
(lp68583
sg16
(lp68584
I111
asg26
(lp68585
ssS'verlag'
p68586
(dp68587
g230
(lp68588
sg174
(lp68589
sg262
(lp68590
sg116
(lp68591
sg59
(lp68592
sg85
(lp68593
sg138
(lp68594
sg429
(lp68595
sg535
(lp68596
sg14
(lp68597
sg16
(lp68598
sg20
(lp68599
sg221
(lp68600
sg106
(lp68601
I2746
asg354
(lp68602
ssS'timemultiplex'
p68603
(dp68604
g22
(lp68605
I1066
assS'differenti'
p68606
(dp68607
g68
(lp68608
sg70
(lp68609
sg281
(lp68610
sg287
(lp68611
sg484
(lp68612
sg38
(lp68613
sg85
(lp68614
sg91
(lp68615
sg245
(lp68616
sg46
(lp68617
sg96
(lp68618
sg99
(lp68619
sg535
(lp68620
sg223
(lp68621
sg230
(lp68622
sg216
(lp68623
sg329
(lp68624
sg32
(lp68625
sg318
(lp68626
sg34
(lp68627
sg124
(lp68628
sg126
(lp68629
sg341
(lp68630
sg313
(lp68631
sg130
(lp68632
sg132
(lp68633
sg14
(lp68634
sg16
(lp68635
sg135
(lp68636
sg50
(lp68637
I43
assS'cpqfjt'
p68638
(dp68639
g26
(lp68640
I2113
assS'kohonel'
p68641
(dp68642
g116
(lp68643
I1781
assS'alway'
p68644
(dp68645
g26
(lp68646
sg72
(lp68647
sg30
(lp68648
sg74
(lp68649
sg145
(lp68650
sg80
(lp68651
sg295
(lp68652
sg183
(lp68653
sg59
(lp68654
sg484
(lp68655
sg83
(lp68656
sg85
(lp68657
sg303
(lp68658
sg42
(lp68659
I619
asg306
(lp68660
sg89
(lp68661
sg91
(lp68662
sg245
(lp68663
sg46
(lp68664
sg20
(lp68665
sg118
(lp68666
sg12
(lp68667
sg94
(lp68668
sg52
(lp68669
sg230
(lp68670
sg329
(lp68671
sg440
(lp68672
sg34
(lp68673
sg36
(lp68674
sg384
(lp68675
sg68
(lp68676
sg126
(lp68677
sg341
(lp68678
sg128
(lp68679
sg140
(lp68680
sg354
(lp68681
ssS'vesaliw'
p68682
(dp68683
g484
(lp68684
I49
assS'rubin'
p68685
(dp68686
g74
(lp68687
sg460
(lp68688
sg72
(lp68689
sg440
(lp68690
sg91
(lp68691
sg221
(lp68692
sg313
(lp68693
I2131
assS'anyon'
p68694
(dp68695
g94
(lp68696
sg108
(lp68697
I1867
asg163
(lp68698
sg10
(lp68699
sg78
(lp68700
ssS'fourth'
p68701
(dp68702
g216
(lp68703
sg116
(lp68704
sg70
(lp68705
sg80
(lp68706
sg6
(lp68707
sg163
(lp68708
sg72
(lp68709
sg281
(lp68710
sg89
(lp68711
sg108
(lp68712
I2132
asg110
(lp68713
sg52
(lp68714
ssS'minpi'
p68715
(dp68716
g34
(lp68717
I1934
assS'hampson'
p68718
(dp68719
g110
(lp68720
I744
assS'bujioiii'
p68721
(dp68722
g135
(lp68723
I918
assS'paramount'
p68724
(dp68725
g52
(lp68726
I1997
assS'cocktail'
p68727
(dp68728
g318
(lp68729
I252
assS'levi'
p68730
(dp68731
g438
(lp68732
I2421
asg106
(lp68733
ssS'dtfj'
p68734
(dp68735
g12
(lp68736
I1027
assS'eighth'
p68737
(dp68738
g126
(lp68739
I1892
asg114
(lp68740
ssS'repeat'
p68741
(dp68742
g30
(lp68743
sg32
(lp68744
sg4
(lp68745
sg76
(lp68746
sg34
(lp68747
sg72
(lp68748
sg484
(lp68749
sg126
(lp68750
sg281
(lp68751
sg40
(lp68752
sg42
(lp68753
I1470
asg26
(lp68754
sg63
(lp68755
sg89
(lp68756
sg91
(lp68757
sg104
(lp68758
sg94
(lp68759
sg106
(lp68760
sg135
(lp68761
sg96
(lp68762
sg354
(lp68763
ssS'innerv'
p68764
(dp68765
g174
(lp68766
I312
assS'practic'
p68767
(dp68768
g124
(lp68769
sg70
(lp68770
sg277
(lp68771
sg163
(lp68772
sg281
(lp68773
sg40
(lp68774
sg30
(lp68775
sg287
(lp68776
sg74
(lp68777
sg145
(lp68778
sg293
(lp68779
sg295
(lp68780
sg183
(lp68781
sg59
(lp68782
sg83
(lp68783
sg85
(lp68784
sg42
(lp68785
I570
asg306
(lp68786
sg89
(lp68787
sg91
(lp68788
sg245
(lp68789
sg20
(lp68790
sg221
(lp68791
sg535
(lp68792
sg44
(lp68793
sg329
(lp68794
sg102
(lp68795
sg104
(lp68796
sg63
(lp68797
sg114
(lp68798
sg438
(lp68799
sg440
(lp68800
sg318
(lp68801
sg121
(lp68802
sg181
(lp68803
sg8
(lp68804
sg99
(lp68805
sg460
(lp68806
sg235
(lp68807
sg126
(lp68808
sg341
(lp68809
sg313
(lp68810
sg344
(lp68811
sg36
(lp68812
sg132
(lp68813
sg14
(lp68814
sg16
(lp68815
sg50
(lp68816
ssS'frick'
p68817
(dp68818
g384
(lp68819
I2394
assS'vanadium'
p68820
(dp68821
g14
(lp68822
I2732
assS'kohonen'
p68823
(dp68824
g116
(lp68825
sg34
(lp68826
sg106
(lp68827
I62
asg20
(lp68828
sg52
(lp68829
sg149
(lp68830
ssS'multispectr'
p68831
(dp68832
g281
(lp68833
I1695
assS'osborn'
p68834
(dp68835
g118
(lp68836
sg114
(lp68837
I1633
assS'inform'
p68838
(dp68839
g344
(lp68840
sg68
(lp68841
sg70
(lp68842
sg78
(lp68843
sg277
(lp68844
sg163
(lp68845
sg72
(lp68846
sg281
(lp68847
sg283
(lp68848
sg36
(lp68849
sg181
(lp68850
sg303
(lp68851
sg26
(lp68852
sg30
(lp68853
sg350
(lp68854
sg74
(lp68855
sg176
(lp68856
sg145
(lp68857
sg256
(lp68858
sg76
(lp68859
sg262
(lp68860
sg295
(lp68861
sg183
(lp68862
sg59
(lp68863
sg484
(lp68864
sg38
(lp68865
sg83
(lp68866
sg85
(lp68867
sg63
(lp68868
sg42
(lp68869
I1138
asg306
(lp68870
sg87
(lp68871
sg80
(lp68872
sg91
(lp68873
sg12
(lp68874
sg94
(lp68875
sg96
(lp68876
sg48
(lp68877
sg99
(lp68878
sg313
(lp68879
sg44
(lp68880
sg149
(lp68881
sg116
(lp68882
sg329
(lp68883
sg293
(lp68884
sg460
(lp68885
sg178
(lp68886
sg245
(lp68887
sg429
(lp68888
sg318
(lp68889
sg102
(lp68890
sg104
(lp68891
sg106
(lp68892
sg20
(lp68893
sg52
(lp68894
sg114
(lp68895
sg230
(lp68896
sg174
(lp68897
sg440
(lp68898
sg18
(lp68899
sg121
(lp68900
sg4
(lp68901
sg6
(lp68902
sg8
(lp68903
sg34
(lp68904
sg221
(lp68905
sg384
(lp68906
sg124
(lp68907
sg126
(lp68908
sg341
(lp68909
sg118
(lp68910
sg287
(lp68911
sg223
(lp68912
sg130
(lp68913
sg132
(lp68914
sg14
(lp68915
sg16
(lp68916
sg135
(lp68917
sg50
(lp68918
sg138
(lp68919
sg140
(lp68920
sg354
(lp68921
ssS'preced'
p68922
(dp68923
g287
(lp68924
sg178
(lp68925
sg4
(lp68926
sg223
(lp68927
sg130
(lp68928
I828
asg121
(lp68929
sg52
(lp68930
sg256
(lp68931
ssS'combin'
p68932
(dp68933
g124
(lp68934
sg78
(lp68935
sg277
(lp68936
sg72
(lp68937
sg283
(lp68938
sg303
(lp68939
sg26
(lp68940
sg30
(lp68941
sg74
(lp68942
sg145
(lp68943
sg80
(lp68944
sg76
(lp68945
sg293
(lp68946
sg295
(lp68947
sg183
(lp68948
sg484
(lp68949
sg83
(lp68950
sg85
(lp68951
sg63
(lp68952
sg42
(lp68953
I522
asg306
(lp68954
sg87
(lp68955
sg89
(lp68956
sg91
(lp68957
sg245
(lp68958
sg46
(lp68959
sg96
(lp68960
sg48
(lp68961
sg313
(lp68962
sg44
(lp68963
sg149
(lp68964
sg116
(lp68965
sg32
(lp68966
sg350
(lp68967
sg68
(lp68968
sg102
(lp68969
sg104
(lp68970
sg106
(lp68971
sg110
(lp68972
sg20
(lp68973
sg52
(lp68974
sg230
(lp68975
sg174
(lp68976
sg440
(lp68977
sg178
(lp68978
sg4
(lp68979
sg181
(lp68980
sg8
(lp68981
sg460
(lp68982
sg235
(lp68983
sg126
(lp68984
sg341
(lp68985
sg223
(lp68986
sg130
(lp68987
sg132
(lp68988
sg14
(lp68989
sg16
(lp68990
sg135
(lp68991
sg138
(lp68992
sg140
(lp68993
ssS'practis'
p68994
(dp68995
g126
(lp68996
I2246
assS'acceleromet'
p68997
(dp68998
g78
(lp68999
I455
assS'isjust'
p69000
(dp69001
g72
(lp69002
I1886
assS'geneal'
p69003
(dp69004
g295
(lp69005
I4026
asg183
(lp69006
ssS'superscript'
p69007
(dp69008
g102
(lp69009
I467
asg306
(lp69010
sg124
(lp69011
sg176
(lp69012
sg293
(lp69013
ssS'extractor'
p69014
(dp69015
g178
(lp69016
I1120
asg281
(lp69017
sg63
(lp69018
ssS'anticip'
p69019
(dp69020
g245
(lp69021
I865
asg70
(lp69022
sg80
(lp69023
sg350
(lp69024
ssS'microscop'
p69025
(dp69026
g384
(lp69027
I525
asg283
(lp69028
ssS'bangalor'
p69029
(dp69030
g329
(lp69031
I3004
assS'lx'
p69032
(dp69033
g32
(lp69034
sg145
(lp69035
sg181
(lp69036
sg124
(lp69037
sg87
(lp69038
sg313
(lp69039
I1059
assS'gra'
p69040
(dp69041
g63
(lp69042
I456
assS'denker'
p69043
(dp69044
g181
(lp69045
sg344
(lp69046
sg183
(lp69047
sg341
(lp69048
sg44
(lp69049
sg36
(lp69050
sg132
(lp69051
sg94
(lp69052
sg50
(lp69053
I1649
asg63
(lp69054
sg223
(lp69055
sg114
(lp69056
ssS'nimh'
p69057
(dp69058
g256
(lp69059
I2093
assS'tectur'
p69060
(dp69061
g128
(lp69062
I51
assS'apex'
p69063
(dp69064
g96
(lp69065
I2434
assS'passag'
p69066
(dp69067
g42
(lp69068
I2302
asg116
(lp69069
sg262
(lp69070
ssS'wallach'
p69071
(dp69072
g118
(lp69073
I279
assS'xihi'
p69074
(dp69075
g130
(lp69076
I451
assS'vestibular'
p69077
(dp69078
g350
(lp69079
I1356
assS'gtv'
p69080
(dp69081
g34
(lp69082
I1218
assS'directionselect'
p69083
(dp69084
g70
(lp69085
I1531
assS'ahn'
p69086
(dp69087
g223
(lp69088
I3092
assS'xlcrj'
p69089
(dp69090
g341
(lp69091
I2305
assS'hamilton'
p69092
(dp69093
g14
(lp69094
I4694
asg124
(lp69095
sg126
(lp69096
ssS'wcnn'
p69097
(dp69098
g72
(lp69099
I3634
assS'shelham'
p69100
(dp69101
g350
(lp69102
I361
assS'tern'
p69103
(dp69104
g48
(lp69105
I1124
assS'term'
p69106
(dp69107
g329
(lp69108
sg70
(lp69109
sg26
(lp69110
sg163
(lp69111
sg283
(lp69112
sg460
(lp69113
sg36
(lp69114
sg181
(lp69115
sg30
(lp69116
sg287
(lp69117
sg74
(lp69118
sg176
(lp69119
sg145
(lp69120
sg76
(lp69121
sg262
(lp69122
sg295
(lp69123
sg183
(lp69124
sg80
(lp69125
sg38
(lp69126
sg85
(lp69127
sg124
(lp69128
sg306
(lp69129
sg87
(lp69130
sg89
(lp69131
sg68
(lp69132
sg12
(lp69133
sg96
(lp69134
sg18
(lp69135
sg99
(lp69136
sg313
(lp69137
sg223
(lp69138
sg350
(lp69139
sg118
(lp69140
sg230
(lp69141
sg174
(lp69142
sg32
(lp69143
sg245
(lp69144
sg429
(lp69145
sg318
(lp69146
sg102
(lp69147
sg104
(lp69148
sg106
(lp69149
sg108
(lp69150
sg110
(lp69151
sg20
(lp69152
sg52
(lp69153
sg114
(lp69154
sg216
(lp69155
sg438
(lp69156
I274
asg440
(lp69157
sg332
(lp69158
sg121
(lp69159
sg4
(lp69160
sg6
(lp69161
sg8
(lp69162
sg34
(lp69163
sg221
(lp69164
sg384
(lp69165
sg235
(lp69166
sg126
(lp69167
sg281
(lp69168
sg535
(lp69169
sg344
(lp69170
sg63
(lp69171
sg128
(lp69172
sg130
(lp69173
sg132
(lp69174
sg14
(lp69175
sg16
(lp69176
sg135
(lp69177
sg50
(lp69178
sg138
(lp69179
sg140
(lp69180
sg354
(lp69181
ssS'retinex'
p69182
(dp69183
g118
(lp69184
I2534
assS'name'
p69185
(dp69186
g283
(lp69187
sg287
(lp69188
sg145
(lp69189
sg85
(lp69190
sg303
(lp69191
sg42
(lp69192
I268
asg94
(lp69193
sg20
(lp69194
sg99
(lp69195
sg223
(lp69196
sg32
(lp69197
sg429
(lp69198
sg46
(lp69199
sg102
(lp69200
sg104
(lp69201
sg52
(lp69202
sg114
(lp69203
sg440
(lp69204
sg8
(lp69205
sg34
(lp69206
sg460
(lp69207
sg40
(lp69208
sg138
(lp69209
ssS'rmse'
p69210
(dp69211
g329
(lp69212
sg354
(lp69213
I2396
assS'opera'
p69214
(dp69215
g42
(lp69216
I2534
assS'cerebr'
p69217
(dp69218
g176
(lp69219
sg99
(lp69220
I3218
asg80
(lp69221
ssS'qx'
p69222
(dp69223
g230
(lp69224
I2685
assS'riken'
p69225
(dp69226
g36
(lp69227
I48
assS'realism'
p69228
(dp69229
g48
(lp69230
I1774
assS'ahw'
p69231
(dp69232
g341
(lp69233
I1410
assS'spinor'
p69234
(dp69235
g32
(lp69236
I474
assS'notabl'
p69237
(dp69238
g306
(lp69239
sg283
(lp69240
sg138
(lp69241
I1507
asg10
(lp69242
sg83
(lp69243
ssS'individu'
p69244
(dp69245
g283
(lp69246
sg70
(lp69247
sg163
(lp69248
sg30
(lp69249
sg80
(lp69250
sg293
(lp69251
sg295
(lp69252
sg183
(lp69253
sg484
(lp69254
sg83
(lp69255
sg306
(lp69256
sg94
(lp69257
sg96
(lp69258
sg48
(lp69259
sg221
(lp69260
sg350
(lp69261
sg110
(lp69262
sg63
(lp69263
sg114
(lp69264
sg116
(lp69265
sg438
(lp69266
I273
asg121
(lp69267
sg6
(lp69268
sg181
(lp69269
sg235
(lp69270
sg34
(lp69271
sg126
(lp69272
sg10
(lp69273
sg128
(lp69274
sg78
(lp69275
sg132
(lp69276
sg22
(lp69277
sg50
(lp69278
sg140
(lp69279
ssS'goldmann'
p69280
(dp69281
g344
(lp69282
sg40
(lp69283
I827
assS'ctiqt'
p69284
(dp69285
g87
(lp69286
I1033
assS'andapproximabilityoffind'
p69287
(dp69288
g40
(lp69289
I2470
assS'zebra'
p69290
(dp69291
g116
(lp69292
I2560
assS'begun'
p69293
(dp69294
g44
(lp69295
I2523
assS'choukri'
p69296
(dp69297
g96
(lp69298
I2917
assS'ejj'
p69299
(dp69300
g149
(lp69301
I924
assS'dredi'
p69302
(dp69303
g80
(lp69304
I35
assS'crosscorrelogram'
p69305
(dp69306
g6
(lp69307
I796
assS'mkii'
p69308
(dp69309
g14
(lp69310
I3532
assS'faculti'
p69311
(dp69312
g80
(lp69313
I476
assS'mkin'
p69314
(dp69315
g178
(lp69316
I709
assS'profil'
p69317
(dp69318
g116
(lp69319
sg118
(lp69320
sg283
(lp69321
sg181
(lp69322
sg295
(lp69323
sg183
(lp69324
sg83
(lp69325
sg303
(lp69326
sg110
(lp69327
sg99
(lp69328
I915
asg149
(lp69329
ssS'ruck'
p69330
(dp69331
g70
(lp69332
I2482
assS'wakoshi'
p69333
(dp69334
g36
(lp69335
I49
assS'factori'
p69336
(dp69337
g72
(lp69338
sg74
(lp69339
sg50
(lp69340
I1693
asg163
(lp69341
ssS'qg'
p69342
(dp69343
g50
(lp69344
I826
assS'hull'
p69345
(dp69346
g42
(lp69347
I3384
assS'kepeat'
p69348
(dp69349
g72
(lp69350
I2412
assS'neuromodulatori'
p69351
(dp69352
g4
(lp69353
I752
assS'theori'
p69354
(dp69355
g68
(lp69356
sg70
(lp69357
sg78
(lp69358
sg163
(lp69359
sg40
(lp69360
sg26
(lp69361
sg30
(lp69362
sg287
(lp69363
sg74
(lp69364
sg176
(lp69365
sg145
(lp69366
sg80
(lp69367
sg262
(lp69368
sg295
(lp69369
sg183
(lp69370
sg85
(lp69371
sg303
(lp69372
sg306
(lp69373
sg91
(lp69374
sg12
(lp69375
sg46
(lp69376
sg96
(lp69377
sg99
(lp69378
sg535
(lp69379
sg223
(lp69380
sg118
(lp69381
sg174
(lp69382
sg293
(lp69383
sg32
(lp69384
sg318
(lp69385
sg102
(lp69386
sg104
(lp69387
sg106
(lp69388
sg110
(lp69389
sg63
(lp69390
sg114
(lp69391
sg438
(lp69392
I4
asg440
(lp69393
sg332
(lp69394
sg178
(lp69395
sg8
(lp69396
sg36
(lp69397
sg384
(lp69398
sg124
(lp69399
sg72
(lp69400
sg281
(lp69401
sg313
(lp69402
sg344
(lp69403
sg44
(lp69404
sg128
(lp69405
sg130
(lp69406
sg138
(lp69407
sg140
(lp69408
sg354
(lp69409
ssS'ension'
p69410
(dp69411
g287
(lp69412
I3555
assS'prescrib'
p69413
(dp69414
g102
(lp69415
I1663
asg277
(lp69416
ssS'refut'
p69417
(dp69418
g85
(lp69419
I3773
assS'synchron'
p69420
(dp69421
g174
(lp69422
sg332
(lp69423
sg22
(lp69424
sg38
(lp69425
sg10
(lp69426
sg104
(lp69427
sg149
(lp69428
I2444
assS'motion'
p69429
(dp69430
g216
(lp69431
sg70
(lp69432
sg32
(lp69433
sg178
(lp69434
sg80
(lp69435
sg6
(lp69436
sg262
(lp69437
sg460
(lp69438
sg293
(lp69439
sg38
(lp69440
sg83
(lp69441
sg42
(lp69442
I2202
asg245
(lp69443
sg46
(lp69444
sg114
(lp69445
sg350
(lp69446
ssS'turn'
p69447
(dp69448
g26
(lp69449
sg163
(lp69450
sg80
(lp69451
sg344
(lp69452
sg83
(lp69453
sg303
(lp69454
sg42
(lp69455
I2249
asg20
(lp69456
sg18
(lp69457
sg99
(lp69458
sg350
(lp69459
sg116
(lp69460
sg329
(lp69461
sg429
(lp69462
sg102
(lp69463
sg110
(lp69464
sg63
(lp69465
sg216
(lp69466
sg174
(lp69467
sg32
(lp69468
sg34
(lp69469
sg384
(lp69470
sg68
(lp69471
sg72
(lp69472
sg341
(lp69473
sg40
(lp69474
sg50
(lp69475
sg140
(lp69476
ssS'ledgement'
p69477
(dp69478
g40
(lp69479
sg8
(lp69480
I2370
assS'place'
p69481
(dp69482
g80
(lp69483
sg74
(lp69484
sg256
(lp69485
sg76
(lp69486
sg293
(lp69487
sg295
(lp69488
sg183
(lp69489
sg59
(lp69490
sg484
(lp69491
sg303
(lp69492
sg42
(lp69493
I1736
asg94
(lp69494
sg18
(lp69495
sg99
(lp69496
sg313
(lp69497
sg149
(lp69498
sg329
(lp69499
sg106
(lp69500
sg63
(lp69501
sg52
(lp69502
sg114
(lp69503
sg174
(lp69504
sg178
(lp69505
sg22
(lp69506
sg341
(lp69507
sg78
(lp69508
sg14
(lp69509
sg135
(lp69510
sg50
(lp69511
sg138
(lp69512
ssS'swing'
p69513
(dp69514
g245
(lp69515
I2427
assS'qn'
p69516
(dp69517
g440
(lp69518
I358
asg76
(lp69519
ssS'imposs'
p69520
(dp69521
g329
(lp69522
sg440
(lp69523
sg318
(lp69524
sg36
(lp69525
sg460
(lp69526
sg68
(lp69527
sg306
(lp69528
sg128
(lp69529
sg130
(lp69530
I1960
asg94
(lp69531
sg63
(lp69532
ssS'jwg'
p69533
(dp69534
g114
(lp69535
I2127
assS'origin'
p69536
(dp69537
g329
(lp69538
sg277
(lp69539
sg287
(lp69540
sg74
(lp69541
sg145
(lp69542
sg80
(lp69543
sg293
(lp69544
sg295
(lp69545
sg183
(lp69546
sg59
(lp69547
sg484
(lp69548
sg85
(lp69549
sg303
(lp69550
sg91
(lp69551
sg12
(lp69552
sg96
(lp69553
sg48
(lp69554
sg221
(lp69555
sg313
(lp69556
sg223
(lp69557
sg350
(lp69558
sg174
(lp69559
sg429
(lp69560
sg318
(lp69561
sg106
(lp69562
sg108
(lp69563
sg230
(lp69564
sg438
(lp69565
I882
asg440
(lp69566
sg332
(lp69567
sg178
(lp69568
sg22
(lp69569
sg6
(lp69570
sg8
(lp69571
sg68
(lp69572
sg126
(lp69573
sg281
(lp69574
sg10
(lp69575
sg535
(lp69576
sg130
(lp69577
sg132
(lp69578
sg14
(lp69579
sg149
(lp69580
sg50
(lp69581
sg138
(lp69582
ssS'suspend'
p69583
(dp69584
g104
(lp69585
I1797
assS'stonaj'
p69586
(dp69587
g132
(lp69588
I694
assS'blumberg'
p69589
(dp69590
g293
(lp69591
I3100
assS'array'
p69592
(dp69593
g108
(lp69594
sg70
(lp69595
sg256
(lp69596
sg183
(lp69597
sg176
(lp69598
sg10
(lp69599
sg245
(lp69600
sg332
(lp69601
sg89
(lp69602
sg102
(lp69603
sg14
(lp69604
sg135
(lp69605
sg138
(lp69606
I1572
asg52
(lp69607
sg114
(lp69608
ssS'predefin'
p69609
(dp69610
g59
(lp69611
I1421
asg429
(lp69612
ssS'bertseka'
p69613
(dp69614
g230
(lp69615
sg306
(lp69616
I248
assS'given'
p69617
(dp69618
g80
(lp69619
sg293
(lp69620
sg344
(lp69621
sg78
(lp69622
sg59
(lp69623
sg484
(lp69624
sg38
(lp69625
sg83
(lp69626
sg85
(lp69627
sg118
(lp69628
sg34
(lp69629
sg36
(lp69630
sg460
(lp69631
sg68
(lp69632
sg72
(lp69633
sg281
(lp69634
sg40
(lp69635
sg283
(lp69636
sg70
(lp69637
sg277
(lp69638
sg163
(lp69639
sg89
(lp69640
sg91
(lp69641
sg12
(lp69642
sg94
(lp69643
sg96
(lp69644
sg48
(lp69645
sg99
(lp69646
sg313
(lp69647
sg44
(lp69648
sg429
(lp69649
sg102
(lp69650
sg104
(lp69651
sg106
(lp69652
sg108
(lp69653
sg110
(lp69654
sg63
(lp69655
sg52
(lp69656
sg114
(lp69657
sg128
(lp69658
sg130
(lp69659
sg132
(lp69660
sg14
(lp69661
sg16
(lp69662
sg135
(lp69663
sg50
(lp69664
sg138
(lp69665
sg140
(lp69666
sg354
(lp69667
sg306
(lp69668
sg87
(lp69669
sg245
(lp69670
sg46
(lp69671
sg20
(lp69672
sg18
(lp69673
sg221
(lp69674
sg535
(lp69675
sg223
(lp69676
sg350
(lp69677
sg216
(lp69678
sg438
(lp69679
sg440
(lp69680
sg332
(lp69681
sg4
(lp69682
sg8
(lp69683
sg126
(lp69684
sg341
(lp69685
sg30
(lp69686
sg287
(lp69687
sg74
(lp69688
sg176
(lp69689
sg145
(lp69690
sg76
(lp69691
sg262
(lp69692
sg295
(lp69693
sg183
(lp69694
sg42
(lp69695
I103
asg230
(lp69696
sg329
(lp69697
sg32
(lp69698
sg318
(lp69699
sg178
(lp69700
sg22
(lp69701
sg235
(lp69702
sg384
(lp69703
sg124
(lp69704
ssS'stuck'
p69705
(dp69706
g295
(lp69707
sg183
(lp69708
sg121
(lp69709
I925
assS'reli'
p69710
(dp69711
g287
(lp69712
sg34
(lp69713
sg78
(lp69714
sg68
(lp69715
sg85
(lp69716
sg306
(lp69717
sg89
(lp69718
sg130
(lp69719
sg132
(lp69720
sg104
(lp69721
sg350
(lp69722
sg313
(lp69723
sg223
(lp69724
sg354
(lp69725
I307
assS'plastic'
p69726
(dp69727
g438
(lp69728
I676
asg318
(lp69729
sg50
(lp69730
sg429
(lp69731
sg176
(lp69732
sg106
(lp69733
sg99
(lp69734
sg149
(lp69735
ssS'credibl'
p69736
(dp69737
g178
(lp69738
I104
assS'adject'
p69739
(dp69740
g94
(lp69741
I1182
assS'iay'
p69742
(dp69743
g223
(lp69744
I1391
assS'bleak'
p69745
(dp69746
g85
(lp69747
I3107
assS'circl'
p69748
(dp69749
g438
(lp69750
I248
asg70
(lp69751
sg121
(lp69752
sg295
(lp69753
sg183
(lp69754
sg72
(lp69755
sg429
(lp69756
sg12
(lp69757
sg14
(lp69758
sg106
(lp69759
sg16
(lp69760
sg52
(lp69761
ssS'white'
p69762
(dp69763
g116
(lp69764
sg438
(lp69765
sg118
(lp69766
sg318
(lp69767
sg256
(lp69768
sg262
(lp69769
sg174
(lp69770
sg26
(lp69771
sg235
(lp69772
sg149
(lp69773
sg114
(lp69774
sg313
(lp69775
sg42
(lp69776
I2386
asg12
(lp69777
sg68
(lp69778
sg80
(lp69779
sg130
(lp69780
sg132
(lp69781
sg108
(lp69782
sg138
(lp69783
sg354
(lp69784
ssS'holden'
p69785
(dp69786
g277
(lp69787
sg223
(lp69788
I3497
assS'hue'
p69789
(dp69790
g12
(lp69791
I2498
asg181
(lp69792
ssS'werbo'
p69793
(dp69794
g76
(lp69795
I1515
assS'cope'
p69796
(dp69797
g332
(lp69798
sg295
(lp69799
sg183
(lp69800
sg34
(lp69801
sg14
(lp69802
sg16
(lp69803
I1775
assS'haberw'
p69804
(dp69805
g176
(lp69806
I2518
assS'copi'
p69807
(dp69808
g116
(lp69809
sg76
(lp69810
sg183
(lp69811
sg124
(lp69812
sg18
(lp69813
I1852
asg114
(lp69814
ssS'adaboostbas'
p69815
(dp69816
g344
(lp69817
I2184
assS'specifi'
p69818
(dp69819
g277
(lp69820
sg126
(lp69821
sg76
(lp69822
sg183
(lp69823
sg59
(lp69824
sg38
(lp69825
sg87
(lp69826
sg89
(lp69827
sg12
(lp69828
sg46
(lp69829
sg20
(lp69830
sg223
(lp69831
sg350
(lp69832
sg429
(lp69833
sg102
(lp69834
sg104
(lp69835
sg110
(lp69836
sg63
(lp69837
sg52
(lp69838
sg230
(lp69839
sg118
(lp69840
sg36
(lp69841
sg460
(lp69842
sg124
(lp69843
sg72
(lp69844
sg40
(lp69845
sg44
(lp69846
sg132
(lp69847
I3768
asg149
(lp69848
ssS'eagl'
p69849
(dp69850
g59
(lp69851
I1045
assS'hut'
p69852
(dp69853
g48
(lp69854
I1700
assS'ilgt'
p69855
(dp69856
g40
(lp69857
I1649
assS'iiali'
p69858
(dp69859
g145
(lp69860
I1182
assS'ofresult'
p69861
(dp69862
g89
(lp69863
I870
assS'necessarili'
p69864
(dp69865
g118
(lp69866
sg484
(lp69867
sg8
(lp69868
sg34
(lp69869
sg78
(lp69870
sg124
(lp69871
sg126
(lp69872
sg281
(lp69873
sg85
(lp69874
sg42
(lp69875
I1050
asg128
(lp69876
sg130
(lp69877
sg12
(lp69878
sg163
(lp69879
sg140
(lp69880
sg354
(lp69881
ssS'photograph'
p69882
(dp69883
g181
(lp69884
I86
assS'serv'
p69885
(dp69886
g32
(lp69887
sg332
(lp69888
sg256
(lp69889
sg6
(lp69890
sg344
(lp69891
sg460
(lp69892
sg80
(lp69893
sg74
(lp69894
sg10
(lp69895
sg281
(lp69896
sg306
(lp69897
sg429
(lp69898
sg94
(lp69899
I2693
asg20
(lp69900
sg59
(lp69901
ssS'wide'
p69902
(dp69903
g283
(lp69904
sg70
(lp69905
sg26
(lp69906
sg181
(lp69907
sg303
(lp69908
sg256
(lp69909
sg262
(lp69910
sg295
(lp69911
sg183
(lp69912
sg59
(lp69913
sg85
(lp69914
sg63
(lp69915
sg42
(lp69916
I3277
asg245
(lp69917
sg48
(lp69918
sg221
(lp69919
sg535
(lp69920
sg223
(lp69921
sg350
(lp69922
sg429
(lp69923
sg332
(lp69924
sg104
(lp69925
sg108
(lp69926
sg110
(lp69927
sg178
(lp69928
sg22
(lp69929
sg329
(lp69930
sg440
(lp69931
sg318
(lp69932
sg121
(lp69933
sg4
(lp69934
sg6
(lp69935
sg235
(lp69936
sg293
(lp69937
sg10
(lp69938
sg344
(lp69939
sg44
(lp69940
sg128
(lp69941
sg14
(lp69942
sg16
(lp69943
sg149
(lp69944
ssS'icslp'
p69945
(dp69946
g96
(lp69947
I2782
assS'lman'
p69948
(dp69949
g116
(lp69950
I373
assS'nakahara'
p69951
(dp69952
g18
(lp69953
I11
assS'r'
p69954
(dp69955
g80
(lp69956
sg293
(lp69957
sg344
(lp69958
sg78
(lp69959
sg59
(lp69960
sg484
(lp69961
sg38
(lp69962
sg83
(lp69963
sg85
(lp69964
sg303
(lp69965
sg438
(lp69966
sg116
(lp69967
sg118
(lp69968
sg34
(lp69969
sg36
(lp69970
sg460
(lp69971
sg72
(lp69972
sg281
(lp69973
sg10
(lp69974
sg40
(lp69975
sg283
(lp69976
sg70
(lp69977
sg26
(lp69978
sg277
(lp69979
sg163
(lp69980
sg89
(lp69981
sg91
(lp69982
sg12
(lp69983
sg94
(lp69984
sg96
(lp69985
sg48
(lp69986
sg99
(lp69987
sg313
(lp69988
sg44
(lp69989
sg149
(lp69990
sg429
(lp69991
sg102
(lp69992
sg104
(lp69993
sg106
(lp69994
sg108
(lp69995
sg110
(lp69996
sg63
(lp69997
sg52
(lp69998
sg114
(lp69999
sg128
(lp70000
sg130
(lp70001
sg132
(lp70002
sg14
(lp70003
sg16
(lp70004
sg135
(lp70005
sg50
(lp70006
sg138
(lp70007
sg140
(lp70008
sg354
(lp70009
sg306
(lp70010
sg87
(lp70011
sg245
(lp70012
sg46
(lp70013
sg20
(lp70014
sg18
(lp70015
sg221
(lp70016
sg535
(lp70017
sg223
(lp70018
sg350
(lp70019
sg216
(lp70020
sg174
(lp70021
sg440
(lp70022
sg332
(lp70023
sg121
(lp70024
sg4
(lp70025
sg6
(lp70026
sg8
(lp70027
sg126
(lp70028
sg341
(lp70029
sg30
(lp70030
sg287
(lp70031
sg74
(lp70032
sg176
(lp70033
sg145
(lp70034
sg256
(lp70035
sg76
(lp70036
sg262
(lp70037
sg295
(lp70038
sg183
(lp70039
sg42
(lp70040
I1506
asg230
(lp70041
sg329
(lp70042
sg32
(lp70043
sg318
(lp70044
sg178
(lp70045
sg22
(lp70046
sg181
(lp70047
sg235
(lp70048
sg384
(lp70049
sg124
(lp70050
ssS'balanc'
p70051
(dp70052
g329
(lp70053
sg26
(lp70054
sg262
(lp70055
sg78
(lp70056
sg83
(lp70057
sg10
(lp70058
sg42
(lp70059
I3301
asg14
(lp70060
sg106
(lp70061
sg16
(lp70062
sg114
(lp70063
ssS'emphasis'
p70064
(dp70065
g283
(lp70066
I853
assS'posit'
p70067
(dp70068
g124
(lp70069
sg70
(lp70070
sg26
(lp70071
sg163
(lp70072
sg283
(lp70073
sg350
(lp70074
sg176
(lp70075
sg80
(lp70076
sg262
(lp70077
sg295
(lp70078
sg183
(lp70079
sg59
(lp70080
sg484
(lp70081
sg38
(lp70082
sg83
(lp70083
sg85
(lp70084
sg303
(lp70085
sg42
(lp70086
I1775
asg12
(lp70087
sg46
(lp70088
sg20
(lp70089
sg99
(lp70090
sg535
(lp70091
sg223
(lp70092
sg149
(lp70093
sg174
(lp70094
sg293
(lp70095
sg245
(lp70096
sg429
(lp70097
sg102
(lp70098
sg178
(lp70099
sg106
(lp70100
sg108
(lp70101
sg52
(lp70102
sg114
(lp70103
sg230
(lp70104
sg438
(lp70105
sg32
(lp70106
sg332
(lp70107
sg121
(lp70108
sg4
(lp70109
sg181
(lp70110
sg8
(lp70111
sg34
(lp70112
sg36
(lp70113
sg460
(lp70114
sg235
(lp70115
sg126
(lp70116
sg341
(lp70117
sg118
(lp70118
sg344
(lp70119
sg132
(lp70120
sg14
(lp70121
sg16
(lp70122
sg135
(lp70123
sg50
(lp70124
sg138
(lp70125
sg140
(lp70126
ssS'contextdepend'
p70127
(dp70128
g87
(lp70129
I347
assS'seri'
p70130
(dp70131
g68
(lp70132
sg70
(lp70133
sg163
(lp70134
sg176
(lp70135
sg145
(lp70136
sg76
(lp70137
sg295
(lp70138
sg183
(lp70139
sg91
(lp70140
sg99
(lp70141
sg313
(lp70142
sg223
(lp70143
sg32
(lp70144
sg102
(lp70145
sg178
(lp70146
sg106
(lp70147
I1598
asg108
(lp70148
sg329
(lp70149
sg440
(lp70150
sg332
(lp70151
sg121
(lp70152
sg181
(lp70153
sg8
(lp70154
sg34
(lp70155
sg124
(lp70156
sg126
(lp70157
sg281
(lp70158
sg128
(lp70159
sg78
(lp70160
sg14
(lp70161
sg16
(lp70162
ssS'and'
p70163
(dp70164
g80
(lp70165
sg293
(lp70166
sg344
(lp70167
sg78
(lp70168
sg59
(lp70169
sg484
(lp70170
sg38
(lp70171
sg83
(lp70172
sg85
(lp70173
sg303
(lp70174
sg438
(lp70175
sg116
(lp70176
sg118
(lp70177
sg34
(lp70178
sg36
(lp70179
sg460
(lp70180
sg68
(lp70181
sg72
(lp70182
sg281
(lp70183
sg10
(lp70184
sg40
(lp70185
sg283
(lp70186
sg70
(lp70187
sg26
(lp70188
sg277
(lp70189
sg163
(lp70190
sg89
(lp70191
sg91
(lp70192
sg12
(lp70193
sg94
(lp70194
sg96
(lp70195
sg48
(lp70196
sg99
(lp70197
sg313
(lp70198
sg44
(lp70199
sg149
(lp70200
sg429
(lp70201
sg102
(lp70202
sg104
(lp70203
sg106
(lp70204
sg108
(lp70205
sg110
(lp70206
sg63
(lp70207
sg52
(lp70208
sg114
(lp70209
sg128
(lp70210
sg130
(lp70211
sg132
(lp70212
sg14
(lp70213
sg16
(lp70214
sg135
(lp70215
sg50
(lp70216
sg138
(lp70217
sg140
(lp70218
sg354
(lp70219
sg306
(lp70220
sg87
(lp70221
sg245
(lp70222
sg46
(lp70223
sg20
(lp70224
sg18
(lp70225
sg221
(lp70226
sg535
(lp70227
sg223
(lp70228
sg350
(lp70229
sg216
(lp70230
sg174
(lp70231
sg440
(lp70232
sg332
(lp70233
sg121
(lp70234
sg4
(lp70235
sg6
(lp70236
sg8
(lp70237
sg126
(lp70238
sg341
(lp70239
sg30
(lp70240
sg287
(lp70241
sg74
(lp70242
sg176
(lp70243
sg145
(lp70244
sg256
(lp70245
sg76
(lp70246
sg262
(lp70247
sg295
(lp70248
sg183
(lp70249
sg42
(lp70250
I6
asg230
(lp70251
sg329
(lp70252
sg32
(lp70253
sg318
(lp70254
sg178
(lp70255
sg22
(lp70256
sg181
(lp70257
sg235
(lp70258
sg384
(lp70259
sg124
(lp70260
ssS'prd'
p70261
(dp70262
g344
(lp70263
I857
assS'diui'
p70264
(dp70265
g18
(lp70266
I976
assS'ana'
p70267
(dp70268
g38
(lp70269
I2743
assS'prb'
p70270
(dp70271
g46
(lp70272
I2542
assS'prl'
p70273
(dp70274
g36
(lp70275
I3312
assS'ann'
p70276
(dp70277
g440
(lp70278
sg283
(lp70279
sg178
(lp70280
sg344
(lp70281
sg36
(lp70282
sg281
(lp70283
sg10
(lp70284
sg12
(lp70285
sg14
(lp70286
I2750
asg99
(lp70287
sg350
(lp70288
ssS'pri'
p70289
(dp70290
g176
(lp70291
I1572
asg85
(lp70292
ssS'ani'
p70293
(dp70294
g124
(lp70295
sg70
(lp70296
sg78
(lp70297
sg163
(lp70298
sg281
(lp70299
sg26
(lp70300
sg30
(lp70301
sg287
(lp70302
sg74
(lp70303
sg176
(lp70304
sg145
(lp70305
sg76
(lp70306
sg262
(lp70307
sg344
(lp70308
sg183
(lp70309
sg59
(lp70310
sg38
(lp70311
sg83
(lp70312
sg85
(lp70313
sg303
(lp70314
sg42
(lp70315
I42
asg306
(lp70316
sg87
(lp70317
sg89
(lp70318
sg91
(lp70319
sg12
(lp70320
sg94
(lp70321
sg96
(lp70322
sg68
(lp70323
sg535
(lp70324
sg44
(lp70325
sg350
(lp70326
sg230
(lp70327
sg329
(lp70328
sg293
(lp70329
sg245
(lp70330
sg429
(lp70331
sg318
(lp70332
sg46
(lp70333
sg102
(lp70334
sg104
(lp70335
sg106
(lp70336
sg108
(lp70337
sg110
(lp70338
sg63
(lp70339
sg52
(lp70340
sg114
(lp70341
sg216
(lp70342
sg174
(lp70343
sg32
(lp70344
sg332
(lp70345
sg8
(lp70346
sg34
(lp70347
sg460
(lp70348
sg235
(lp70349
sg72
(lp70350
sg341
(lp70351
sg10
(lp70352
sg40
(lp70353
sg223
(lp70354
sg128
(lp70355
sg130
(lp70356
sg14
(lp70357
sg50
(lp70358
sg138
(lp70359
sg140
(lp70360
ssS'mateo'
p70361
(dp70362
g26
(lp70363
sg287
(lp70364
sg440
(lp70365
sg318
(lp70366
sg178
(lp70367
sg4
(lp70368
sg34
(lp70369
sg256
(lp70370
sg124
(lp70371
sg126
(lp70372
sg12
(lp70373
sg223
(lp70374
sg91
(lp70375
sg132
(lp70376
sg94
(lp70377
sg20
(lp70378
sg50
(lp70379
sg140
(lp70380
sg354
(lp70381
I2959
assS'subroutin'
p70382
(dp70383
g145
(lp70384
sg89
(lp70385
I2471
assS'prt'
p70386
(dp70387
g96
(lp70388
I2409
assS'bonato'
p70389
(dp70390
g118
(lp70391
I2343
assS'prs'
p70392
(dp70393
g46
(lp70394
I2530
assS'prr'
p70395
(dp70396
g96
(lp70397
I2411
assS'laex'
p70398
(dp70399
g26
(lp70400
I1401
assS'techniqu'
p70401
(dp70402
g283
(lp70403
sg163
(lp70404
sg303
(lp70405
sg40
(lp70406
sg30
(lp70407
sg74
(lp70408
sg145
(lp70409
sg293
(lp70410
sg295
(lp70411
sg183
(lp70412
sg484
(lp70413
sg83
(lp70414
sg63
(lp70415
sg42
(lp70416
I2867
asg89
(lp70417
sg460
(lp70418
sg245
(lp70419
sg46
(lp70420
sg96
(lp70421
sg99
(lp70422
sg313
(lp70423
sg223
(lp70424
sg429
(lp70425
sg94
(lp70426
sg106
(lp70427
sg110
(lp70428
sg20
(lp70429
sg52
(lp70430
sg114
(lp70431
sg230
(lp70432
sg174
(lp70433
sg440
(lp70434
sg318
(lp70435
sg22
(lp70436
sg8
(lp70437
sg384
(lp70438
sg124
(lp70439
sg10
(lp70440
sg535
(lp70441
sg44
(lp70442
sg78
(lp70443
sg132
(lp70444
sg14
(lp70445
sg16
(lp70446
sg135
(lp70447
sg50
(lp70448
sg138
(lp70449
sg140
(lp70450
ssS'moreov'
p70451
(dp70452
g287
(lp70453
sg32
(lp70454
sg318
(lp70455
sg8
(lp70456
sg34
(lp70457
sg78
(lp70458
sg460
(lp70459
sg72
(lp70460
sg74
(lp70461
sg303
(lp70462
sg42
(lp70463
I1958
asg344
(lp70464
sg102
(lp70465
sg110
(lp70466
sg44
(lp70467
ssS'datapath'
p70468
(dp70469
g10
(lp70470
I721
assS'eubank'
p70471
(dp70472
g295
(lp70473
I3820
asg183
(lp70474
ssS'tweten'
p70475
(dp70476
g216
(lp70477
I927
assS'detenninist'
p70478
(dp70479
g110
(lp70480
I63
assS'other'
p70481
(dp70482
g80
(lp70483
sg293
(lp70484
sg344
(lp70485
sg78
(lp70486
sg59
(lp70487
sg484
(lp70488
sg38
(lp70489
sg83
(lp70490
sg85
(lp70491
sg303
(lp70492
sg438
(lp70493
sg116
(lp70494
sg118
(lp70495
sg34
(lp70496
sg36
(lp70497
sg460
(lp70498
sg68
(lp70499
sg72
(lp70500
sg281
(lp70501
sg10
(lp70502
sg40
(lp70503
sg283
(lp70504
sg70
(lp70505
sg26
(lp70506
sg277
(lp70507
sg89
(lp70508
sg91
(lp70509
sg12
(lp70510
sg94
(lp70511
sg96
(lp70512
sg48
(lp70513
sg99
(lp70514
sg44
(lp70515
sg149
(lp70516
sg429
(lp70517
sg102
(lp70518
sg104
(lp70519
sg106
(lp70520
sg108
(lp70521
sg110
(lp70522
sg63
(lp70523
sg52
(lp70524
sg114
(lp70525
sg128
(lp70526
sg130
(lp70527
sg132
(lp70528
sg14
(lp70529
sg135
(lp70530
sg50
(lp70531
sg138
(lp70532
sg140
(lp70533
sg354
(lp70534
sg306
(lp70535
sg245
(lp70536
sg46
(lp70537
sg20
(lp70538
sg18
(lp70539
sg221
(lp70540
sg535
(lp70541
sg223
(lp70542
sg350
(lp70543
sg216
(lp70544
sg174
(lp70545
sg440
(lp70546
sg332
(lp70547
sg121
(lp70548
sg4
(lp70549
sg8
(lp70550
sg126
(lp70551
sg341
(lp70552
sg30
(lp70553
sg287
(lp70554
sg74
(lp70555
sg256
(lp70556
sg76
(lp70557
sg262
(lp70558
sg295
(lp70559
sg183
(lp70560
sg42
(lp70561
I364
asg230
(lp70562
sg329
(lp70563
sg32
(lp70564
sg318
(lp70565
sg178
(lp70566
sg22
(lp70567
sg181
(lp70568
sg235
(lp70569
sg384
(lp70570
sg124
(lp70571
ssS'ideal'
p70572
(dp70573
g438
(lp70574
I147
asg22
(lp70575
sg277
(lp70576
sg329
(lp70577
sg295
(lp70578
sg183
(lp70579
sg460
(lp70580
sg85
(lp70581
sg52
(lp70582
sg245
(lp70583
sg91
(lp70584
sg94
(lp70585
sg132
(lp70586
sg14
(lp70587
sg106
(lp70588
sg20
(lp70589
sg138
(lp70590
sg140
(lp70591
sg350
(lp70592
ssS'imageri'
p70593
(dp70594
g293
(lp70595
I610
assS'hyperpolar'
p70596
(dp70597
g106
(lp70598
I262
assS'cotpput'
p70599
(dp70600
g102
(lp70601
I2388
assS'spheric'
p70602
(dp70603
g30
(lp70604
sg384
(lp70605
I2325
asg283
(lp70606
sg85
(lp70607
ssS'cnrs'
p70608
(dp70609
g287
(lp70610
I20
assS'sure'
p70611
(dp70612
g42
(lp70613
I874
asg36
(lp70614
sg140
(lp70615
sg354
(lp70616
ssS'multipli'
p70617
(dp70618
g287
(lp70619
sg32
(lp70620
sg283
(lp70621
sg70
(lp70622
sg4
(lp70623
sg344
(lp70624
sg10
(lp70625
sg303
(lp70626
sg87
(lp70627
sg102
(lp70628
sg14
(lp70629
sg16
(lp70630
I1903
asg22
(lp70631
sg221
(lp70632
sg44
(lp70633
sg350
(lp70634
ssS'sherman'
p70635
(dp70636
g295
(lp70637
I1784
asg183
(lp70638
ssS'connor'
p70639
(dp70640
g6
(lp70641
I1362
assS'iijij'
p70642
(dp70643
g99
(lp70644
I1924
assS'ddimension'
p70645
(dp70646
g341
(lp70647
I2204
assS'uyl'
p70648
(dp70649
g313
(lp70650
I1423
assS'icon'
p70651
(dp70652
g63
(lp70653
I3077
assS'successfulji'
p70654
(dp70655
g130
(lp70656
I334
assS'neuro'
p70657
(dp70658
g174
(lp70659
I2541
asg59
(lp70660
sg10
(lp70661
ssS'dordrecht'
p70662
(dp70663
g26
(lp70664
sg149
(lp70665
I3142
assS'neura'
p70666
(dp70667
g48
(lp70668
I2480
assS'adopt'
p70669
(dp70670
g76
(lp70671
sg293
(lp70672
sg484
(lp70673
sg85
(lp70674
sg42
(lp70675
I2660
asg87
(lp70676
sg128
(lp70677
sg132
(lp70678
sg96
(lp70679
sg108
(lp70680
sg535
(lp70681
ssS'eiior'
p70682
(dp70683
g76
(lp70684
I3260
assS'quantiti'
p70685
(dp70686
g230
(lp70687
sg438
(lp70688
I2038
asg287
(lp70689
sg329
(lp70690
sg8
(lp70691
sg295
(lp70692
sg183
(lp70693
sg460
(lp70694
sg235
(lp70695
sg85
(lp70696
sg124
(lp70697
sg102
(lp70698
sg429
(lp70699
sg176
(lp70700
sg12
(lp70701
sg14
(lp70702
sg16
(lp70703
sg99
(lp70704
sg44
(lp70705
sg354
(lp70706
ssS'cortex'
p70707
(dp70708
g80
(lp70709
sg438
(lp70710
I10
asg18
(lp70711
sg70
(lp70712
sg4
(lp70713
sg6
(lp70714
sg118
(lp70715
sg50
(lp70716
sg303
(lp70717
sg176
(lp70718
sg63
(lp70719
sg245
(lp70720
sg318
(lp70721
sg12
(lp70722
sg181
(lp70723
sg106
(lp70724
sg48
(lp70725
sg99
(lp70726
sg535
(lp70727
sg149
(lp70728
ssS'senior'
p70729
(dp70730
g76
(lp70731
I9
assS'idiosyncraci'
p70732
(dp70733
g83
(lp70734
I2616
assS'adaequ'
p70735
(dp70736
g36
(lp70737
I547
assS'lium'
p70738
(dp70739
g350
(lp70740
I1880
assS'slope'
p70741
(dp70742
g256
(lp70743
sg76
(lp70744
sg262
(lp70745
sg91
(lp70746
sg132
(lp70747
I925
asg18
(lp70748
sg110
(lp70749
sg223
(lp70750
ssS'avgwait'
p70751
(dp70752
g83
(lp70753
I2313
assS'uncondit'
p70754
(dp70755
g91
(lp70756
I2524
asg281
(lp70757
ssS'tkl'
p70758
(dp70759
g130
(lp70760
I2781
assS'cheap'
p70761
(dp70762
g429
(lp70763
I150
assS'polysilicon'
p70764
(dp70765
g20
(lp70766
sg135
(lp70767
I1027
assS'multicub'
p70768
(dp70769
g283
(lp70770
I388
assS'liebermann'
p70771
(dp70772
g32
(lp70773
I3113
assS'itm'
p70774
(dp70775
g484
(lp70776
I1655
assS'recombin'
p70777
(dp70778
g70
(lp70779
I1507
assS'permiss'
p70780
(dp70781
g295
(lp70782
sg183
(lp70783
sg145
(lp70784
sg535
(lp70785
I1315
assS'itj'
p70786
(dp70787
g74
(lp70788
sg89
(lp70789
I2427
asg91
(lp70790
ssS'nll'
p70791
(dp70792
g68
(lp70793
I2335
assS'pavlidi'
p70794
(dp70795
g63
(lp70796
I165
assS'leigh'
p70797
(dp70798
g350
(lp70799
I2896
assS'dlr'
p70800
(dp70801
g221
(lp70802
I528
assS'written'
p70803
(dp70804
g26
(lp70805
sg74
(lp70806
sg76
(lp70807
sg295
(lp70808
sg183
(lp70809
sg38
(lp70810
sg42
(lp70811
I1066
asg91
(lp70812
sg221
(lp70813
sg313
(lp70814
sg350
(lp70815
sg63
(lp70816
sg114
(lp70817
sg230
(lp70818
sg32
(lp70819
sg235
(lp70820
sg384
(lp70821
sg68
(lp70822
sg126
(lp70823
sg10
(lp70824
sg40
(lp70825
sg50
(lp70826
ssS'devroy'
p70827
(dp70828
g281
(lp70829
I818
assS'multiscal'
p70830
(dp70831
g118
(lp70832
sg8
(lp70833
I2
assS'analyz'
p70834
(dp70835
g277
(lp70836
sg74
(lp70837
sg293
(lp70838
sg344
(lp70839
sg183
(lp70840
sg484
(lp70841
sg38
(lp70842
sg85
(lp70843
sg89
(lp70844
sg245
(lp70845
sg18
(lp70846
sg535
(lp70847
sg102
(lp70848
sg110
(lp70849
sg438
(lp70850
I761
asg318
(lp70851
sg178
(lp70852
sg6
(lp70853
sg36
(lp70854
sg40
(lp70855
sg295
(lp70856
sg132
(lp70857
ssS'analyt'
p70858
(dp70859
g124
(lp70860
sg163
(lp70861
sg295
(lp70862
sg183
(lp70863
sg38
(lp70864
sg91
(lp70865
sg221
(lp70866
sg535
(lp70867
sg102
(lp70868
sg230
(lp70869
sg4
(lp70870
sg235
(lp70871
sg36
(lp70872
sg384
(lp70873
sg68
(lp70874
sg126
(lp70875
sg281
(lp70876
sg132
(lp70877
sg14
(lp70878
sg16
(lp70879
sg460
(lp70880
sg140
(lp70881
sg354
(lp70882
I1285
assS'analys'
p70883
(dp70884
g216
(lp70885
sg32
(lp70886
sg176
(lp70887
sg4
(lp70888
sg235
(lp70889
sg281
(lp70890
sg85
(lp70891
sg110
(lp70892
sg44
(lp70893
I1729
asg256
(lp70894
ssS'viewpoint'
p70895
(dp70896
g138
(lp70897
I3526
asg181
(lp70898
sg313
(lp70899
ssS'dekker'
p70900
(dp70901
g295
(lp70902
sg183
(lp70903
sg96
(lp70904
I2842
asg91
(lp70905
ssS'lighten'
p70906
(dp70907
g118
(lp70908
I2183
assS'ssp'
p70909
(dp70910
g96
(lp70911
I3000
assS'dendrit'
p70912
(dp70913
g438
(lp70914
I1142
asg106
(lp70915
sg70
(lp70916
ssS'dyi'
p70917
(dp70918
g118
(lp70919
I1095
assS'lighter'
p70920
(dp70921
g118
(lp70922
I653
assS'sse'
p70923
(dp70924
g277
(lp70925
I942
assS'reveal'
p70926
(dp70927
g438
(lp70928
I2296
asg74
(lp70929
sg176
(lp70930
sg4
(lp70931
sg181
(lp70932
sg38
(lp70933
sg281
(lp70934
sg40
(lp70935
sg12
(lp70936
sg303
(lp70937
sg48
(lp70938
sg99
(lp70939
sg149
(lp70940
ssS'kademb'
p70941
(dp70942
g22
(lp70943
I2558
assS'musculatur'
p70944
(dp70945
g116
(lp70946
I1641
assS'outperform'
p70947
(dp70948
g277
(lp70949
sg34
(lp70950
sg460
(lp70951
sg126
(lp70952
sg63
(lp70953
sg94
(lp70954
sg221
(lp70955
sg313
(lp70956
I1806
asg223
(lp70957
ssS'dramat'
p70958
(dp70959
g438
(lp70960
I1028
asg145
(lp70961
sg484
(lp70962
sg94
(lp70963
sg96
(lp70964
sg221
(lp70965
ssS'intrins'
p70966
(dp70967
g332
(lp70968
sg6
(lp70969
sg38
(lp70970
sg283
(lp70971
sg104
(lp70972
sg12
(lp70973
sg14
(lp70974
sg106
(lp70975
I1923
asg99
(lp70976
sg16
(lp70977
sg149
(lp70978
ssS'preliminarili'
p70979
(dp70980
g42
(lp70981
I208
assS'rms'
p70982
(dp70983
g108
(lp70984
I1885
assS'rmp'
p70985
(dp70986
g106
(lp70987
I1881
assS'lnx'
p70988
(dp70989
g8
(lp70990
I1918
assS'lnp'
p70991
(dp70992
g163
(lp70993
I496
assS'mtt'
p70994
(dp70995
g306
(lp70996
I2050
assS'insitut'
p70997
(dp70998
g341
(lp70999
I2866
assS'rmj'
p71000
(dp71001
g281
(lp71002
I1297
assS'lnf'
p71003
(dp71004
g102
(lp71005
I441
assS'lna'
p71006
(dp71007
g429
(lp71008
I1281
assS'neuman'
p71009
(dp71010
g91
(lp71011
I1676
assS'detect'
p71012
(dp71013
g283
(lp71014
sg70
(lp71015
sg277
(lp71016
sg163
(lp71017
sg30
(lp71018
sg145
(lp71019
sg80
(lp71020
sg293
(lp71021
sg295
(lp71022
sg183
(lp71023
sg59
(lp71024
sg484
(lp71025
sg303
(lp71026
sg42
(lp71027
I1751
asg245
(lp71028
sg20
(lp71029
sg18
(lp71030
sg221
(lp71031
sg149
(lp71032
sg118
(lp71033
sg12
(lp71034
sg318
(lp71035
sg63
(lp71036
sg114
(lp71037
sg216
(lp71038
sg174
(lp71039
sg332
(lp71040
sg8
(lp71041
sg124
(lp71042
sg78
(lp71043
sg132
(lp71044
sg135
(lp71045
sg50
(lp71046
sg138
(lp71047
ssS'electrogram'
p71048
(dp71049
g135
(lp71050
I117
assS'federal'
p71051
(dp71052
g40
(lp71053
I2479
assS'review'
p71054
(dp71055
g277
(lp71056
sg163
(lp71057
sg30
(lp71058
sg287
(lp71059
sg74
(lp71060
sg176
(lp71061
sg145
(lp71062
sg76
(lp71063
sg295
(lp71064
sg183
(lp71065
sg80
(lp71066
sg85
(lp71067
sg313
(lp71068
sg149
(lp71069
sg104
(lp71070
sg106
(lp71071
I2977
asg118
(lp71072
sg178
(lp71073
sg4
(lp71074
sg235
(lp71075
sg36
(lp71076
sg384
(lp71077
sg68
(lp71078
sg344
(lp71079
sg128
(lp71080
sg130
(lp71081
sg354
(lp71082
ssS'minmj'
p71083
(dp71084
g72
(lp71085
I1678
assS'plymouth'
p71086
(dp71087
g332
(lp71088
I20
assS'hist'
p71089
(dp71090
g174
(lp71091
I2426
assS'wkck'
p71092
(dp71093
g235
(lp71094
I510
assS'inhibitori'
p71095
(dp71096
g216
(lp71097
sg438
(lp71098
I600
asg118
(lp71099
sg332
(lp71100
sg4
(lp71101
sg174
(lp71102
sg68
(lp71103
sg102
(lp71104
sg176
(lp71105
sg12
(lp71106
sg106
(lp71107
sg350
(lp71108
sg50
(lp71109
sg535
(lp71110
sg149
(lp71111
ssS'mtl'
p71112
(dp71113
g277
(lp71114
I1015
assS'vendrik'
p71115
(dp71116
g118
(lp71117
I780
assS'essick'
p71118
(dp71119
g303
(lp71120
I2771
assS'comp'
p71121
(dp71122
g329
(lp71123
I1831
asg38
(lp71124
sg287
(lp71125
ssS'gabaerg'
p71126
(dp71127
g438
(lp71128
I1151
assS'mtrr'
p71129
(dp71130
g306
(lp71131
I2280
assS'cycl'
p71132
(dp71133
g74
(lp71134
sg76
(lp71135
sg78
(lp71136
sg68
(lp71137
sg10
(lp71138
sg94
(lp71139
sg14
(lp71140
sg135
(lp71141
I402
assS'comm'
p71142
(dp71143
g344
(lp71144
sg287
(lp71145
I3700
asg110
(lp71146
ssS'enddo'
p71147
(dp71148
g344
(lp71149
I1030
assS'beichter'
p71150
(dp71151
g10
(lp71152
I2855
assS'come'
p71153
(dp71154
g30
(lp71155
sg121
(lp71156
sg256
(lp71157
sg80
(lp71158
sg295
(lp71159
sg183
(lp71160
sg124
(lp71161
sg126
(lp71162
sg85
(lp71163
sg42
(lp71164
I1808
asg12
(lp71165
sg72
(lp71166
sg91
(lp71167
sg36
(lp71168
sg132
(lp71169
sg104
(lp71170
sg106
(lp71171
sg48
(lp71172
sg178
(lp71173
sg145
(lp71174
sg149
(lp71175
ssS'vetterl'
p71176
(dp71177
g34
(lp71178
I2904
assS'reaction'
p71179
(dp71180
g4
(lp71181
I1475
asg303
(lp71182
ssS'repertoir'
p71183
(dp71184
g116
(lp71185
I867
assS'regiol'
p71186
(dp71187
g108
(lp71188
I1655
assS'region'
p71189
(dp71190
g277
(lp71191
sg176
(lp71192
sg256
(lp71193
sg118
(lp71194
sg295
(lp71195
sg183
(lp71196
sg59
(lp71197
sg89
(lp71198
sg245
(lp71199
sg20
(lp71200
sg48
(lp71201
sg99
(lp71202
sg149
(lp71203
sg329
(lp71204
sg102
(lp71205
sg178
(lp71206
sg108
(lp71207
sg96
(lp71208
sg52
(lp71209
sg114
(lp71210
sg230
(lp71211
sg438
(lp71212
I745
asg332
(lp71213
sg121
(lp71214
sg22
(lp71215
sg6
(lp71216
sg8
(lp71217
sg34
(lp71218
sg36
(lp71219
sg384
(lp71220
sg124
(lp71221
sg126
(lp71222
sg341
(lp71223
sg344
(lp71224
sg14
(lp71225
sg16
(lp71226
sg50
(lp71227
sg354
(lp71228
ssS'beckman'
p71229
(dp71230
g36
(lp71231
sg48
(lp71232
I18
assS'contract'
p71233
(dp71234
g230
(lp71235
sg438
(lp71236
I2378
asg221
(lp71237
sg10
(lp71238
sg306
(lp71239
sg12
(lp71240
sg46
(lp71241
sg99
(lp71242
ssS'icecub'
p71243
(dp71244
g48
(lp71245
I256
assS'leibler'
p71246
(dp71247
g36
(lp71248
sg130
(lp71249
I1017
assS'subthreshold'
p71250
(dp71251
g20
(lp71252
I394
asg256
(lp71253
sg262
(lp71254
ssS'fahlman'
p71255
(dp71256
g89
(lp71257
I2321
assS'jitter'
p71258
(dp71259
g135
(lp71260
I568
assS'ixil'
p71261
(dp71262
g438
(lp71263
I1400
asg130
(lp71264
ssS'inspir'
p71265
(dp71266
g74
(lp71267
sg256
(lp71268
sg181
(lp71269
I35
asg293
(lp71270
sg85
(lp71271
sg40
(lp71272
sg26
(lp71273
ssS'period'
p71274
(dp71275
g26
(lp71276
sg181
(lp71277
sg176
(lp71278
sg262
(lp71279
sg78
(lp71280
sg42
(lp71281
I3032
asg12
(lp71282
sg46
(lp71283
sg18
(lp71284
sg99
(lp71285
sg535
(lp71286
sg174
(lp71287
sg293
(lp71288
sg102
(lp71289
sg4
(lp71290
sg116
(lp71291
sg438
(lp71292
sg332
(lp71293
sg22
(lp71294
sg6
(lp71295
sg8
(lp71296
sg36
(lp71297
sg68
(lp71298
sg14
(lp71299
ssS'duti'
p71300
(dp71301
g78
(lp71302
sg135
(lp71303
I2064
assS'pole'
p71304
(dp71305
g121
(lp71306
I492
asg22
(lp71307
ssS'pola'
p71308
(dp71309
g245
(lp71310
I70
assS'subsampl'
p71311
(dp71312
g183
(lp71313
sg85
(lp71314
sg178
(lp71315
sg181
(lp71316
sg223
(lp71317
I2195
assS'pod'
p71318
(dp71319
g277
(lp71320
I577
assS'poll'
p71321
(dp71322
g83
(lp71323
I2896
assS'poli'
p71324
(dp71325
g440
(lp71326
I227
asg20
(lp71327
ssS'alongsid'
p71328
(dp71329
g277
(lp71330
I2751
assS'nathwani'
p71331
(dp71332
g91
(lp71333
I2985
assS'peng'
p71334
(dp71335
g329
(lp71336
I1745
assS'zimmermann'
p71337
(dp71338
g59
(lp71339
I3436
assS'coupl'
p71340
(dp71341
g283
(lp71342
sg26
(lp71343
sg30
(lp71344
sg176
(lp71345
sg256
(lp71346
sg80
(lp71347
sg293
(lp71348
sg38
(lp71349
sg42
(lp71350
I1909
asg94
(lp71351
sg20
(lp71352
sg99
(lp71353
sg535
(lp71354
sg350
(lp71355
sg106
(lp71356
sg96
(lp71357
sg174
(lp71358
sg332
(lp71359
sg384
(lp71360
sg128
(lp71361
sg130
(lp71362
sg460
(lp71363
ssS'howard'
p71364
(dp71365
g74
(lp71366
sg318
(lp71367
sg6
(lp71368
sg181
(lp71369
sg484
(lp71370
sg303
(lp71371
sg94
(lp71372
I3557
asg63
(lp71373
sg350
(lp71374
ssS'shih'
p71375
(dp71376
g256
(lp71377
I8
assS'cutoff'
p71378
(dp71379
g22
(lp71380
I1141
assS'andrew'
p71381
(dp71382
g121
(lp71383
sg76
(lp71384
sg341
(lp71385
sg85
(lp71386
sg83
(lp71387
sg89
(lp71388
I14
assS'superieur'
p71389
(dp71390
g287
(lp71391
I17
assS'unfavor'
p71392
(dp71393
g429
(lp71394
I1966
assS'workspac'
p71395
(dp71396
g59
(lp71397
sg181
(lp71398
I965
assS'andrea'
p71399
(dp71400
g59
(lp71401
I43
assS'spirit'
p71402
(dp71403
g306
(lp71404
sg68
(lp71405
sg74
(lp71406
sg181
(lp71407
sg130
(lp71408
I640
assS'case'
p71409
(dp71410
g124
(lp71411
sg78
(lp71412
sg277
(lp71413
sg163
(lp71414
sg72
(lp71415
sg293
(lp71416
sg281
(lp71417
sg283
(lp71418
sg85
(lp71419
sg460
(lp71420
sg36
(lp71421
sg40
(lp71422
sg26
(lp71423
sg30
(lp71424
sg350
(lp71425
sg74
(lp71426
sg176
(lp71427
sg145
(lp71428
sg80
(lp71429
sg262
(lp71430
sg295
(lp71431
sg183
(lp71432
sg59
(lp71433
sg38
(lp71434
sg83
(lp71435
sg114
(lp71436
sg303
(lp71437
sg42
(lp71438
I335
asg306
(lp71439
sg87
(lp71440
sg89
(lp71441
sg91
(lp71442
sg12
(lp71443
sg46
(lp71444
sg96
(lp71445
sg48
(lp71446
sg99
(lp71447
sg313
(lp71448
sg44
(lp71449
sg149
(lp71450
sg118
(lp71451
sg230
(lp71452
sg287
(lp71453
sg18
(lp71454
sg32
(lp71455
sg245
(lp71456
sg429
(lp71457
sg68
(lp71458
sg102
(lp71459
sg104
(lp71460
sg108
(lp71461
sg110
(lp71462
sg63
(lp71463
sg22
(lp71464
sg216
(lp71465
sg329
(lp71466
sg440
(lp71467
sg332
(lp71468
sg121
(lp71469
sg181
(lp71470
sg6
(lp71471
sg8
(lp71472
sg34
(lp71473
sg221
(lp71474
sg384
(lp71475
sg235
(lp71476
sg126
(lp71477
sg341
(lp71478
sg535
(lp71479
sg344
(lp71480
sg128
(lp71481
sg130
(lp71482
sg132
(lp71483
sg135
(lp71484
sg50
(lp71485
sg138
(lp71486
sg140
(lp71487
sg354
(lp71488
ssS'refractori'
p71489
(dp71490
g174
(lp71491
I1287
asg262
(lp71492
ssS'pca'
p71493
(dp71494
g96
(lp71495
I1111
asg318
(lp71496
sg72
(lp71497
sg163
(lp71498
ssS'amend'
p71499
(dp71500
g176
(lp71501
I594
assS'mount'
p71502
(dp71503
g14
(lp71504
sg16
(lp71505
I2090
asg59
(lp71506
sg256
(lp71507
sg78
(lp71508
ssS'cast'
p71509
(dp71510
g230
(lp71511
sg46
(lp71512
sg429
(lp71513
sg108
(lp71514
I1189
assS'ipij'
p71515
(dp71516
g30
(lp71517
I1192
assS'refineri'
p71518
(dp71519
g78
(lp71520
I2806
assS'vess'
p71521
(dp71522
g91
(lp71523
I2141
assS'trentin'
p71524
(dp71525
g96
(lp71526
I40
assS'clutter'
p71527
(dp71528
g181
(lp71529
I2298
assS'perez'
p71530
(dp71531
g460
(lp71532
I638
assS'edelson'
p71533
(dp71534
g110
(lp71535
I3146
assS'author'
p71536
(dp71537
g174
(lp71538
sg74
(lp71539
sg48
(lp71540
sg283
(lp71541
sg76
(lp71542
sg118
(lp71543
sg68
(lp71544
sg114
(lp71545
sg42
(lp71546
I2155
asg318
(lp71547
sg223
(lp71548
sg46
(lp71549
sg132
(lp71550
sg94
(lp71551
sg96
(lp71552
sg135
(lp71553
sg138
(lp71554
sg140
(lp71555
sg350
(lp71556
ssS'alphabet'
p71557
(dp71558
g104
(lp71559
sg138
(lp71560
I2007
assS'bowl'
p71561
(dp71562
g85
(lp71563
I3242
assS'html'
p71564
(dp71565
g124
(lp71566
I2980
assS'ecolog'
p71567
(dp71568
g174
(lp71569
I2558
asg181
(lp71570
ssS'eventu'
p71571
(dp71572
g174
(lp71573
sg332
(lp71574
sg80
(lp71575
sg110
(lp71576
sg126
(lp71577
sg176
(lp71578
sg38
(lp71579
sg46
(lp71580
sg104
(lp71581
sg99
(lp71582
I1578
asg149
(lp71583
ssS'todor'
p71584
(dp71585
g32
(lp71586
I3100
assS'week'
p71587
(dp71588
g132
(lp71589
I2608
asg94
(lp71590
ssS'hassoun'
p71591
(dp71592
g36
(lp71593
I217
assS'ween'
p71594
(dp71595
g48
(lp71596
I1665
assS'kelvin'
p71597
(dp71598
g14
(lp71599
sg16
(lp71600
I283
assS'nest'
p71601
(dp71602
g8
(lp71603
sg63
(lp71604
sg85
(lp71605
sg354
(lp71606
I2065
assS'driver'
p71607
(dp71608
g245
(lp71609
I2189
asg293
(lp71610
ssS'director'
p71611
(dp71612
g12
(lp71613
I2539
assS'campbel'
p71614
(dp71615
g132
(lp71616
I3522
asg12
(lp71617
sg32
(lp71618
ssS'vowel'
p71619
(dp71620
g30
(lp71621
sg440
(lp71622
sg87
(lp71623
sg121
(lp71624
I1367
assS'footprint'
p71625
(dp71626
g83
(lp71627
I1938
assS'tix'
p71628
(dp71629
g18
(lp71630
I760
assS'makhoul'
p71631
(dp71632
g87
(lp71633
I417
assS'ijcai'
p71634
(dp71635
g332
(lp71636
I2684
assS'yhi'
p71637
(dp71638
g230
(lp71639
I2091
assS'moder'
p71640
(dp71641
g295
(lp71642
sg94
(lp71643
I344
asg183
(lp71644
sg341
(lp71645
sg8
(lp71646
ssS'multifold'
p71647
(dp71648
g85
(lp71649
I1091
assS'pennsylvania'
p71650
(dp71651
g245
(lp71652
I31
assS'facad'
p71653
(dp71654
g118
(lp71655
I52
assS'justifi'
p71656
(dp71657
g183
(lp71658
sg306
(lp71659
sg176
(lp71660
sg22
(lp71661
sg8
(lp71662
I2225
assS'without'
p71663
(dp71664
g68
(lp71665
sg70
(lp71666
sg78
(lp71667
sg277
(lp71668
sg163
(lp71669
sg303
(lp71670
sg145
(lp71671
sg256
(lp71672
sg80
(lp71673
sg293
(lp71674
sg295
(lp71675
sg183
(lp71676
sg484
(lp71677
sg83
(lp71678
sg85
(lp71679
sg63
(lp71680
sg42
(lp71681
I3085
asg91
(lp71682
sg12
(lp71683
sg94
(lp71684
sg20
(lp71685
sg18
(lp71686
sg221
(lp71687
sg350
(lp71688
sg116
(lp71689
sg178
(lp71690
sg318
(lp71691
sg102
(lp71692
sg104
(lp71693
sg106
(lp71694
sg110
(lp71695
sg96
(lp71696
sg114
(lp71697
sg216
(lp71698
sg329
(lp71699
sg332
(lp71700
sg121
(lp71701
sg6
(lp71702
sg8
(lp71703
sg34
(lp71704
sg36
(lp71705
sg235
(lp71706
sg126
(lp71707
sg341
(lp71708
sg40
(lp71709
sg130
(lp71710
sg132
(lp71711
sg14
(lp71712
sg135
(lp71713
sg50
(lp71714
ssS'fptd'
p71715
(dp71716
g262
(lp71717
I897
assS'deceler'
p71718
(dp71719
g83
(lp71720
I594
assS'model'
p71721
(dp71722
g329
(lp71723
sg70
(lp71724
sg78
(lp71725
sg277
(lp71726
sg72
(lp71727
sg283
(lp71728
sg460
(lp71729
sg36
(lp71730
sg40
(lp71731
sg26
(lp71732
sg30
(lp71733
sg287
(lp71734
sg74
(lp71735
sg176
(lp71736
sg145
(lp71737
sg256
(lp71738
sg76
(lp71739
sg262
(lp71740
sg295
(lp71741
sg183
(lp71742
sg80
(lp71743
sg83
(lp71744
sg85
(lp71745
sg124
(lp71746
sg42
(lp71747
I3368
asg87
(lp71748
sg89
(lp71749
sg91
(lp71750
sg12
(lp71751
sg94
(lp71752
sg96
(lp71753
sg48
(lp71754
sg99
(lp71755
sg313
(lp71756
sg44
(lp71757
sg149
(lp71758
sg118
(lp71759
sg230
(lp71760
sg174
(lp71761
sg293
(lp71762
sg116
(lp71763
sg32
(lp71764
sg245
(lp71765
sg429
(lp71766
sg318
(lp71767
sg46
(lp71768
sg102
(lp71769
sg104
(lp71770
sg106
(lp71771
sg108
(lp71772
sg110
(lp71773
sg178
(lp71774
sg52
(lp71775
sg22
(lp71776
sg216
(lp71777
sg438
(lp71778
sg440
(lp71779
sg332
(lp71780
sg121
(lp71781
sg4
(lp71782
sg181
(lp71783
sg8
(lp71784
sg34
(lp71785
sg221
(lp71786
sg384
(lp71787
sg235
(lp71788
sg126
(lp71789
sg281
(lp71790
sg535
(lp71791
sg344
(lp71792
sg63
(lp71793
sg223
(lp71794
sg128
(lp71795
sg130
(lp71796
sg132
(lp71797
sg14
(lp71798
sg16
(lp71799
sg350
(lp71800
sg138
(lp71801
sg303
(lp71802
sg354
(lp71803
ssS'reward'
p71804
(dp71805
g329
(lp71806
sg283
(lp71807
sg89
(lp71808
I147
asg83
(lp71809
sg293
(lp71810
ssS'nktj'
p71811
(dp71812
g68
(lp71813
I2895
assS'when'
p71814
(dp71815
g344
(lp71816
sg329
(lp71817
sg70
(lp71818
sg78
(lp71819
sg277
(lp71820
sg163
(lp71821
sg72
(lp71822
sg68
(lp71823
sg80
(lp71824
sg36
(lp71825
sg26
(lp71826
sg30
(lp71827
sg350
(lp71828
sg74
(lp71829
sg176
(lp71830
sg145
(lp71831
sg256
(lp71832
sg76
(lp71833
sg262
(lp71834
sg295
(lp71835
sg183
(lp71836
sg59
(lp71837
sg484
(lp71838
sg83
(lp71839
sg85
(lp71840
sg303
(lp71841
sg42
(lp71842
I452
asg306
(lp71843
sg87
(lp71844
sg89
(lp71845
sg91
(lp71846
sg12
(lp71847
sg94
(lp71848
sg96
(lp71849
sg18
(lp71850
sg99
(lp71851
sg313
(lp71852
sg44
(lp71853
sg149
(lp71854
sg230
(lp71855
sg174
(lp71856
sg293
(lp71857
sg116
(lp71858
sg32
(lp71859
sg178
(lp71860
sg245
(lp71861
sg429
(lp71862
sg318
(lp71863
sg46
(lp71864
sg102
(lp71865
sg104
(lp71866
sg106
(lp71867
sg108
(lp71868
sg20
(lp71869
sg52
(lp71870
sg114
(lp71871
sg216
(lp71872
sg438
(lp71873
sg440
(lp71874
sg332
(lp71875
sg121
(lp71876
sg4
(lp71877
sg181
(lp71878
sg8
(lp71879
sg34
(lp71880
sg221
(lp71881
sg460
(lp71882
sg235
(lp71883
sg126
(lp71884
sg341
(lp71885
sg118
(lp71886
sg287
(lp71887
sg223
(lp71888
sg128
(lp71889
sg130
(lp71890
sg132
(lp71891
sg135
(lp71892
sg50
(lp71893
sg140
(lp71894
sg354
(lp71895
ssS'macrostructur'
p71896
(dp71897
g283
(lp71898
I2062
assS'oxford'
p71899
(dp71900
g118
(lp71901
sg80
(lp71902
sg384
(lp71903
sg303
(lp71904
sg106
(lp71905
I2926
asg223
(lp71906
ssS'kill'
p71907
(dp71908
g32
(lp71909
I3032
assS'ithaca'
p71910
(dp71911
g132
(lp71912
I3833
assS'halfway'
p71913
(dp71914
g216
(lp71915
I1702
assS'feedforward'
p71916
(dp71917
g163
(lp71918
sg40
(lp71919
sg287
(lp71920
sg295
(lp71921
sg183
(lp71922
sg83
(lp71923
sg91
(lp71924
sg12
(lp71925
sg94
(lp71926
sg96
(lp71927
sg18
(lp71928
sg535
(lp71929
sg104
(lp71930
sg110
(lp71931
sg52
(lp71932
sg114
(lp71933
sg216
(lp71934
sg178
(lp71935
sg181
(lp71936
sg34
(lp71937
sg36
(lp71938
sg68
(lp71939
sg313
(lp71940
sg128
(lp71941
sg78
(lp71942
sg14
(lp71943
sg16
(lp71944
sg50
(lp71945
sg138
(lp71946
sg354
(lp71947
I3033
assS'nx'
p71948
(dp71949
g287
(lp71950
I2414
assS'mosfet'
p71951
(dp71952
g20
(lp71953
I389
assS'miscellan'
p71954
(dp71955
g48
(lp71956
I367
assS'hint'
p71957
(dp71958
g42
(lp71959
I2718
asg429
(lp71960
sg110
(lp71961
sg277
(lp71962
sg223
(lp71963
ssS'olivi'
p71964
(dp71965
g350
(lp71966
I15
assS'rose'
p71967
(dp71968
g14
(lp71969
I2701
asg74
(lp71970
ssS'except'
p71971
(dp71972
g124
(lp71973
sg26
(lp71974
sg72
(lp71975
sg283
(lp71976
sg287
(lp71977
sg74
(lp71978
sg76
(lp71979
sg262
(lp71980
sg295
(lp71981
sg183
(lp71982
sg38
(lp71983
sg83
(lp71984
sg303
(lp71985
sg42
(lp71986
I3072
asg87
(lp71987
sg48
(lp71988
sg223
(lp71989
sg350
(lp71990
sg293
(lp71991
sg102
(lp71992
sg63
(lp71993
sg230
(lp71994
sg6
(lp71995
sg68
(lp71996
sg126
(lp71997
sg281
(lp71998
sg10
(lp71999
sg344
(lp72000
sg128
(lp72001
sg138
(lp72002
sg354
(lp72003
ssS'xtiai'
p72004
(dp72005
g76
(lp72006
I2305
assS'collicujus'
p72007
(dp72008
g303
(lp72009
I1236
assS'bypixel'
p72010
(dp72011
g70
(lp72012
I1694
assS'lett'
p72013
(dp72014
g106
(lp72015
I3002
asg384
(lp72016
ssS'pwlx'
p72017
(dp72018
g313
(lp72019
I543
assS'rost'
p72020
(dp72021
g26
(lp72022
I3268
assS'vulner'
p72023
(dp72024
g99
(lp72025
I3003
assS'disrupt'
p72026
(dp72027
g116
(lp72028
sg14
(lp72029
I3035
asg68
(lp72030
ssS'ross'
p72031
(dp72032
g118
(lp72033
sg99
(lp72034
I2929
asg4
(lp72035
ssS'shachter'
p72036
(dp72037
g183
(lp72038
I6780
assS'obermay'
p72039
(dp72040
g48
(lp72041
sg149
(lp72042
I319
assS'nq'
p72043
(dp72044
g83
(lp72045
I2201
assS'nr'
p72046
(dp72047
g121
(lp72048
I370
assS'color'
p72049
(dp72050
g118
(lp72051
sg74
(lp72052
sg178
(lp72053
sg181
(lp72054
sg59
(lp72055
sg12
(lp72056
I2476
asg223
(lp72057
ssS'divorc'
p72058
(dp72059
g72
(lp72060
I1005
assS'techn'
p72061
(dp72062
g59
(lp72063
I36
assS'patrick'
p72064
(dp72065
g63
(lp72066
I2992
assS'shrink'
p72067
(dp72068
g295
(lp72069
sg183
(lp72070
sg124
(lp72071
I2682
asg38
(lp72072
ssS'kennedi'
p72073
(dp72074
g124
(lp72075
sg126
(lp72076
I2926
assS'tic'
p72077
(dp72078
g96
(lp72079
I2352
asg89
(lp72080
sg535
(lp72081
ssS'wbi'
p72082
(dp72083
g72
(lp72084
I2126
assS'hackprop'
p72085
(dp72086
g108
(lp72087
I2106
assS'interlock'
p72088
(dp72089
g10
(lp72090
I1302
assS'dicarlo'
p72091
(dp72092
g135
(lp72093
I2486
assS'intact'
p72094
(dp72095
g4
(lp72096
I3080
asg181
(lp72097
sg350
(lp72098
ssS'garnett'
p72099
(dp72100
g281
(lp72101
I929
assS'eurosci'
p72102
(dp72103
g6
(lp72104
I2279
assS'slice'
p72105
(dp72106
g46
(lp72107
sg106
(lp72108
I862
asg318
(lp72109
sg10
(lp72110
ssS'easili'
p72111
(dp72112
g293
(lp72113
sg295
(lp72114
sg183
(lp72115
sg38
(lp72116
sg85
(lp72117
sg42
(lp72118
I1125
asg48
(lp72119
sg313
(lp72120
sg118
(lp72121
sg102
(lp72122
sg104
(lp72123
sg63
(lp72124
sg329
(lp72125
sg318
(lp72126
sg181
(lp72127
sg235
(lp72128
sg36
(lp72129
sg460
(lp72130
sg68
(lp72131
sg341
(lp72132
sg40
(lp72133
sg14
(lp72134
sg138
(lp72135
ssS'highest'
p72136
(dp72137
g30
(lp72138
sg329
(lp72139
sg74
(lp72140
sg318
(lp72141
sg178
(lp72142
sg80
(lp72143
sg293
(lp72144
sg124
(lp72145
sg126
(lp72146
sg83
(lp72147
sg303
(lp72148
sg283
(lp72149
sg70
(lp72150
sg138
(lp72151
I477
assS'landscap'
p72152
(dp72153
g42
(lp72154
I2739
assS'moon'
p72155
(dp72156
g52
(lp72157
I26
assS'subproblem'
p72158
(dp72159
g8
(lp72160
I1511
assS'intract'
p72161
(dp72162
g74
(lp72163
sg83
(lp72164
sg85
(lp72165
sg306
(lp72166
sg89
(lp72167
I201
asg91
(lp72168
ssS'moor'
p72169
(dp72170
g174
(lp72171
sg332
(lp72172
sg295
(lp72173
sg183
(lp72174
sg306
(lp72175
sg89
(lp72176
sg245
(lp72177
sg108
(lp72178
I391
asg110
(lp72179
sg223
(lp72180
ssS'corespond'
p72181
(dp72182
g460
(lp72183
I478
assS'complic'
p72184
(dp72185
g332
(lp72186
sg178
(lp72187
sg181
(lp72188
sg6
(lp72189
sg295
(lp72190
sg183
(lp72191
sg384
(lp72192
sg126
(lp72193
sg83
(lp72194
sg85
(lp72195
sg52
(lp72196
sg91
(lp72197
sg130
(lp72198
sg102
(lp72199
sg140
(lp72200
I2251
asg38
(lp72201
ssS'venugop'
p72202
(dp72203
g163
(lp72204
I1832
assS'optima'
p72205
(dp72206
g354
(lp72207
I1167
assS'immun'
p72208
(dp72209
g174
(lp72210
sg145
(lp72211
sg89
(lp72212
I864
asg283
(lp72213
ssS'inspect'
p72214
(dp72215
g40
(lp72216
sg44
(lp72217
sg130
(lp72218
sg14
(lp72219
sg135
(lp72220
I2119
asg52
(lp72221
ssS'microcod'
p72222
(dp72223
g10
(lp72224
I1670
assS'oo'
p72225
(dp72226
g230
(lp72227
sg287
(lp72228
sg332
(lp72229
sg8
(lp72230
I910
asg34
(lp72231
sg341
(lp72232
sg87
(lp72233
sg46
(lp72234
ssS'on'
p72235
(dp72236
g80
(lp72237
sg293
(lp72238
sg344
(lp72239
sg78
(lp72240
sg59
(lp72241
sg484
(lp72242
sg38
(lp72243
sg83
(lp72244
sg85
(lp72245
sg303
(lp72246
sg438
(lp72247
sg116
(lp72248
sg118
(lp72249
sg34
(lp72250
sg36
(lp72251
sg460
(lp72252
sg68
(lp72253
sg72
(lp72254
sg281
(lp72255
sg10
(lp72256
sg40
(lp72257
sg283
(lp72258
sg70
(lp72259
sg26
(lp72260
sg277
(lp72261
sg163
(lp72262
sg89
(lp72263
sg91
(lp72264
sg12
(lp72265
sg94
(lp72266
sg96
(lp72267
sg48
(lp72268
sg99
(lp72269
sg313
(lp72270
sg44
(lp72271
sg149
(lp72272
sg429
(lp72273
sg102
(lp72274
sg104
(lp72275
sg106
(lp72276
sg108
(lp72277
sg110
(lp72278
sg63
(lp72279
sg52
(lp72280
sg114
(lp72281
sg128
(lp72282
sg130
(lp72283
sg132
(lp72284
sg14
(lp72285
sg16
(lp72286
sg135
(lp72287
sg50
(lp72288
sg138
(lp72289
sg140
(lp72290
sg354
(lp72291
sg306
(lp72292
sg87
(lp72293
sg245
(lp72294
sg46
(lp72295
sg20
(lp72296
sg18
(lp72297
sg221
(lp72298
sg535
(lp72299
sg223
(lp72300
sg350
(lp72301
sg216
(lp72302
sg174
(lp72303
sg440
(lp72304
sg332
(lp72305
sg121
(lp72306
sg4
(lp72307
sg6
(lp72308
sg8
(lp72309
sg126
(lp72310
sg341
(lp72311
sg30
(lp72312
sg287
(lp72313
sg74
(lp72314
sg176
(lp72315
sg145
(lp72316
sg256
(lp72317
sg76
(lp72318
sg262
(lp72319
sg295
(lp72320
sg183
(lp72321
sg42
(lp72322
I192
asg230
(lp72323
sg329
(lp72324
sg32
(lp72325
sg318
(lp72326
sg178
(lp72327
sg22
(lp72328
sg181
(lp72329
sg235
(lp72330
sg384
(lp72331
sg124
(lp72332
ssS'om'
p72333
(dp72334
g132
(lp72335
I1350
asg30
(lp72336
sg36
(lp72337
sg91
(lp72338
sg350
(lp72339
ssS'ol'
p72340
(dp72341
g332
(lp72342
sg8
(lp72343
sg460
(lp72344
sg68
(lp72345
sg126
(lp72346
sg281
(lp72347
sg10
(lp72348
sg12
(lp72349
sg14
(lp72350
I4315
asg108
(lp72351
sg63
(lp72352
sg350
(lp72353
ssS'ok'
p72354
(dp72355
g384
(lp72356
sg178
(lp72357
sg460
(lp72358
sg8
(lp72359
I1473
assS'oj'
p72360
(dp72361
g32
(lp72362
sg108
(lp72363
I2582
asg76
(lp72364
sg8
(lp72365
sg262
(lp72366
sg87
(lp72367
sg91
(lp72368
sg245
(lp72369
sg18
(lp72370
sg535
(lp72371
sg44
(lp72372
sg350
(lp72373
ssS'aanna'
p72374
(dp72375
g230
(lp72376
I38
assS'oh'
p72377
(dp72378
g440
(lp72379
I2147
assS'of'
p72380
(dp72381
g80
(lp72382
sg293
(lp72383
sg344
(lp72384
sg78
(lp72385
sg59
(lp72386
sg484
(lp72387
sg38
(lp72388
sg83
(lp72389
sg85
(lp72390
sg303
(lp72391
sg438
(lp72392
sg116
(lp72393
sg118
(lp72394
sg34
(lp72395
sg36
(lp72396
sg460
(lp72397
sg68
(lp72398
sg72
(lp72399
sg281
(lp72400
sg10
(lp72401
sg40
(lp72402
sg283
(lp72403
sg70
(lp72404
sg26
(lp72405
sg277
(lp72406
sg163
(lp72407
sg89
(lp72408
sg91
(lp72409
sg12
(lp72410
sg94
(lp72411
sg96
(lp72412
sg48
(lp72413
sg99
(lp72414
sg313
(lp72415
sg44
(lp72416
sg149
(lp72417
sg429
(lp72418
sg102
(lp72419
sg104
(lp72420
sg106
(lp72421
sg108
(lp72422
sg110
(lp72423
sg63
(lp72424
sg52
(lp72425
sg114
(lp72426
sg128
(lp72427
sg130
(lp72428
sg132
(lp72429
sg14
(lp72430
sg16
(lp72431
sg135
(lp72432
sg50
(lp72433
sg138
(lp72434
sg140
(lp72435
sg354
(lp72436
sg306
(lp72437
sg87
(lp72438
sg245
(lp72439
sg46
(lp72440
sg20
(lp72441
sg18
(lp72442
sg221
(lp72443
sg535
(lp72444
sg223
(lp72445
sg350
(lp72446
sg216
(lp72447
sg174
(lp72448
sg440
(lp72449
sg332
(lp72450
sg121
(lp72451
sg4
(lp72452
sg6
(lp72453
sg8
(lp72454
sg126
(lp72455
sg341
(lp72456
sg30
(lp72457
sg287
(lp72458
sg74
(lp72459
sg176
(lp72460
sg145
(lp72461
sg256
(lp72462
sg76
(lp72463
sg262
(lp72464
sg295
(lp72465
sg183
(lp72466
sg42
(lp72467
I3
asg230
(lp72468
sg329
(lp72469
sg32
(lp72470
sg318
(lp72471
sg178
(lp72472
sg22
(lp72473
sg181
(lp72474
sg235
(lp72475
sg384
(lp72476
sg124
(lp72477
ssS'gregori'
p72478
(dp72479
g46
(lp72480
I17
assS'od'
p72481
(dp72482
g48
(lp72483
I2263
assS'oc'
p72484
(dp72485
g178
(lp72486
I1430
asg114
(lp72487
ssS'ob'
p72488
(dp72489
g230
(lp72490
sg108
(lp72491
I1216
asg38
(lp72492
ssS'oa'
p72493
(dp72494
g295
(lp72495
I2806
asg183
(lp72496
ssS'stanc'
p72497
(dp72498
g350
(lp72499
I2248
assS'oz'
p72500
(dp72501
g32
(lp72502
sg135
(lp72503
I38
assS'oy'
p72504
(dp72505
g32
(lp72506
sg313
(lp72507
I718
assS'ox'
p72508
(dp72509
g384
(lp72510
sg14
(lp72511
sg16
(lp72512
I57
asg108
(lp72513
sg32
(lp72514
ssS'ow'
p72515
(dp72516
g313
(lp72517
I724
assS'ov'
p72518
(dp72519
g132
(lp72520
sg14
(lp72521
I3660
asg20
(lp72522
ssS'oflargescal'
p72523
(dp72524
g8
(lp72525
I94
assS'ot'
p72526
(dp72527
g230
(lp72528
sg108
(lp72529
I1217
asg277
(lp72530
ssS'os'
p72531
(dp72532
g116
(lp72533
sg174
(lp72534
sg83
(lp72535
sg245
(lp72536
sg132
(lp72537
sg135
(lp72538
I1643
assS'or'
p72539
(dp72540
g80
(lp72541
sg293
(lp72542
sg344
(lp72543
sg78
(lp72544
sg59
(lp72545
sg484
(lp72546
sg38
(lp72547
sg83
(lp72548
sg85
(lp72549
sg303
(lp72550
sg438
(lp72551
sg116
(lp72552
sg118
(lp72553
sg34
(lp72554
sg36
(lp72555
sg460
(lp72556
sg68
(lp72557
sg72
(lp72558
sg281
(lp72559
sg10
(lp72560
sg40
(lp72561
sg283
(lp72562
sg70
(lp72563
sg26
(lp72564
sg277
(lp72565
sg163
(lp72566
sg89
(lp72567
sg91
(lp72568
sg12
(lp72569
sg94
(lp72570
sg96
(lp72571
sg48
(lp72572
sg99
(lp72573
sg313
(lp72574
sg44
(lp72575
sg149
(lp72576
sg429
(lp72577
sg102
(lp72578
sg104
(lp72579
sg106
(lp72580
sg108
(lp72581
sg110
(lp72582
sg63
(lp72583
sg52
(lp72584
sg114
(lp72585
sg128
(lp72586
sg130
(lp72587
sg132
(lp72588
sg14
(lp72589
sg16
(lp72590
sg135
(lp72591
sg50
(lp72592
sg138
(lp72593
sg140
(lp72594
sg354
(lp72595
sg306
(lp72596
sg87
(lp72597
sg245
(lp72598
sg46
(lp72599
sg20
(lp72600
sg18
(lp72601
sg221
(lp72602
sg535
(lp72603
sg223
(lp72604
sg350
(lp72605
sg174
(lp72606
sg440
(lp72607
sg332
(lp72608
sg121
(lp72609
sg4
(lp72610
sg6
(lp72611
sg8
(lp72612
sg126
(lp72613
sg341
(lp72614
sg30
(lp72615
sg287
(lp72616
sg74
(lp72617
sg176
(lp72618
sg145
(lp72619
sg256
(lp72620
sg76
(lp72621
sg262
(lp72622
sg295
(lp72623
sg183
(lp72624
sg42
(lp72625
I109
asg230
(lp72626
sg329
(lp72627
sg32
(lp72628
sg318
(lp72629
sg178
(lp72630
sg22
(lp72631
sg181
(lp72632
sg235
(lp72633
sg384
(lp72634
sg124
(lp72635
ssS'op'
p72636
(dp72637
g332
(lp72638
I2264
assS'routin'
p72639
(dp72640
g96
(lp72641
I1213
asg63
(lp72642
sg10
(lp72643
sg293
(lp72644
ssS'darmstadt'
p72645
(dp72646
g535
(lp72647
I23
assS'harm'
p72648
(dp72649
g484
(lp72650
I2329
assS'eoordin'
p72651
(dp72652
g63
(lp72653
I1241
assS'bauemendspielen'
p72654
(dp72655
g132
(lp72656
I3712
assS'detennin'
p72657
(dp72658
g110
(lp72659
sg91
(lp72660
sg149
(lp72661
I1729
assS'kehl'
p72662
(dp72663
g106
(lp72664
I2599
assS'there'
p72665
(dp72666
g329
(lp72667
sg70
(lp72668
sg78
(lp72669
sg277
(lp72670
sg163
(lp72671
sg68
(lp72672
sg281
(lp72673
sg85
(lp72674
sg460
(lp72675
sg26
(lp72676
sg30
(lp72677
sg287
(lp72678
sg74
(lp72679
sg176
(lp72680
sg145
(lp72681
sg80
(lp72682
sg76
(lp72683
sg262
(lp72684
sg295
(lp72685
sg183
(lp72686
sg59
(lp72687
sg484
(lp72688
sg83
(lp72689
sg114
(lp72690
sg42
(lp72691
I618
asg306
(lp72692
sg87
(lp72693
sg89
(lp72694
sg91
(lp72695
sg12
(lp72696
sg94
(lp72697
sg48
(lp72698
sg99
(lp72699
sg313
(lp72700
sg44
(lp72701
sg149
(lp72702
sg230
(lp72703
sg174
(lp72704
sg293
(lp72705
sg32
(lp72706
sg429
(lp72707
sg318
(lp72708
sg46
(lp72709
sg102
(lp72710
sg178
(lp72711
sg106
(lp72712
sg108
(lp72713
sg110
(lp72714
sg63
(lp72715
sg52
(lp72716
sg22
(lp72717
sg216
(lp72718
sg438
(lp72719
sg440
(lp72720
sg18
(lp72721
sg121
(lp72722
sg4
(lp72723
sg6
(lp72724
sg34
(lp72725
sg221
(lp72726
sg384
(lp72727
sg124
(lp72728
sg341
(lp72729
sg10
(lp72730
sg40
(lp72731
sg344
(lp72732
sg223
(lp72733
sg128
(lp72734
sg36
(lp72735
sg132
(lp72736
sg14
(lp72737
sg16
(lp72738
sg138
(lp72739
sg140
(lp72740
sg354
(lp72741
ssS'tphysic'
p72742
(dp72743
g350
(lp72744
I46
assS'strict'
p72745
(dp72746
g287
(lp72747
sg74
(lp72748
sg26
(lp72749
sg163
(lp72750
sg34
(lp72751
sg59
(lp72752
sg135
(lp72753
I64
assS'interfac'
p72754
(dp72755
g22
(lp72756
sg59
(lp72757
sg10
(lp72758
sg94
(lp72759
sg14
(lp72760
sg106
(lp72761
I1034
asg135
(lp72762
sg96
(lp72763
ssS'therw'
p72764
(dp72765
g40
(lp72766
I427
assS'morrison'
p72767
(dp72768
g295
(lp72769
I1785
asg183
(lp72770
ssS'hodg'
p72771
(dp72772
g281
(lp72773
I661
assS'razhorov'
p72774
(dp72775
g344
(lp72776
I1482
assS'gibb'
p72777
(dp72778
g74
(lp72779
sg124
(lp72780
sg126
(lp72781
sg130
(lp72782
I409
assS'mcgaugh'
p72783
(dp72784
g106
(lp72785
I2918
assS'juj'
p72786
(dp72787
g68
(lp72788
I351
assS'tupl'
p72789
(dp72790
g52
(lp72791
I52
asg293
(lp72792
ssS'regard'
p72793
(dp72794
g230
(lp72795
sg440
(lp72796
sg318
(lp72797
sg283
(lp72798
sg4
(lp72799
sg6
(lp72800
sg76
(lp72801
sg124
(lp72802
sg130
(lp72803
sg85
(lp72804
sg535
(lp72805
sg42
(lp72806
I1823
asg34
(lp72807
sg176
(lp72808
sg94
(lp72809
sg14
(lp72810
sg16
(lp72811
sg18
(lp72812
sg344
(lp72813
sg96
(lp72814
sg22
(lp72815
ssS'diffus'
p72816
(dp72817
g118
(lp72818
sg80
(lp72819
sg329
(lp72820
sg384
(lp72821
sg50
(lp72822
I94
asg149
(lp72823
ssS'jut'
p72824
(dp72825
g42
(lp72826
I2827
assS'sollich'
p72827
(dp72828
g235
(lp72829
sg354
(lp72830
I300
assS'illus'
p72831
(dp72832
g12
(lp72833
I563
asg118
(lp72834
ssS'hemispac'
p72835
(dp72836
g303
(lp72837
I136
assS'longer'
p72838
(dp72839
g216
(lp72840
sg74
(lp72841
sg332
(lp72842
sg178
(lp72843
sg4
(lp72844
sg6
(lp72845
sg76
(lp72846
sg256
(lp72847
sg78
(lp72848
sg38
(lp72849
sg83
(lp72850
sg102
(lp72851
sg176
(lp72852
sg89
(lp72853
sg94
(lp72854
sg132
(lp72855
sg14
(lp72856
sg181
(lp72857
sg18
(lp72858
sg138
(lp72859
I2560
asg149
(lp72860
ssS'herault'
p72861
(dp72862
g178
(lp72863
I2515
assS'notat'
p72864
(dp72865
g8
(lp72866
sg429
(lp72867
sg68
(lp72868
sg341
(lp72869
sg85
(lp72870
sg42
(lp72871
I758
asg306
(lp72872
sg102
(lp72873
sg104
(lp72874
sg96
(lp72875
sg108
(lp72876
sg221
(lp72877
sg535
(lp72878
sg350
(lp72879
ssS'uster'
p72880
(dp72881
g52
(lp72882
I942
assS'bostock'
p72883
(dp72884
g80
(lp72885
I2543
assS'stunaj'
p72886
(dp72887
g132
(lp72888
I640
assS'grasp'
p72889
(dp72890
g460
(lp72891
sg99
(lp72892
I540
assS'forwardbackward'
p72893
(dp72894
g76
(lp72895
I95
assS'taylor'
p72896
(dp72897
g32
(lp72898
sg176
(lp72899
sg8
(lp72900
sg295
(lp72901
sg183
(lp72902
sg384
(lp72903
sg40
(lp72904
sg34
(lp72905
sg283
(lp72906
sg14
(lp72907
sg16
(lp72908
sg108
(lp72909
sg138
(lp72910
I506
assS'cbh'
p72911
(dp72912
g32
(lp72913
I1237
assS'integratorreset'
p72914
(dp72915
g80
(lp72916
I2180
assS'linecancel'
p72917
(dp72918
g303
(lp72919
I2466
assS'rearrang'
p72920
(dp72921
g10
(lp72922
I2061
assS'intra'
p72923
(dp72924
g438
(lp72925
I210
assS'acet'
p72926
(dp72927
g106
(lp72928
I1051
assS'incorrect'
p72929
(dp72930
g318
(lp72931
sg121
(lp72932
sg4
(lp72933
sg76
(lp72934
sg183
(lp72935
sg281
(lp72936
sg42
(lp72937
I1948
asg91
(lp72938
sg94
(lp72939
sg110
(lp72940
ssS'krug'
p72941
(dp72942
g99
(lp72943
I15
assS'purdu'
p72944
(dp72945
g484
(lp72946
I2523
assS'compel'
p72947
(dp72948
g4
(lp72949
I505
assS'mentat'
p72950
(dp72951
g295
(lp72952
I2140
asg183
(lp72953
ssS'shamma'
p72954
(dp72955
g22
(lp72956
I2486
assS'compet'
p72957
(dp72958
g74
(lp72959
sg178
(lp72960
sg277
(lp72961
sg8
(lp72962
sg295
(lp72963
sg183
(lp72964
sg10
(lp72965
sg313
(lp72966
sg34
(lp72967
sg429
(lp72968
sg78
(lp72969
sg102
(lp72970
sg70
(lp72971
sg303
(lp72972
sg138
(lp72973
I1249
assS'igl'
p72974
(dp72975
g350
(lp72976
I570
assS'neuropsycholog'
p72977
(dp72978
g4
(lp72979
I47
asg303
(lp72980
ssS'cytolog'
p72981
(dp72982
g484
(lp72983
I1730
assS'limllld'
p72984
(dp72985
g46
(lp72986
I1802
assS'umform'
p72987
(dp72988
g460
(lp72989
I2214
assS'symbol'
p72990
(dp72991
g332
(lp72992
sg277
(lp72993
sg80
(lp72994
sg344
(lp72995
sg10
(lp72996
sg42
(lp72997
I3407
asg132
(lp72998
sg104
(lp72999
sg48
(lp73000
sg535
(lp73001
sg223
(lp73002
sg350
(lp73003
ssS'briefli'
p73004
(dp73005
g32
(lp73006
sg48
(lp73007
sg145
(lp73008
sg235
(lp73009
sg293
(lp73010
sg68
(lp73011
sg281
(lp73012
sg108
(lp73013
sg132
(lp73014
sg135
(lp73015
I750
asg110
(lp73016
sg223
(lp73017
ssS'nucleus'
p73018
(dp73019
g116
(lp73020
sg174
(lp73021
sg70
(lp73022
sg303
(lp73023
sg106
(lp73024
I2444
asg350
(lp73025
ssS'commissur'
p73026
(dp73027
g106
(lp73028
I1023
assS'stavenga'
p73029
(dp73030
g256
(lp73031
I2149
assS'unbalanc'
p73032
(dp73033
g78
(lp73034
I1280
assS'wife'
p73035
(dp73036
g72
(lp73037
I2559
assS'invest'
p73038
(dp73039
g132
(lp73040
I3342
asg277
(lp73041
ssS'rutger'
p73042
(dp73043
g287
(lp73044
I31
assS'potenti'
p73045
(dp73046
g72
(lp73047
sg287
(lp73048
sg176
(lp73049
sg256
(lp73050
sg76
(lp73051
sg262
(lp73052
sg295
(lp73053
sg183
(lp73054
sg484
(lp73055
sg85
(lp73056
sg306
(lp73057
sg89
(lp73058
sg94
(lp73059
sg20
(lp73060
sg48
(lp73061
sg313
(lp73062
sg223
(lp73063
sg149
(lp73064
sg429
(lp73065
sg46
(lp73066
sg106
(lp73067
sg110
(lp73068
sg63
(lp73069
sg22
(lp73070
sg438
(lp73071
I1764
asg18
(lp73072
sg6
(lp73073
sg181
(lp73074
sg235
(lp73075
sg36
(lp73076
sg384
(lp73077
sg124
(lp73078
sg126
(lp73079
sg281
(lp73080
sg10
(lp73081
sg344
(lp73082
sg130
(lp73083
sg132
(lp73084
sg14
(lp73085
sg16
(lp73086
sg135
(lp73087
sg354
(lp73088
ssS'degrad'
p73089
(dp73090
g245
(lp73091
sg116
(lp73092
sg332
(lp73093
I1986
asg484
(lp73094
sg78
(lp73095
ssS'hmms'
p73096
(dp73097
g460
(lp73098
sg96
(lp73099
I1878
asg87
(lp73100
sg440
(lp73101
ssS'all'
p73102
(dp73103
g80
(lp73104
sg293
(lp73105
sg344
(lp73106
sg78
(lp73107
sg59
(lp73108
sg484
(lp73109
sg38
(lp73110
sg83
(lp73111
sg85
(lp73112
sg303
(lp73113
sg438
(lp73114
sg116
(lp73115
sg118
(lp73116
sg34
(lp73117
sg36
(lp73118
sg460
(lp73119
sg68
(lp73120
sg72
(lp73121
sg281
(lp73122
sg10
(lp73123
sg40
(lp73124
sg283
(lp73125
sg70
(lp73126
sg26
(lp73127
sg277
(lp73128
sg163
(lp73129
sg89
(lp73130
sg91
(lp73131
sg12
(lp73132
sg94
(lp73133
sg96
(lp73134
sg48
(lp73135
sg99
(lp73136
sg313
(lp73137
sg44
(lp73138
sg149
(lp73139
sg429
(lp73140
sg102
(lp73141
sg104
(lp73142
sg108
(lp73143
sg110
(lp73144
sg63
(lp73145
sg52
(lp73146
sg114
(lp73147
sg128
(lp73148
sg130
(lp73149
sg132
(lp73150
sg14
(lp73151
sg16
(lp73152
sg135
(lp73153
sg50
(lp73154
sg138
(lp73155
sg140
(lp73156
sg354
(lp73157
sg87
(lp73158
sg245
(lp73159
sg46
(lp73160
sg20
(lp73161
sg18
(lp73162
sg221
(lp73163
sg535
(lp73164
sg223
(lp73165
sg350
(lp73166
sg216
(lp73167
sg174
(lp73168
sg440
(lp73169
sg121
(lp73170
sg4
(lp73171
sg6
(lp73172
sg8
(lp73173
sg126
(lp73174
sg341
(lp73175
sg30
(lp73176
sg287
(lp73177
sg74
(lp73178
sg176
(lp73179
sg145
(lp73180
sg76
(lp73181
sg262
(lp73182
sg295
(lp73183
sg183
(lp73184
sg42
(lp73185
I769
asg230
(lp73186
sg329
(lp73187
sg32
(lp73188
sg318
(lp73189
sg178
(lp73190
sg22
(lp73191
sg181
(lp73192
sg235
(lp73193
sg384
(lp73194
sg124
(lp73195
ssS'ali'
p73196
(dp73197
g535
(lp73198
I2186
assS'lack'
p73199
(dp73200
g329
(lp73201
sg440
(lp73202
sg145
(lp73203
sg76
(lp73204
sg6
(lp73205
sg287
(lp73206
sg281
(lp73207
sg303
(lp73208
sg42
(lp73209
I198
asg223
(lp73210
sg130
(lp73211
sg14
(lp73212
sg48
(lp73213
sg110
(lp73214
sg138
(lp73215
sg44
(lp73216
ssS'ala'
p73217
(dp73218
g132
(lp73219
I2395
assS'scalar'
p73220
(dp73221
g230
(lp73222
sg438
(lp73223
I2027
asg32
(lp73224
sg163
(lp73225
sg329
(lp73226
sg38
(lp73227
sg124
(lp73228
sg126
(lp73229
sg10
(lp73230
sg287
(lp73231
sg102
(lp73232
sg46
(lp73233
sg354
(lp73234
sg221
(lp73235
sg535
(lp73236
sg149
(lp73237
ssS'disc'
p73238
(dp73239
g40
(lp73240
I2534
assS'abil'
p73241
(dp73242
g30
(lp73243
sg293
(lp73244
sg344
(lp73245
sg38
(lp73246
sg303
(lp73247
sg245
(lp73248
sg94
(lp73249
sg18
(lp73250
sg99
(lp73251
sg313
(lp73252
sg223
(lp73253
sg429
(lp73254
sg106
(lp73255
I1497
asg216
(lp73256
sg332
(lp73257
sg4
(lp73258
sg8
(lp73259
sg34
(lp73260
sg36
(lp73261
sg460
(lp73262
sg40
(lp73263
sg128
(lp73264
sg50
(lp73265
sg138
(lp73266
ssS'follow'
p73267
(dp73268
g329
(lp73269
sg26
(lp73270
sg72
(lp73271
sg68
(lp73272
sg293
(lp73273
sg281
(lp73274
sg85
(lp73275
sg36
(lp73276
sg303
(lp73277
sg350
(lp73278
sg74
(lp73279
sg176
(lp73280
sg145
(lp73281
sg256
(lp73282
sg76
(lp73283
sg262
(lp73284
sg344
(lp73285
sg78
(lp73286
sg59
(lp73287
sg38
(lp73288
sg83
(lp73289
sg114
(lp73290
sg124
(lp73291
sg42
(lp73292
I688
asg306
(lp73293
sg87
(lp73294
sg89
(lp73295
sg91
(lp73296
sg12
(lp73297
sg94
(lp73298
sg20
(lp73299
sg48
(lp73300
sg99
(lp73301
sg535
(lp73302
sg44
(lp73303
sg149
(lp73304
sg118
(lp73305
sg116
(lp73306
sg174
(lp73307
sg18
(lp73308
sg32
(lp73309
sg245
(lp73310
sg429
(lp73311
sg318
(lp73312
sg46
(lp73313
sg102
(lp73314
sg104
(lp73315
sg106
(lp73316
sg108
(lp73317
sg110
(lp73318
sg178
(lp73319
sg22
(lp73320
sg230
(lp73321
sg438
(lp73322
sg440
(lp73323
sg332
(lp73324
sg121
(lp73325
sg4
(lp73326
sg181
(lp73327
sg8
(lp73328
sg34
(lp73329
sg221
(lp73330
sg384
(lp73331
sg235
(lp73332
sg126
(lp73333
sg341
(lp73334
sg10
(lp73335
sg40
(lp73336
sg287
(lp73337
sg63
(lp73338
sg128
(lp73339
sg130
(lp73340
sg132
(lp73341
sg14
(lp73342
sg16
(lp73343
sg135
(lp73344
sg460
(lp73345
sg140
(lp73346
sg354
(lp73347
ssS'disk'
p73348
(dp73349
g118
(lp73350
sg176
(lp73351
I292
assS'pts'
p73352
(dp73353
g18
(lp73354
I761
assS'alp'
p73355
(dp73356
g14
(lp73357
I4656
asg128
(lp73358
ssS'tsitsik'
p73359
(dp73360
g306
(lp73361
I20
assS'hanazawa'
p73362
(dp73363
g94
(lp73364
sg108
(lp73365
I2592
assS'vicario'
p73366
(dp73367
g116
(lp73368
I383
assS'mijkl'
p73369
(dp73370
g74
(lp73371
I1496
assS'program'
p73372
(dp73373
g287
(lp73374
sg74
(lp73375
sg76
(lp73376
sg344
(lp73377
sg183
(lp73378
sg83
(lp73379
sg85
(lp73380
sg306
(lp73381
sg89
(lp73382
sg46
(lp73383
sg96
(lp73384
sg18
(lp73385
sg99
(lp73386
sg94
(lp73387
sg104
(lp73388
sg110
(lp73389
sg114
(lp73390
sg230
(lp73391
sg178
(lp73392
sg10
(lp73393
sg40
(lp73394
sg130
(lp73395
sg132
(lp73396
sg14
(lp73397
sg138
(lp73398
sg140
(lp73399
I2057
assS'ciativ'
p73400
(dp73401
g106
(lp73402
I1174
asg178
(lp73403
ssS'aleksand'
p73404
(dp73405
g138
(lp73406
I3387
assS'neglig'
p73407
(dp73408
g145
(lp73409
sg295
(lp73410
sg183
(lp73411
sg384
(lp73412
sg83
(lp73413
sg42
(lp73414
I669
asg102
(lp73415
sg59
(lp73416
sg44
(lp73417
ssS'fax'
p73418
(dp73419
g87
(lp73420
I30
assS'consum'
p73421
(dp73422
g6
(lp73423
sg163
(lp73424
sg20
(lp73425
sg91
(lp73426
sg14
(lp73427
sg96
(lp73428
sg135
(lp73429
sg138
(lp73430
sg140
(lp73431
I2239
asg114
(lp73432
ssS'fjjf'
p73433
(dp73434
g20
(lp73435
I1746
assS'ingredi'
p73436
(dp73437
g295
(lp73438
I1751
asg183
(lp73439
ssS'far'
p73440
(dp73441
g26
(lp73442
sg277
(lp73443
sg163
(lp73444
sg287
(lp73445
sg74
(lp73446
sg145
(lp73447
sg76
(lp73448
sg262
(lp73449
sg295
(lp73450
sg183
(lp73451
sg59
(lp73452
sg306
(lp73453
sg87
(lp73454
sg91
(lp73455
sg94
(lp73456
sg20
(lp73457
sg18
(lp73458
sg44
(lp73459
sg350
(lp73460
sg102
(lp73461
sg52
(lp73462
sg114
(lp73463
sg216
(lp73464
sg174
(lp73465
sg318
(lp73466
sg181
(lp73467
sg235
(lp73468
sg34
(lp73469
sg384
(lp73470
sg40
(lp73471
sg344
(lp73472
sg128
(lp73473
sg78
(lp73474
sg132
(lp73475
sg138
(lp73476
I909
assS'rod'
p73477
(dp73478
g245
(lp73479
I1305
asg256
(lp73480
sg83
(lp73481
ssS'fat'
p73482
(dp73483
g34
(lp73484
sg318
(lp73485
I2371
assS'llv'
p73486
(dp73487
g262
(lp73488
I1251
assS'lyapunov'
p73489
(dp73490
g12
(lp73491
I1102
asg230
(lp73492
sg535
(lp73493
ssS'sloan'
p73494
(dp73495
g256
(lp73496
I2102
asg40
(lp73497
ssS'ical'
p73498
(dp73499
g149
(lp73500
I2854
assS'fan'
p73501
(dp73502
g287
(lp73503
sg429
(lp73504
sg135
(lp73505
I1039
asg40
(lp73506
ssS'llu'
p73507
(dp73508
g256
(lp73509
sg8
(lp73510
I2474
assS'failur'
p73511
(dp73512
g42
(lp73513
I3133
asg78
(lp73514
sg145
(lp73515
sg4
(lp73516
sg44
(lp73517
ssS'lnlpui'
p73518
(dp73519
g245
(lp73520
I2509
assS'gearbox'
p73521
(dp73522
g78
(lp73523
I2977
assS'induct'
p73524
(dp73525
g30
(lp73526
sg287
(lp73527
sg277
(lp73528
sg344
(lp73529
sg183
(lp73530
sg341
(lp73531
sg91
(lp73532
sg128
(lp73533
sg78
(lp73534
sg132
(lp73535
sg106
(lp73536
I1220
asg223
(lp73537
ssS'hamiltonian'
p73538
(dp73539
g46
(lp73540
sg124
(lp73541
sg126
(lp73542
I1107
assS'stimulus'
p73543
(dp73544
g216
(lp73545
sg118
(lp73546
sg74
(lp73547
sg332
(lp73548
sg70
(lp73549
sg4
(lp73550
sg6
(lp73551
sg262
(lp73552
sg116
(lp73553
sg303
(lp73554
sg176
(lp73555
sg78
(lp73556
sg12
(lp73557
sg106
(lp73558
I534
asg99
(lp73559
sg535
(lp73560
sg256
(lp73561
ssS'list'
p73562
(dp73563
g287
(lp73564
sg440
(lp73565
sg121
(lp73566
sg262
(lp73567
sg8
(lp73568
sg78
(lp73569
sg145
(lp73570
sg32
(lp73571
sg85
(lp73572
sg132
(lp73573
I335
asg94
(lp73574
sg293
(lp73575
sg48
(lp73576
sg99
(lp73577
sg63
(lp73578
sg44
(lp73579
sg350
(lp73580
ssS'lli'
p73581
(dp73582
g104
(lp73583
I2454
asg20
(lp73584
ssS'monkeyus'
p73585
(dp73586
g350
(lp73587
I2803
assS'stepsiz'
p73588
(dp73589
g216
(lp73590
I1355
assS'becker'
p73591
(dp73592
g34
(lp73593
sg318
(lp73594
I203
assS'programma'
p73595
(dp73596
g96
(lp73597
I2689
assS'align'
p73598
(dp73599
g440
(lp73600
sg318
(lp73601
sg80
(lp73602
sg76
(lp73603
sg87
(lp73604
sg130
(lp73605
I1726
asg96
(lp73606
sg48
(lp73607
ssS'fakultiit'
p73608
(dp73609
g313
(lp73610
I2313
assS'synergi'
p73611
(dp73612
g223
(lp73613
I462
assS'tei'
p73614
(dp73615
g221
(lp73616
I460
assS'tel'
p73617
(dp73618
g87
(lp73619
I29
asg145
(lp73620
ssS'tem'
p73621
(dp73622
g460
(lp73623
sg145
(lp73624
I3210
asg83
(lp73625
ssS'ten'
p73626
(dp73627
g116
(lp73628
sg108
(lp73629
sg70
(lp73630
sg78
(lp73631
sg6
(lp73632
sg8
(lp73633
sg183
(lp73634
sg277
(lp73635
sg87
(lp73636
sg48
(lp73637
sg104
(lp73638
sg14
(lp73639
sg16
(lp73640
sg135
(lp73641
sg138
(lp73642
I1243
asg44
(lp73643
sg114
(lp73644
ssS'foreground'
p73645
(dp73646
g332
(lp73647
I724
assS'ted'
p73648
(dp73649
g106
(lp73650
I958
asg318
(lp73651
ssS'dtpt'
p73652
(dp73653
g384
(lp73654
I445
assS'biolog'
p73655
(dp73656
g26
(lp73657
sg176
(lp73658
sg80
(lp73659
sg344
(lp73660
sg59
(lp73661
sg91
(lp73662
sg245
(lp73663
sg46
(lp73664
sg18
(lp73665
sg535
(lp73666
sg350
(lp73667
sg429
(lp73668
sg102
(lp73669
sg106
(lp73670
I2943
asg110
(lp73671
sg63
(lp73672
sg116
(lp73673
sg174
(lp73674
sg32
(lp73675
sg318
(lp73676
sg4
(lp73677
sg6
(lp73678
sg68
(lp73679
sg40
(lp73680
sg48
(lp73681
sg130
(lp73682
sg149
(lp73683
sg50
(lp73684
sg138
(lp73685
ssS'pressur'
p73686
(dp73687
g174
(lp73688
sg277
(lp73689
sg91
(lp73690
sg14
(lp73691
sg16
(lp73692
I525
asg114
(lp73693
ssS'design'
p73694
(dp73695
g283
(lp73696
sg70
(lp73697
sg303
(lp73698
sg256
(lp73699
sg344
(lp73700
sg183
(lp73701
sg59
(lp73702
sg63
(lp73703
sg42
(lp73704
I346
asg89
(lp73705
sg245
(lp73706
sg46
(lp73707
sg20
(lp73708
sg99
(lp73709
sg313
(lp73710
sg429
(lp73711
sg94
(lp73712
sg102
(lp73713
sg178
(lp73714
sg110
(lp73715
sg96
(lp73716
sg114
(lp73717
sg230
(lp73718
sg329
(lp73719
sg121
(lp73720
sg22
(lp73721
sg181
(lp73722
sg8
(lp73723
sg34
(lp73724
sg68
(lp73725
sg281
(lp73726
sg10
(lp73727
sg535
(lp73728
sg128
(lp73729
sg78
(lp73730
sg132
(lp73731
sg14
(lp73732
sg16
(lp73733
sg135
(lp73734
sg138
(lp73735
sg354
(lp73736
ssS'reinf'
p73737
(dp73738
g329
(lp73739
I1830
assS'ter'
p73740
(dp73741
g42
(lp73742
I2778
assS'yw'
p73743
(dp73744
g535
(lp73745
I1170
assS'widrow'
p73746
(dp73747
g104
(lp73748
I408
asg114
(lp73749
ssS'proxi'
p73750
(dp73751
g354
(lp73752
I1812
assS'unsuit'
p73753
(dp73754
g14
(lp73755
sg16
(lp73756
I781
assS'what'
p73757
(dp73758
g329
(lp73759
sg70
(lp73760
sg26
(lp73761
sg277
(lp73762
sg287
(lp73763
sg76
(lp73764
sg262
(lp73765
sg295
(lp73766
sg183
(lp73767
sg59
(lp73768
sg38
(lp73769
sg83
(lp73770
sg303
(lp73771
sg42
(lp73772
I1371
asg91
(lp73773
sg12
(lp73774
sg94
(lp73775
sg20
(lp73776
sg99
(lp73777
sg313
(lp73778
sg223
(lp73779
sg350
(lp73780
sg174
(lp73781
sg293
(lp73782
sg429
(lp73783
sg102
(lp73784
sg110
(lp73785
sg63
(lp73786
sg52
(lp73787
sg438
(lp73788
sg318
(lp73789
sg178
(lp73790
sg80
(lp73791
sg34
(lp73792
sg36
(lp73793
sg68
(lp73794
sg344
(lp73795
ssS'nonlinear'
p73796
(dp73797
g70
(lp73798
sg277
(lp73799
sg163
(lp73800
sg30
(lp73801
sg76
(lp73802
sg295
(lp73803
sg183
(lp73804
sg85
(lp73805
sg303
(lp73806
sg89
(lp73807
sg46
(lp73808
sg96
(lp73809
sg18
(lp73810
sg535
(lp73811
sg350
(lp73812
sg102
(lp73813
sg106
(lp73814
I2938
asg108
(lp73815
sg110
(lp73816
sg230
(lp73817
sg121
(lp73818
sg181
(lp73819
sg8
(lp73820
sg36
(lp73821
sg235
(lp73822
sg128
(lp73823
sg130
(lp73824
sg50
(lp73825
sg138
(lp73826
sg354
(lp73827
ssS'sub'
p73828
(dp73829
g116
(lp73830
sg32
(lp73831
sg63
(lp73832
sg14
(lp73833
sg16
(lp73834
I1753
asg96
(lp73835
sg52
(lp73836
ssS'sun'
p73837
(dp73838
g132
(lp73839
sg14
(lp73840
sg16
(lp73841
I240
asg10
(lp73842
ssS'sum'
p73843
(dp73844
g68
(lp73845
sg70
(lp73846
sg78
(lp73847
sg277
(lp73848
sg163
(lp73849
sg281
(lp73850
sg40
(lp73851
sg287
(lp73852
sg74
(lp73853
sg176
(lp73854
sg145
(lp73855
sg262
(lp73856
sg295
(lp73857
sg183
(lp73858
sg83
(lp73859
sg89
(lp73860
sg91
(lp73861
sg245
(lp73862
sg46
(lp73863
sg20
(lp73864
sg18
(lp73865
sg99
(lp73866
sg313
(lp73867
sg149
(lp73868
sg118
(lp73869
sg293
(lp73870
sg32
(lp73871
sg429
(lp73872
sg318
(lp73873
sg104
(lp73874
sg110
(lp73875
sg52
(lp73876
sg216
(lp73877
sg438
(lp73878
I401
asg440
(lp73879
sg332
(lp73880
sg178
(lp73881
sg181
(lp73882
sg235
(lp73883
sg221
(lp73884
sg384
(lp73885
sg124
(lp73886
sg126
(lp73887
sg341
(lp73888
sg535
(lp73889
sg344
(lp73890
sg128
(lp73891
sg130
(lp73892
sg132
(lp73893
sg14
(lp73894
sg16
(lp73895
sg135
(lp73896
sg50
(lp73897
sg138
(lp73898
sg140
(lp73899
ssS'brief'
p73900
(dp73901
g174
(lp73902
sg70
(lp73903
sg4
(lp73904
sg8
(lp73905
sg40
(lp73906
sg104
(lp73907
sg138
(lp73908
I611
assS'higuchi'
p73909
(dp73910
g20
(lp73911
I2637
assS'version'
p73912
(dp73913
g26
(lp73914
sg163
(lp73915
sg287
(lp73916
sg256
(lp73917
sg344
(lp73918
sg183
(lp73919
sg83
(lp73920
sg85
(lp73921
sg63
(lp73922
sg42
(lp73923
I1254
asg306
(lp73924
sg87
(lp73925
sg91
(lp73926
sg245
(lp73927
sg96
(lp73928
sg48
(lp73929
sg221
(lp73930
sg223
(lp73931
sg329
(lp73932
sg104
(lp73933
sg108
(lp73934
sg110
(lp73935
sg20
(lp73936
sg52
(lp73937
sg174
(lp73938
sg4
(lp73939
sg8
(lp73940
sg34
(lp73941
sg36
(lp73942
sg384
(lp73943
sg124
(lp73944
sg126
(lp73945
sg10
(lp73946
sg44
(lp73947
sg132
(lp73948
sg50
(lp73949
sg460
(lp73950
sg354
(lp73951
ssS'sus'
p73952
(dp73953
g341
(lp73954
I1506
assS'sur'
p73955
(dp73956
g176
(lp73957
I2560
assS'transconduct'
p73958
(dp73959
g135
(lp73960
I1151
assS'califano'
p73961
(dp73962
g181
(lp73963
I2304
assS'nonselect'
p73964
(dp73965
g438
(lp73966
I784
assS'brier'
p73967
(dp73968
g106
(lp73969
I91
assS'discern'
p73970
(dp73971
g256
(lp73972
I1296
assS'themselv'
p73973
(dp73974
g440
(lp73975
sg121
(lp73976
sg4
(lp73977
I670
asg181
(lp73978
sg262
(lp73979
sg295
(lp73980
sg183
(lp73981
sg384
(lp73982
sg38
(lp73983
sg70
(lp73984
sg114
(lp73985
ssS'sauthoff'
p73986
(dp73987
g14
(lp73988
sg16
(lp73989
I2621
assS'behaviour'
p73990
(dp73991
g332
(lp73992
I87
asg178
(lp73993
sg235
(lp73994
sg124
(lp73995
sg126
(lp73996
sg283
(lp73997
sg176
(lp73998
sg535
(lp73999
ssS'koiran'
p74000
(dp74001
g287
(lp74002
I8
assS'berkeley'
p74003
(dp74004
g440
(lp74005
sg121
(lp74006
sg183
(lp74007
sg484
(lp74008
sg10
(lp74009
sg12
(lp74010
I18
asg221
(lp74011
ssS'mcclelland'
p74012
(dp74013
g181
(lp74014
sg76
(lp74015
sg344
(lp74016
sg78
(lp74017
sg40
(lp74018
sg223
(lp74019
I3407
asg114
(lp74020
ssS'siglul'
p74021
(dp74022
g22
(lp74023
I678
assS'naval'
p74024
(dp74025
g438
(lp74026
I2370
asg440
(lp74027
sg6
(lp74028
sg181
(lp74029
sg118
(lp74030
sg106
(lp74031
ssS'mac'
p74032
(dp74033
g221
(lp74034
I1410
assS'mcdonnel'
p74035
(dp74036
g50
(lp74037
I1573
asg313
(lp74038
sg181
(lp74039
sg350
(lp74040
ssS'magnitud'
p74041
(dp74042
g438
(lp74043
I1578
asg74
(lp74044
sg22
(lp74045
sg293
(lp74046
sg295
(lp74047
sg183
(lp74048
sg126
(lp74049
sg341
(lp74050
sg40
(lp74051
sg245
(lp74052
sg87
(lp74053
sg52
(lp74054
sg78
(lp74055
sg132
(lp74056
sg20
(lp74057
sg50
(lp74058
sg535
(lp74059
sg44
(lp74060
sg350
(lp74061
ssS'reiniti'
p74062
(dp74063
g295
(lp74064
I2134
asg183
(lp74065
ssS'glasberg'
p74066
(dp74067
g174
(lp74068
I678
asg332
(lp74069
ssS'wkak'
p74070
(dp74071
g235
(lp74072
I525
assS'pedorm'
p74073
(dp74074
g283
(lp74075
I95
assS'overfit'
p74076
(dp74077
g277
(lp74078
sg235
(lp74079
sg36
(lp74080
sg59
(lp74081
sg126
(lp74082
sg281
(lp74083
sg221
(lp74084
sg138
(lp74085
sg140
(lp74086
I2197
assS'heurist'
p74087
(dp74088
g74
(lp74089
sg72
(lp74090
sg126
(lp74091
sg83
(lp74092
sg85
(lp74093
sg63
(lp74094
sg42
(lp74095
I1102
asg306
(lp74096
sg128
(lp74097
sg94
(lp74098
sg96
(lp74099
sg313
(lp74100
sg52
(lp74101
ssS'denerv'
p74102
(dp74103
g176
(lp74104
I2364
assS'proceed'
p74105
(dp74106
g68
(lp74107
sg78
(lp74108
sg277
(lp74109
sg26
(lp74110
sg30
(lp74111
sg287
(lp74112
sg176
(lp74113
sg76
(lp74114
sg118
(lp74115
sg183
(lp74116
sg59
(lp74117
sg484
(lp74118
sg83
(lp74119
sg85
(lp74120
sg42
(lp74121
I844
asg89
(lp74122
sg245
(lp74123
sg94
(lp74124
sg221
(lp74125
sg223
(lp74126
sg149
(lp74127
sg230
(lp74128
sg329
(lp74129
sg429
(lp74130
sg108
(lp74131
sg63
(lp74132
sg114
(lp74133
sg216
(lp74134
sg174
(lp74135
sg440
(lp74136
sg318
(lp74137
sg121
(lp74138
sg80
(lp74139
sg34
(lp74140
sg124
(lp74141
sg130
(lp74142
sg132
(lp74143
sg14
(lp74144
sg16
(lp74145
sg50
(lp74146
sg138
(lp74147
sg140
(lp74148
ssS'ofmuitipl'
p74149
(dp74150
g332
(lp74151
I2278
assS'yvf'
p74152
(dp74153
g130
(lp74154
I1302
assS'coverag'
p74155
(dp74156
g440
(lp74157
I997
asg87
(lp74158
ssS'branden'
p74159
(dp74160
g26
(lp74161
I3226
assS'ansti'
p74162
(dp74163
g332
(lp74164
I717
assS'autonom'
p74165
(dp74166
g277
(lp74167
sg293
(lp74168
sg384
(lp74169
sg42
(lp74170
I89
asg94
(lp74171
sg18
(lp74172
sg223
(lp74173
ssS'minor'
p74174
(dp74175
g59
(lp74176
sg295
(lp74177
sg183
(lp74178
sg384
(lp74179
sg14
(lp74180
sg16
(lp74181
I958
asg96
(lp74182
sg44
(lp74183
ssS'flat'
p74184
(dp74185
g89
(lp74186
I1004
asg91
(lp74187
sg85
(lp74188
sg341
(lp74189
ssS'israel'
p74190
(dp74191
g32
(lp74192
sg145
(lp74193
sg6
(lp74194
sg140
(lp74195
I3170
asg99
(lp74196
sg223
(lp74197
ssS'mellon'
p74198
(dp74199
g4
(lp74200
sg80
(lp74201
sg277
(lp74202
sg306
(lp74203
sg89
(lp74204
sg91
(lp74205
sg132
(lp74206
I3493
asg94
(lp74207
sg221
(lp74208
sg313
(lp74209
sg223
(lp74210
ssS'probabilist'
p74211
(dp74212
g74
(lp74213
sg178
(lp74214
sg22
(lp74215
sg76
(lp74216
sg262
(lp74217
sg295
(lp74218
sg183
(lp74219
sg440
(lp74220
sg83
(lp74221
sg91
(lp74222
sg110
(lp74223
sg354
(lp74224
I3073
assS'conson'
p74225
(dp74226
g74
(lp74227
I2413
assS'stick'
p74228
(dp74229
g460
(lp74230
I2160
asg429
(lp74231
ssS'known'
p74232
(dp74233
g68
(lp74234
sg70
(lp74235
sg78
(lp74236
sg163
(lp74237
sg303
(lp74238
sg287
(lp74239
sg74
(lp74240
sg176
(lp74241
sg76
(lp74242
sg262
(lp74243
sg183
(lp74244
sg59
(lp74245
sg484
(lp74246
sg83
(lp74247
sg85
(lp74248
sg63
(lp74249
sg42
(lp74250
I1784
asg306
(lp74251
sg89
(lp74252
sg91
(lp74253
sg46
(lp74254
sg96
(lp74255
sg48
(lp74256
sg221
(lp74257
sg313
(lp74258
sg223
(lp74259
sg350
(lp74260
sg32
(lp74261
sg318
(lp74262
sg106
(lp74263
sg108
(lp74264
sg20
(lp74265
sg52
(lp74266
sg230
(lp74267
sg438
(lp74268
sg440
(lp74269
sg332
(lp74270
sg4
(lp74271
sg181
(lp74272
sg34
(lp74273
sg384
(lp74274
sg124
(lp74275
sg281
(lp74276
sg40
(lp74277
sg128
(lp74278
sg130
(lp74279
sg132
(lp74280
sg50
(lp74281
sg140
(lp74282
sg354
(lp74283
ssS'cochlear'
p74284
(dp74285
g174
(lp74286
I53
asg332
(lp74287
sg22
(lp74288
ssS'detriment'
p74289
(dp74290
g484
(lp74291
I2284
assS'ensu'
p74292
(dp74293
g216
(lp74294
I1693
assS'edej'
p74295
(dp74296
g138
(lp74297
I537
assS'iklay'
p74298
(dp74299
g76
(lp74300
I1030
assS'levay'
p74301
(dp74302
g48
(lp74303
I2227
assS'lnlng'
p74304
(dp74305
g128
(lp74306
I1589
assS'outlin'
p74307
(dp74308
g216
(lp74309
sg230
(lp74310
sg76
(lp74311
sg126
(lp74312
sg341
(lp74313
sg40
(lp74314
sg89
(lp74315
sg12
(lp74316
sg104
(lp74317
sg96
(lp74318
sg108
(lp74319
sg138
(lp74320
I3139
assS'autotypist'
p74321
(dp74322
g94
(lp74323
I71
assS'v'
p74324
(dp74325
g68
(lp74326
sg70
(lp74327
sg163
(lp74328
sg116
(lp74329
sg293
(lp74330
sg281
(lp74331
sg283
(lp74332
sg85
(lp74333
sg40
(lp74334
sg287
(lp74335
sg74
(lp74336
sg176
(lp74337
sg145
(lp74338
sg256
(lp74339
sg80
(lp74340
sg262
(lp74341
sg183
(lp74342
sg484
(lp74343
sg38
(lp74344
sg114
(lp74345
sg306
(lp74346
sg87
(lp74347
sg91
(lp74348
sg12
(lp74349
sg46
(lp74350
sg20
(lp74351
sg48
(lp74352
sg221
(lp74353
sg313
(lp74354
sg44
(lp74355
sg149
(lp74356
sg230
(lp74357
sg329
(lp74358
sg18
(lp74359
sg32
(lp74360
sg245
(lp74361
sg318
(lp74362
sg102
(lp74363
sg104
(lp74364
sg106
(lp74365
I1988
asg108
(lp74366
sg110
(lp74367
sg178
(lp74368
sg22
(lp74369
sg216
(lp74370
sg174
(lp74371
sg440
(lp74372
sg332
(lp74373
sg121
(lp74374
sg4
(lp74375
sg8
(lp74376
sg34
(lp74377
sg36
(lp74378
sg460
(lp74379
sg124
(lp74380
sg72
(lp74381
sg341
(lp74382
sg535
(lp74383
sg130
(lp74384
sg132
(lp74385
sg14
(lp74386
sg135
(lp74387
sg140
(lp74388
sg354
(lp74389
ssS'duan'
p74390
(dp74391
g124
(lp74392
sg126
(lp74393
I437
assS'orlean'
p74394
(dp74395
g230
(lp74396
I3470
assS'omit'
p74397
(dp74398
g287
(lp74399
sg74
(lp74400
sg332
(lp74401
sg6
(lp74402
sg295
(lp74403
sg183
(lp74404
sg341
(lp74405
sg85
(lp74406
sg34
(lp74407
sg36
(lp74408
sg130
(lp74409
I1184
assS'methyl'
p74410
(dp74411
g106
(lp74412
I1209
assS'rectif'
p74413
(dp74414
g174
(lp74415
I492
assS'scriptiv'
p74416
(dp74417
g48
(lp74418
I628
assS'cours'
p74419
(dp74420
g26
(lp74421
sg277
(lp74422
sg181
(lp74423
sg287
(lp74424
sg76
(lp74425
sg262
(lp74426
sg295
(lp74427
sg183
(lp74428
sg83
(lp74429
sg85
(lp74430
sg42
(lp74431
I2584
asg48
(lp74432
sg18
(lp74433
sg44
(lp74434
sg350
(lp74435
sg293
(lp74436
sg104
(lp74437
sg106
(lp74438
sg332
(lp74439
sg4
(lp74440
sg6
(lp74441
sg235
(lp74442
sg68
(lp74443
sg344
(lp74444
sg138
(lp74445
ssS'goal'
p74446
(dp74447
g70
(lp74448
sg277
(lp74449
sg163
(lp74450
sg176
(lp74451
sg145
(lp74452
sg76
(lp74453
sg293
(lp74454
sg78
(lp74455
sg59
(lp74456
sg484
(lp74457
sg85
(lp74458
sg306
(lp74459
sg89
(lp74460
sg91
(lp74461
sg94
(lp74462
sg96
(lp74463
sg18
(lp74464
sg221
(lp74465
sg313
(lp74466
sg223
(lp74467
sg104
(lp74468
sg110
(lp74469
sg230
(lp74470
sg329
(lp74471
sg440
(lp74472
sg80
(lp74473
sg181
(lp74474
sg99
(lp74475
sg460
(lp74476
sg341
(lp74477
sg10
(lp74478
sg44
(lp74479
sg132
(lp74480
I567
assS'divid'
p74481
(dp74482
g163
(lp74483
sg74
(lp74484
sg256
(lp74485
sg76
(lp74486
sg78
(lp74487
sg484
(lp74488
sg42
(lp74489
I1901
asg87
(lp74490
sg63
(lp74491
sg114
(lp74492
sg116
(lp74493
sg174
(lp74494
sg32
(lp74495
sg22
(lp74496
sg36
(lp74497
sg126
(lp74498
sg40
(lp74499
sg128
(lp74500
sg183
(lp74501
sg14
(lp74502
sg16
(lp74503
sg135
(lp74504
sg138
(lp74505
ssS'rather'
p74506
(dp74507
g68
(lp74508
sg163
(lp74509
sg30
(lp74510
sg74
(lp74511
sg176
(lp74512
sg256
(lp74513
sg80
(lp74514
sg293
(lp74515
sg295
(lp74516
sg183
(lp74517
sg59
(lp74518
sg83
(lp74519
sg85
(lp74520
sg303
(lp74521
sg87
(lp74522
sg89
(lp74523
sg245
(lp74524
sg46
(lp74525
sg48
(lp74526
sg221
(lp74527
sg44
(lp74528
sg174
(lp74529
sg318
(lp74530
sg94
(lp74531
sg102
(lp74532
sg106
(lp74533
sg110
(lp74534
sg63
(lp74535
sg114
(lp74536
sg216
(lp74537
sg438
(lp74538
I394
asg440
(lp74539
sg332
(lp74540
sg4
(lp74541
sg181
(lp74542
sg34
(lp74543
sg99
(lp74544
sg384
(lp74545
sg124
(lp74546
sg126
(lp74547
sg281
(lp74548
sg40
(lp74549
sg36
(lp74550
sg132
(lp74551
sg14
(lp74552
sg16
(lp74553
sg50
(lp74554
sg354
(lp74555
ssS'nearestneighbor'
p74556
(dp74557
g281
(lp74558
I766
assS'divis'
p74559
(dp74560
g36
(lp74561
sg68
(lp74562
sg85
(lp74563
sg40
(lp74564
sg12
(lp74565
I2544
asg223
(lp74566
ssS'perman'
p74567
(dp74568
g36
(lp74569
I244
asg429
(lp74570
ssS'falavigna'
p74571
(dp74572
g96
(lp74573
I2725
assS'sandwich'
p74574
(dp74575
g14
(lp74576
I2730
assS'resourc'
p74577
(dp74578
g8
(lp74579
sg293
(lp74580
sg72
(lp74581
sg87
(lp74582
sg128
(lp74583
I799
asg102
(lp74584
sg96
(lp74585
ssS'algebra'
p74586
(dp74587
g32
(lp74588
sg34
(lp74589
sg10
(lp74590
sg429
(lp74591
sg46
(lp74592
sg104
(lp74593
sg140
(lp74594
I638
assS'discusf'
p74595
(dp74596
g181
(lp74597
I2315
assS'reflect'
p74598
(dp74599
g283
(lp74600
sg70
(lp74601
sg163
(lp74602
sg76
(lp74603
sg295
(lp74604
sg183
(lp74605
sg38
(lp74606
sg42
(lp74607
I2544
asg94
(lp74608
sg96
(lp74609
sg221
(lp74610
sg313
(lp74611
sg44
(lp74612
sg350
(lp74613
sg104
(lp74614
sg108
(lp74615
sg110
(lp74616
sg52
(lp74617
sg118
(lp74618
sg332
(lp74619
sg121
(lp74620
sg4
(lp74621
sg8
(lp74622
sg130
(lp74623
sg354
(lp74624
ssS'langley'
p74625
(dp74626
g91
(lp74627
I2926
assS'tession'
p74628
(dp74629
g106
(lp74630
I1253
assS'performanceofth'
p74631
(dp74632
g350
(lp74633
I2909
assS'short'
p74634
(dp74635
g283
(lp74636
sg277
(lp74637
sg176
(lp74638
sg262
(lp74639
sg85
(lp74640
sg303
(lp74641
sg87
(lp74642
sg89
(lp74643
sg12
(lp74644
sg96
(lp74645
sg99
(lp74646
sg535
(lp74647
sg149
(lp74648
sg116
(lp74649
sg429
(lp74650
sg102
(lp74651
sg20
(lp74652
sg114
(lp74653
sg216
(lp74654
sg174
(lp74655
sg32
(lp74656
sg121
(lp74657
sg4
(lp74658
sg6
(lp74659
sg36
(lp74660
sg68
(lp74661
sg72
(lp74662
sg132
(lp74663
sg138
(lp74664
I2261
assS'lha'
p74665
(dp74666
g72
(lp74667
I2696
assS'susan'
p74668
(dp74669
g332
(lp74670
I6
assS'rlght'
p74671
(dp74672
g245
(lp74673
I1796
assS'trevor'
p74674
(dp74675
g293
(lp74676
I8
assS'ambigu'
p74677
(dp74678
g102
(lp74679
sg80
(lp74680
sg181
(lp74681
sg140
(lp74682
I51
asg235
(lp74683
ssS'caus'
p74684
(dp74685
g277
(lp74686
sg163
(lp74687
sg72
(lp74688
sg181
(lp74689
sg74
(lp74690
sg176
(lp74691
sg145
(lp74692
sg256
(lp74693
sg76
(lp74694
sg293
(lp74695
sg295
(lp74696
sg183
(lp74697
sg484
(lp74698
sg83
(lp74699
sg303
(lp74700
sg89
(lp74701
sg245
(lp74702
sg94
(lp74703
sg20
(lp74704
sg18
(lp74705
sg221
(lp74706
sg149
(lp74707
sg116
(lp74708
sg174
(lp74709
sg46
(lp74710
sg102
(lp74711
sg63
(lp74712
sg52
(lp74713
sg216
(lp74714
sg438
(lp74715
I1885
asg32
(lp74716
sg332
(lp74717
sg4
(lp74718
sg6
(lp74719
sg235
(lp74720
sg99
(lp74721
sg126
(lp74722
sg128
(lp74723
sg78
(lp74724
sg132
(lp74725
sg14
(lp74726
sg135
(lp74727
sg354
(lp74728
ssS'shade'
p74729
(dp74730
g116
(lp74731
sg145
(lp74732
sg429
(lp74733
sg14
(lp74734
sg16
(lp74735
I429
asg63
(lp74736
sg44
(lp74737
ssS'multivari'
p74738
(dp74739
g440
(lp74740
sg70
(lp74741
sg295
(lp74742
sg183
(lp74743
sg124
(lp74744
sg126
(lp74745
sg130
(lp74746
I3179
asg102
(lp74747
sg96
(lp74748
sg221
(lp74749
sg313
(lp74750
sg38
(lp74751
ssS'woxt'
p74752
(dp74753
g341
(lp74754
I2662
assS'pseudorandom'
p74755
(dp74756
g281
(lp74757
I1449
assS'folklor'
p74758
(dp74759
g36
(lp74760
I171
assS'stride'
p74761
(dp74762
g10
(lp74763
I1182
assS'boun'
p74764
(dp74765
g178
(lp74766
I24
assS'style'
p74767
(dp74768
g42
(lp74769
I70
asg70
(lp74770
sg63
(lp74771
sg76
(lp74772
sg114
(lp74773
ssS'mefl'
p74774
(dp74775
g104
(lp74776
I1231
assS'trairq'
p74777
(dp74778
g59
(lp74779
I2547
assS'resort'
p74780
(dp74781
g230
(lp74782
sg104
(lp74783
sg460
(lp74784
sg50
(lp74785
I1474
assS'eclo'
p74786
(dp74787
g40
(lp74788
I2587
assS'stephen'
p74789
(dp74790
g30
(lp74791
sg78
(lp74792
sg32
(lp74793
sg135
(lp74794
I17
asg283
(lp74795
ssS'sharedhidoen'
p74796
(dp74797
g277
(lp74798
I2003
assS'might'
p74799
(dp74800
g70
(lp74801
sg277
(lp74802
sg163
(lp74803
sg281
(lp74804
sg30
(lp74805
sg74
(lp74806
sg76
(lp74807
sg78
(lp74808
sg59
(lp74809
sg484
(lp74810
sg303
(lp74811
sg306
(lp74812
sg221
(lp74813
sg223
(lp74814
sg429
(lp74815
sg318
(lp74816
sg216
(lp74817
sg332
(lp74818
sg4
(lp74819
sg181
(lp74820
sg34
(lp74821
sg99
(lp74822
sg68
(lp74823
sg126
(lp74824
sg341
(lp74825
sg44
(lp74826
sg128
(lp74827
sg130
(lp74828
sg132
(lp74829
sg140
(lp74830
I2199
assS'alter'
p74831
(dp74832
g174
(lp74833
sg332
(lp74834
sg22
(lp74835
sg118
(lp74836
sg183
(lp74837
sg46
(lp74838
sg14
(lp74839
sg106
(lp74840
I700
asg20
(lp74841
sg99
(lp74842
sg16
(lp74843
sg149
(lp74844
ssS'northwest'
p74845
(dp74846
g80
(lp74847
I247
assS'qnlqn'
p74848
(dp74849
g440
(lp74850
I446
assS'return'
p74851
(dp74852
g332
(lp74853
sg145
(lp74854
sg344
(lp74855
sg83
(lp74856
sg303
(lp74857
sg42
(lp74858
I350
asg89
(lp74859
sg223
(lp74860
sg46
(lp74861
sg18
(lp74862
sg99
(lp74863
sg63
(lp74864
sg44
(lp74865
sg149
(lp74866
ssS'visuospati'
p74867
(dp74868
g80
(lp74869
I2454
assS'framework'
p74870
(dp74871
g72
(lp74872
sg30
(lp74873
sg287
(lp74874
sg74
(lp74875
sg176
(lp74876
sg76
(lp74877
sg344
(lp74878
sg484
(lp74879
sg303
(lp74880
sg42
(lp74881
I3272
asg306
(lp74882
sg87
(lp74883
sg245
(lp74884
sg46
(lp74885
sg99
(lp74886
sg223
(lp74887
sg149
(lp74888
sg32
(lp74889
sg12
(lp74890
sg429
(lp74891
sg104
(lp74892
sg63
(lp74893
sg114
(lp74894
sg440
(lp74895
sg4
(lp74896
sg8
(lp74897
sg34
(lp74898
sg124
(lp74899
sg126
(lp74900
sg281
(lp74901
sg130
(lp74902
sg138
(lp74903
sg354
(lp74904
ssS'semicirtular'
p74905
(dp74906
g350
(lp74907
I1755
assS'ficat'
p74908
(dp74909
g114
(lp74910
I344
assS'callaway'
p74911
(dp74912
g149
(lp74913
I217
assS'marwan'
p74914
(dp74915
g135
(lp74916
I12
assS'refresh'
p74917
(dp74918
g14
(lp74919
I2860
assS'compris'
p74920
(dp74921
g32
(lp74922
sg332
(lp74923
sg132
(lp74924
sg14
(lp74925
I2983
asg110
(lp74926
sg114
(lp74927
ssS'enoi'
p74928
(dp74929
g183
(lp74930
I5220
assS'ceas'
p74931
(dp74932
g216
(lp74933
sg174
(lp74934
I1601
asg76
(lp74935
ssS'blicher'
p74936
(dp74937
g30
(lp74938
I2593
assS'ikiiiki'
p74939
(dp74940
g440
(lp74941
I492
assS'truncat'
p74942
(dp74943
g70
(lp74944
sg8
(lp74945
sg126
(lp74946
sg68
(lp74947
sg38
(lp74948
sg281
(lp74949
sg83
(lp74950
sg42
(lp74951
I542
asg163
(lp74952
ssS'marcum'
p74953
(dp74954
g114
(lp74955
I1338
assS'backwardlik'
p74956
(dp74957
g440
(lp74958
I1158
assS'mansour'
p74959
(dp74960
g145
(lp74961
I9
asg85
(lp74962
ssS'weight'
p74963
(dp74964
g68
(lp74965
sg78
(lp74966
sg116
(lp74967
sg303
(lp74968
sg281
(lp74969
sg40
(lp74970
sg26
(lp74971
sg30
(lp74972
sg287
(lp74973
sg74
(lp74974
sg176
(lp74975
sg76
(lp74976
sg118
(lp74977
sg295
(lp74978
sg183
(lp74979
sg484
(lp74980
sg38
(lp74981
sg85
(lp74982
sg63
(lp74983
sg42
(lp74984
I2227
asg306
(lp74985
sg87
(lp74986
sg89
(lp74987
sg94
(lp74988
sg96
(lp74989
sg18
(lp74990
sg99
(lp74991
sg313
(lp74992
sg223
(lp74993
sg149
(lp74994
sg230
(lp74995
sg329
(lp74996
sg293
(lp74997
sg318
(lp74998
sg102
(lp74999
sg178
(lp75000
sg108
(lp75001
sg110
(lp75002
sg20
(lp75003
sg52
(lp75004
sg114
(lp75005
sg216
(lp75006
sg174
(lp75007
sg332
(lp75008
sg121
(lp75009
sg4
(lp75010
sg6
(lp75011
sg235
(lp75012
sg34
(lp75013
sg221
(lp75014
sg460
(lp75015
sg124
(lp75016
sg126
(lp75017
sg341
(lp75018
sg10
(lp75019
sg535
(lp75020
sg344
(lp75021
sg128
(lp75022
sg36
(lp75023
sg132
(lp75024
sg14
(lp75025
sg16
(lp75026
sg135
(lp75027
sg50
(lp75028
sg138
(lp75029
sg140
(lp75030
sg354
(lp75031
ssS'needless'
p75032
(dp75033
g145
(lp75034
I2146
assS'stroop'
p75035
(dp75036
g4
(lp75037
I603
assS'expect'
p75038
(dp75039
g68
(lp75040
sg70
(lp75041
sg283
(lp75042
sg36
(lp75043
sg30
(lp75044
sg287
(lp75045
sg74
(lp75046
sg176
(lp75047
sg145
(lp75048
sg295
(lp75049
sg183
(lp75050
sg484
(lp75051
sg83
(lp75052
sg85
(lp75053
sg303
(lp75054
sg306
(lp75055
sg89
(lp75056
sg91
(lp75057
sg94
(lp75058
sg96
(lp75059
sg18
(lp75060
sg221
(lp75061
sg313
(lp75062
sg429
(lp75063
sg318
(lp75064
sg102
(lp75065
sg110
(lp75066
sg63
(lp75067
sg22
(lp75068
sg329
(lp75069
sg32
(lp75070
sg332
(lp75071
sg121
(lp75072
sg4
(lp75073
sg6
(lp75074
sg8
(lp75075
sg34
(lp75076
sg99
(lp75077
sg460
(lp75078
sg235
(lp75079
sg126
(lp75080
sg281
(lp75081
sg535
(lp75082
sg344
(lp75083
sg130
(lp75084
sg132
(lp75085
sg138
(lp75086
sg140
(lp75087
sg354
(lp75088
I273
assS'http'
p75089
(dp75090
g121
(lp75091
I227
asg295
(lp75092
sg183
(lp75093
sg124
(lp75094
sg126
(lp75095
sg223
(lp75096
ssS'anw'
p75097
(dp75098
g83
(lp75099
I2788
assS'alcohol'
p75100
(dp75101
g221
(lp75102
I2234
assS'izl'
p75103
(dp75104
g176
(lp75105
I727
assS'loll'
p75106
(dp75107
g110
(lp75108
I1659
assS'health'
p75109
(dp75110
g36
(lp75111
sg484
(lp75112
sg78
(lp75113
sg277
(lp75114
sg149
(lp75115
I3172
assS'hill'
p75116
(dp75117
g163
(lp75118
sg183
(lp75119
sg303
(lp75120
sg429
(lp75121
sg89
(lp75122
sg44
(lp75123
sg128
(lp75124
I2842
asg223
(lp75125
sg149
(lp75126
ssS'etal'
p75127
(dp75128
g12
(lp75129
I2487
assS'pomerleau'
p75130
(dp75131
g94
(lp75132
I16
assS'benjamin'
p75133
(dp75134
g306
(lp75135
I14
assS'fiber'
p75136
(dp75137
g438
(lp75138
I198
asg32
(lp75139
sg70
(lp75140
sg350
(lp75141
ssS'wisconsin'
p75142
(dp75143
g344
(lp75144
sg484
(lp75145
sg70
(lp75146
sg4
(lp75147
I599
assS'advanc'
p75148
(dp75149
g70
(lp75150
sg78
(lp75151
sg277
(lp75152
sg163
(lp75153
sg303
(lp75154
sg30
(lp75155
sg287
(lp75156
sg74
(lp75157
sg176
(lp75158
sg145
(lp75159
sg256
(lp75160
sg262
(lp75161
sg295
(lp75162
sg183
(lp75163
sg59
(lp75164
sg83
(lp75165
sg63
(lp75166
sg42
(lp75167
I237
asg306
(lp75168
sg89
(lp75169
sg91
(lp75170
sg12
(lp75171
sg46
(lp75172
sg20
(lp75173
sg48
(lp75174
sg221
(lp75175
sg313
(lp75176
sg223
(lp75177
sg350
(lp75178
sg293
(lp75179
sg94
(lp75180
sg96
(lp75181
sg52
(lp75182
sg116
(lp75183
sg174
(lp75184
sg440
(lp75185
sg318
(lp75186
sg178
(lp75187
sg34
(lp75188
sg384
(lp75189
sg124
(lp75190
sg72
(lp75191
sg344
(lp75192
sg44
(lp75193
sg130
(lp75194
sg132
(lp75195
sg50
(lp75196
sg138
(lp75197
sg140
(lp75198
sg354
(lp75199
ssS'differ'
p75200
(dp75201
g80
(lp75202
sg293
(lp75203
sg344
(lp75204
sg78
(lp75205
sg59
(lp75206
sg484
(lp75207
sg38
(lp75208
sg83
(lp75209
sg85
(lp75210
sg303
(lp75211
sg438
(lp75212
sg116
(lp75213
sg118
(lp75214
sg34
(lp75215
sg36
(lp75216
sg460
(lp75217
sg68
(lp75218
sg72
(lp75219
sg281
(lp75220
sg10
(lp75221
sg40
(lp75222
sg283
(lp75223
sg70
(lp75224
sg26
(lp75225
sg277
(lp75226
sg163
(lp75227
sg89
(lp75228
sg91
(lp75229
sg12
(lp75230
sg94
(lp75231
sg96
(lp75232
sg48
(lp75233
sg99
(lp75234
sg44
(lp75235
sg149
(lp75236
sg429
(lp75237
sg102
(lp75238
sg106
(lp75239
sg108
(lp75240
sg110
(lp75241
sg63
(lp75242
sg52
(lp75243
sg114
(lp75244
sg128
(lp75245
sg130
(lp75246
sg132
(lp75247
sg14
(lp75248
sg135
(lp75249
sg50
(lp75250
sg138
(lp75251
sg140
(lp75252
sg354
(lp75253
sg306
(lp75254
sg87
(lp75255
sg245
(lp75256
sg46
(lp75257
sg20
(lp75258
sg18
(lp75259
sg221
(lp75260
sg535
(lp75261
sg223
(lp75262
sg350
(lp75263
sg216
(lp75264
sg174
(lp75265
sg440
(lp75266
sg332
(lp75267
sg121
(lp75268
sg4
(lp75269
sg6
(lp75270
sg8
(lp75271
sg126
(lp75272
sg30
(lp75273
sg287
(lp75274
sg74
(lp75275
sg176
(lp75276
sg145
(lp75277
sg256
(lp75278
sg76
(lp75279
sg262
(lp75280
sg295
(lp75281
sg183
(lp75282
sg42
(lp75283
I951
asg230
(lp75284
sg329
(lp75285
sg32
(lp75286
sg318
(lp75287
sg178
(lp75288
sg22
(lp75289
sg181
(lp75290
sg235
(lp75291
sg124
(lp75292
ssS'eklj'
p75293
(dp75294
g130
(lp75295
I1172
assS'teach'
p75296
(dp75297
g42
(lp75298
I1927
asg104
(lp75299
sg341
(lp75300
sg83
(lp75301
ssS'roweth'
p75302
(dp75303
g124
(lp75304
sg126
(lp75305
I2931
assS'lag'
p75306
(dp75307
g245
(lp75308
sg70
(lp75309
sg6
(lp75310
I449
assS'neurolog'
p75311
(dp75312
g106
(lp75313
I296
asg303
(lp75314
ssS'miniefh'
p75315
(dp75316
g78
(lp75317
I2299
assS'wlt'
p75318
(dp75319
g110
(lp75320
I2967
assS'wlw'
p75321
(dp75322
g74
(lp75323
I1264
assS'perhap'
p75324
(dp75325
g287
(lp75326
sg74
(lp75327
sg332
(lp75328
sg6
(lp75329
sg344
(lp75330
sg183
(lp75331
sg85
(lp75332
sg176
(lp75333
sg89
(lp75334
sg128
(lp75335
I501
asg94
(lp75336
sg44
(lp75337
ssS'wll'
p75338
(dp75339
g318
(lp75340
I1805
assS'circuit'
p75341
(dp75342
g70
(lp75343
sg145
(lp75344
sg22
(lp75345
sg256
(lp75346
sg68
(lp75347
sg341
(lp75348
sg10
(lp75349
sg40
(lp75350
sg42
(lp75351
I3366
asg104
(lp75352
sg245
(lp75353
sg14
(lp75354
sg20
(lp75355
sg135
(lp75356
sg63
(lp75357
sg149
(lp75358
ssS'tebrat'
p75359
(dp75360
g256
(lp75361
I262
assS'fcom'
p75362
(dp75363
g484
(lp75364
I678
assS'feed'
p75365
(dp75366
g287
(lp75367
sg108
(lp75368
sg283
(lp75369
sg163
(lp75370
sg36
(lp75371
sg59
(lp75372
sg68
(lp75373
sg126
(lp75374
sg10
(lp75375
sg87
(lp75376
sg128
(lp75377
sg245
(lp75378
sg96
(lp75379
sg135
(lp75380
sg110
(lp75381
sg138
(lp75382
sg140
(lp75383
I1385
asg114
(lp75384
ssS'temporauy'
p75385
(dp75386
g102
(lp75387
I3032
assS'unrestrict'
p75388
(dp75389
g438
(lp75390
I2320
assS'notifi'
p75391
(dp75392
g78
(lp75393
I330
assS'feel'
p75394
(dp75395
g4
(lp75396
I2748
asg63
(lp75397
ssS'summaris'
p75398
(dp75399
g332
(lp75400
sg135
(lp75401
I657
asg283
(lp75402
ssS'utarg'
p75403
(dp75404
g74
(lp75405
I1864
assS'least'
p75406
(dp75407
g277
(lp75408
sg163
(lp75409
sg72
(lp75410
sg287
(lp75411
sg74
(lp75412
sg145
(lp75413
sg256
(lp75414
sg262
(lp75415
sg295
(lp75416
sg183
(lp75417
sg484
(lp75418
sg85
(lp75419
sg42
(lp75420
I1706
asg306
(lp75421
sg91
(lp75422
sg46
(lp75423
sg48
(lp75424
sg313
(lp75425
sg94
(lp75426
sg106
(lp75427
sg110
(lp75428
sg63
(lp75429
sg216
(lp75430
sg174
(lp75431
sg4
(lp75432
sg6
(lp75433
sg235
(lp75434
sg34
(lp75435
sg460
(lp75436
sg126
(lp75437
sg341
(lp75438
sg344
(lp75439
sg128
(lp75440
sg14
(lp75441
sg16
(lp75442
sg135
(lp75443
sg140
(lp75444
ssS'blank'
p75445
(dp75446
g104
(lp75447
I2873
assS'xili'
p75448
(dp75449
g72
(lp75450
I2240
assS'gpi'
p75451
(dp75452
g26
(lp75453
I1904
assS'interact'
p75454
(dp75455
g70
(lp75456
sg26
(lp75457
sg30
(lp75458
sg74
(lp75459
sg176
(lp75460
sg80
(lp75461
sg293
(lp75462
sg183
(lp75463
sg59
(lp75464
sg42
(lp75465
I1931
asg12
(lp75466
sg99
(lp75467
sg535
(lp75468
sg149
(lp75469
sg116
(lp75470
sg429
(lp75471
sg318
(lp75472
sg106
(lp75473
sg63
(lp75474
sg52
(lp75475
sg216
(lp75476
sg118
(lp75477
sg332
(lp75478
sg384
(lp75479
sg68
(lp75480
sg126
(lp75481
sg130
(lp75482
sg135
(lp75483
sg50
(lp75484
ssS'luiz'
p75485
(dp75486
g118
(lp75487
I9
assS'gpc'
p75488
(dp75489
g178
(lp75490
I1471
assS'stork'
p75491
(dp75492
g44
(lp75493
I26
assS'stori'
p75494
(dp75495
g306
(lp75496
I387
asg83
(lp75497
sg85
(lp75498
ssS'contin'
p75499
(dp75500
g256
(lp75501
I2253
assS'morett'
p75502
(dp75503
g32
(lp75504
I3135
assS'store'
p75505
(dp75506
g283
(lp75507
sg76
(lp75508
sg293
(lp75509
sg484
(lp75510
sg83
(lp75511
sg306
(lp75512
sg87
(lp75513
sg245
(lp75514
sg20
(lp75515
sg99
(lp75516
sg535
(lp75517
sg44
(lp75518
sg429
(lp75519
sg104
(lp75520
sg106
(lp75521
sg63
(lp75522
sg52
(lp75523
sg116
(lp75524
sg438
(lp75525
I2052
asg178
(lp75526
sg384
(lp75527
sg10
(lp75528
sg132
(lp75529
sg14
(lp75530
sg16
(lp75531
sg135
(lp75532
sg50
(lp75533
ssS'necklac'
p75534
(dp75535
g181
(lp75536
I1574
assS'luckili'
p75537
(dp75538
g89
(lp75539
I975
assS'imperfect'
p75540
(dp75541
g42
(lp75542
I1092
asg14
(lp75543
sg89
(lp75544
ssS'option'
p75545
(dp75546
g126
(lp75547
sg135
(lp75548
I1677
asg121
(lp75549
sg114
(lp75550
ssS'pump'
p75551
(dp75552
g78
(lp75553
sg135
(lp75554
I409
assS'dol'
p75555
(dp75556
g110
(lp75557
I1478
assS'excitatorili'
p75558
(dp75559
g174
(lp75560
I849
assS'pronunci'
p75561
(dp75562
g30
(lp75563
sg96
(lp75564
I1990
asg87
(lp75565
ssS'king'
p75566
(dp75567
g132
(lp75568
I866
asg384
(lp75569
sg176
(lp75570
sg350
(lp75571
ssS'kind'
p75572
(dp75573
g30
(lp75574
sg32
(lp75575
sg181
(lp75576
sg6
(lp75577
sg8
(lp75578
sg235
(lp75579
sg126
(lp75580
sg74
(lp75581
sg42
(lp75582
I933
asg12
(lp75583
sg20
(lp75584
sg163
(lp75585
sg140
(lp75586
ssS'simplist'
p75587
(dp75588
g70
(lp75589
I1713
assS'whenev'
p75590
(dp75591
g216
(lp75592
sg283
(lp75593
sg145
(lp75594
sg4
(lp75595
sg80
(lp75596
sg235
(lp75597
sg295
(lp75598
sg183
(lp75599
sg293
(lp75600
sg341
(lp75601
sg10
(lp75602
sg42
(lp75603
I1041
asg306
(lp75604
sg132
(lp75605
sg63
(lp75606
ssS'remot'
p75607
(dp75608
g102
(lp75609
sg14
(lp75610
I4448
asg10
(lp75611
ssS'remov'
p75612
(dp75613
g22
(lp75614
sg6
(lp75615
sg235
(lp75616
sg295
(lp75617
sg183
(lp75618
sg484
(lp75619
sg85
(lp75620
sg76
(lp75621
sg344
(lp75622
sg104
(lp75623
sg102
(lp75624
sg14
(lp75625
I4510
asg20
(lp75626
sg18
(lp75627
sg63
(lp75628
sg256
(lp75629
ssS'goddard'
p75630
(dp75631
g429
(lp75632
I2411
assS'architect'
p75633
(dp75634
g59
(lp75635
I494
assS'postur'
p75636
(dp75637
g59
(lp75638
sg303
(lp75639
sg350
(lp75640
I351
assS'dand'
p75641
(dp75642
g85
(lp75643
I1961
assS'dana'
p75644
(dp75645
g429
(lp75646
sg85
(lp75647
I4006
assS'claustrum'
p75648
(dp75649
g70
(lp75650
I2521
assS'strengthen'
p75651
(dp75652
g332
(lp75653
sg149
(lp75654
I1245
assS'gale'
p75655
(dp75656
g145
(lp75657
I271
assS'willshaw'
p75658
(dp75659
g176
(lp75660
sg8
(lp75661
I512
assS'dedic'
p75662
(dp75663
g104
(lp75664
I503
asg59
(lp75665
ssS'entireti'
p75666
(dp75667
g94
(lp75668
I109
assS'inferotempor'
p75669
(dp75670
g181
(lp75671
I275
assS'cieillo'
p75672
(dp75673
g221
(lp75674
I1131
assS'violat'
p75675
(dp75676
g118
(lp75677
sg74
(lp75678
sg318
(lp75679
sg85
(lp75680
sg429
(lp75681
sg130
(lp75682
sg132
(lp75683
I2086
assS'pmi'
p75684
(dp75685
g72
(lp75686
I2841
assS'supercomput'
p75687
(dp75688
g149
(lp75689
I2745
assS'striat'
p75690
(dp75691
g12
(lp75692
sg438
(lp75693
I1155
asg48
(lp75694
sg149
(lp75695
ssS'ypist'
p75696
(dp75697
g94
(lp75698
I2888
assS'gile'
p75699
(dp75700
g174
(lp75701
sg318
(lp75702
sg124
(lp75703
sg89
(lp75704
sg223
(lp75705
sg128
(lp75706
sg132
(lp75707
sg50
(lp75708
I1726
asg44
(lp75709
ssS'reach'
p75710
(dp75711
g438
(lp75712
I1383
asg32
(lp75713
sg18
(lp75714
sg329
(lp75715
sg4
(lp75716
sg262
(lp75717
sg174
(lp75718
sg34
(lp75719
sg183
(lp75720
sg460
(lp75721
sg235
(lp75722
sg126
(lp75723
sg303
(lp75724
sg110
(lp75725
sg89
(lp75726
sg91
(lp75727
sg36
(lp75728
sg145
(lp75729
sg108
(lp75730
sg99
(lp75731
sg535
(lp75732
ssS'dhve'
p75733
(dp75734
g102
(lp75735
I1314
assS'react'
p75736
(dp75737
g293
(lp75738
I966
assS'duart'
p75739
(dp75740
g46
(lp75741
I3662
assS'amar'
p75742
(dp75743
g72
(lp75744
I499
assS'sandro'
p75745
(dp75746
g99
(lp75747
I40
assS'exey'
p75748
(dp75749
g6
(lp75750
I411
assS'dictat'
p75751
(dp75752
g429
(lp75753
sg83
(lp75754
I507
assS'transfrom'
p75755
(dp75756
g138
(lp75757
I3274
assS'adaboost'
p75758
(dp75759
g344
(lp75760
sg183
(lp75761
I4194
assS'phototransduct'
p75762
(dp75763
g245
(lp75764
I1165
asg256
(lp75765
ssS'notch'
p75766
(dp75767
g78
(lp75768
I2170
assS'fono'
p75769
(dp75770
g106
(lp75771
I2275
assS'mimet'
p75772
(dp75773
g174
(lp75774
I2542
assS'cornput'
p75775
(dp75776
g350
(lp75777
I1234
assS'cda'
p75778
(dp75779
g313
(lp75780
I2036
assS'rtn'
p75781
(dp75782
g230
(lp75783
I2275
assS'dtij'
p75784
(dp75785
g12
(lp75786
I1123
assS'hesk'
p75787
(dp75788
g36
(lp75789
I1394
assS'rtr'
p75790
(dp75791
g36
(lp75792
I1007
assS'font'
p75793
(dp75794
g30
(lp75795
sg63
(lp75796
sg223
(lp75797
I2452
assS'penalti'
p75798
(dp75799
g230
(lp75800
sg283
(lp75801
sg295
(lp75802
sg183
(lp75803
sg83
(lp75804
sg429
(lp75805
sg135
(lp75806
sg221
(lp75807
sg138
(lp75808
I2073
assS'givat'
p75809
(dp75810
g6
(lp75811
I37
assS'iet'
p75812
(dp75813
g85
(lp75814
I1969
assS'grace'
p75815
(dp75816
g306
(lp75817
I97
assS'iei'
p75818
(dp75819
g108
(lp75820
I1927
assS'wren'
p75821
(dp75822
g293
(lp75823
I3318
assS'hip'
p75824
(dp75825
g18
(lp75826
sg313
(lp75827
I1188
assS'sacking'
p75828
(dp75829
g183
(lp75830
I6590
assS'his'
p75831
(dp75832
g116
(lp75833
sg332
(lp75834
sg256
(lp75835
sg36
(lp75836
sg126
(lp75837
sg83
(lp75838
sg10
(lp75839
sg85
(lp75840
sg12
(lp75841
sg176
(lp75842
sg140
(lp75843
I3050
asg132
(lp75844
sg18
(lp75845
sg99
(lp75846
sg44
(lp75847
ssS'spurious'
p75848
(dp75849
g429
(lp75850
sg341
(lp75851
sg8
(lp75852
I304
assS'tcr'
p75853
(dp75854
g145
(lp75855
I2711
assS'procel'
p75856
(dp75857
g83
(lp75858
I2877
assS'longest'
p75859
(dp75860
g83
(lp75861
sg6
(lp75862
I1379
assS'hif'
p75863
(dp75864
g332
(lp75865
I1609
assS'sawaguchi'
p75866
(dp75867
g4
(lp75868
I2981
assS'reprint'
p75869
(dp75870
g74
(lp75871
I2664
assS'him'
p75872
(dp75873
g94
(lp75874
I2397
asg176
(lp75875
sg293
(lp75876
ssS'hin'
p75877
(dp75878
g38
(lp75879
sg72
(lp75880
sg313
(lp75881
I1192
assS'neroda'
p75882
(dp75883
g104
(lp75884
I2977
assS'stratum'
p75885
(dp75886
g106
(lp75887
I1008
assS'statist'
p75888
(dp75889
g124
(lp75890
sg78
(lp75891
sg277
(lp75892
sg163
(lp75893
sg72
(lp75894
sg287
(lp75895
sg74
(lp75896
sg145
(lp75897
sg76
(lp75898
sg295
(lp75899
sg183
(lp75900
sg59
(lp75901
sg484
(lp75902
sg85
(lp75903
sg87
(lp75904
sg91
(lp75905
sg12
(lp75906
sg46
(lp75907
sg96
(lp75908
sg221
(lp75909
sg313
(lp75910
sg223
(lp75911
sg149
(lp75912
sg460
(lp75913
sg429
(lp75914
sg102
(lp75915
sg178
(lp75916
sg106
(lp75917
I2947
asg110
(lp75918
sg114
(lp75919
sg329
(lp75920
sg440
(lp75921
sg318
(lp75922
sg121
(lp75923
sg181
(lp75924
sg8
(lp75925
sg36
(lp75926
sg384
(lp75927
sg235
(lp75928
sg126
(lp75929
sg281
(lp75930
sg344
(lp75931
sg44
(lp75932
sg128
(lp75933
sg130
(lp75934
sg132
(lp75935
sg135
(lp75936
sg138
(lp75937
sg354
(lp75938
ssS'vincent'
p75939
(dp75940
g40
(lp75941
I22
assS'arr'
p75942
(dp75943
g96
(lp75944
I2405
assS'chamberi'
p75945
(dp75946
g344
(lp75947
I3430
assS'art'
p75948
(dp75949
g96
(lp75950
sg126
(lp75951
sg63
(lp75952
sg44
(lp75953
I348
assS'detrano'
p75954
(dp75955
g91
(lp75956
I2811
assS'arx'
p75957
(dp75958
g245
(lp75959
I1219
assS'invis'
p75960
(dp75961
g18
(lp75962
I1559
assS'proactiv'
p75963
(dp75964
g99
(lp75965
I2753
assS'roc'
p75966
(dp75967
g114
(lp75968
I1194
assS'arc'
p75969
(dp75970
g80
(lp75971
sg460
(lp75972
sg108
(lp75973
I1606
asg63
(lp75974
sg44
(lp75975
ssS'bare'
p75976
(dp75977
g216
(lp75978
I2077
assS'are'
p75979
(dp75980
g80
(lp75981
sg293
(lp75982
sg344
(lp75983
sg78
(lp75984
sg59
(lp75985
sg484
(lp75986
sg38
(lp75987
sg83
(lp75988
sg85
(lp75989
sg303
(lp75990
sg438
(lp75991
sg116
(lp75992
sg118
(lp75993
sg34
(lp75994
sg36
(lp75995
sg460
(lp75996
sg68
(lp75997
sg72
(lp75998
sg281
(lp75999
sg10
(lp76000
sg40
(lp76001
sg283
(lp76002
sg70
(lp76003
sg26
(lp76004
sg277
(lp76005
sg163
(lp76006
sg89
(lp76007
sg91
(lp76008
sg12
(lp76009
sg94
(lp76010
sg96
(lp76011
sg48
(lp76012
sg99
(lp76013
sg313
(lp76014
sg44
(lp76015
sg149
(lp76016
sg429
(lp76017
sg102
(lp76018
sg104
(lp76019
sg106
(lp76020
sg108
(lp76021
sg110
(lp76022
sg63
(lp76023
sg52
(lp76024
sg114
(lp76025
sg128
(lp76026
sg130
(lp76027
sg132
(lp76028
sg14
(lp76029
sg16
(lp76030
sg135
(lp76031
sg50
(lp76032
sg138
(lp76033
sg140
(lp76034
sg354
(lp76035
sg306
(lp76036
sg87
(lp76037
sg245
(lp76038
sg46
(lp76039
sg20
(lp76040
sg18
(lp76041
sg221
(lp76042
sg535
(lp76043
sg223
(lp76044
sg350
(lp76045
sg216
(lp76046
sg174
(lp76047
sg440
(lp76048
sg332
(lp76049
sg121
(lp76050
sg4
(lp76051
sg6
(lp76052
sg8
(lp76053
sg126
(lp76054
sg341
(lp76055
sg30
(lp76056
sg287
(lp76057
sg74
(lp76058
sg176
(lp76059
sg145
(lp76060
sg256
(lp76061
sg76
(lp76062
sg262
(lp76063
sg295
(lp76064
sg183
(lp76065
sg42
(lp76066
I93
asg230
(lp76067
sg329
(lp76068
sg32
(lp76069
sg318
(lp76070
sg178
(lp76071
sg22
(lp76072
sg181
(lp76073
sg235
(lp76074
sg384
(lp76075
sg124
(lp76076
ssS'matic'
p76077
(dp76078
g135
(lp76079
I2558
assS'disadvantag'
p76080
(dp76081
g283
(lp76082
sg313
(lp76083
I855
assS'arm'
p76084
(dp76085
g174
(lp76086
sg32
(lp76087
sg59
(lp76088
sg293
(lp76089
sg295
(lp76090
sg183
(lp76091
sg460
(lp76092
sg124
(lp76093
sg99
(lp76094
I214
asg313
(lp76095
ssS'aro'
p76096
(dp76097
g306
(lp76098
I2917
assS'cohnqpsych'
p76099
(dp76100
g313
(lp76101
I15
assS'constan'
p76102
(dp76103
g438
(lp76104
I478
assS'valavani'
p76105
(dp76106
g245
(lp76107
I2899
assS'various'
p76108
(dp76109
g283
(lp76110
sg26
(lp76111
sg176
(lp76112
sg145
(lp76113
sg80
(lp76114
sg78
(lp76115
sg59
(lp76116
sg38
(lp76117
sg85
(lp76118
sg303
(lp76119
sg42
(lp76120
I519
asg306
(lp76121
sg245
(lp76122
sg48
(lp76123
sg221
(lp76124
sg313
(lp76125
sg223
(lp76126
sg329
(lp76127
sg12
(lp76128
sg102
(lp76129
sg110
(lp76130
sg63
(lp76131
sg52
(lp76132
sg174
(lp76133
sg230
(lp76134
sg438
(lp76135
sg32
(lp76136
sg178
(lp76137
sg4
(lp76138
sg181
(lp76139
sg8
(lp76140
sg68
(lp76141
sg126
(lp76142
sg10
(lp76143
sg128
(lp76144
sg132
(lp76145
sg14
(lp76146
sg16
(lp76147
sg22
(lp76148
ssS'nontrivi'
p76149
(dp76150
g384
(lp76151
I2056
asg85
(lp76152
ssS'nestor'
p76153
(dp76154
g438
(lp76155
I34
assS'syntact'
p76156
(dp76157
g429
(lp76158
sg8
(lp76159
I530
assS'soli'
p76160
(dp76161
g36
(lp76162
I1404
assS'coeffici'
p76163
(dp76164
g30
(lp76165
sg318
(lp76166
sg121
(lp76167
sg26
(lp76168
sg6
(lp76169
sg163
(lp76170
sg295
(lp76171
sg183
(lp76172
sg460
(lp76173
sg68
(lp76174
sg281
(lp76175
sg40
(lp76176
sg42
(lp76177
I576
asg78
(lp76178
sg145
(lp76179
sg96
(lp76180
sg99
(lp76181
ssS'induc'
p76182
(dp76183
g30
(lp76184
sg74
(lp76185
sg6
(lp76186
sg181
(lp76187
sg344
(lp76188
sg78
(lp76189
sg124
(lp76190
sg303
(lp76191
sg91
(lp76192
sg12
(lp76193
sg106
(lp76194
I677
asg149
(lp76195
ssS'sole'
p76196
(dp76197
g4
(lp76198
sg344
(lp76199
sg303
(lp76200
sg132
(lp76201
I149
asg46
(lp76202
sg44
(lp76203
sg350
(lp76204
ssS'brasher'
p76205
(dp76206
g99
(lp76207
I14
assS'succeed'
p76208
(dp76209
g121
(lp76210
sg344
(lp76211
sg341
(lp76212
sg42
(lp76213
I1763
asg89
(lp76214
sg50
(lp76215
sg63
(lp76216
ssS'solv'
p76217
(dp76218
g68
(lp76219
sg26
(lp76220
sg277
(lp76221
sg163
(lp76222
sg74
(lp76223
sg262
(lp76224
sg295
(lp76225
sg183
(lp76226
sg59
(lp76227
sg83
(lp76228
sg42
(lp76229
I2167
asg306
(lp76230
sg91
(lp76231
sg46
(lp76232
sg18
(lp76233
sg293
(lp76234
sg429
(lp76235
sg102
(lp76236
sg108
(lp76237
sg110
(lp76238
sg230
(lp76239
sg318
(lp76240
sg4
(lp76241
sg8
(lp76242
sg34
(lp76243
sg384
(lp76244
sg235
(lp76245
sg72
(lp76246
sg40
(lp76247
sg130
(lp76248
sg14
(lp76249
sg16
(lp76250
sg50
(lp76251
sg460
(lp76252
ssS'ernu'
p76253
(dp76254
g6
(lp76255
I779
assS'electrica'
p76256
(dp76257
g256
(lp76258
I2165
assS'condi'
p76259
(dp76260
g18
(lp76261
I594
assS'inertia'
p76262
(dp76263
g440
(lp76264
I920
asg124
(lp76265
sg126
(lp76266
ssS'c'
p76267
(dp76268
g80
(lp76269
sg293
(lp76270
sg344
(lp76271
sg78
(lp76272
sg59
(lp76273
sg484
(lp76274
sg38
(lp76275
sg83
(lp76276
sg85
(lp76277
sg303
(lp76278
sg438
(lp76279
sg116
(lp76280
sg118
(lp76281
sg34
(lp76282
sg36
(lp76283
sg460
(lp76284
sg68
(lp76285
sg72
(lp76286
sg281
(lp76287
sg10
(lp76288
sg283
(lp76289
sg70
(lp76290
sg26
(lp76291
sg277
(lp76292
sg163
(lp76293
sg89
(lp76294
sg12
(lp76295
sg94
(lp76296
sg96
(lp76297
sg48
(lp76298
sg99
(lp76299
sg313
(lp76300
sg44
(lp76301
sg149
(lp76302
sg429
(lp76303
sg102
(lp76304
sg104
(lp76305
sg106
(lp76306
sg108
(lp76307
sg110
(lp76308
sg63
(lp76309
sg52
(lp76310
sg114
(lp76311
sg128
(lp76312
sg130
(lp76313
sg132
(lp76314
sg14
(lp76315
sg16
(lp76316
sg135
(lp76317
sg50
(lp76318
sg138
(lp76319
sg140
(lp76320
sg354
(lp76321
sg87
(lp76322
sg245
(lp76323
sg46
(lp76324
sg20
(lp76325
sg18
(lp76326
sg221
(lp76327
sg535
(lp76328
sg223
(lp76329
sg350
(lp76330
sg174
(lp76331
sg440
(lp76332
sg332
(lp76333
sg121
(lp76334
sg4
(lp76335
sg6
(lp76336
sg8
(lp76337
sg126
(lp76338
sg30
(lp76339
sg287
(lp76340
sg74
(lp76341
sg176
(lp76342
sg145
(lp76343
sg256
(lp76344
sg76
(lp76345
sg262
(lp76346
sg295
(lp76347
sg183
(lp76348
sg42
(lp76349
I2066
asg230
(lp76350
sg329
(lp76351
sg32
(lp76352
sg318
(lp76353
sg178
(lp76354
sg22
(lp76355
sg181
(lp76356
sg235
(lp76357
sg384
(lp76358
sg124
(lp76359
ssS'shafranov'
p76360
(dp76361
g14
(lp76362
sg16
(lp76363
I662
assS'zador'
p76364
(dp76365
g262
(lp76366
I26
assS'risto'
p76367
(dp76368
g149
(lp76369
I20
assS'automatica'
p76370
(dp76371
g104
(lp76372
I2931
assS'context'
p76373
(dp76374
g70
(lp76375
sg26
(lp76376
sg287
(lp76377
sg76
(lp76378
sg59
(lp76379
sg83
(lp76380
sg85
(lp76381
sg306
(lp76382
sg87
(lp76383
sg89
(lp76384
sg91
(lp76385
sg94
(lp76386
sg96
(lp76387
sg223
(lp76388
sg350
(lp76389
sg32
(lp76390
sg332
(lp76391
sg102
(lp76392
sg178
(lp76393
sg106
(lp76394
I2367
asg118
(lp76395
sg440
(lp76396
sg318
(lp76397
sg121
(lp76398
sg4
(lp76399
sg181
(lp76400
sg235
(lp76401
sg34
(lp76402
sg132
(lp76403
sg14
(lp76404
sg16
(lp76405
sg50
(lp76406
sg138
(lp76407
sg140
(lp76408
ssS'benni'
p76409
(dp76410
g26
(lp76411
I3193
assS'sky'
p76412
(dp76413
g80
(lp76414
I2740
assS'anantharaman'
p76415
(dp76416
g132
(lp76417
I3478
assS'handprint'
p76418
(dp76419
g178
(lp76420
sg138
(lp76421
I3506
asg63
(lp76422
ssS'residu'
p76423
(dp76424
g74
(lp76425
sg22
(lp76426
sg181
(lp76427
sg38
(lp76428
sg72
(lp76429
sg96
(lp76430
sg14
(lp76431
sg16
(lp76432
I2315
asg313
(lp76433
ssS'lar'
p76434
(dp76435
g174
(lp76436
sg140
(lp76437
I3063
assS'mistak'
p76438
(dp76439
g94
(lp76440
I2934
assS'seiko'
p76441
(dp76442
g295
(lp76443
I54
asg183
(lp76444
ssS'pr'
p76445
(dp76446
g174
(lp76447
I1893
asg145
(lp76448
sg76
(lp76449
sg262
(lp76450
sg344
(lp76451
sg460
(lp76452
sg72
(lp76453
sg87
(lp76454
ssS'ps'
p76455
(dp76456
g46
(lp76457
sg145
(lp76458
sg126
(lp76459
sg76
(lp76460
sg8
(lp76461
I2645
assS'pp'
p76462
(dp76463
g26
(lp76464
sg277
(lp76465
sg72
(lp76466
sg30
(lp76467
sg287
(lp76468
sg256
(lp76469
sg295
(lp76470
sg183
(lp76471
sg42
(lp76472
I3370
asg87
(lp76473
sg91
(lp76474
sg245
(lp76475
sg96
(lp76476
sg48
(lp76477
sg99
(lp76478
sg535
(lp76479
sg44
(lp76480
sg102
(lp76481
sg108
(lp76482
sg20
(lp76483
sg114
(lp76484
sg118
(lp76485
sg332
(lp76486
sg22
(lp76487
sg460
(lp76488
sg126
(lp76489
sg281
(lp76490
sg40
(lp76491
sg344
(lp76492
sg78
(lp76493
sg135
(lp76494
sg354
(lp76495
ssS'pv'
p76496
(dp76497
g130
(lp76498
I970
assS'pw'
p76499
(dp76500
g63
(lp76501
I2236
assS'pt'
p76502
(dp76503
g74
(lp76504
sg262
(lp76505
sg36
(lp76506
sg384
(lp76507
sg87
(lp76508
sg46
(lp76509
sg18
(lp76510
I769
assS'pendleton'
p76511
(dp76512
g124
(lp76513
sg126
(lp76514
I2929
assS'px'
p76515
(dp76516
g230
(lp76517
I2472
asg176
(lp76518
ssS'py'
p76519
(dp76520
g262
(lp76521
I1391
assS'due'
p76522
(dp76523
g329
(lp76524
sg70
(lp76525
sg281
(lp76526
sg283
(lp76527
sg85
(lp76528
sg287
(lp76529
sg176
(lp76530
sg145
(lp76531
sg80
(lp76532
sg76
(lp76533
sg118
(lp76534
sg295
(lp76535
sg183
(lp76536
sg59
(lp76537
sg484
(lp76538
sg83
(lp76539
sg114
(lp76540
sg303
(lp76541
sg306
(lp76542
sg87
(lp76543
sg89
(lp76544
sg91
(lp76545
sg12
(lp76546
sg94
(lp76547
sg20
(lp76548
sg48
(lp76549
sg99
(lp76550
sg44
(lp76551
sg149
(lp76552
sg230
(lp76553
sg174
(lp76554
sg460
(lp76555
sg245
(lp76556
sg429
(lp76557
sg68
(lp76558
sg104
(lp76559
sg106
(lp76560
sg110
(lp76561
sg22
(lp76562
sg216
(lp76563
sg438
(lp76564
I1466
asg440
(lp76565
sg332
(lp76566
sg121
(lp76567
sg4
(lp76568
sg6
(lp76569
sg8
(lp76570
sg34
(lp76571
sg221
(lp76572
sg384
(lp76573
sg235
(lp76574
sg126
(lp76575
sg341
(lp76576
sg40
(lp76577
sg223
(lp76578
sg128
(lp76579
sg36
(lp76580
sg132
(lp76581
sg14
(lp76582
sg16
(lp76583
sg135
(lp76584
sg50
(lp76585
sg138
(lp76586
ssS'pb'
p76587
(dp76588
g68
(lp76589
I3262
assS'pc'
p76590
(dp76591
g178
(lp76592
sg163
(lp76593
sg78
(lp76594
sg14
(lp76595
sg96
(lp76596
sg135
(lp76597
I1434
asg221
(lp76598
sg44
(lp76599
sg114
(lp76600
ssS'pa'
p76601
(dp76602
g484
(lp76603
sg4
(lp76604
sg80
(lp76605
sg344
(lp76606
sg130
(lp76607
sg72
(lp76608
sg277
(lp76609
sg10
(lp76610
sg245
(lp76611
sg89
(lp76612
sg91
(lp76613
sg96
(lp76614
sg132
(lp76615
sg94
(lp76616
sg106
(lp76617
I952
asg313
(lp76618
sg223
(lp76619
ssS'pf'
p76620
(dp76621
g230
(lp76622
I2675
assS'pg'
p76623
(dp76624
g12
(lp76625
I2765
assS'pd'
p76626
(dp76627
g295
(lp76628
sg183
(lp76629
sg221
(lp76630
I603
assS'pe'
p76631
(dp76632
g74
(lp76633
sg99
(lp76634
I3284
asg76
(lp76635
ssS'pj'
p76636
(dp76637
g429
(lp76638
sg70
(lp76639
sg91
(lp76640
sg44
(lp76641
I1032
assS'pk'
p76642
(dp76643
g344
(lp76644
sg106
(lp76645
I2840
asg18
(lp76646
sg235
(lp76647
ssS'ph'
p76648
(dp76649
g6
(lp76650
sg76
(lp76651
sg8
(lp76652
sg72
(lp76653
sg245
(lp76654
sg12
(lp76655
sg44
(lp76656
I1067
assS'pi'
p76657
(dp76658
g230
(lp76659
sg329
(lp76660
sg283
(lp76661
sg80
(lp76662
sg6
(lp76663
sg344
(lp76664
sg59
(lp76665
sg341
(lp76666
sg281
(lp76667
sg91
(lp76668
sg130
(lp76669
I2076
asg104
(lp76670
sg221
(lp76671
sg46
(lp76672
sg44
(lp76673
sg26
(lp76674
ssS'pn'
p76675
(dp76676
g295
(lp76677
I1682
asg183
(lp76678
sg277
(lp76679
ssS'po'
p76680
(dp76681
g329
(lp76682
sg332
(lp76683
sg72
(lp76684
sg176
(lp76685
sg130
(lp76686
I1071
asg245
(lp76687
sg44
(lp76688
ssS'pl'
p76689
(dp76690
g102
(lp76691
I247
asg332
(lp76692
sg22
(lp76693
sg68
(lp76694
ssS'pm'
p76695
(dp76696
g102
(lp76697
I298
asg329
(lp76698
sg20
(lp76699
sg72
(lp76700
sg10
(lp76701
ssS'flight'
p76702
(dp76703
g78
(lp76704
I3037
assS'epworth'
p76705
(dp76706
g63
(lp76707
I2999
assS'mullilay'
p76708
(dp76709
g128
(lp76710
I1523
assS'morimoto'
p76711
(dp76712
g20
(lp76713
I17
assS'demand'
p76714
(dp76715
g174
(lp76716
sg176
(lp76717
sg4
(lp76718
sg42
(lp76719
I372
asg429
(lp76720
sg102
(lp76721
sg14
(lp76722
sg20
(lp76723
sg18
(lp76724
ssS'tangenti'
p76725
(dp76726
g99
(lp76727
I913
assS'heighten'
p76728
(dp76729
g42
(lp76730
I717
assS'georgia'
p76731
(dp76732
g295
(lp76733
I43
asg183
(lp76734
sg78
(lp76735
ssS'fontinu'
p76736
(dp76737
g89
(lp76738
I1068
assS'negyect'
p76739
(dp76740
g262
(lp76741
I1603
assS'arcsin'
p76742
(dp76743
g38
(lp76744
I1603
assS'batch'
p76745
(dp76746
g295
(lp76747
sg36
(lp76748
sg460
(lp76749
sg126
(lp76750
sg91
(lp76751
sg183
(lp76752
sg50
(lp76753
I665
assS'nakai'
p76754
(dp76755
g20
(lp76756
I15
assS'zippelius'
p76757
(dp76758
g384
(lp76759
I2342
assS'abov'
p76760
(dp76761
g68
(lp76762
sg70
(lp76763
sg26
(lp76764
sg163
(lp76765
sg72
(lp76766
sg281
(lp76767
sg283
(lp76768
sg287
(lp76769
sg74
(lp76770
sg176
(lp76771
sg145
(lp76772
sg256
(lp76773
sg76
(lp76774
sg118
(lp76775
sg295
(lp76776
sg183
(lp76777
sg59
(lp76778
sg38
(lp76779
sg83
(lp76780
sg114
(lp76781
sg42
(lp76782
I1796
asg87
(lp76783
sg89
(lp76784
sg91
(lp76785
sg12
(lp76786
sg94
(lp76787
sg18
(lp76788
sg535
(lp76789
sg223
(lp76790
sg149
(lp76791
sg329
(lp76792
sg293
(lp76793
sg32
(lp76794
sg429
(lp76795
sg318
(lp76796
sg102
(lp76797
sg110
(lp76798
sg52
(lp76799
sg22
(lp76800
sg230
(lp76801
sg438
(lp76802
sg440
(lp76803
sg332
(lp76804
sg178
(lp76805
sg4
(lp76806
sg6
(lp76807
sg235
(lp76808
sg34
(lp76809
sg36
(lp76810
sg124
(lp76811
sg126
(lp76812
sg341
(lp76813
sg40
(lp76814
sg344
(lp76815
sg48
(lp76816
sg44
(lp76817
sg78
(lp76818
sg132
(lp76819
sg138
(lp76820
sg140
(lp76821
ssS'introdu'
p76822
(dp76823
g221
(lp76824
I1293
assS'rix'
p76825
(dp76826
g176
(lp76827
I1102
assS'riw'
p76828
(dp76829
g329
(lp76830
I1673
assS'rin'
p76831
(dp76832
g38
(lp76833
I678
assS'rio'
p76834
(dp76835
g118
(lp76836
sg10
(lp76837
I2355
assS'mixturewj'
p76838
(dp76839
g91
(lp76840
I859
assS'unitari'
p76841
(dp76842
g216
(lp76843
I789
asg32
(lp76844
ssS'rii'
p76845
(dp76846
g38
(lp76847
I2947
asg26
(lp76848
ssS'rid'
p76849
(dp76850
g34
(lp76851
I1053
assS'illinoi'
p76852
(dp76853
g102
(lp76854
sg245
(lp76855
sg48
(lp76856
I22
asg36
(lp76857
ssS'reprogram'
p76858
(dp76859
g14
(lp76860
I2892
assS'blackwood'
p76861
(dp76862
g174
(lp76863
I532
assS'noooi'
p76864
(dp76865
g438
(lp76866
I2379
assS'letxi'
p76867
(dp76868
g230
(lp76869
I807
assS'minim'
p76870
(dp76871
g78
(lp76872
sg277
(lp76873
sg163
(lp76874
sg72
(lp76875
sg281
(lp76876
sg26
(lp76877
sg30
(lp76878
sg74
(lp76879
sg145
(lp76880
sg295
(lp76881
sg183
(lp76882
sg38
(lp76883
sg83
(lp76884
sg85
(lp76885
sg42
(lp76886
I2178
asg306
(lp76887
sg91
(lp76888
sg96
(lp76889
sg313
(lp76890
sg223
(lp76891
sg429
(lp76892
sg102
(lp76893
sg110
(lp76894
sg63
(lp76895
sg230
(lp76896
sg329
(lp76897
sg440
(lp76898
sg318
(lp76899
sg178
(lp76900
sg22
(lp76901
sg181
(lp76902
sg8
(lp76903
sg34
(lp76904
sg36
(lp76905
sg235
(lp76906
sg126
(lp76907
sg341
(lp76908
sg40
(lp76909
sg344
(lp76910
sg128
(lp76911
sg130
(lp76912
sg132
(lp76913
sg14
(lp76914
sg16
(lp76915
sg50
(lp76916
sg138
(lp76917
sg140
(lp76918
sg354
(lp76919
ssS'hesten'
p76920
(dp76921
g32
(lp76922
I475
assS'lengthi'
p76923
(dp76924
g91
(lp76925
I1750
assS'viola'
p76926
(dp76927
g318
(lp76928
sg50
(lp76929
I1567
asg350
(lp76930
ssS'subalgebra'
p76931
(dp76932
g32
(lp76933
I2099
assS'higher'
p76934
(dp76935
g283
(lp76936
sg70
(lp76937
sg277
(lp76938
sg163
(lp76939
sg181
(lp76940
sg30
(lp76941
sg287
(lp76942
sg74
(lp76943
sg80
(lp76944
sg295
(lp76945
sg183
(lp76946
sg484
(lp76947
sg303
(lp76948
sg94
(lp76949
sg96
(lp76950
sg18
(lp76951
sg118
(lp76952
sg102
(lp76953
sg106
(lp76954
I2481
asg108
(lp76955
sg63
(lp76956
sg230
(lp76957
sg329
(lp76958
sg332
(lp76959
sg121
(lp76960
sg4
(lp76961
sg6
(lp76962
sg36
(lp76963
sg68
(lp76964
sg10
(lp76965
sg78
(lp76966
sg140
(lp76967
ssS'fogel'
p76968
(dp76969
g18
(lp76970
I1334
assS'indebt'
p76971
(dp76972
g48
(lp76973
I1936
assS'wherea'
p76974
(dp76975
g230
(lp76976
sg32
(lp76977
sg59
(lp76978
sg26
(lp76979
sg163
(lp76980
sg384
(lp76981
sg341
(lp76982
sg44
(lp76983
sg132
(lp76984
sg135
(lp76985
I2016
asg221
(lp76986
sg63
(lp76987
sg223
(lp76988
ssS'covari'
p76989
(dp76990
g30
(lp76991
sg329
(lp76992
sg318
(lp76993
sg178
(lp76994
sg163
(lp76995
sg36
(lp76996
sg124
(lp76997
sg138
(lp76998
sg91
(lp76999
sg130
(lp77000
sg102
(lp77001
sg106
(lp77002
I2
asg108
(lp77003
sg50
(lp77004
sg313
(lp77005
sg96
(lp77006
ssS'sekul'
p77007
(dp77008
g216
(lp77009
I187
assS'elospiza'
p77010
(dp77011
g116
(lp77012
I41
assS'bartlett'
p77013
(dp77014
g181
(lp77015
I14
assS'robust'
p77016
(dp77017
g116
(lp77018
sg183
(lp77019
sg74
(lp77020
sg332
(lp77021
sg178
(lp77022
sg4
(lp77023
sg181
(lp77024
sg235
(lp77025
sg295
(lp77026
sg18
(lp77027
sg59
(lp77028
sg72
(lp77029
sg281
(lp77030
sg76
(lp77031
sg318
(lp77032
sg89
(lp77033
sg48
(lp77034
sg130
(lp77035
sg104
(lp77036
sg135
(lp77037
I77
assS'provabl'
p77038
(dp77039
g46
(lp77040
I418
assS'lower'
p77041
(dp77042
g26
(lp77043
sg277
(lp77044
sg40
(lp77045
sg30
(lp77046
sg287
(lp77047
sg74
(lp77048
sg76
(lp77049
sg262
(lp77050
sg295
(lp77051
sg183
(lp77052
sg59
(lp77053
sg484
(lp77054
sg38
(lp77055
sg83
(lp77056
sg89
(lp77057
sg12
(lp77058
sg46
(lp77059
sg48
(lp77060
sg221
(lp77061
sg313
(lp77062
sg44
(lp77063
sg318
(lp77064
sg329
(lp77065
sg332
(lp77066
sg102
(lp77067
sg104
(lp77068
sg106
(lp77069
I1641
asg110
(lp77070
sg114
(lp77071
sg174
(lp77072
sg118
(lp77073
sg18
(lp77074
sg121
(lp77075
sg22
(lp77076
sg6
(lp77077
sg235
(lp77078
sg34
(lp77079
sg99
(lp77080
sg384
(lp77081
sg126
(lp77082
sg281
(lp77083
sg535
(lp77084
sg78
(lp77085
sg135
(lp77086
sg138
(lp77087
sg140
(lp77088
sg354
(lp77089
ssS'earmark'
p77090
(dp77091
g72
(lp77092
I3347
assS'machineri'
p77093
(dp77094
g124
(lp77095
I278
asg114
(lp77096
ssS'discourag'
p77097
(dp77098
g26
(lp77099
I2285
assS'darel'
p77100
(dp77101
g59
(lp77102
I3121
assS'neigbbor'
p77103
(dp77104
g149
(lp77105
I2137
assS'corinna'
p77106
(dp77107
g183
(lp77108
I4045
assS'romerstr'
p77109
(dp77110
g132
(lp77111
I18
asg223
(lp77112
ssS'lowei'
p77113
(dp77114
g149
(lp77115
I1895
assS'lowel'
p77116
(dp77117
g149
(lp77118
I242
assS'propos'
p77119
(dp77120
g70
(lp77121
sg163
(lp77122
sg30
(lp77123
sg74
(lp77124
sg118
(lp77125
sg78
(lp77126
sg484
(lp77127
sg85
(lp77128
sg63
(lp77129
sg42
(lp77130
I29
asg87
(lp77131
sg245
(lp77132
sg46
(lp77133
sg96
(lp77134
sg18
(lp77135
sg221
(lp77136
sg223
(lp77137
sg350
(lp77138
sg116
(lp77139
sg174
(lp77140
sg293
(lp77141
sg12
(lp77142
sg318
(lp77143
sg102
(lp77144
sg178
(lp77145
sg20
(lp77146
sg52
(lp77147
sg230
(lp77148
sg438
(lp77149
sg440
(lp77150
sg332
(lp77151
sg121
(lp77152
sg8
(lp77153
sg34
(lp77154
sg36
(lp77155
sg124
(lp77156
sg72
(lp77157
sg281
(lp77158
sg128
(lp77159
sg130
(lp77160
sg149
(lp77161
sg50
(lp77162
sg140
(lp77163
sg354
(lp77164
ssS'facial'
p77165
(dp77166
g223
(lp77167
I756
asg293
(lp77168
ssS'blpij'
p77169
(dp77170
g306
(lp77171
I894
assS'pater'
p77172
(dp77173
g132
(lp77174
I3721
assS'semant'
p77175
(dp77176
g59
(lp77177
I3239
assS'zerocross'
p77178
(dp77179
g245
(lp77180
I1543
assS'relianc'
p77181
(dp77182
g14
(lp77183
I3700
assS'foreshorten'
p77184
(dp77185
g181
(lp77186
I656
assS'hertfordshir'
p77187
(dp77188
g283
(lp77189
I15
assS'stimul'
p77190
(dp77191
g438
(lp77192
I977
asg176
(lp77193
sg6
(lp77194
sg104
(lp77195
sg46
(lp77196
sg106
(lp77197
sg50
(lp77198
ssS'muenchen'
p77199
(dp77200
g221
(lp77201
I27
assS'theoret'
p77202
(dp77203
g70
(lp77204
sg26
(lp77205
sg163
(lp77206
sg287
(lp77207
sg74
(lp77208
sg176
(lp77209
sg145
(lp77210
sg80
(lp77211
sg262
(lp77212
sg344
(lp77213
sg183
(lp77214
sg59
(lp77215
sg83
(lp77216
sg85
(lp77217
sg89
(lp77218
sg12
(lp77219
sg18
(lp77220
sg535
(lp77221
sg44
(lp77222
sg350
(lp77223
sg329
(lp77224
sg102
(lp77225
sg110
(lp77226
sg63
(lp77227
sg174
(lp77228
sg22
(lp77229
sg8
(lp77230
sg34
(lp77231
sg36
(lp77232
sg384
(lp77233
sg124
(lp77234
sg40
(lp77235
sg138
(lp77236
sg354
(lp77237
I57
assS'contractor'
p77238
(dp77239
g63
(lp77240
I594
assS'hsin'
p77241
(dp77242
g230
(lp77243
I8
assS'plateau'
p77244
(dp77245
g38
(lp77246
sg121
(lp77247
I1075
assS'theorem'
p77248
(dp77249
g230
(lp77250
sg287
(lp77251
sg295
(lp77252
sg183
(lp77253
sg68
(lp77254
sg341
(lp77255
I1776
asg85
(lp77256
sg40
(lp77257
sg34
(lp77258
sg306
(lp77259
sg36
(lp77260
sg46
(lp77261
sg221
(lp77262
sg535
(lp77263
ssS'bakerhausdorff'
p77264
(dp77265
g32
(lp77266
I71
assS'oftopograph'
p77267
(dp77268
g149
(lp77269
I3157
assS'omologo'
p77270
(dp77271
g96
(lp77272
I2765
assS'yopt'
p77273
(dp77274
g85
(lp77275
I3021
assS'loss'
p77276
(dp77277
g438
(lp77278
sg78
(lp77279
sg295
(lp77280
sg183
(lp77281
sg124
(lp77282
sg341
(lp77283
sg114
(lp77284
sg40
(lp77285
sg42
(lp77286
I2790
asg36
(lp77287
sg89
(lp77288
sg130
(lp77289
sg132
(lp77290
sg350
(lp77291
sg99
(lp77292
sg354
(lp77293
ssS'moodi'
p77294
(dp77295
g121
(lp77296
sg36
(lp77297
sg313
(lp77298
sg44
(lp77299
sg132
(lp77300
sg178
(lp77301
sg138
(lp77302
I3348
asg223
(lp77303
ssS'discriminatori'
p77304
(dp77305
g281
(lp77306
I2397
assS'lji'
p77307
(dp77308
g438
(lp77309
I476
assS'ljk'
p77310
(dp77311
g438
(lp77312
I312
assS'ljj'
p77313
(dp77314
g32
(lp77315
I2549
assS'ljl'
p77316
(dp77317
g102
(lp77318
I476
assS'gradientenmethod'
p77319
(dp77320
g34
(lp77321
I2920
assS'ljn'
p77322
(dp77323
g283
(lp77324
I428
assS'lje'
p77325
(dp77326
g104
(lp77327
I1537
assS'aurisch'
p77328
(dp77329
g132
(lp77330
I3464
assS'collect'
p77331
(dp77332
g283
(lp77333
sg277
(lp77334
sg30
(lp77335
sg74
(lp77336
sg344
(lp77337
sg78
(lp77338
sg484
(lp77339
sg42
(lp77340
I1063
asg89
(lp77341
sg94
(lp77342
sg96
(lp77343
sg223
(lp77344
sg32
(lp77345
sg104
(lp77346
sg63
(lp77347
sg114
(lp77348
sg438
(lp77349
sg440
(lp77350
sg235
(lp77351
sg124
(lp77352
sg126
(lp77353
sg40
(lp77354
sg14
(lp77355
sg16
(lp77356
sg135
(lp77357
sg138
(lp77358
sg354
(lp77359
ssS'markey'
p77360
(dp77361
g83
(lp77362
I1680
assS'sibisi'
p77363
(dp77364
g26
(lp77365
I3311
assS'play'
p77366
(dp77367
g116
(lp77368
sg74
(lp77369
sg318
(lp77370
sg145
(lp77371
sg4
(lp77372
sg6
(lp77373
sg295
(lp77374
sg183
(lp77375
sg460
(lp77376
sg124
(lp77377
sg126
(lp77378
sg83
(lp77379
sg42
(lp77380
I1576
asg12
(lp77381
sg132
(lp77382
sg20
(lp77383
sg135
(lp77384
sg223
(lp77385
sg38
(lp77386
ssS'understood'
p77387
(dp77388
g32
(lp77389
sg318
(lp77390
sg256
(lp77391
sg429
(lp77392
sg68
(lp77393
sg74
(lp77394
sg306
(lp77395
sg14
(lp77396
sg16
(lp77397
I642
asg63
(lp77398
ssS'surpris'
p77399
(dp77400
g230
(lp77401
sg287
(lp77402
sg318
(lp77403
sg70
(lp77404
sg26
(lp77405
sg181
(lp77406
sg235
(lp77407
sg293
(lp77408
sg68
(lp77409
sg83
(lp77410
sg176
(lp77411
sg89
(lp77412
sg91
(lp77413
sg128
(lp77414
sg132
(lp77415
I2819
asg99
(lp77416
ssS'currencstlt'
p77417
(dp77418
g440
(lp77419
I682
assS'distribut'
p77420
(dp77421
g124
(lp77422
sg78
(lp77423
sg277
(lp77424
sg163
(lp77425
sg72
(lp77426
sg281
(lp77427
sg283
(lp77428
sg36
(lp77429
sg30
(lp77430
sg287
(lp77431
sg74
(lp77432
sg145
(lp77433
sg76
(lp77434
sg262
(lp77435
sg295
(lp77436
sg183
(lp77437
sg59
(lp77438
sg484
(lp77439
sg38
(lp77440
sg83
(lp77441
sg85
(lp77442
sg42
(lp77443
I2031
asg306
(lp77444
sg87
(lp77445
sg89
(lp77446
sg91
(lp77447
sg94
(lp77448
sg96
(lp77449
sg48
(lp77450
sg99
(lp77451
sg313
(lp77452
sg223
(lp77453
sg329
(lp77454
sg293
(lp77455
sg460
(lp77456
sg318
(lp77457
sg102
(lp77458
sg108
(lp77459
sg110
(lp77460
sg52
(lp77461
sg114
(lp77462
sg216
(lp77463
sg438
(lp77464
sg440
(lp77465
sg332
(lp77466
sg4
(lp77467
sg181
(lp77468
sg8
(lp77469
sg221
(lp77470
sg384
(lp77471
sg235
(lp77472
sg126
(lp77473
sg341
(lp77474
sg10
(lp77475
sg40
(lp77476
sg344
(lp77477
sg128
(lp77478
sg130
(lp77479
sg14
(lp77480
sg16
(lp77481
sg50
(lp77482
sg138
(lp77483
sg140
(lp77484
sg354
(lp77485
ssS'affili'
p77486
(dp77487
g440
(lp77488
I223
asg223
(lp77489
ssS'rieh'
p77490
(dp77491
g223
(lp77492
I711
assS'medi'
p77493
(dp77494
g277
(lp77495
I453
assS'prop'
p77496
(dp77497
g277
(lp77498
sg10
(lp77499
sg429
(lp77500
sg44
(lp77501
sg132
(lp77502
I1567
asg223
(lp77503
ssS'waveform'
p77504
(dp77505
g116
(lp77506
sg14
(lp77507
sg20
(lp77508
sg135
(lp77509
I436
asg350
(lp77510
ssS'retri'
p77511
(dp77512
g145
(lp77513
I2101
assS'proj'
p77514
(dp77515
g32
(lp77516
I2015
assS'prof'
p77517
(dp77518
g59
(lp77519
I1076
assS'leftmost'
p77520
(dp77521
g118
(lp77522
sg91
(lp77523
I2451
asg303
(lp77524
ssS'proc'
p77525
(dp77526
g26
(lp77527
sg72
(lp77528
sg287
(lp77529
sg176
(lp77530
sg293
(lp77531
sg295
(lp77532
sg183
(lp77533
sg59
(lp77534
sg63
(lp77535
sg12
(lp77536
sg20
(lp77537
sg48
(lp77538
sg99
(lp77539
sg313
(lp77540
sg44
(lp77541
sg174
(lp77542
sg104
(lp77543
sg106
(lp77544
sg96
(lp77545
sg52
(lp77546
sg438
(lp77547
I2440
asg440
(lp77548
sg22
(lp77549
sg181
(lp77550
sg8
(lp77551
sg36
(lp77552
sg126
(lp77553
sg10
(lp77554
sg40
(lp77555
sg344
(lp77556
sg128
(lp77557
sg14
(lp77558
ssS'prob'
p77559
(dp77560
g295
(lp77561
sg183
(lp77562
sg83
(lp77563
sg223
(lp77564
I1559
assS'oversampl'
p77565
(dp77566
g22
(lp77567
I95
assS'nonrealiz'
p77568
(dp77569
g341
(lp77570
I385
assS'informatica'
p77571
(dp77572
g44
(lp77573
I16
assS'palmer'
p77574
(dp77575
g438
(lp77576
I2483
asg26
(lp77577
sg130
(lp77578
ssS'perpendicular'
p77579
(dp77580
g32
(lp77581
sg70
(lp77582
sg26
(lp77583
sg295
(lp77584
sg183
(lp77585
sg34
(lp77586
sg48
(lp77587
I1532
asg350
(lp77588
ssS'wawrzynek'
p77589
(dp77590
g174
(lp77591
I2616
asg10
(lp77592
ssS'devlin'
p77593
(dp77594
g313
(lp77595
I2065
assS'tinker'
p77596
(dp77597
g135
(lp77598
I1755
assS'schaffer'
p77599
(dp77600
g106
(lp77601
I574
assS'transpos'
p77602
(dp77603
g102
(lp77604
I472
asg46
(lp77605
sg306
(lp77606
sg235
(lp77607
ssS'sunvideo'
p77608
(dp77609
g181
(lp77610
I1189
assS'ylx'
p77611
(dp77612
g329
(lp77613
sg36
(lp77614
sg72
(lp77615
sg126
(lp77616
sg313
(lp77617
sg354
(lp77618
I1113
assS'modal'
p77619
(dp77620
g94
(lp77621
I296
asg384
(lp77622
sg318
(lp77623
sg59
(lp77624
sg293
(lp77625
ssS'highfrequ'
p77626
(dp77627
g106
(lp77628
I566
assS'gamma'
p77629
(dp77630
g126
(lp77631
sg440
(lp77632
sg121
(lp77633
sg83
(lp77634
sg128
(lp77635
I360
assS'yli'
p77636
(dp77637
g318
(lp77638
sg6
(lp77639
I1798
assS'abrash'
p77640
(dp77641
g87
(lp77642
I343
assS'emphasi'
p77643
(dp77644
g12
(lp77645
sg174
(lp77646
sg18
(lp77647
sg99
(lp77648
I3374
asg181
(lp77649
ssS'laili'
p77650
(dp77651
g46
(lp77652
I2197
assS'gaver'
p77653
(dp77654
g174
(lp77655
I1603
assS'jn'
p77656
(dp77657
g245
(lp77658
I2527
asg235
(lp77659
ssS'cij'
p77660
(dp77661
g18
(lp77662
I991
assS'mginal'
p77663
(dp77664
g183
(lp77665
I5290
assS'artifici'
p77666
(dp77667
g283
(lp77668
sg78
(lp77669
sg281
(lp77670
sg36
(lp77671
sg104
(lp77672
sg287
(lp77673
sg74
(lp77674
sg293
(lp77675
sg295
(lp77676
sg183
(lp77677
sg59
(lp77678
sg484
(lp77679
sg83
(lp77680
sg42
(lp77681
I1573
asg91
(lp77682
sg94
(lp77683
sg96
(lp77684
sg221
(lp77685
sg223
(lp77686
sg429
(lp77687
sg178
(lp77688
sg108
(lp77689
sg110
(lp77690
sg63
(lp77691
sg114
(lp77692
sg438
(lp77693
sg440
(lp77694
sg332
(lp77695
sg121
(lp77696
sg181
(lp77697
sg34
(lp77698
sg99
(lp77699
sg68
(lp77700
sg72
(lp77701
sg341
(lp77702
sg10
(lp77703
sg40
(lp77704
sg344
(lp77705
sg130
(lp77706
sg132
(lp77707
sg138
(lp77708
sg354
(lp77709
ssS'canal'
p77710
(dp77711
g174
(lp77712
I205
asg350
(lp77713
ssS'ixi'
p77714
(dp77715
g104
(lp77716
sg130
(lp77717
I1301
assS'isabell'
p77718
(dp77719
g128
(lp77720
I2734
assS'question'
p77721
(dp77722
g277
(lp77723
sg287
(lp77724
sg80
(lp77725
sg344
(lp77726
sg306
(lp77727
sg91
(lp77728
sg94
(lp77729
sg99
(lp77730
sg313
(lp77731
sg350
(lp77732
sg110
(lp77733
sg63
(lp77734
sg114
(lp77735
sg116
(lp77736
sg121
(lp77737
sg34
(lp77738
sg36
(lp77739
sg68
(lp77740
sg126
(lp77741
sg40
(lp77742
sg130
(lp77743
sg50
(lp77744
I346
assS'lond'
p77745
(dp77746
g106
(lp77747
I2588
asg48
(lp77748
sg176
(lp77749
sg256
(lp77750
ssS'fast'
p77751
(dp77752
g283
(lp77753
sg295
(lp77754
sg183
(lp77755
sg59
(lp77756
sg38
(lp77757
sg42
(lp77758
I382
asg87
(lp77759
sg91
(lp77760
sg245
(lp77761
sg20
(lp77762
sg221
(lp77763
sg535
(lp77764
sg44
(lp77765
sg104
(lp77766
sg96
(lp77767
sg52
(lp77768
sg116
(lp77769
sg32
(lp77770
sg4
(lp77771
sg181
(lp77772
sg34
(lp77773
sg68
(lp77774
sg10
(lp77775
sg40
(lp77776
sg132
(lp77777
sg14
(lp77778
sg16
(lp77779
ssS'adjac'
p77780
(dp77781
g174
(lp77782
sg6
(lp77783
sg76
(lp77784
sg118
(lp77785
sg80
(lp77786
sg42
(lp77787
I2840
asg102
(lp77788
sg149
(lp77789
ssS'bourlard'
p77790
(dp77791
g440
(lp77792
I20
asg87
(lp77793
sg76
(lp77794
ssS'arithmet'
p77795
(dp77796
g14
(lp77797
I3053
asg74
(lp77798
sg429
(lp77799
sg10
(lp77800
ssS'etch'
p77801
(dp77802
g85
(lp77803
I619
assS'ixt'
p77804
(dp77805
g341
(lp77806
I622
assS'melodia'
p77807
(dp77808
g116
(lp77809
I42
assS'kilpatr'
p77810
(dp77811
g74
(lp77812
I3253
assS'potassium'
p77813
(dp77814
g6
(lp77815
I1340
assS'raction'
p77816
(dp77817
g85
(lp77818
I2391
assS'delta'
p77819
(dp77820
g440
(lp77821
sg76
(lp77822
sg235
(lp77823
sg295
(lp77824
sg183
(lp77825
sg108
(lp77826
I543
asg313
(lp77827
ssS'upright'
p77828
(dp77829
g223
(lp77830
I3332
assS'bradburi'
p77831
(dp77832
g116
(lp77833
I2355
assS'hinton'
p77834
(dp77835
g70
(lp77836
sg26
(lp77837
sg72
(lp77838
sg30
(lp77839
sg74
(lp77840
sg76
(lp77841
sg344
(lp77842
sg78
(lp77843
sg59
(lp77844
sg94
(lp77845
sg223
(lp77846
sg108
(lp77847
sg114
(lp77848
sg329
(lp77849
sg318
(lp77850
sg121
(lp77851
sg34
(lp77852
sg36
(lp77853
sg460
(lp77854
sg126
(lp77855
sg295
(lp77856
sg128
(lp77857
sg183
(lp77858
sg138
(lp77859
I20
assS'aramaki'
p77860
(dp77861
g20
(lp77862
I33
assS'tsutomu'
p77863
(dp77864
g20
(lp77865
I14
assS'consist'
p77866
(dp77867
g329
(lp77868
sg70
(lp77869
sg78
(lp77870
sg277
(lp77871
sg72
(lp77872
sg281
(lp77873
sg283
(lp77874
sg85
(lp77875
sg36
(lp77876
sg181
(lp77877
sg26
(lp77878
sg30
(lp77879
sg287
(lp77880
sg74
(lp77881
sg176
(lp77882
sg145
(lp77883
sg256
(lp77884
sg76
(lp77885
sg118
(lp77886
sg295
(lp77887
sg183
(lp77888
sg59
(lp77889
sg484
(lp77890
sg114
(lp77891
sg303
(lp77892
sg42
(lp77893
I2080
asg80
(lp77894
sg460
(lp77895
sg12
(lp77896
sg46
(lp77897
sg96
(lp77898
sg48
(lp77899
sg99
(lp77900
sg44
(lp77901
sg149
(lp77902
sg174
(lp77903
sg293
(lp77904
sg350
(lp77905
sg429
(lp77906
sg68
(lp77907
sg104
(lp77908
sg106
(lp77909
sg108
(lp77910
sg110
(lp77911
sg63
(lp77912
sg22
(lp77913
sg216
(lp77914
sg438
(lp77915
sg440
(lp77916
sg332
(lp77917
sg121
(lp77918
sg4
(lp77919
sg6
(lp77920
sg8
(lp77921
sg221
(lp77922
sg384
(lp77923
sg124
(lp77924
sg126
(lp77925
sg341
(lp77926
sg40
(lp77927
sg344
(lp77928
sg223
(lp77929
sg128
(lp77930
sg130
(lp77931
sg14
(lp77932
sg16
(lp77933
sg135
(lp77934
sg138
(lp77935
sg140
(lp77936
sg354
(lp77937
ssS'oocl'
p77938
(dp77939
g18
(lp77940
I1109
assS'caller'
p77941
(dp77942
g440
(lp77943
I2114
assS'retinotop'
p77944
(dp77945
g118
(lp77946
sg535
(lp77947
I154
asg303
(lp77948
ssS'sheu'
p77949
(dp77950
g20
(lp77951
I2691
assS'fukada'
p77952
(dp77953
g181
(lp77954
I2554
assS'highlight'
p77955
(dp77956
g329
(lp77957
sg429
(lp77958
sg108
(lp77959
I2028
assS'z'
p77960
(dp77961
g287
(lp77962
sg74
(lp77963
sg176
(lp77964
sg145
(lp77965
sg76
(lp77966
sg262
(lp77967
sg295
(lp77968
sg183
(lp77969
sg484
(lp77970
sg38
(lp77971
sg83
(lp77972
sg85
(lp77973
sg303
(lp77974
sg306
(lp77975
sg89
(lp77976
sg91
(lp77977
sg12
(lp77978
sg94
(lp77979
sg96
(lp77980
sg99
(lp77981
sg313
(lp77982
sg223
(lp77983
sg350
(lp77984
sg329
(lp77985
sg245
(lp77986
sg46
(lp77987
sg102
(lp77988
sg108
(lp77989
sg110
(lp77990
sg22
(lp77991
sg116
(lp77992
sg174
(lp77993
sg32
(lp77994
sg318
(lp77995
sg121
(lp77996
sg181
(lp77997
sg6
(lp77998
sg8
(lp77999
sg221
(lp78000
sg384
(lp78001
sg68
(lp78002
sg126
(lp78003
sg341
(lp78004
sg344
(lp78005
sg128
(lp78006
sg130
(lp78007
sg14
(lp78008
sg16
(lp78009
sg50
(lp78010
sg138
(lp78011
I349
assS'landsat'
p78012
(dp78013
g281
(lp78014
I2267
assS'oocu'
p78015
(dp78016
g350
(lp78017
I2912
assS'hydrophob'
p78018
(dp78019
g26
(lp78020
I189
assS'dill'
p78021
(dp78022
g96
(lp78023
I2578
assS'enlarg'
p78024
(dp78025
g89
(lp78026
I1034
assS'sorkand'
p78027
(dp78028
g277
(lp78029
I1735
assS'spontan'
p78030
(dp78031
g48
(lp78032
I1092
assS'steeper'
p78033
(dp78034
g262
(lp78035
I1322
assS'kamar'
p78036
(dp78037
g26
(lp78038
I24
assS'raint'
p78039
(dp78040
g48
(lp78041
I761
assS'nick'
p78042
(dp78043
g176
(lp78044
I2465
assS'nici'
p78045
(dp78046
g50
(lp78047
I14
assS'kroghlnordita'
p78048
(dp78049
g140
(lp78050
I239
assS'gro'
p78051
(dp78052
g535
(lp78053
I2162
assS'nice'
p78054
(dp78055
g174
(lp78056
I2138
asg74
(lp78057
sg36
(lp78058
sg384
(lp78059
sg124
(lp78060
sg85
(lp78061
sg104
(lp78062
sg63
(lp78063
ssS'dala'
p78064
(dp78065
g85
(lp78066
I4119
assS'tvjqj'
p78067
(dp78068
g130
(lp78069
I1552
assS'victorri'
p78070
(dp78071
g132
(lp78072
I3757
asg223
(lp78073
ssS'stimlus'
p78074
(dp78075
g70
(lp78076
I1930
assS'meaning'
p78077
(dp78078
g74
(lp78079
sg8
(lp78080
sg344
(lp78081
sg293
(lp78082
sg306
(lp78083
sg94
(lp78084
sg108
(lp78085
I1676
asg63
(lp78086
sg44
(lp78087
ssS'speakerindepend'
p78088
(dp78089
g30
(lp78090
I178
assS'vigil'
p78091
(dp78092
g104
(lp78093
I1468
assS'grenand'
p78094
(dp78095
g138
(lp78096
I794
assS'vice'
p78097
(dp78098
g429
(lp78099
sg332
(lp78100
sg70
(lp78101
sg176
(lp78102
sg140
(lp78103
I1637
assS'we'
p78104
(dp78105
g80
(lp78106
sg293
(lp78107
sg344
(lp78108
sg78
(lp78109
sg59
(lp78110
sg484
(lp78111
sg38
(lp78112
sg83
(lp78113
sg85
(lp78114
sg303
(lp78115
sg438
(lp78116
sg118
(lp78117
sg36
(lp78118
sg460
(lp78119
sg68
(lp78120
sg72
(lp78121
sg281
(lp78122
sg10
(lp78123
sg40
(lp78124
sg26
(lp78125
sg277
(lp78126
sg163
(lp78127
sg89
(lp78128
sg91
(lp78129
sg12
(lp78130
sg94
(lp78131
sg96
(lp78132
sg48
(lp78133
sg99
(lp78134
sg313
(lp78135
sg44
(lp78136
sg149
(lp78137
sg429
(lp78138
sg102
(lp78139
sg104
(lp78140
sg106
(lp78141
sg108
(lp78142
sg110
(lp78143
sg63
(lp78144
sg114
(lp78145
sg128
(lp78146
sg130
(lp78147
sg132
(lp78148
sg14
(lp78149
sg16
(lp78150
sg135
(lp78151
sg50
(lp78152
sg138
(lp78153
sg140
(lp78154
sg354
(lp78155
sg306
(lp78156
sg46
(lp78157
sg20
(lp78158
sg18
(lp78159
sg221
(lp78160
sg535
(lp78161
sg223
(lp78162
sg350
(lp78163
sg174
(lp78164
sg440
(lp78165
sg332
(lp78166
sg121
(lp78167
sg4
(lp78168
sg6
(lp78169
sg8
(lp78170
sg126
(lp78171
sg341
(lp78172
sg30
(lp78173
sg287
(lp78174
sg74
(lp78175
sg176
(lp78176
sg145
(lp78177
sg256
(lp78178
sg76
(lp78179
sg262
(lp78180
sg295
(lp78181
sg183
(lp78182
sg42
(lp78183
I167
asg230
(lp78184
sg329
(lp78185
sg32
(lp78186
sg318
(lp78187
sg178
(lp78188
sg22
(lp78189
sg181
(lp78190
sg235
(lp78191
sg384
(lp78192
sg124
(lp78193
ssS'xxxxxxxxxxxx'
p78194
(dp78195
g52
(lp78196
I1163
assS'bulletin'
p78197
(dp78198
g287
(lp78199
I3502
assS'nasa'
p78200
(dp78201
g110
(lp78202
I3048
assS'leak'
p78203
(dp78204
g118
(lp78205
sg262
(lp78206
I1206
assS'konishi'
p78207
(dp78208
g116
(lp78209
I178
assS'buffalo'
p78210
(dp78211
g245
(lp78212
sg183
(lp78213
sg138
(lp78214
I3492
asg63
(lp78215
ssS'sector'
p78216
(dp78217
g18
(lp78218
I1446
asg83
(lp78219
ssS'windo'
p78220
(dp78221
g116
(lp78222
I664
assS'wavenumb'
p78223
(dp78224
g102
(lp78225
I1454
assS'kanal'
p78226
(dp78227
g287
(lp78228
I3539
asg281
(lp78229
sg183
(lp78230
ssS'ofrecuii'
p78231
(dp78232
g76
(lp78233
I3235
assS'edi'
p78234
(dp78235
g384
(lp78236
I1365
assS'levick'
p78237
(dp78238
g245
(lp78239
I1432
assS'steve'
p78240
(dp78241
g110
(lp78242
sg121
(lp78243
I8
asg83
(lp78244
ssS'jjij'
p78245
(dp78246
g149
(lp78247
I881
assS'omnuci'
p78248
(dp78249
g83
(lp78250
I1298
assS'edu'
p78251
(dp78252
g68
(lp78253
sg277
(lp78254
sg281
(lp78255
sg287
(lp78256
sg74
(lp78257
sg256
(lp78258
sg80
(lp78259
sg262
(lp78260
sg295
(lp78261
sg183
(lp78262
sg484
(lp78263
sg83
(lp78264
sg303
(lp78265
sg306
(lp78266
sg89
(lp78267
sg91
(lp78268
sg94
(lp78269
sg18
(lp78270
sg99
(lp78271
sg313
(lp78272
sg350
(lp78273
sg116
(lp78274
sg118
(lp78275
sg293
(lp78276
sg178
(lp78277
sg230
(lp78278
sg329
(lp78279
sg440
(lp78280
sg318
(lp78281
sg121
(lp78282
sg22
(lp78283
sg181
(lp78284
sg8
(lp78285
sg460
(lp78286
sg124
(lp78287
sg126
(lp78288
sg341
(lp78289
sg10
(lp78290
sg40
(lp78291
sg344
(lp78292
sg149
(lp78293
sg50
(lp78294
I10
assS'superfici'
p78295
(dp78296
g48
(lp78297
I393
assS'huebner'
p78298
(dp78299
g350
(lp78300
I2902
assS'iiyposioooi'
p78301
(dp78302
g350
(lp78303
I1261
assS'simon'
p78304
(dp78305
g94
(lp78306
I3471
assS'noorden'
p78307
(dp78308
g332
(lp78309
I2726
assS'stabli'
p78310
(dp78311
g230
(lp78312
sg46
(lp78313
I114
assS'jqj'
p78314
(dp78315
g102
(lp78316
I723
assS'mainstream'
p78317
(dp78318
g174
(lp78319
I578
assS'smolen'
p78320
(dp78321
g80
(lp78322
I2739
assS'rlllng'
p78323
(dp78324
g118
(lp78325
I1044
assS'electron'
p78326
(dp78327
g26
(lp78328
sg283
(lp78329
sg121
(lp78330
sg22
(lp78331
sg256
(lp78332
sg40
(lp78333
sg140
(lp78334
I17
asg245
(lp78335
sg14
(lp78336
sg20
(lp78337
sg110
(lp78338
sg52
(lp78339
sg114
(lp78340
ssS'acknow'
p78341
(dp78342
g40
(lp78343
sg8
(lp78344
I2369
assS'electrod'
p78345
(dp78346
g14
(lp78347
sg106
(lp78348
I1054
asg6
(lp78349
ssS'vsel'
p78350
(dp78351
g14
(lp78352
I3651
assS'relev'
p78353
(dp78354
g124
(lp78355
sg287
(lp78356
sg74
(lp78357
sg293
(lp78358
sg295
(lp78359
sg183
(lp78360
sg38
(lp78361
sg44
(lp78362
sg350
(lp78363
sg104
(lp78364
sg63
(lp78365
sg116
(lp78366
sg4
(lp78367
sg384
(lp78368
sg68
(lp78369
sg126
(lp78370
sg344
(lp78371
sg128
(lp78372
sg130
(lp78373
sg132
(lp78374
sg460
(lp78375
sg354
(lp78376
I1856
assS'techfak'
p78377
(dp78378
g59
(lp78379
I45
assS'dealt'
p78380
(dp78381
g287
(lp78382
sg59
(lp78383
sg76
(lp78384
sg140
(lp78385
I2828
asg40
(lp78386
ssS'thalam'
p78387
(dp78388
g80
(lp78389
I156
assS'rro'
p78390
(dp78391
g36
(lp78392
I3092
assS'ulvllj'
p78393
(dp78394
g106
(lp78395
I2000
assS'eomput'
p78396
(dp78397
g63
(lp78398
I1225
assS'smaller'
p78399
(dp78400
g287
(lp78401
sg74
(lp78402
sg145
(lp78403
sg256
(lp78404
sg262
(lp78405
sg295
(lp78406
sg183
(lp78407
sg484
(lp78408
sg85
(lp78409
sg89
(lp78410
sg94
(lp78411
sg20
(lp78412
sg44
(lp78413
sg350
(lp78414
sg118
(lp78415
sg102
(lp78416
sg106
(lp78417
I807
asg108
(lp78418
sg63
(lp78419
sg52
(lp78420
sg329
(lp78421
sg32
(lp78422
sg318
(lp78423
sg178
(lp78424
sg22
(lp78425
sg8
(lp78426
sg34
(lp78427
sg36
(lp78428
sg235
(lp78429
sg281
(lp78430
sg10
(lp78431
sg40
(lp78432
sg14
(lp78433
sg16
(lp78434
sg149
(lp78435
sg138
(lp78436
sg140
(lp78437
ssS'fallsid'
p78438
(dp78439
g108
(lp78440
sg128
(lp78441
I639
assS'rre'
p78442
(dp78443
g332
(lp78444
I1094
assS'acit'
p78445
(dp78446
g262
(lp78447
I1788
assS'arendra'
p78448
(dp78449
g128
(lp78450
I1695
assS'fold'
p78451
(dp78452
g344
(lp78453
sg484
(lp78454
sg26
(lp78455
sg52
(lp78456
I2502
assS'helicopt'
p78457
(dp78458
g78
(lp78459
I2976
assS'acid'
p78460
(dp78461
g344
(lp78462
sg106
(lp78463
I1531
asg138
(lp78464
sg26
(lp78465
ssS'rrr'
p78466
(dp78467
g256
(lp78468
I828
assS'multinet'
p78469
(dp78470
g135
(lp78471
I2604
assS'koenig'
p78472
(dp78473
g70
(lp78474
I2455
assS'compar'
p78475
(dp78476
g68
(lp78477
sg70
(lp78478
sg78
(lp78479
sg277
(lp78480
sg163
(lp78481
sg116
(lp78482
sg283
(lp78483
sg36
(lp78484
sg26
(lp78485
sg30
(lp78486
sg287
(lp78487
sg74
(lp78488
sg145
(lp78489
sg80
(lp78490
sg76
(lp78491
sg262
(lp78492
sg344
(lp78493
sg183
(lp78494
sg59
(lp78495
sg484
(lp78496
sg83
(lp78497
sg85
(lp78498
sg303
(lp78499
sg42
(lp78500
I482
asg87
(lp78501
sg89
(lp78502
sg91
(lp78503
sg12
(lp78504
sg94
(lp78505
sg96
(lp78506
sg48
(lp78507
sg99
(lp78508
sg313
(lp78509
sg44
(lp78510
sg350
(lp78511
sg118
(lp78512
sg230
(lp78513
sg329
(lp78514
sg318
(lp78515
sg46
(lp78516
sg102
(lp78517
sg178
(lp78518
sg108
(lp78519
sg110
(lp78520
sg63
(lp78521
sg22
(lp78522
sg216
(lp78523
sg438
(lp78524
sg32
(lp78525
sg332
(lp78526
sg121
(lp78527
sg4
(lp78528
sg8
(lp78529
sg34
(lp78530
sg221
(lp78531
sg384
(lp78532
sg124
(lp78533
sg126
(lp78534
sg281
(lp78535
sg10
(lp78536
sg40
(lp78537
sg223
(lp78538
sg128
(lp78539
sg130
(lp78540
sg132
(lp78541
sg14
(lp78542
sg16
(lp78543
sg135
(lp78544
sg50
(lp78545
sg138
(lp78546
sg354
(lp78547
ssS'juggl'
p78548
(dp78549
g313
(lp78550
I2289
assS'vibrat'
p78551
(dp78552
g116
(lp78553
sg174
(lp78554
I214
asg78
(lp78555
sg303
(lp78556
ssS'chose'
p78557
(dp78558
g440
(lp78559
sg178
(lp78560
sg26
(lp78561
sg74
(lp78562
sg10
(lp78563
sg83
(lp78564
sg128
(lp78565
sg221
(lp78566
sg138
(lp78567
sg140
(lp78568
I1226
assS'equalis'
p78569
(dp78570
g283
(lp78571
I892
assS'usag'
p78572
(dp78573
g118
(lp78574
sg145
(lp78575
sg26
(lp78576
sg85
(lp78577
sg94
(lp78578
sg14
(lp78579
sg135
(lp78580
I1264
assS'tabul'
p78581
(dp78582
g76
(lp78583
I819
assS'usaf'
p78584
(dp78585
g281
(lp78586
I2226
assS'hki'
p78587
(dp78588
g221
(lp78589
I593
assS'transmembran'
p78590
(dp78591
g256
(lp78592
I557
assS'vectm'
p78593
(dp78594
g183
(lp78595
I5336
assS'larger'
p78596
(dp78597
g70
(lp78598
sg72
(lp78599
sg30
(lp78600
sg74
(lp78601
sg176
(lp78602
sg145
(lp78603
sg256
(lp78604
sg78
(lp78605
sg59
(lp78606
sg484
(lp78607
sg38
(lp78608
sg85
(lp78609
sg303
(lp78610
sg87
(lp78611
sg245
(lp78612
sg94
(lp78613
sg20
(lp78614
sg221
(lp78615
sg118
(lp78616
sg102
(lp78617
sg104
(lp78618
sg110
(lp78619
sg63
(lp78620
sg329
(lp78621
sg178
(lp78622
sg4
(lp78623
sg235
(lp78624
sg34
(lp78625
sg124
(lp78626
sg126
(lp78627
sg10
(lp78628
sg40
(lp78629
sg14
(lp78630
sg16
(lp78631
sg135
(lp78632
sg140
(lp78633
sg354
(lp78634
I807
assS'spatia'
p78635
(dp78636
g70
(lp78637
sg293
(lp78638
I257
assS'treisman'
p78639
(dp78640
g178
(lp78641
I516
assS'posch'
p78642
(dp78643
g59
(lp78644
I3224
assS'typic'
p78645
(dp78646
g70
(lp78647
sg26
(lp78648
sg277
(lp78649
sg30
(lp78650
sg287
(lp78651
sg74
(lp78652
sg145
(lp78653
sg76
(lp78654
sg78
(lp78655
sg59
(lp78656
sg38
(lp78657
sg83
(lp78658
sg85
(lp78659
sg303
(lp78660
sg42
(lp78661
I2621
asg87
(lp78662
sg91
(lp78663
sg94
(lp78664
sg20
(lp78665
sg18
(lp78666
sg99
(lp78667
sg223
(lp78668
sg149
(lp78669
sg116
(lp78670
sg135
(lp78671
sg429
(lp78672
sg318
(lp78673
sg106
(lp78674
sg108
(lp78675
sg110
(lp78676
sg63
(lp78677
sg22
(lp78678
sg230
(lp78679
sg329
(lp78680
sg440
(lp78681
sg48
(lp78682
sg178
(lp78683
sg181
(lp78684
sg6
(lp78685
sg235
(lp78686
sg34
(lp78687
sg221
(lp78688
sg384
(lp78689
sg68
(lp78690
sg281
(lp78691
sg10
(lp78692
sg44
(lp78693
sg128
(lp78694
sg130
(lp78695
sg132
(lp78696
sg14
(lp78697
sg16
(lp78698
sg350
(lp78699
sg354
(lp78700
ssS'spatio'
p78701
(dp78702
g256
(lp78703
I132
asg293
(lp78704
ssS'hardi'
p78705
(dp78706
g96
(lp78707
I894
asg256
(lp78708
ssS'shovel'
p78709
(dp78710
g181
(lp78711
I75
assS'redu'
p78712
(dp78713
g4
(lp78714
I2853
assS'iong'
p78715
(dp78716
g106
(lp78717
I967
assS'appli'
p78718
(dp78719
g329
(lp78720
sg78
(lp78721
sg277
(lp78722
sg163
(lp78723
sg283
(lp78724
sg26
(lp78725
sg30
(lp78726
sg287
(lp78727
sg74
(lp78728
sg176
(lp78729
sg145
(lp78730
sg76
(lp78731
sg293
(lp78732
sg295
(lp78733
sg183
(lp78734
sg59
(lp78735
sg38
(lp78736
sg85
(lp78737
sg63
(lp78738
sg306
(lp78739
sg87
(lp78740
sg89
(lp78741
sg460
(lp78742
sg12
(lp78743
sg94
(lp78744
sg96
(lp78745
sg221
(lp78746
sg313
(lp78747
sg44
(lp78748
sg230
(lp78749
sg174
(lp78750
sg32
(lp78751
sg68
(lp78752
sg46
(lp78753
sg102
(lp78754
sg104
(lp78755
sg106
(lp78756
sg108
(lp78757
sg110
(lp78758
sg178
(lp78759
sg52
(lp78760
sg216
(lp78761
sg438
(lp78762
I1642
asg440
(lp78763
sg318
(lp78764
sg121
(lp78765
sg22
(lp78766
sg181
(lp78767
sg8
(lp78768
sg34
(lp78769
sg36
(lp78770
sg384
(lp78771
sg124
(lp78772
sg126
(lp78773
sg281
(lp78774
sg535
(lp78775
sg344
(lp78776
sg223
(lp78777
sg128
(lp78778
sg130
(lp78779
sg132
(lp78780
sg14
(lp78781
sg50
(lp78782
sg138
(lp78783
sg140
(lp78784
sg354
(lp78785
ssS'approxim'
p78786
(dp78787
g124
(lp78788
sg70
(lp78789
sg26
(lp78790
sg163
(lp78791
sg72
(lp78792
sg281
(lp78793
sg283
(lp78794
sg460
(lp78795
sg36
(lp78796
sg303
(lp78797
sg30
(lp78798
sg287
(lp78799
sg74
(lp78800
sg176
(lp78801
sg145
(lp78802
sg76
(lp78803
sg262
(lp78804
sg295
(lp78805
sg183
(lp78806
sg59
(lp78807
sg38
(lp78808
sg83
(lp78809
sg85
(lp78810
sg63
(lp78811
sg42
(lp78812
I539
asg306
(lp78813
sg87
(lp78814
sg89
(lp78815
sg91
(lp78816
sg12
(lp78817
sg94
(lp78818
sg96
(lp78819
sg48
(lp78820
sg99
(lp78821
sg313
(lp78822
sg44
(lp78823
sg149
(lp78824
sg116
(lp78825
sg293
(lp78826
sg32
(lp78827
sg68
(lp78828
sg46
(lp78829
sg102
(lp78830
sg108
(lp78831
sg110
(lp78832
sg20
(lp78833
sg52
(lp78834
sg114
(lp78835
sg230
(lp78836
sg438
(lp78837
sg440
(lp78838
sg318
(lp78839
sg121
(lp78840
sg22
(lp78841
sg181
(lp78842
sg8
(lp78843
sg34
(lp78844
sg221
(lp78845
sg384
(lp78846
sg235
(lp78847
sg126
(lp78848
sg341
(lp78849
sg10
(lp78850
sg535
(lp78851
sg344
(lp78852
sg223
(lp78853
sg128
(lp78854
sg130
(lp78855
sg132
(lp78856
sg14
(lp78857
sg16
(lp78858
sg135
(lp78859
sg50
(lp78860
sg138
(lp78861
sg140
(lp78862
sg354
(lp78863
ssS'pxp'
p78864
(dp78865
g178
(lp78866
I352
assS'inequ'
p78867
(dp78868
g230
(lp78869
sg281
(lp78870
sg306
(lp78871
sg130
(lp78872
I280
asg110
(lp78873
sg535
(lp78874
sg223
(lp78875
ssS'dehaen'
p78876
(dp78877
g4
(lp78878
I574
assS'motor'
p78879
(dp78880
g116
(lp78881
sg174
(lp78882
sg178
(lp78883
sg4
(lp78884
sg293
(lp78885
sg295
(lp78886
sg183
(lp78887
sg68
(lp78888
sg303
(lp78889
sg34
(lp78890
sg78
(lp78891
sg245
(lp78892
sg104
(lp78893
sg18
(lp78894
sg99
(lp78895
I59
asg223
(lp78896
sg350
(lp78897
ssS'razborov'
p78898
(dp78899
g344
(lp78900
sg40
(lp78901
I2493
assS'greg'
p78902
(dp78903
g277
(lp78904
I3000
assS'scalei'
p78905
(dp78906
g181
(lp78907
I1215
assS'fed'
p78908
(dp78909
g438
(lp78910
I2121
asg178
(lp78911
sg174
(lp78912
sg68
(lp78913
sg287
(lp78914
sg128
(lp78915
sg20
(lp78916
sg99
(lp78917
sg138
(lp78918
sg354
(lp78919
ssS'ljung'
p78920
(dp78921
g295
(lp78922
I1673
asg183
(lp78923
ssS'iceg'
p78924
(dp78925
g135
(lp78926
I1
assS'usa'
p78927
(dp78928
g216
(lp78929
sg438
(lp78930
I2444
asg440
(lp78931
sg176
(lp78932
sg178
(lp78933
sg76
(lp78934
sg8
(lp78935
sg38
(lp78936
sg40
(lp78937
sg287
(lp78938
sg87
(lp78939
sg174
(lp78940
sg12
(lp78941
sg106
(lp78942
sg48
(lp78943
sg138
(lp78944
sg44
(lp78945
sg149
(lp78946
ssS'fel'
p78947
(dp78948
g281
(lp78949
I326
assS'ush'
p78950
(dp78951
g230
(lp78952
I2041
assS'few'
p78953
(dp78954
g283
(lp78955
sg181
(lp78956
sg145
(lp78957
sg256
(lp78958
sg76
(lp78959
sg344
(lp78960
sg183
(lp78961
sg80
(lp78962
sg42
(lp78963
I399
asg89
(lp78964
sg91
(lp78965
sg46
(lp78966
sg96
(lp78967
sg48
(lp78968
sg221
(lp78969
sg313
(lp78970
sg149
(lp78971
sg429
(lp78972
sg94
(lp78973
sg104
(lp78974
sg108
(lp78975
sg110
(lp78976
sg63
(lp78977
sg52
(lp78978
sg114
(lp78979
sg178
(lp78980
sg4
(lp78981
sg6
(lp78982
sg34
(lp78983
sg124
(lp78984
sg128
(lp78985
sg132
(lp78986
sg14
(lp78987
sg16
(lp78988
sg138
(lp78989
sg354
(lp78990
ssS'feu'
p78991
(dp78992
g460
(lp78993
I1244
assS'gert'
p78994
(dp78995
g22
(lp78996
I13
assS'ineu'
p78997
(dp78998
g14
(lp78999
I3680
assS'sort'
p79000
(dp79001
g4
(lp79002
sg76
(lp79003
sg34
(lp79004
sg78
(lp79005
sg68
(lp79006
sg277
(lp79007
sg44
(lp79008
I181
asg20
(lp79009
sg63
(lp79010
sg223
(lp79011
ssS'clever'
p79012
(dp79013
g42
(lp79014
I2102
assS'impress'
p79015
(dp79016
g221
(lp79017
sg484
(lp79018
sg89
(lp79019
I302
assS'sore'
p79020
(dp79021
g20
(lp79022
I1850
assS'rabbit'
p79023
(dp79024
g245
(lp79025
sg106
(lp79026
I2579
assS'saito'
p79027
(dp79028
g181
(lp79029
I2552
assS'kautz'
p79030
(dp79031
g40
(lp79032
I2572
assS'annoy'
p79033
(dp79034
g94
(lp79035
I670
assS'tricub'
p79036
(dp79037
g313
(lp79038
I1340
assS'augment'
p79039
(dp79040
g178
(lp79041
sg293
(lp79042
sg295
(lp79043
sg183
(lp79044
sg124
(lp79045
sg126
(lp79046
sg87
(lp79047
sg89
(lp79048
sg132
(lp79049
I135
assS'annot'
p79050
(dp79051
g104
(lp79052
sg91
(lp79053
I2373
assS'column'
p79054
(dp79055
g329
(lp79056
sg32
(lp79057
sg48
(lp79058
sg121
(lp79059
sg460
(lp79060
sg72
(lp79061
sg281
(lp79062
sg102
(lp79063
sg306
(lp79064
sg283
(lp79065
sg12
(lp79066
sg104
(lp79067
sg96
(lp79068
sg135
(lp79069
I604
asg221
(lp79070
sg149
(lp79071
ssS'chii'
p79072
(dp79073
g256
(lp79074
I9
assS'counterexampl'
p79075
(dp79076
g104
(lp79077
I2895
asg223
(lp79078
ssS'proof'
p79079
(dp79080
g287
(lp79081
sg70
(lp79082
sg145
(lp79083
sg34
(lp79084
sg36
(lp79085
sg68
(lp79086
sg341
(lp79087
sg85
(lp79088
sg40
(lp79089
sg344
(lp79090
sg306
(lp79091
sg46
(lp79092
sg48
(lp79093
I465
assS'tau'
p79094
(dp79095
g145
(lp79096
I265
assS'tap'
p79097
(dp79098
g121
(lp79099
sg4
(lp79100
sg128
(lp79101
I311
asg22
(lp79102
ssS'tar'
p79103
(dp79104
g121
(lp79105
I1490
assS'carrol'
p79106
(dp79107
g74
(lp79108
I660
assS'rapt'
p79109
(dp79110
g36
(lp79111
I2933
assS'proprietari'
p79112
(dp79113
g91
(lp79114
I2408
assS'tab'
p79115
(dp79116
g48
(lp79117
I744
asg72
(lp79118
ssS'tac'
p79119
(dp79120
g89
(lp79121
I834
assS'tal'
p79122
(dp79123
g52
(lp79124
I333
assS'tam'
p79125
(dp79126
g230
(lp79127
sg14
(lp79128
sg135
(lp79129
I2457
assS'serial'
p79130
(dp79131
g135
(lp79132
I1651
asg110
(lp79133
sg10
(lp79134
sg52
(lp79135
sg63
(lp79136
ssS'tao'
p79137
(dp79138
g130
(lp79139
I1390
assS'eigenfunct'
p79140
(dp79141
g262
(lp79142
I1007
assS'unus'
p79143
(dp79144
g135
(lp79145
I1814
asg72
(lp79146
sg277
(lp79147
ssS'sis'
p79148
(dp79149
g283
(lp79150
sg277
(lp79151
I287
assS'siu'
p79152
(dp79153
g287
(lp79154
I3643
asg145
(lp79155
sg40
(lp79156
ssS'sit'
p79157
(dp79158
g14
(lp79159
I3040
asg293
(lp79160
sg26
(lp79161
sg94
(lp79162
ssS'six'
p79163
(dp79164
g230
(lp79165
sg116
(lp79166
sg176
(lp79167
sg70
(lp79168
sg256
(lp79169
sg59
(lp79170
sg124
(lp79171
sg281
(lp79172
sg63
(lp79173
sg283
(lp79174
sg128
(lp79175
sg104
(lp79176
sg94
(lp79177
sg48
(lp79178
sg138
(lp79179
I1332
assS'serum'
p79180
(dp79181
g91
(lp79182
I2113
assS'tdistanc'
p79183
(dp79184
g44
(lp79185
I1390
assS'brian'
p79186
(dp79187
g10
(lp79188
I20
assS'iontophoret'
p79189
(dp79190
g106
(lp79191
I2399
assS'sig'
p79192
(dp79193
g14
(lp79194
I4257
asg128
(lp79195
ssS'instead'
p79196
(dp79197
g70
(lp79198
sg78
(lp79199
sg277
(lp79200
sg163
(lp79201
sg72
(lp79202
sg26
(lp79203
sg287
(lp79204
sg80
(lp79205
sg293
(lp79206
sg295
(lp79207
sg183
(lp79208
sg59
(lp79209
sg484
(lp79210
sg303
(lp79211
sg42
(lp79212
I756
asg87
(lp79213
sg89
(lp79214
sg91
(lp79215
sg12
(lp79216
sg94
(lp79217
sg223
(lp79218
sg149
(lp79219
sg429
(lp79220
sg63
(lp79221
sg52
(lp79222
sg216
(lp79223
sg174
(lp79224
sg32
(lp79225
sg318
(lp79226
sg22
(lp79227
sg8
(lp79228
sg34
(lp79229
sg235
(lp79230
sg126
(lp79231
sg344
(lp79232
sg128
(lp79233
sg130
(lp79234
sg132
(lp79235
sg50
(lp79236
sg140
(lp79237
sg354
(lp79238
ssS'dwi'
p79239
(dp79240
g216
(lp79241
I1039
assS'sin'
p79242
(dp79243
g230
(lp79244
sg30
(lp79245
sg32
(lp79246
sg484
(lp79247
sg181
(lp79248
sg36
(lp79249
sg384
(lp79250
sg124
(lp79251
sg46
(lp79252
I2884
assS'sil'
p79253
(dp79254
g102
(lp79255
I2365
assS'tension'
p79256
(dp79257
g116
(lp79258
I1633
assS'lesion'
p79259
(dp79260
g116
(lp79261
sg303
(lp79262
sg350
(lp79263
sg4
(lp79264
sg149
(lp79265
I2539
assS'attend'
p79266
(dp79267
g332
(lp79268
I1018
asg178
(lp79269
sg176
(lp79270
ssS'outlier'
p79271
(dp79272
g295
(lp79273
sg183
(lp79274
sg318
(lp79275
I1874
assS'bursari'
p79276
(dp79277
g176
(lp79278
I2467
assS'iearn'
p79279
(dp79280
g484
(lp79281
sg89
(lp79282
I442
asg83
(lp79283
sg293
(lp79284
ssS'hazard'
p79285
(dp79286
g10
(lp79287
I1299
assS'exemplifi'
p79288
(dp79289
g332
(lp79290
I304
asg145
(lp79291
sg235
(lp79292
ssS'ecdjjij'
p79293
(dp79294
g149
(lp79295
I852
assS'attent'
p79296
(dp79297
g332
(lp79298
sg178
(lp79299
sg4
(lp79300
sg6
(lp79301
sg8
(lp79302
sg295
(lp79303
sg183
(lp79304
sg59
(lp79305
sg293
(lp79306
sg277
(lp79307
sg306
(lp79308
sg176
(lp79309
sg140
(lp79310
sg181
(lp79311
sg18
(lp79312
sg221
(lp79313
sg44
(lp79314
sg354
(lp79315
I320
assS'rpist'
p79316
(dp79317
g94
(lp79318
I2630
assS'topograph'
p79319
(dp79320
g176
(lp79321
sg535
(lp79322
sg149
(lp79323
I2296
assS'bertin'
p79324
(dp79325
g52
(lp79326
I2601
assS'birkhaus'
p79327
(dp79328
g22
(lp79329
I2370
assS'orchestr'
p79330
(dp79331
g104
(lp79332
I811
asg10
(lp79333
ssS'aftereffect'
p79334
(dp79335
g12
(lp79336
I1390
asg216
(lp79337
ssS'oftre'
p79338
(dp79339
g42
(lp79340
I3126
assS'light'
p79341
(dp79342
g216
(lp79343
sg438
(lp79344
I247
asg440
(lp79345
sg176
(lp79346
sg145
(lp79347
sg256
(lp79348
sg118
(lp79349
sg110
(lp79350
sg460
(lp79351
sg40
(lp79352
sg283
(lp79353
sg89
(lp79354
sg130
(lp79355
sg245
(lp79356
sg70
(lp79357
sg99
(lp79358
sg63
(lp79359
sg223
(lp79360
sg149
(lp79361
ssS'kindermann'
p79362
(dp79363
g313
(lp79364
sg354
(lp79365
I11
assS'chapman'
p79366
(dp79367
g235
(lp79368
sg295
(lp79369
sg183
(lp79370
sg484
(lp79371
sg221
(lp79372
sg140
(lp79373
I3131
assS'lirst'
p79374
(dp79375
g96
(lp79376
I22
assS'battiti'
p79377
(dp79378
g34
(lp79379
I2805
assS'timedepend'
p79380
(dp79381
g262
(lp79382
I1137
assS'ouput'
p79383
(dp79384
g262
(lp79385
I211
assS'whilst'
p79386
(dp79387
g87
(lp79388
I189
assS'iimwt'
p79389
(dp79390
g306
(lp79391
I1659
assS'centripet'
p79392
(dp79393
g99
(lp79394
I787
assS'decornpos'
p79395
(dp79396
g350
(lp79397
I813
assS'superior'
p79398
(dp79399
g174
(lp79400
sg121
(lp79401
sg4
(lp79402
sg235
(lp79403
sg183
(lp79404
sg85
(lp79405
sg40
(lp79406
sg20
(lp79407
sg303
(lp79408
sg221
(lp79409
sg44
(lp79410
I275
assS'inaij'
p79411
(dp79412
g429
(lp79413
I580
assS'inlet'
p79414
(dp79415
g78
(lp79416
I2836
assS'ebnn'
p79417
(dp79418
g132
(lp79419
I428
asg223
(lp79420
ssS'lausann'
p79421
(dp79422
g174
(lp79423
I2545
asg40
(lp79424
ssS'hajto'
p79425
(dp79426
g14
(lp79427
I2688
assS'vital'
p79428
(dp79429
g245
(lp79430
I182
asg176
(lp79431
ssS'somatotop'
p79432
(dp79433
g535
(lp79434
I156
assS'nonneg'
p79435
(dp79436
g102
(lp79437
sg74
(lp79438
sg68
(lp79439
sg89
(lp79440
I521
asg535
(lp79441
ssS'bernoulli'
p79442
(dp79443
g281
(lp79444
I1818
assS'precess'
p79445
(dp79446
g80
(lp79447
I2789
assS'ferguson'
p79448
(dp79449
g283
(lp79450
I11
assS'hildreth'
p79451
(dp79452
g174
(lp79453
I1063
assS'flee'
p79454
(dp79455
g104
(lp79456
I3078
assS'kawamata'
p79457
(dp79458
g20
(lp79459
I2632
assS'profound'
p79460
(dp79461
g245
(lp79462
I207
assS'edit'
p79463
(dp79464
g174
(lp79465
sg145
(lp79466
sg26
(lp79467
sg163
(lp79468
sg262
(lp79469
sg94
(lp79470
I1660
asg114
(lp79471
ssS'tran'
p79472
(dp79473
g174
(lp79474
sg440
(lp79475
sg48
(lp79476
sg22
(lp79477
sg181
(lp79478
sg8
(lp79479
sg36
(lp79480
sg460
(lp79481
sg235
(lp79482
sg72
(lp79483
sg281
(lp79484
sg40
(lp79485
sg42
(lp79486
I3434
asg63
(lp79487
sg20
(lp79488
sg128
(lp79489
sg96
(lp79490
sg108
(lp79491
sg313
(lp79492
ssS'trai'
p79493
(dp79494
g223
(lp79495
I1888
assS'iiiit'
p79496
(dp79497
g306
(lp79498
I2300
assS'vramp'
p79499
(dp79500
g14
(lp79501
I3659
asg20
(lp79502
ssS'iiiii'
p79503
(dp79504
g6
(lp79505
I1796
assS'weather'
p79506
(dp79507
g70
(lp79508
I566
assS'trap'
p79509
(dp79510
g34
(lp79511
sg429
(lp79512
sg38
(lp79513
sg8
(lp79514
I341
assS'biehl'
p79515
(dp79516
g38
(lp79517
I3315
assS'oun'
p79518
(dp79519
g277
(lp79520
I1990
assS'barnett'
p79521
(dp79522
g91
(lp79523
sg354
(lp79524
I3219
assS'imposit'
p79525
(dp79526
g68
(lp79527
I3195
assS'greyscal'
p79528
(dp79529
g138
(lp79530
I1863
assS'interplay'
p79531
(dp79532
g332
(lp79533
I1394
assS'our'
p79534
(dp79535
g124
(lp79536
sg78
(lp79537
sg277
(lp79538
sg163
(lp79539
sg281
(lp79540
sg85
(lp79541
sg36
(lp79542
sg40
(lp79543
sg26
(lp79544
sg30
(lp79545
sg287
(lp79546
sg74
(lp79547
sg176
(lp79548
sg145
(lp79549
sg80
(lp79550
sg293
(lp79551
sg295
(lp79552
sg183
(lp79553
sg59
(lp79554
sg38
(lp79555
sg83
(lp79556
sg114
(lp79557
sg303
(lp79558
sg306
(lp79559
sg87
(lp79560
sg89
(lp79561
sg460
(lp79562
sg12
(lp79563
sg94
(lp79564
sg96
(lp79565
sg48
(lp79566
sg99
(lp79567
sg313
(lp79568
sg44
(lp79569
sg118
(lp79570
sg32
(lp79571
sg429
(lp79572
sg318
(lp79573
sg46
(lp79574
sg102
(lp79575
sg178
(lp79576
sg106
(lp79577
I978
asg108
(lp79578
sg110
(lp79579
sg63
(lp79580
sg52
(lp79581
sg22
(lp79582
sg230
(lp79583
sg329
(lp79584
sg440
(lp79585
sg18
(lp79586
sg121
(lp79587
sg4
(lp79588
sg8
(lp79589
sg221
(lp79590
sg384
(lp79591
sg235
(lp79592
sg341
(lp79593
sg10
(lp79594
sg535
(lp79595
sg344
(lp79596
sg223
(lp79597
sg130
(lp79598
sg132
(lp79599
sg14
(lp79600
sg50
(lp79601
sg138
(lp79602
sg140
(lp79603
sg354
(lp79604
ssS'out'
p79605
(dp79606
g124
(lp79607
sg70
(lp79608
sg26
(lp79609
sg277
(lp79610
sg116
(lp79611
sg85
(lp79612
sg287
(lp79613
sg74
(lp79614
sg176
(lp79615
sg183
(lp79616
sg59
(lp79617
sg484
(lp79618
sg114
(lp79619
sg303
(lp79620
sg42
(lp79621
I214
asg306
(lp79622
sg87
(lp79623
sg89
(lp79624
sg91
(lp79625
sg46
(lp79626
sg20
(lp79627
sg48
(lp79628
sg221
(lp79629
sg223
(lp79630
sg149
(lp79631
sg230
(lp79632
sg429
(lp79633
sg68
(lp79634
sg94
(lp79635
sg106
(lp79636
sg108
(lp79637
sg110
(lp79638
sg63
(lp79639
sg22
(lp79640
sg216
(lp79641
sg32
(lp79642
sg318
(lp79643
sg178
(lp79644
sg4
(lp79645
sg181
(lp79646
sg8
(lp79647
sg384
(lp79648
sg235
(lp79649
sg126
(lp79650
sg341
(lp79651
sg10
(lp79652
sg40
(lp79653
sg78
(lp79654
sg132
(lp79655
sg14
(lp79656
sg135
(lp79657
sg50
(lp79658
sg138
(lp79659
sg140
(lp79660
ssS'performa'
p79661
(dp79662
g4
(lp79663
I1539
assS'frontier'
p79664
(dp79665
g138
(lp79666
I3487
assS'categori'
p79667
(dp79668
g26
(lp79669
sg181
(lp79670
sg110
(lp79671
sg42
(lp79672
I2161
asg44
(lp79673
sg104
(lp79674
sg94
(lp79675
sg48
(lp79676
sg221
(lp79677
sg63
(lp79678
sg223
(lp79679
sg114
(lp79680
ssS'clockwis'
p79681
(dp79682
g44
(lp79683
I1370
assS'disappear'
p79684
(dp79685
g132
(lp79686
I2940
asg94
(lp79687
sg18
(lp79688
sg245
(lp79689
ssS'aoba'
p79690
(dp79691
g20
(lp79692
I32
assS'respecti'
p79693
(dp79694
g36
(lp79695
sg341
(lp79696
I1665
assS'plural'
p79697
(dp79698
g42
(lp79699
I3064
assS'ducret'
p79700
(dp79701
g83
(lp79702
I1128
assS'uasf'
p79703
(dp79704
g281
(lp79705
I2407
assS'volker'
p79706
(dp79707
g221
(lp79708
I29
assS'neostriatum'
p79709
(dp79710
g116
(lp79711
I380
assS'uncoupl'
p79712
(dp79713
g121
(lp79714
I183
assS'york'
p79715
(dp79716
g26
(lp79717
sg163
(lp79718
sg341
(lp79719
sg303
(lp79720
sg74
(lp79721
sg80
(lp79722
sg295
(lp79723
sg183
(lp79724
sg59
(lp79725
sg85
(lp79726
sg63
(lp79727
sg91
(lp79728
sg245
(lp79729
sg94
(lp79730
sg20
(lp79731
sg313
(lp79732
sg149
(lp79733
sg12
(lp79734
sg106
(lp79735
I2930
asg96
(lp79736
sg52
(lp79737
sg230
(lp79738
sg318
(lp79739
sg4
(lp79740
sg6
(lp79741
sg281
(lp79742
sg130
(lp79743
sg138
(lp79744
sg354
(lp79745
ssS'dictionari'
p79746
(dp79747
g94
(lp79748
I877
assS'philip'
p79749
(dp79750
g329
(lp79751
I6
assS'ioniz'
p79752
(dp79753
g14
(lp79754
sg16
(lp79755
I308
assS'ltp'
p79756
(dp79757
g106
(lp79758
I113
assS'tadashi'
p79759
(dp79760
g20
(lp79761
I12
assS'g'
p79762
(dp79763
g80
(lp79764
sg293
(lp79765
sg344
(lp79766
sg78
(lp79767
sg59
(lp79768
sg38
(lp79769
sg83
(lp79770
sg85
(lp79771
sg303
(lp79772
sg438
(lp79773
sg116
(lp79774
sg118
(lp79775
sg34
(lp79776
sg36
(lp79777
sg460
(lp79778
sg68
(lp79779
sg72
(lp79780
sg281
(lp79781
sg10
(lp79782
sg40
(lp79783
sg283
(lp79784
sg70
(lp79785
sg26
(lp79786
sg277
(lp79787
sg163
(lp79788
sg89
(lp79789
sg91
(lp79790
sg12
(lp79791
sg94
(lp79792
sg96
(lp79793
sg48
(lp79794
sg99
(lp79795
sg313
(lp79796
sg44
(lp79797
sg149
(lp79798
sg429
(lp79799
sg102
(lp79800
sg104
(lp79801
sg106
(lp79802
sg108
(lp79803
sg110
(lp79804
sg63
(lp79805
sg52
(lp79806
sg114
(lp79807
sg128
(lp79808
sg130
(lp79809
sg132
(lp79810
sg14
(lp79811
sg16
(lp79812
sg135
(lp79813
sg50
(lp79814
sg138
(lp79815
sg140
(lp79816
sg354
(lp79817
sg306
(lp79818
sg87
(lp79819
sg245
(lp79820
sg46
(lp79821
sg20
(lp79822
sg18
(lp79823
sg535
(lp79824
sg223
(lp79825
sg350
(lp79826
sg216
(lp79827
sg174
(lp79828
sg440
(lp79829
sg332
(lp79830
sg121
(lp79831
sg4
(lp79832
sg6
(lp79833
sg8
(lp79834
sg126
(lp79835
sg341
(lp79836
sg30
(lp79837
sg287
(lp79838
sg74
(lp79839
sg176
(lp79840
sg145
(lp79841
sg256
(lp79842
sg76
(lp79843
sg262
(lp79844
sg295
(lp79845
sg183
(lp79846
sg42
(lp79847
I607
asg230
(lp79848
sg329
(lp79849
sg32
(lp79850
sg318
(lp79851
sg178
(lp79852
sg22
(lp79853
sg181
(lp79854
sg235
(lp79855
sg384
(lp79856
sg124
(lp79857
ssS'zoologist'
p79858
(dp79859
g116
(lp79860
I2546
assS'organis'
p79861
(dp79862
g332
(lp79863
I722
asg176
(lp79864
sg52
(lp79865
ssS'ltf'
p79866
(dp79867
g106
(lp79868
I983
assS'ltd'
p79869
(dp79870
g106
(lp79871
I222
asg135
(lp79872
sg283
(lp79873
ssS'lowfrequ'
p79874
(dp79875
g106
(lp79876
I342
assS'lti'
p79877
(dp79878
g223
(lp79879
I2146
assS'lth'
p79880
(dp79881
g36
(lp79882
I2228
assS'lto'
p79883
(dp79884
g106
(lp79885
I273
assS'organiz'
p79886
(dp79887
g68
(lp79888
sg535
(lp79889
I258
assS'ltm'
p79890
(dp79891
g535
(lp79892
I293
assS'ltl'
p79893
(dp79894
g52
(lp79895
I1013
assS'tokamak'
p79896
(dp79897
g14
(lp79898
sg16
(lp79899
I6
assS'echo'
p79900
(dp79901
g174
(lp79902
I1623
assS'walter'
p79903
(dp79904
g59
(lp79905
sg354
(lp79906
I333
assS'iizuka'
p79907
(dp79908
g68
(lp79909
I3300
assS'prioriti'
p79910
(dp79911
g83
(lp79912
I2110
asg80
(lp79913
ssS'salient'
p79914
(dp79915
g74
(lp79916
sg178
(lp79917
I781
asg63
(lp79918
sg76
(lp79919
sg293
(lp79920
ssS'unknown'
p79921
(dp79922
g74
(lp79923
sg80
(lp79924
sg293
(lp79925
sg59
(lp79926
sg38
(lp79927
sg83
(lp79928
sg85
(lp79929
sg42
(lp79930
I476
asg91
(lp79931
sg46
(lp79932
sg535
(lp79933
sg223
(lp79934
sg230
(lp79935
sg32
(lp79936
sg318
(lp79937
sg235
(lp79938
sg460
(lp79939
sg68
(lp79940
sg130
(lp79941
sg132
(lp79942
sg50
(lp79943
sg140
(lp79944
sg354
(lp79945
ssS'accent'
p79946
(dp79947
g30
(lp79948
sg96
(lp79949
I1302
assS'capac'
p79950
(dp79951
g216
(lp79952
sg438
(lp79953
sg74
(lp79954
sg4
(lp79955
sg287
(lp79956
sg72
(lp79957
sg83
(lp79958
sg42
(lp79959
I2772
asg102
(lp79960
sg104
(lp79961
sg354
(lp79962
ssS'rasmussen'
p79963
(dp79964
g124
(lp79965
sg126
(lp79966
I11
assS'their'
p79967
(dp79968
g329
(lp79969
sg70
(lp79970
sg277
(lp79971
sg72
(lp79972
sg303
(lp79973
sg80
(lp79974
sg293
(lp79975
sg283
(lp79976
sg460
(lp79977
sg181
(lp79978
sg40
(lp79979
sg30
(lp79980
sg350
(lp79981
sg74
(lp79982
sg145
(lp79983
sg256
(lp79984
sg76
(lp79985
sg262
(lp79986
sg295
(lp79987
sg183
(lp79988
sg59
(lp79989
sg484
(lp79990
sg38
(lp79991
sg83
(lp79992
sg85
(lp79993
sg124
(lp79994
sg306
(lp79995
sg87
(lp79996
sg89
(lp79997
sg91
(lp79998
sg12
(lp79999
sg94
(lp80000
sg96
(lp80001
sg48
(lp80002
sg99
(lp80003
sg313
(lp80004
sg44
(lp80005
sg149
(lp80006
sg118
(lp80007
sg230
(lp80008
sg174
(lp80009
sg18
(lp80010
sg116
(lp80011
sg32
(lp80012
sg178
(lp80013
sg245
(lp80014
sg429
(lp80015
sg68
(lp80016
sg102
(lp80017
sg104
(lp80018
sg110
(lp80019
sg20
(lp80020
sg52
(lp80021
sg22
(lp80022
sg216
(lp80023
sg438
(lp80024
I535
asg440
(lp80025
sg332
(lp80026
sg121
(lp80027
sg4
(lp80028
sg6
(lp80029
sg8
(lp80030
sg221
(lp80031
sg384
(lp80032
sg235
(lp80033
sg126
(lp80034
sg535
(lp80035
sg344
(lp80036
sg63
(lp80037
sg223
(lp80038
sg128
(lp80039
sg132
(lp80040
sg14
(lp80041
sg135
(lp80042
sg50
(lp80043
sg138
(lp80044
sg140
(lp80045
sg354
(lp80046
ssS'salienc'
p80047
(dp80048
g344
(lp80049
sg70
(lp80050
sg74
(lp80051
sg178
(lp80052
sg8
(lp80053
I1101
assS'siam'
p80054
(dp80055
g174
(lp80056
sg176
(lp80057
sg145
(lp80058
sg8
(lp80059
I2529
asg344
(lp80060
sg40
(lp80061
sg104
(lp80062
ssS'shell'
p80063
(dp80064
g89
(lp80065
I1840
assS'shelf'
p80066
(dp80067
g181
(lp80068
I1183
assS'dhmm'
p80069
(dp80070
g440
(lp80071
I2038
assS'shallow'
p80072
(dp80073
g102
(lp80074
I2556
asg68
(lp80075
sg181
(lp80076
ssS'tsodyk'
p80077
(dp80078
g262
(lp80079
I547
assS'academi'
p80080
(dp80081
g216
(lp80082
sg8
(lp80083
sg145
(lp80084
sg40
(lp80085
sg149
(lp80086
I2929
assS'nonmodifi'
p80087
(dp80088
g438
(lp80089
I258
assS'blind'
p80090
(dp80091
g78
(lp80092
sg318
(lp80093
sg50
(lp80094
I1632
asg76
(lp80095
ssS'minaea'
p80096
(dp80097
g89
(lp80098
I2445
assS'holland'
p80099
(dp80100
g32
(lp80101
I3145
asg283
(lp80102
sg281
(lp80103
ssS'colliculus'
p80104
(dp80105
g174
(lp80106
I330
asg303
(lp80107
ssS'emac'
p80108
(dp80109
g94
(lp80110
I565
assS'dgvctz'
p80111
(dp80112
g74
(lp80113
I2430
assS'heifa'
p80114
(dp80115
g140
(lp80116
I3169
assS'siemen'
p80117
(dp80118
g78
(lp80119
sg221
(lp80120
sg313
(lp80121
I2047
asg10
(lp80122
sg163
(lp80123
ssS'ybe'
p80124
(dp80125
g318
(lp80126
I1118
assS'hz'
p80127
(dp80128
g174
(lp80129
sg332
(lp80130
sg22
(lp80131
sg78
(lp80132
sg106
(lp80133
I553
asg135
(lp80134
ssS'clip'
p80135
(dp80136
g114
(lp80137
I1216
assS'irwin'
p80138
(dp80139
g99
(lp80140
I2796
assS'cohen'
p80141
(dp80142
g46
(lp80143
sg118
(lp80144
sg87
(lp80145
sg4
(lp80146
I21
asg535
(lp80147
ssS'linker'
p80148
(dp80149
g10
(lp80150
I578
assS'jnumber'
p80151
(dp80152
g116
(lp80153
I1171
assS'pfinder'
p80154
(dp80155
g293
(lp80156
I3326
assS'coher'
p80157
(dp80158
g216
(lp80159
sg332
(lp80160
sg6
(lp80161
sg44
(lp80162
I532
asg149
(lp80163
ssS'counterstream'
p80164
(dp80165
g70
(lp80166
I2635
assS'disjoint'
p80167
(dp80168
g74
(lp80169
sg145
(lp80170
sg36
(lp80171
sg341
(lp80172
sg85
(lp80173
sg104
(lp80174
sg94
(lp80175
sg149
(lp80176
I2403
assS'soderstrom'
p80177
(dp80178
g295
(lp80179
I1674
asg183
(lp80180
ssS'predic'
p80181
(dp80182
g145
(lp80183
I1111
asg277
(lp80184
ssS'tishbi'
p80185
(dp80186
g85
(lp80187
sg140
(lp80188
I3207
asg130
(lp80189
ssS'hopfield'
p80190
(dp80191
g438
(lp80192
sg8
(lp80193
sg384
(lp80194
sg40
(lp80195
sg42
(lp80196
I3356
asg429
(lp80197
sg12
(lp80198
ssS'manand'
p80199
(dp80200
g230
(lp80201
I1733
assS'diverg'
p80202
(dp80203
g230
(lp80204
sg329
(lp80205
sg36
(lp80206
sg72
(lp80207
sg306
(lp80208
sg89
(lp80209
sg130
(lp80210
I1018
asg245
(lp80211
sg350
(lp80212
ssS'rout'
p80213
(dp80214
g42
(lp80215
I2323
asg14
(lp80216
sg329
(lp80217
sg8
(lp80218
ssS'which'
p80219
(dp80220
g80
(lp80221
sg293
(lp80222
sg344
(lp80223
sg78
(lp80224
sg59
(lp80225
sg484
(lp80226
sg38
(lp80227
sg83
(lp80228
sg85
(lp80229
sg303
(lp80230
sg438
(lp80231
sg116
(lp80232
sg118
(lp80233
sg34
(lp80234
sg36
(lp80235
sg460
(lp80236
sg68
(lp80237
sg72
(lp80238
sg281
(lp80239
sg10
(lp80240
sg40
(lp80241
sg283
(lp80242
sg70
(lp80243
sg26
(lp80244
sg277
(lp80245
sg163
(lp80246
sg89
(lp80247
sg91
(lp80248
sg12
(lp80249
sg94
(lp80250
sg96
(lp80251
sg48
(lp80252
sg99
(lp80253
sg313
(lp80254
sg44
(lp80255
sg149
(lp80256
sg429
(lp80257
sg102
(lp80258
sg104
(lp80259
sg106
(lp80260
sg108
(lp80261
sg110
(lp80262
sg63
(lp80263
sg52
(lp80264
sg114
(lp80265
sg128
(lp80266
sg130
(lp80267
sg132
(lp80268
sg14
(lp80269
sg16
(lp80270
sg135
(lp80271
sg50
(lp80272
sg138
(lp80273
sg140
(lp80274
sg354
(lp80275
sg306
(lp80276
sg87
(lp80277
sg245
(lp80278
sg46
(lp80279
sg20
(lp80280
sg18
(lp80281
sg221
(lp80282
sg535
(lp80283
sg223
(lp80284
sg350
(lp80285
sg216
(lp80286
sg174
(lp80287
sg440
(lp80288
sg332
(lp80289
sg121
(lp80290
sg4
(lp80291
sg6
(lp80292
sg8
(lp80293
sg126
(lp80294
sg341
(lp80295
sg30
(lp80296
sg287
(lp80297
sg74
(lp80298
sg176
(lp80299
sg145
(lp80300
sg256
(lp80301
sg76
(lp80302
sg262
(lp80303
sg295
(lp80304
sg183
(lp80305
sg42
(lp80306
I850
asg230
(lp80307
sg329
(lp80308
sg32
(lp80309
sg318
(lp80310
sg178
(lp80311
sg22
(lp80312
sg181
(lp80313
sg235
(lp80314
sg384
(lp80315
sg124
(lp80316
ssS'divers'
p80317
(dp80318
g277
(lp80319
sg235
(lp80320
sg102
(lp80321
sg48
(lp80322
I592
asg110
(lp80323
sg350
(lp80324
ssS'kahan'
p80325
(dp80326
g63
(lp80327
I164
assS'combat'
p80328
(dp80329
g245
(lp80330
I2662
assS'fdlpl'
p80331
(dp80332
g102
(lp80333
I311
assS'who'
p80334
(dp80335
g118
(lp80336
sg4
(lp80337
sg344
(lp80338
sg221
(lp80339
sg59
(lp80340
sg484
(lp80341
sg126
(lp80342
sg83
(lp80343
sg85
(lp80344
sg72
(lp80345
sg110
(lp80346
sg78
(lp80347
sg132
(lp80348
I3430
asg94
(lp80349
sg96
(lp80350
sg99
(lp80351
ssS'whi'
p80352
(dp80353
g216
(lp80354
sg70
(lp80355
sg74
(lp80356
sg176
(lp80357
sg121
(lp80358
sg26
(lp80359
sg277
(lp80360
sg262
(lp80361
sg34
(lp80362
sg303
(lp80363
sg12
(lp80364
sg132
(lp80365
I848
asg145
(lp80366
sg110
(lp80367
sg149
(lp80368
ssS'hu'
p80369
(dp80370
g83
(lp80371
I2400
assS'class'
p80372
(dp80373
g68
(lp80374
sg70
(lp80375
sg277
(lp80376
sg163
(lp80377
sg283
(lp80378
sg30
(lp80379
sg287
(lp80380
sg74
(lp80381
sg145
(lp80382
sg256
(lp80383
sg76
(lp80384
sg344
(lp80385
sg183
(lp80386
sg484
(lp80387
sg83
(lp80388
sg85
(lp80389
sg42
(lp80390
I3319
asg306
(lp80391
sg87
(lp80392
sg89
(lp80393
sg460
(lp80394
sg12
(lp80395
sg46
(lp80396
sg96
(lp80397
sg48
(lp80398
sg221
(lp80399
sg535
(lp80400
sg223
(lp80401
sg350
(lp80402
sg429
(lp80403
sg178
(lp80404
sg108
(lp80405
sg52
(lp80406
sg230
(lp80407
sg438
(lp80408
sg318
(lp80409
sg121
(lp80410
sg181
(lp80411
sg8
(lp80412
sg34
(lp80413
sg36
(lp80414
sg384
(lp80415
sg124
(lp80416
sg281
(lp80417
sg10
(lp80418
sg40
(lp80419
sg44
(lp80420
sg128
(lp80421
sg14
(lp80422
sg16
(lp80423
sg135
(lp80424
sg50
(lp80425
sg138
(lp80426
ssS'logea'
p80427
(dp80428
g318
(lp80429
I1604
assS'hk'
p80430
(dp80431
g72
(lp80432
I19
assS'guarantte'
p80433
(dp80434
g12
(lp80435
I1294
assS'hh'
p80436
(dp80437
g6
(lp80438
I991
assS'fick'
p80439
(dp80440
g32
(lp80441
I1421
assS'interocular'
p80442
(dp80443
g350
(lp80444
I415
assS'determin'
p80445
(dp80446
g124
(lp80447
sg70
(lp80448
sg78
(lp80449
sg277
(lp80450
sg72
(lp80451
sg68
(lp80452
sg283
(lp80453
sg36
(lp80454
sg303
(lp80455
sg26
(lp80456
sg30
(lp80457
sg176
(lp80458
sg145
(lp80459
sg80
(lp80460
sg76
(lp80461
sg262
(lp80462
sg295
(lp80463
sg183
(lp80464
sg59
(lp80465
sg484
(lp80466
sg38
(lp80467
sg83
(lp80468
sg85
(lp80469
sg63
(lp80470
sg42
(lp80471
I405
asg91
(lp80472
sg12
(lp80473
sg94
(lp80474
sg96
(lp80475
sg18
(lp80476
sg99
(lp80477
sg535
(lp80478
sg44
(lp80479
sg149
(lp80480
sg118
(lp80481
sg329
(lp80482
sg293
(lp80483
sg32
(lp80484
sg245
(lp80485
sg429
(lp80486
sg318
(lp80487
sg46
(lp80488
sg102
(lp80489
sg104
(lp80490
sg106
(lp80491
sg110
(lp80492
sg20
(lp80493
sg230
(lp80494
sg174
(lp80495
sg440
(lp80496
sg332
(lp80497
sg121
(lp80498
sg4
(lp80499
sg6
(lp80500
sg8
(lp80501
sg34
(lp80502
sg221
(lp80503
sg460
(lp80504
sg235
(lp80505
sg126
(lp80506
sg281
(lp80507
sg10
(lp80508
sg40
(lp80509
sg344
(lp80510
sg223
(lp80511
sg128
(lp80512
sg130
(lp80513
sg132
(lp80514
sg14
(lp80515
sg16
(lp80516
sg350
(lp80517
sg138
(lp80518
sg354
(lp80519
ssS'guinea'
p80520
(dp80521
g106
(lp80522
I2996
assS'syring'
p80523
(dp80524
g116
(lp80525
I1618
assS'nmin'
p80526
(dp80527
g438
(lp80528
I1431
assS'gain'
p80529
(dp80530
g283
(lp80531
sg26
(lp80532
sg74
(lp80533
sg176
(lp80534
sg145
(lp80535
sg256
(lp80536
sg262
(lp80537
sg295
(lp80538
sg183
(lp80539
sg59
(lp80540
sg484
(lp80541
sg85
(lp80542
sg87
(lp80543
sg89
(lp80544
sg91
(lp80545
sg245
(lp80546
sg94
(lp80547
sg20
(lp80548
sg313
(lp80549
sg350
(lp80550
sg12
(lp80551
sg429
(lp80552
sg102
(lp80553
sg178
(lp80554
sg108
(lp80555
sg118
(lp80556
sg440
(lp80557
sg318
(lp80558
sg121
(lp80559
sg4
(lp80560
sg235
(lp80561
sg36
(lp80562
sg68
(lp80563
sg126
(lp80564
sg10
(lp80565
sg135
(lp80566
sg50
(lp80567
sg138
(lp80568
sg140
(lp80569
sg354
(lp80570
I41
assS'dorothi'
p80571
(dp80572
g114
(lp80573
I9
assS'perfon'
p80574
(dp80575
g110
(lp80576
I1502
assS'dijf'
p80577
(dp80578
g535
(lp80579
I1689
assS'perfor'
p80580
(dp80581
g106
(lp80582
I2584
assS'technician'
p80583
(dp80584
g104
(lp80585
I2825
assS'atiya'
p80586
(dp80587
g68
(lp80588
I3327
assS'envisag'
p80589
(dp80590
g87
(lp80591
I723
assS'utter'
p80592
(dp80593
g174
(lp80594
sg96
(lp80595
I429
asg87
(lp80596
sg440
(lp80597
ssS'fear'
p80598
(dp80599
g126
(lp80600
I109
assS'feat'
p80601
(dp80602
g59
(lp80603
I641
assS'nearer'
p80604
(dp80605
g138
(lp80606
I2387
assS'ha'
p80607
(dp80608
g72
(lp80609
sg429
(lp80610
sg178
(lp80611
sg130
(lp80612
I1536
assS'locat'
p80613
(dp80614
g283
(lp80615
sg70
(lp80616
sg78
(lp80617
sg277
(lp80618
sg303
(lp80619
sg30
(lp80620
sg176
(lp80621
sg80
(lp80622
sg293
(lp80623
sg295
(lp80624
sg183
(lp80625
sg59
(lp80626
sg83
(lp80627
sg63
(lp80628
sg42
(lp80629
I657
asg245
(lp80630
sg46
(lp80631
sg20
(lp80632
sg48
(lp80633
sg223
(lp80634
sg149
(lp80635
sg118
(lp80636
sg429
(lp80637
sg94
(lp80638
sg178
(lp80639
sg108
(lp80640
sg96
(lp80641
sg52
(lp80642
sg216
(lp80643
sg438
(lp80644
sg121
(lp80645
sg22
(lp80646
sg8
(lp80647
sg34
(lp80648
sg384
(lp80649
sg130
(lp80650
sg14
(lp80651
sg16
(lp80652
sg350
(lp80653
sg138
(lp80654
sg354
(lp80655
ssS'straightfol'
p80656
(dp80657
g181
(lp80658
I839
assS'kaynak'
p80659
(dp80660
g178
(lp80661
I2447
assS'holdsworth'
p80662
(dp80663
g174
(lp80664
I451
assS'hg'
p80665
(dp80666
g130
(lp80667
I1534
assS'lemma'
p80668
(dp80669
g344
(lp80670
sg36
(lp80671
sg341
(lp80672
I1913
asg40
(lp80673
ssS'smallest'
p80674
(dp80675
g34
(lp80676
sg124
(lp80677
sg126
(lp80678
sg281
(lp80679
sg40
(lp80680
sg344
(lp80681
sg14
(lp80682
sg16
(lp80683
sg20
(lp80684
sg44
(lp80685
sg354
(lp80686
I2310
assS'micron'
p80687
(dp80688
g14
(lp80689
I3214
assS'prespecifi'
p80690
(dp80691
g277
(lp80692
I637
assS'he'
p80693
(dp80694
g438
(lp80695
sg4
(lp80696
sg174
(lp80697
sg344
(lp80698
sg221
(lp80699
sg460
(lp80700
sg124
(lp80701
sg42
(lp80702
I1965
asg110
(lp80703
sg130
(lp80704
sg132
(lp80705
sg94
(lp80706
sg48
(lp80707
sg99
(lp80708
sg59
(lp80709
sg52
(lp80710
ssS'local'
p80711
(dp80712
g68
(lp80713
sg70
(lp80714
sg26
(lp80715
sg72
(lp80716
sg283
(lp80717
sg460
(lp80718
sg30
(lp80719
sg74
(lp80720
sg176
(lp80721
sg256
(lp80722
sg80
(lp80723
sg118
(lp80724
sg295
(lp80725
sg183
(lp80726
sg59
(lp80727
sg42
(lp80728
I1079
asg306
(lp80729
sg89
(lp80730
sg91
(lp80731
sg245
(lp80732
sg46
(lp80733
sg96
(lp80734
sg48
(lp80735
sg221
(lp80736
sg313
(lp80737
sg223
(lp80738
sg149
(lp80739
sg116
(lp80740
sg329
(lp80741
sg32
(lp80742
sg429
(lp80743
sg102
(lp80744
sg178
(lp80745
sg108
(lp80746
sg52
(lp80747
sg216
(lp80748
sg438
(lp80749
sg440
(lp80750
sg332
(lp80751
sg121
(lp80752
sg181
(lp80753
sg6
(lp80754
sg8
(lp80755
sg34
(lp80756
sg36
(lp80757
sg384
(lp80758
sg124
(lp80759
sg126
(lp80760
sg341
(lp80761
sg128
(lp80762
sg130
(lp80763
sg132
(lp80764
sg14
(lp80765
sg16
(lp80766
sg50
(lp80767
sg138
(lp80768
sg354
(lp80769
ssS'vigor'
p80770
(dp80771
g135
(lp80772
I302
assS'chipl'
p80773
(dp80774
g14
(lp80775
I4155
assS'contribut'
p80776
(dp80777
g287
(lp80778
sg74
(lp80779
sg176
(lp80780
sg145
(lp80781
sg80
(lp80782
sg183
(lp80783
sg59
(lp80784
sg38
(lp80785
sg89
(lp80786
sg46
(lp80787
sg48
(lp80788
sg535
(lp80789
sg350
(lp80790
sg174
(lp80791
sg102
(lp80792
sg104
(lp80793
sg63
(lp80794
sg216
(lp80795
sg438
(lp80796
I2356
asg332
(lp80797
sg6
(lp80798
sg181
(lp80799
sg235
(lp80800
sg124
(lp80801
sg10
(lp80802
sg40
(lp80803
sg130
(lp80804
sg132
(lp80805
sg14
(lp80806
sg16
(lp80807
sg138
(lp80808
sg140
(lp80809
ssS'jekl'
p80810
(dp80811
g130
(lp80812
I1053
assS'rgen'
p80813
(dp80814
g36
(lp80815
I937
assS'suen'
p80816
(dp80817
g42
(lp80818
I3423
asg63
(lp80819
ssS'cerebellar'
p80820
(dp80821
g350
(lp80822
I2813
assS'catecholaminerg'
p80823
(dp80824
g4
(lp80825
I2887
assS'passband'
p80826
(dp80827
g22
(lp80828
I1959
assS'psalti'
p80829
(dp80830
g281
(lp80831
I1118
assS'meguro'
p80832
(dp80833
g18
(lp80834
I24
assS'qp'
p80835
(dp80836
g76
(lp80837
I2529
assS'qu'
p80838
(dp80839
g38
(lp80840
I741
assS'qt'
p80841
(dp80842
g230
(lp80843
sg46
(lp80844
I1539
asg87
(lp80845
sg34
(lp80846
sg262
(lp80847
ssS'bisect'
p80848
(dp80849
g303
(lp80850
I2526
assS'yaleu'
p80851
(dp80852
g429
(lp80853
sg8
(lp80854
I2632
assS'displac'
p80855
(dp80856
g118
(lp80857
sg32
(lp80858
sg256
(lp80859
sg68
(lp80860
sg102
(lp80861
sg138
(lp80862
I1787
assS'qa'
p80863
(dp80864
g72
(lp80865
sg262
(lp80866
I1182
assS'macintyr'
p80867
(dp80868
g287
(lp80869
I1153
assS'qc'
p80870
(dp80871
g102
(lp80872
I2722
assS'qe'
p80873
(dp80874
g34
(lp80875
sg32
(lp80876
I1829
assS'qd'
p80877
(dp80878
g460
(lp80879
sg87
(lp80880
I894
assS'partit'
p80881
(dp80882
g30
(lp80883
sg329
(lp80884
sg332
(lp80885
sg8
(lp80886
sg295
(lp80887
sg183
(lp80888
sg460
(lp80889
sg484
(lp80890
sg126
(lp80891
sg85
(lp80892
sg287
(lp80893
sg36
(lp80894
sg163
(lp80895
sg128
(lp80896
sg130
(lp80897
I1788
asg245
(lp80898
sg104
(lp80899
sg221
(lp80900
sg46
(lp80901
sg52
(lp80902
ssS'qf'
p80903
(dp80904
g34
(lp80905
I1903
assS'qi'
p80906
(dp80907
g76
(lp80908
sg460
(lp80909
sg38
(lp80910
sg87
(lp80911
sg102
(lp80912
I2366
asg46
(lp80913
ssS'qk'
p80914
(dp80915
g440
(lp80916
I396
assS'qj'
p80917
(dp80918
g438
(lp80919
I1922
asg460
(lp80920
sg293
(lp80921
ssS'qm'
p80922
(dp80923
g102
(lp80924
I1096
asg72
(lp80925
ssS'ql'
p80926
(dp80927
g102
(lp80928
I457
asg440
(lp80929
sg76
(lp80930
ssS'view'
p80931
(dp80932
g26
(lp80933
sg74
(lp80934
sg80
(lp80935
sg262
(lp80936
sg295
(lp80937
sg183
(lp80938
sg59
(lp80939
sg303
(lp80940
sg306
(lp80941
sg460
(lp80942
sg12
(lp80943
sg96
(lp80944
sg18
(lp80945
sg99
(lp80946
sg223
(lp80947
sg350
(lp80948
sg293
(lp80949
sg429
(lp80950
sg102
(lp80951
sg108
(lp80952
sg110
(lp80953
sg216
(lp80954
sg329
(lp80955
sg32
(lp80956
sg332
(lp80957
sg121
(lp80958
sg181
(lp80959
sg34
(lp80960
sg384
(lp80961
sg124
(lp80962
sg72
(lp80963
sg344
(lp80964
sg132
(lp80965
sg50
(lp80966
sg138
(lp80967
sg354
(lp80968
I450
assS'signatur'
p80969
(dp80970
g114
(lp80971
I7
assS'ebl'
p80972
(dp80973
g132
(lp80974
I1052
assS'knowledg'
p80975
(dp80976
g283
(lp80977
sg277
(lp80978
sg30
(lp80979
sg76
(lp80980
sg293
(lp80981
sg295
(lp80982
sg183
(lp80983
sg83
(lp80984
sg85
(lp80985
sg42
(lp80986
I467
asg91
(lp80987
sg46
(lp80988
sg221
(lp80989
sg223
(lp80990
sg429
(lp80991
sg94
(lp80992
sg108
(lp80993
sg235
(lp80994
sg460
(lp80995
sg126
(lp80996
sg344
(lp80997
sg132
(lp80998
sg14
(lp80999
sg16
(lp81000
sg50
(lp81001
sg138
(lp81002
sg140
(lp81003
ssS'elast'
p81004
(dp81005
g42
(lp81006
I1617
asg138
(lp81007
sg8
(lp81008
ssS'bptt'
p81009
(dp81010
g128
(lp81011
I1298
assS'troubl'
p81012
(dp81013
g329
(lp81014
sg89
(lp81015
I1507
assS'closer'
p81016
(dp81017
g440
(lp81018
sg318
(lp81019
sg181
(lp81020
sg8
(lp81021
sg40
(lp81022
sg94
(lp81023
sg48
(lp81024
sg138
(lp81025
I2345
asg44
(lp81026
sg149
(lp81027
ssS'dlb'
p81028
(dp81029
g83
(lp81030
I2092
assS'reticulari'
p81031
(dp81032
g350
(lp81033
I1197
assS'cole'
p81034
(dp81035
g174
(lp81036
I167
asg440
(lp81037
ssS'dlf'
p81038
(dp81039
g132
(lp81040
I2256
assS'limho'
p81041
(dp81042
g287
(lp81043
I2953
assS'dlm'
p81044
(dp81045
g116
(lp81046
I366
assS'sekr'
p81047
(dp81048
g34
(lp81049
I13
assS'favor'
p81050
(dp81051
g329
(lp81052
sg74
(lp81053
sg26
(lp81054
sg295
(lp81055
sg183
(lp81056
sg281
(lp81057
sg303
(lp81058
sg102
(lp81059
sg429
(lp81060
sg110
(lp81061
sg128
(lp81062
sg132
(lp81063
sg96
(lp81064
sg48
(lp81065
sg221
(lp81066
sg63
(lp81067
sg354
(lp81068
I2445
assS'physiolog'
p81069
(dp81070
g116
(lp81071
sg438
(lp81072
I1240
asg332
(lp81073
sg4
(lp81074
sg80
(lp81075
sg174
(lp81076
sg303
(lp81077
sg176
(lp81078
sg245
(lp81079
sg99
(lp81080
sg63
(lp81081
sg350
(lp81082
ssS'dlv'
p81083
(dp81084
g20
(lp81085
I1574
assS'crude'
p81086
(dp81087
g176
(lp81088
sg138
(lp81089
I1586
asg181
(lp81090
sg235
(lp81091
ssS'chemic'
p81092
(dp81093
g438
(lp81094
I2288
asg26
(lp81095
sg354
(lp81096
ssS'neurodynam'
p81097
(dp81098
g332
(lp81099
I12
asg535
(lp81100
ssS'thalamus'
p81101
(dp81102
g116
(lp81103
I372
asg70
(lp81104
sg303
(lp81105
ssS'speed'
p81106
(dp81107
g283
(lp81108
sg74
(lp81109
sg256
(lp81110
sg76
(lp81111
sg295
(lp81112
sg183
(lp81113
sg83
(lp81114
sg42
(lp81115
I602
asg87
(lp81116
sg89
(lp81117
sg245
(lp81118
sg94
(lp81119
sg20
(lp81120
sg44
(lp81121
sg350
(lp81122
sg429
(lp81123
sg63
(lp81124
sg52
(lp81125
sg114
(lp81126
sg318
(lp81127
sg8
(lp81128
sg34
(lp81129
sg384
(lp81130
sg128
(lp81131
sg14
(lp81132
sg16
(lp81133
sg460
(lp81134
ssS'jor'
p81135
(dp81136
g83
(lp81137
I1422
assS'exam'
p81138
(dp81139
g48
(lp81140
I794
assS'amen'
p81141
(dp81142
g283
(lp81143
sg121
(lp81144
I2326
assS'aslam'
p81145
(dp81146
g344
(lp81147
I2076
assS'autoassociatorbas'
p81148
(dp81149
g78
(lp81150
I2950
assS'soffki'
p81151
(dp81152
g70
(lp81153
I247
assS'job'
p81154
(dp81155
g132
(lp81156
I3363
assS'entir'
p81157
(dp81158
g30
(lp81159
sg80
(lp81160
sg295
(lp81161
sg183
(lp81162
sg59
(lp81163
sg484
(lp81164
sg85
(lp81165
sg89
(lp81166
sg245
(lp81167
sg94
(lp81168
sg223
(lp81169
sg329
(lp81170
sg429
(lp81171
sg104
(lp81172
sg108
(lp81173
sg63
(lp81174
sg52
(lp81175
sg114
(lp81176
sg174
(lp81177
sg332
(lp81178
sg4
(lp81179
sg181
(lp81180
sg235
(lp81181
sg384
(lp81182
sg68
(lp81183
sg72
(lp81184
sg132
(lp81185
sg14
(lp81186
sg140
(lp81187
I255
assS'ringuett'
p81188
(dp81189
g89
(lp81190
I2330
assS'amer'
p81191
(dp81192
g102
(lp81193
sg99
(lp81194
I3343
assS'swift'
p81195
(dp81196
g277
(lp81197
I240
assS'addit'
p81198
(dp81199
g68
(lp81200
sg78
(lp81201
sg277
(lp81202
sg163
(lp81203
sg72
(lp81204
sg283
(lp81205
sg303
(lp81206
sg30
(lp81207
sg74
(lp81208
sg145
(lp81209
sg76
(lp81210
sg262
(lp81211
sg295
(lp81212
sg183
(lp81213
sg59
(lp81214
sg83
(lp81215
sg85
(lp81216
sg63
(lp81217
sg306
(lp81218
sg91
(lp81219
sg12
(lp81220
sg94
(lp81221
sg20
(lp81222
sg48
(lp81223
sg221
(lp81224
sg223
(lp81225
sg149
(lp81226
sg293
(lp81227
sg32
(lp81228
sg245
(lp81229
sg429
(lp81230
sg318
(lp81231
sg102
(lp81232
sg18
(lp81233
sg106
(lp81234
sg110
(lp81235
sg178
(lp81236
sg22
(lp81237
sg230
(lp81238
sg438
(lp81239
I1491
asg440
(lp81240
sg332
(lp81241
sg121
(lp81242
sg4
(lp81243
sg6
(lp81244
sg235
(lp81245
sg34
(lp81246
sg460
(lp81247
sg124
(lp81248
sg126
(lp81249
sg341
(lp81250
sg10
(lp81251
sg40
(lp81252
sg344
(lp81253
sg130
(lp81254
sg132
(lp81255
sg14
(lp81256
sg16
(lp81257
sg135
(lp81258
sg50
(lp81259
sg138
(lp81260
sg354
(lp81261
ssS'slotin'
p81262
(dp81263
g230
(lp81264
I364
assS'teukolski'
p81265
(dp81266
g34
(lp81267
I2900
assS'duplex'
p81268
(dp81269
g174
(lp81270
I2641
assS'bachmann'
p81271
(dp81272
g438
(lp81273
I1856
assS'committe'
p81274
(dp81275
g484
(lp81276
sg38
(lp81277
sg140
(lp81278
I36
assS'bronx'
p81279
(dp81280
g106
(lp81281
I305
assS'articul'
p81282
(dp81283
g440
(lp81284
I923
asg87
(lp81285
sg181
(lp81286
ssS'unclear'
p81287
(dp81288
g132
(lp81289
sg140
(lp81290
I1705
assS'rpt'
p81291
(dp81292
g104
(lp81293
sg135
(lp81294
I2658
assS'wall'
p81295
(dp81296
g176
(lp81297
sg80
(lp81298
sg293
(lp81299
sg8
(lp81300
sg42
(lp81301
I2576
asg87
(lp81302
ssS'gtl'
p81303
(dp81304
g34
(lp81305
I1235
assS'hyphen'
p81306
(dp81307
g440
(lp81308
I2151
assS'toint'
p81309
(dp81310
g8
(lp81311
I2511
assS'arriv'
p81312
(dp81313
g438
(lp81314
sg32
(lp81315
sg70
(lp81316
sg76
(lp81317
sg174
(lp81318
sg34
(lp81319
sg36
(lp81320
sg83
(lp81321
sg42
(lp81322
I1475
asg130
(lp81323
sg223
(lp81324
ssS'walk'
p81325
(dp81326
g277
(lp81327
sg293
(lp81328
sg124
(lp81329
sg126
(lp81330
sg18
(lp81331
sg99
(lp81332
I2288
asg350
(lp81333
ssS'respect'
p81334
(dp81335
g68
(lp81336
sg163
(lp81337
sg72
(lp81338
sg281
(lp81339
sg36
(lp81340
sg30
(lp81341
sg287
(lp81342
sg74
(lp81343
sg176
(lp81344
sg145
(lp81345
sg76
(lp81346
sg293
(lp81347
sg295
(lp81348
sg183
(lp81349
sg484
(lp81350
sg85
(lp81351
sg303
(lp81352
sg42
(lp81353
I1446
asg306
(lp81354
sg87
(lp81355
sg91
(lp81356
sg12
(lp81357
sg94
(lp81358
sg96
(lp81359
sg48
(lp81360
sg99
(lp81361
sg350
(lp81362
sg230
(lp81363
sg118
(lp81364
sg245
(lp81365
sg429
(lp81366
sg46
(lp81367
sg104
(lp81368
sg108
(lp81369
sg20
(lp81370
sg52
(lp81371
sg216
(lp81372
sg329
(lp81373
sg318
(lp81374
sg178
(lp81375
sg4
(lp81376
sg6
(lp81377
sg221
(lp81378
sg124
(lp81379
sg126
(lp81380
sg341
(lp81381
sg10
(lp81382
sg344
(lp81383
sg128
(lp81384
sg130
(lp81385
sg132
(lp81386
sg14
(lp81387
sg16
(lp81388
sg135
(lp81389
sg140
(lp81390
sg354
(lp81391
ssS'cauchi'
p81392
(dp81393
g306
(lp81394
I2310
asg318
(lp81395
ssS'cienc'
p81396
(dp81397
g318
(lp81398
I2862
assS'worryfre'
p81399
(dp81400
g22
(lp81401
I890
assS'decent'
p81402
(dp81403
g440
(lp81404
I2713
assS'bbd'
p81405
(dp81406
g135
(lp81407
I683
assS'compos'
p81408
(dp81409
g230
(lp81410
sg440
(lp81411
sg4
(lp81412
sg36
(lp81413
sg10
(lp81414
sg42
(lp81415
I917
asg429
(lp81416
sg104
(lp81417
sg245
(lp81418
sg94
(lp81419
sg20
(lp81420
sg18
(lp81421
sg99
(lp81422
sg535
(lp81423
ssS'compon'
p81424
(dp81425
g68
(lp81426
sg78
(lp81427
sg163
(lp81428
sg72
(lp81429
sg30
(lp81430
sg74
(lp81431
sg293
(lp81432
sg295
(lp81433
sg183
(lp81434
sg59
(lp81435
sg38
(lp81436
sg83
(lp81437
sg306
(lp81438
sg91
(lp81439
sg245
(lp81440
sg46
(lp81441
sg96
(lp81442
sg48
(lp81443
sg99
(lp81444
sg350
(lp81445
sg118
(lp81446
sg332
(lp81447
sg102
(lp81448
sg104
(lp81449
sg108
(lp81450
sg20
(lp81451
sg22
(lp81452
sg116
(lp81453
sg438
(lp81454
I807
asg32
(lp81455
sg318
(lp81456
sg4
(lp81457
sg6
(lp81458
sg8
(lp81459
sg34
(lp81460
sg384
(lp81461
sg235
(lp81462
sg126
(lp81463
sg281
(lp81464
sg10
(lp81465
sg130
(lp81466
sg132
(lp81467
sg460
(lp81468
ssS'besid'
p81469
(dp81470
g12
(lp81471
sg59
(lp81472
sg341
(lp81473
sg128
(lp81474
I2571
asg163
(lp81475
ssS'heavisid'
p81476
(dp81477
g287
(lp81478
I643
assS'recanzon'
p81479
(dp81480
g176
(lp81481
I1468
assS'mike'
p81482
(dp81483
g14
(lp81484
sg16
(lp81485
I38
asg87
(lp81486
sg341
(lp81487
sg76
(lp81488
ssS'newsom'
p81489
(dp81490
g6
(lp81491
I2198
asg262
(lp81492
ssS'presenc'
p81493
(dp81494
g216
(lp81495
sg174
(lp81496
sg230
(lp81497
sg6
(lp81498
sg235
(lp81499
sg78
(lp81500
sg460
(lp81501
sg484
(lp81502
sg91
(lp81503
sg181
(lp81504
sg48
(lp81505
sg99
(lp81506
I1584
assS'coxt'
p81507
(dp81508
g20
(lp81509
I1253
assS'hml'
p81510
(dp81511
g72
(lp81512
I1449
assS'hmm'
p81513
(dp81514
g440
(lp81515
sg76
(lp81516
sg460
(lp81517
sg72
(lp81518
sg87
(lp81519
sg96
(lp81520
I85
assS'hmo'
p81521
(dp81522
g281
(lp81523
I2257
assS'present'
p81524
(dp81525
g329
(lp81526
sg70
(lp81527
sg26
(lp81528
sg277
(lp81529
sg163
(lp81530
sg72
(lp81531
sg68
(lp81532
sg85
(lp81533
sg40
(lp81534
sg30
(lp81535
sg350
(lp81536
sg74
(lp81537
sg176
(lp81538
sg145
(lp81539
sg80
(lp81540
sg76
(lp81541
sg118
(lp81542
sg460
(lp81543
sg78
(lp81544
sg59
(lp81545
sg484
(lp81546
sg38
(lp81547
sg83
(lp81548
sg114
(lp81549
sg124
(lp81550
sg306
(lp81551
sg87
(lp81552
sg89
(lp81553
sg91
(lp81554
sg12
(lp81555
sg46
(lp81556
sg96
(lp81557
sg48
(lp81558
sg99
(lp81559
sg313
(lp81560
sg44
(lp81561
sg149
(lp81562
sg230
(lp81563
sg174
(lp81564
sg293
(lp81565
sg116
(lp81566
sg32
(lp81567
sg245
(lp81568
sg318
(lp81569
sg102
(lp81570
sg104
(lp81571
sg106
(lp81572
sg108
(lp81573
sg110
(lp81574
sg20
(lp81575
sg52
(lp81576
sg22
(lp81577
sg216
(lp81578
sg438
(lp81579
I59
asg440
(lp81580
sg332
(lp81581
sg121
(lp81582
sg4
(lp81583
sg181
(lp81584
sg8
(lp81585
sg34
(lp81586
sg36
(lp81587
sg384
(lp81588
sg235
(lp81589
sg126
(lp81590
sg10
(lp81591
sg535
(lp81592
sg344
(lp81593
sg63
(lp81594
sg223
(lp81595
sg128
(lp81596
sg130
(lp81597
sg132
(lp81598
sg14
(lp81599
sg16
(lp81600
sg135
(lp81601
sg50
(lp81602
sg138
(lp81603
sg303
(lp81604
ssS'hmc'
p81605
(dp81606
g124
(lp81607
I1615
assS'reichardt'
p81608
(dp81609
g245
(lp81610
I1429
assS'vanilla'
p81611
(dp81612
g128
(lp81613
I955
assS'cursor'
p81614
(dp81615
g94
(lp81616
I2756
assS'wild'
p81617
(dp81618
g116
(lp81619
sg63
(lp81620
sg128
(lp81621
I2686
assS'abumostafa'
p81622
(dp81623
g110
(lp81624
I2981
assS'observ'
p81625
(dp81626
g68
(lp81627
sg277
(lp81628
sg163
(lp81629
sg281
(lp81630
sg181
(lp81631
sg74
(lp81632
sg176
(lp81633
sg145
(lp81634
sg256
(lp81635
sg76
(lp81636
sg262
(lp81637
sg78
(lp81638
sg80
(lp81639
sg38
(lp81640
sg83
(lp81641
sg85
(lp81642
sg42
(lp81643
I123
asg87
(lp81644
sg91
(lp81645
sg245
(lp81646
sg48
(lp81647
sg99
(lp81648
sg313
(lp81649
sg223
(lp81650
sg149
(lp81651
sg230
(lp81652
sg293
(lp81653
sg32
(lp81654
sg12
(lp81655
sg104
(lp81656
sg106
(lp81657
sg108
(lp81658
sg216
(lp81659
sg440
(lp81660
sg121
(lp81661
sg4
(lp81662
sg6
(lp81663
sg8
(lp81664
sg36
(lp81665
sg384
(lp81666
sg124
(lp81667
sg341
(lp81668
sg10
(lp81669
sg40
(lp81670
sg128
(lp81671
sg132
(lp81672
sg350
(lp81673
sg50
(lp81674
sg460
(lp81675
sg354
(lp81676
ssS'layer'
p81677
(dp81678
g283
(lp81679
sg70
(lp81680
sg26
(lp81681
sg277
(lp81682
sg163
(lp81683
sg256
(lp81684
sg76
(lp81685
sg295
(lp81686
sg183
(lp81687
sg38
(lp81688
sg85
(lp81689
sg303
(lp81690
sg87
(lp81691
sg89
(lp81692
sg245
(lp81693
sg94
(lp81694
sg96
(lp81695
sg18
(lp81696
sg535
(lp81697
sg329
(lp81698
sg102
(lp81699
sg178
(lp81700
sg106
(lp81701
sg108
(lp81702
sg110
(lp81703
sg63
(lp81704
sg52
(lp81705
sg114
(lp81706
sg116
(lp81707
sg438
(lp81708
I6
asg440
(lp81709
sg48
(lp81710
sg121
(lp81711
sg4
(lp81712
sg8
(lp81713
sg36
(lp81714
sg460
(lp81715
sg68
(lp81716
sg126
(lp81717
sg341
(lp81718
sg10
(lp81719
sg40
(lp81720
sg344
(lp81721
sg128
(lp81722
sg78
(lp81723
sg14
(lp81724
sg16
(lp81725
sg135
(lp81726
sg50
(lp81727
sg138
(lp81728
sg140
(lp81729
ssS'kawashima'
p81730
(dp81731
g99
(lp81732
I3061
assS'relabel'
p81733
(dp81734
g87
(lp81735
I1777
asg38
(lp81736
ssS'thus'
p81737
(dp81738
g329
(lp81739
sg70
(lp81740
sg26
(lp81741
sg163
(lp81742
sg72
(lp81743
sg281
(lp81744
sg283
(lp81745
sg181
(lp81746
sg287
(lp81747
sg74
(lp81748
sg145
(lp81749
sg256
(lp81750
sg76
(lp81751
sg262
(lp81752
sg295
(lp81753
sg183
(lp81754
sg59
(lp81755
sg80
(lp81756
sg85
(lp81757
sg124
(lp81758
sg42
(lp81759
I342
asg306
(lp81760
sg12
(lp81761
sg46
(lp81762
sg96
(lp81763
sg48
(lp81764
sg221
(lp81765
sg44
(lp81766
sg149
(lp81767
sg116
(lp81768
sg174
(lp81769
sg293
(lp81770
sg32
(lp81771
sg178
(lp81772
sg245
(lp81773
sg68
(lp81774
sg102
(lp81775
sg104
(lp81776
sg106
(lp81777
sg108
(lp81778
sg110
(lp81779
sg20
(lp81780
sg52
(lp81781
sg114
(lp81782
sg216
(lp81783
sg438
(lp81784
sg440
(lp81785
sg121
(lp81786
sg4
(lp81787
sg6
(lp81788
sg8
(lp81789
sg34
(lp81790
sg460
(lp81791
sg235
(lp81792
sg126
(lp81793
sg341
(lp81794
sg118
(lp81795
sg344
(lp81796
sg63
(lp81797
sg128
(lp81798
sg132
(lp81799
sg14
(lp81800
sg135
(lp81801
sg50
(lp81802
sg140
(lp81803
ssS'avr'
p81804
(dp81805
g256
(lp81806
I1027
assS'motiv'
p81807
(dp81808
g287
(lp81809
sg440
(lp81810
sg318
(lp81811
sg121
(lp81812
sg78
(lp81813
sg40
(lp81814
sg74
(lp81815
sg281
(lp81816
sg306
(lp81817
sg91
(lp81818
sg102
(lp81819
sg145
(lp81820
sg354
(lp81821
I193
asg110
(lp81822
sg63
(lp81823
sg350
(lp81824
ssS'dual'
p81825
(dp81826
g72
(lp81827
I3007
assS'ave'
p81828
(dp81829
g344
(lp81830
sg484
(lp81831
sg70
(lp81832
sg262
(lp81833
I986
assS'fki'
p81834
(dp81835
g30
(lp81836
sg18
(lp81837
I1727
assS'uniti'
p81838
(dp81839
g329
(lp81840
sg22
(lp81841
sg6
(lp81842
sg8
(lp81843
sg183
(lp81844
sg429
(lp81845
sg102
(lp81846
sg108
(lp81847
I1938
asg221
(lp81848
ssS'fko'
p81849
(dp81850
g384
(lp81851
I447
assS'cine'
p81852
(dp81853
g116
(lp81854
I148
assS'cross'
p81855
(dp81856
g70
(lp81857
sg26
(lp81858
sg277
(lp81859
sg72
(lp81860
sg262
(lp81861
sg295
(lp81862
sg183
(lp81863
sg484
(lp81864
sg85
(lp81865
sg303
(lp81866
sg42
(lp81867
I3063
asg87
(lp81868
sg245
(lp81869
sg94
(lp81870
sg18
(lp81871
sg223
(lp81872
sg350
(lp81873
sg329
(lp81874
sg32
(lp81875
sg174
(lp81876
sg440
(lp81877
sg318
(lp81878
sg178
(lp81879
sg4
(lp81880
sg6
(lp81881
sg235
(lp81882
sg36
(lp81883
sg124
(lp81884
sg126
(lp81885
sg118
(lp81886
sg344
(lp81887
sg78
(lp81888
sg14
(lp81889
sg16
(lp81890
sg140
(lp81891
ssS'member'
p81892
(dp81893
g30
(lp81894
sg174
(lp81895
sg74
(lp81896
sg277
(lp81897
sg235
(lp81898
sg183
(lp81899
sg484
(lp81900
sg140
(lp81901
I62
asg46
(lp81902
sg223
(lp81903
ssS'nod'
p81904
(dp81905
g350
(lp81906
I1259
assS'largest'
p81907
(dp81908
g440
(lp81909
sg318
(lp81910
sg145
(lp81911
sg26
(lp81912
sg8
(lp81913
sg34
(lp81914
sg183
(lp81915
sg124
(lp81916
sg126
(lp81917
sg344
(lp81918
sg91
(lp81919
sg102
(lp81920
sg20
(lp81921
sg293
(lp81922
sg99
(lp81923
sg140
(lp81924
I2340
asg38
(lp81925
ssS'frqm'
p81926
(dp81927
g293
(lp81928
I1451
assS'contactless'
p81929
(dp81930
g59
(lp81931
I324
assS'cmajl'
p81932
(dp81933
g145
(lp81934
I2668
assS'disorient'
p81935
(dp81936
g80
(lp81937
I2281
assS'difficult'
p81938
(dp81939
g68
(lp81940
sg277
(lp81941
sg283
(lp81942
sg74
(lp81943
sg344
(lp81944
sg183
(lp81945
sg484
(lp81946
sg83
(lp81947
sg85
(lp81948
sg42
(lp81949
I501
asg87
(lp81950
sg91
(lp81951
sg12
(lp81952
sg99
(lp81953
sg350
(lp81954
sg32
(lp81955
sg332
(lp81956
sg63
(lp81957
sg52
(lp81958
sg230
(lp81959
sg329
(lp81960
sg440
(lp81961
sg318
(lp81962
sg121
(lp81963
sg181
(lp81964
sg8
(lp81965
sg124
(lp81966
sg281
(lp81967
sg40
(lp81968
sg130
(lp81969
sg14
(lp81970
sg16
(lp81971
sg135
(lp81972
sg138
(lp81973
ssS'neocort'
p81974
(dp81975
g6
(lp81976
I1349
assS'lobbi'
p81977
(dp81978
g83
(lp81979
I542
assS'multisensori'
p81980
(dp81981
g32
(lp81982
I3206
assS'defibril'
p81983
(dp81984
g135
(lp81985
I73
assS'offcentr'
p81986
(dp81987
g52
(lp81988
I2428
assS'student'
p81989
(dp81990
g36
(lp81991
sg38
(lp81992
sg277
(lp81993
sg76
(lp81994
sg235
(lp81995
I52
assS'coprocessor'
p81996
(dp81997
g10
(lp81998
I546
assS'decoupl'
p81999
(dp82000
g50
(lp82001
I366
asg535
(lp82002
ssS'permeabl'
p82003
(dp82004
g118
(lp82005
I984
assS'english'
p82006
(dp82007
g42
(lp82008
I1614
asg94
(lp82009
sg74
(lp82010
sg87
(lp82011
ssS'rungekutta'
p82012
(dp82013
g216
(lp82014
I1352
assS'discontin'
p82015
(dp82016
g30
(lp82017
I2563
assS'mfi'
p82018
(dp82019
g332
(lp82020
I1180
assS'ioxlo'
p82021
(dp82022
g183
(lp82023
I5976
assS'mfl'
p82024
(dp82025
g106
(lp82026
I2100
assS'obtain'
p82027
(dp82028
g124
(lp82029
sg26
(lp82030
sg163
(lp82031
sg72
(lp82032
sg68
(lp82033
sg281
(lp82034
sg283
(lp82035
sg85
(lp82036
sg460
(lp82037
sg303
(lp82038
sg287
(lp82039
sg74
(lp82040
sg176
(lp82041
sg145
(lp82042
sg256
(lp82043
sg76
(lp82044
sg262
(lp82045
sg295
(lp82046
sg183
(lp82047
sg59
(lp82048
sg484
(lp82049
sg38
(lp82050
sg83
(lp82051
sg114
(lp82052
sg63
(lp82053
sg42
(lp82054
I540
asg306
(lp82055
sg87
(lp82056
sg91
(lp82057
sg12
(lp82058
sg46
(lp82059
sg96
(lp82060
sg48
(lp82061
sg221
(lp82062
sg535
(lp82063
sg44
(lp82064
sg350
(lp82065
sg174
(lp82066
sg293
(lp82067
sg32
(lp82068
sg245
(lp82069
sg429
(lp82070
sg318
(lp82071
sg102
(lp82072
sg104
(lp82073
sg108
(lp82074
sg110
(lp82075
sg20
(lp82076
sg22
(lp82077
sg230
(lp82078
sg438
(lp82079
sg440
(lp82080
sg18
(lp82081
sg121
(lp82082
sg4
(lp82083
sg181
(lp82084
sg8
(lp82085
sg34
(lp82086
sg36
(lp82087
sg384
(lp82088
sg235
(lp82089
sg126
(lp82090
sg341
(lp82091
sg10
(lp82092
sg118
(lp82093
sg223
(lp82094
sg128
(lp82095
sg78
(lp82096
sg132
(lp82097
sg14
(lp82098
sg16
(lp82099
sg135
(lp82100
sg50
(lp82101
sg138
(lp82102
sg140
(lp82103
sg354
(lp82104
ssS'patric'
p82105
(dp82106
g132
(lp82107
sg106
(lp82108
I17
assS'bifurcat'
p82109
(dp82110
g18
(lp82111
I727
assS'heavili'
p82112
(dp82113
g132
(lp82114
I2883
asg59
(lp82115
sg178
(lp82116
ssS'gillespi'
p82117
(dp82118
g174
(lp82119
I2622
assS'simultan'
p82120
(dp82121
g277
(lp82122
sg80
(lp82123
sg38
(lp82124
sg303
(lp82125
sg12
(lp82126
sg18
(lp82127
sg223
(lp82128
sg149
(lp82129
sg429
(lp82130
sg318
(lp82131
sg102
(lp82132
sg106
(lp82133
I133
asg216
(lp82134
sg118
(lp82135
sg332
(lp82136
sg4
(lp82137
sg6
(lp82138
sg8
(lp82139
sg10
(lp82140
sg130
(lp82141
sg132
(lp82142
sg14
(lp82143
sg16
(lp82144
sg354
(lp82145
ssS'rapid'
p82146
(dp82147
g438
(lp82148
I1083
asg332
(lp82149
sg181
(lp82150
sg8
(lp82151
sg34
(lp82152
sg59
(lp82153
sg68
(lp82154
sg126
(lp82155
sg63
(lp82156
sg350
(lp82157
sg14
(lp82158
sg102
(lp82159
sg94
(lp82160
sg16
(lp82161
sg48
(lp82162
sg110
(lp82163
sg96
(lp82164
sg44
(lp82165
sg174
(lp82166
ssS'preprint'
p82167
(dp82168
g26
(lp82169
sg140
(lp82170
I3165
assS'elsevi'
p82171
(dp82172
g34
(lp82173
sg183
(lp82174
sg384
(lp82175
sg138
(lp82176
sg354
(lp82177
I3114
assS'smith'
p82178
(dp82179
g174
(lp82180
sg145
(lp82181
sg80
(lp82182
sg40
(lp82183
sg14
(lp82184
sg16
(lp82185
I41
assS'ferdinanda'
p82186
(dp82187
g99
(lp82188
I2165
assS'hall'
p82189
(dp82190
g230
(lp82191
sg32
(lp82192
sg283
(lp82193
sg235
(lp82194
sg295
(lp82195
sg183
(lp82196
sg484
(lp82197
sg83
(lp82198
sg10
(lp82199
sg102
(lp82200
sg46
(lp82201
sg108
(lp82202
sg221
(lp82203
sg140
(lp82204
I3132
assS'lowresolut'
p82205
(dp82206
g44
(lp82207
I2422
assS'lsewher'
p82208
(dp82209
g48
(lp82210
I563
assS'galng'
p82211
(dp82212
g295
(lp82213
I1219
asg183
(lp82214
ssS'barrow'
p82215
(dp82216
g429
(lp82217
I347
assS'experentia'
p82218
(dp82219
g174
(lp82220
I2646
assS'agnost'
p82221
(dp82222
g262
(lp82223
I1878
assS'finch'
p82224
(dp82225
g116
(lp82226
I2561
assS'dimmer'
p82227
(dp82228
g318
(lp82229
I2638
assS'know'
p82230
(dp82231
g287
(lp82232
sg145
(lp82233
sg277
(lp82234
sg293
(lp82235
sg36
(lp82236
sg72
(lp82237
sg281
(lp82238
sg85
(lp82239
sg83
(lp82240
sg89
(lp82241
sg91
(lp82242
sg354
(lp82243
I698
asg313
(lp82244
sg223
(lp82245
sg350
(lp82246
ssS'atkeson'
p82247
(dp82248
g295
(lp82249
sg183
(lp82250
sg313
(lp82251
I2287
asg223
(lp82252
ssS'press'
p82253
(dp82254
g283
(lp82255
sg70
(lp82256
sg78
(lp82257
sg303
(lp82258
sg26
(lp82259
sg30
(lp82260
sg287
(lp82261
sg74
(lp82262
sg80
(lp82263
sg262
(lp82264
sg295
(lp82265
sg183
(lp82266
sg59
(lp82267
sg83
(lp82268
sg63
(lp82269
sg91
(lp82270
sg12
(lp82271
sg46
(lp82272
sg96
(lp82273
sg18
(lp82274
sg99
(lp82275
sg313
(lp82276
sg44
(lp82277
sg149
(lp82278
sg118
(lp82279
sg293
(lp82280
sg460
(lp82281
sg429
(lp82282
sg318
(lp82283
sg102
(lp82284
sg106
(lp82285
I2928
asg108
(lp82286
sg20
(lp82287
sg52
(lp82288
sg114
(lp82289
sg116
(lp82290
sg174
(lp82291
sg440
(lp82292
sg332
(lp82293
sg178
(lp82294
sg4
(lp82295
sg181
(lp82296
sg235
(lp82297
sg34
(lp82298
sg36
(lp82299
sg384
(lp82300
sg124
(lp82301
sg281
(lp82302
sg40
(lp82303
sg344
(lp82304
sg223
(lp82305
sg128
(lp82306
sg130
(lp82307
sg132
(lp82308
sg350
(lp82309
sg138
(lp82310
sg354
(lp82311
ssS'shephard'
p82312
(dp82313
g99
(lp82314
I3310
assS'incred'
p82315
(dp82316
g132
(lp82317
I2802
assS'realist'
p82318
(dp82319
g440
(lp82320
sg235
(lp82321
sg59
(lp82322
sg74
(lp82323
sg85
(lp82324
sg102
(lp82325
sg63
(lp82326
sg149
(lp82327
I495
assS'subsystem'
p82328
(dp82329
g40
(lp82330
I2473
assS'exceed'
p82331
(dp82332
g74
(lp82333
sg283
(lp82334
sg262
(lp82335
sg34
(lp82336
sg38
(lp82337
sg89
(lp82338
I2494
asg63
(lp82339
ssS'shapley'
p82340
(dp82341
g118
(lp82342
I400
assS'qee'
p82343
(dp82344
g34
(lp82345
I1741
assS'pomdp'
p82346
(dp82347
g293
(lp82348
I1577
assS'pariti'
p82349
(dp82350
g110
(lp82351
sg145
(lp82352
I2650
assS'growth'
p82353
(dp82354
g38
(lp82355
sg40
(lp82356
I574
assS'holler'
p82357
(dp82358
g14
(lp82359
sg135
(lp82360
I2460
assS'hebbian'
p82361
(dp82362
g438
(lp82363
I741
asg176
(lp82364
sg70
(lp82365
sg50
(lp82366
sg384
(lp82367
sg12
(lp82368
sg106
(lp82369
sg48
(lp82370
sg99
(lp82371
sg535
(lp82372
sg149
(lp82373
ssS'leaf'
p82374
(dp82375
g42
(lp82376
I1482
asg181
(lp82377
ssS'lead'
p82378
(dp82379
g124
(lp82380
sg70
(lp82381
sg78
(lp82382
sg163
(lp82383
sg72
(lp82384
sg283
(lp82385
sg76
(lp82386
sg295
(lp82387
sg183
(lp82388
sg38
(lp82389
sg85
(lp82390
sg303
(lp82391
sg306
(lp82392
sg89
(lp82393
sg91
(lp82394
sg12
(lp82395
sg20
(lp82396
sg221
(lp82397
sg230
(lp82398
sg329
(lp82399
sg116
(lp82400
sg460
(lp82401
sg68
(lp82402
sg102
(lp82403
sg106
(lp82404
sg110
(lp82405
sg114
(lp82406
sg216
(lp82407
sg438
(lp82408
I747
asg440
(lp82409
sg8
(lp82410
sg99
(lp82411
sg384
(lp82412
sg235
(lp82413
sg126
(lp82414
sg341
(lp82415
sg128
(lp82416
sg130
(lp82417
sg132
(lp82418
sg135
(lp82419
sg138
(lp82420
sg354
(lp82421
ssS'retravers'
p82422
(dp82423
g145
(lp82424
I2242
assS'heilman'
p82425
(dp82426
g303
(lp82427
I2816
assS'leam'
p82428
(dp82429
g183
(lp82430
I6725
asg110
(lp82431
ssS'tattersal'
p82432
(dp82433
g52
(lp82434
I551
assS'leap'
p82435
(dp82436
g34
(lp82437
sg181
(lp82438
I2279
assS'leav'
p82439
(dp82440
g484
(lp82441
sg32
(lp82442
sg318
(lp82443
sg26
(lp82444
sg80
(lp82445
sg34
(lp82446
sg384
(lp82447
sg68
(lp82448
sg38
(lp82449
sg74
(lp82450
sg10
(lp82451
sg104
(lp82452
sg96
(lp82453
I1935
assS'settl'
p82454
(dp82455
g438
(lp82456
I2005
asg145
(lp82457
sg287
(lp82458
sg149
(lp82459
sg135
(lp82460
sg138
(lp82461
sg350
(lp82462
ssS'belt'
p82463
(dp82464
g181
(lp82465
I1575
assS'ngle'
p82466
(dp82467
g72
(lp82468
I2673
assS'investig'
p82469
(dp82470
g68
(lp82471
sg283
(lp82472
sg295
(lp82473
sg183
(lp82474
sg59
(lp82475
sg484
(lp82476
sg38
(lp82477
sg83
(lp82478
sg85
(lp82479
sg42
(lp82480
I3354
asg306
(lp82481
sg89
(lp82482
sg91
(lp82483
sg235
(lp82484
sg94
(lp82485
sg20
(lp82486
sg18
(lp82487
sg535
(lp82488
sg223
(lp82489
sg104
(lp82490
sg110
(lp82491
sg52
(lp82492
sg114
(lp82493
sg32
(lp82494
sg332
(lp82495
sg121
(lp82496
sg4
(lp82497
sg8
(lp82498
sg34
(lp82499
sg124
(lp82500
sg10
(lp82501
sg40
(lp82502
sg344
(lp82503
sg48
(lp82504
sg130
(lp82505
sg138
(lp82506
sg140
(lp82507
ssS'obey'
p82508
(dp82509
g32
(lp82510
sg262
(lp82511
sg384
(lp82512
sg68
(lp82513
sg85
(lp82514
sg48
(lp82515
I711
assS'timeargu'
p82516
(dp82517
g384
(lp82518
I696
assS'slur'
p82519
(dp82520
g174
(lp82521
I1979
assS'interburst'
p82522
(dp82523
g106
(lp82524
I1095
assS'wesley'
p82525
(dp82526
g26
(lp82527
sg295
(lp82528
sg183
(lp82529
sg130
(lp82530
sg36
(lp82531
sg12
(lp82532
sg50
(lp82533
I1615
assS'stay'
p82534
(dp82535
g36
(lp82536
sg20
(lp82537
sg18
(lp82538
sg126
(lp82539
I1747
assS'pike'
p82540
(dp82541
g106
(lp82542
I752
assS'rare'
p82543
(dp82544
g70
(lp82545
sg178
(lp82546
sg4
(lp82547
sg277
(lp82548
sg460
(lp82549
sg281
(lp82550
sg42
(lp82551
I2056
asg245
(lp82552
sg91
(lp82553
sg132
(lp82554
sg94
(lp82555
sg26
(lp82556
ssS'hinxton'
p82557
(dp82558
g26
(lp82559
sg235
(lp82560
I293
assS'nda'
p82561
(dp82562
g4
(lp82563
I1590
assS'roychowdhuri'
p82564
(dp82565
g287
(lp82566
I3640
assS'equipartit'
p82567
(dp82568
g384
(lp82569
I1020
assS'nonadapt'
p82570
(dp82571
g50
(lp82572
I95
assS'reglsl'
p82573
(dp82574
g135
(lp82575
I609
assS'disabl'
p82576
(dp82577
g42
(lp82578
I3494
asg135
(lp82579
ssS'owl'
p82580
(dp82581
g176
(lp82582
I108
assS'own'
p82583
(dp82584
g116
(lp82585
sg329
(lp82586
sg32
(lp82587
sg70
(lp82588
sg293
(lp82589
sg126
(lp82590
sg83
(lp82591
sg85
(lp82592
sg313
(lp82593
sg89
(lp82594
sg128
(lp82595
sg14
(lp82596
sg303
(lp82597
sg138
(lp82598
I771
assS'owe'
p82599
(dp82600
g230
(lp82601
sg34
(lp82602
sg20
(lp82603
sg44
(lp82604
I364
assS'eo'
p82605
(dp82606
g332
(lp82607
sg121
(lp82608
sg76
(lp82609
sg36
(lp82610
sg384
(lp82611
sg12
(lp82612
I1728
asg46
(lp82613
sg20
(lp82614
ssS'automat'
p82615
(dp82616
g318
(lp82617
sg59
(lp82618
sg76
(lp82619
sg34
(lp82620
sg72
(lp82621
sg384
(lp82622
sg124
(lp82623
sg126
(lp82624
sg42
(lp82625
I3071
asg344
(lp82626
sg87
(lp82627
sg89
(lp82628
sg78
(lp82629
sg132
(lp82630
sg94
(lp82631
sg96
(lp82632
sg108
(lp82633
sg63
(lp82634
sg52
(lp82635
sg114
(lp82636
ssS'mbsram'
p82637
(dp82638
g10
(lp82639
I478
assS'riiter'
p82640
(dp82641
g59
(lp82642
I2457
assS'brush'
p82643
(dp82644
g42
(lp82645
I2709
assS'shynk'
p82646
(dp82647
g121
(lp82648
I288
assS'vai'
p82649
(dp82650
g20
(lp82651
I723
assS'van'
p82652
(dp82653
g216
(lp82654
sg174
(lp82655
sg32
(lp82656
sg332
(lp82657
sg178
(lp82658
sg8
(lp82659
sg34
(lp82660
sg124
(lp82661
sg245
(lp82662
sg306
(lp82663
sg128
(lp82664
sg12
(lp82665
sg110
(lp82666
sg138
(lp82667
I2287
assS'val'
p82668
(dp82669
g20
(lp82670
I747
assS'propert'
p82671
(dp82672
g59
(lp82673
sg48
(lp82674
I1862
assS'spiral'
p82675
(dp82676
g174
(lp82677
I291
assS'computationalneurosci'
p82678
(dp82679
g149
(lp82680
I3140
assS'appl'
p82681
(dp82682
g174
(lp82683
sg295
(lp82684
sg183
(lp82685
sg104
(lp82686
sg94
(lp82687
sg138
(lp82688
I3226
assS'ivaldi'
p82689
(dp82690
g99
(lp82691
I221
assS'ed'
p82692
(dp82693
g30
(lp82694
sg287
(lp82695
sg74
(lp82696
sg176
(lp82697
sg256
(lp82698
sg76
(lp82699
sg118
(lp82700
sg295
(lp82701
sg183
(lp82702
sg83
(lp82703
sg245
(lp82704
sg20
(lp82705
sg48
(lp82706
sg313
(lp82707
sg44
(lp82708
sg174
(lp82709
sg32
(lp82710
sg12
(lp82711
sg102
(lp82712
sg106
(lp82713
sg108
(lp82714
sg63
(lp82715
sg116
(lp82716
sg438
(lp82717
I2419
asg440
(lp82718
sg18
(lp82719
sg178
(lp82720
sg4
(lp82721
sg235
(lp82722
sg36
(lp82723
sg384
(lp82724
sg126
(lp82725
sg281
(lp82726
sg78
(lp82727
sg14
(lp82728
sg16
(lp82729
sg460
(lp82730
sg354
(lp82731
ssS'var'
p82732
(dp82733
g484
(lp82734
sg318
(lp82735
sg354
(lp82736
I1579
assS'vas'
p82737
(dp82738
g18
(lp82739
I1462
assS'tamar'
p82740
(dp82741
g32
(lp82742
I13
assS'vat'
p82743
(dp82744
g40
(lp82745
I1416
assS'multigrid'
p82746
(dp82747
g8
(lp82748
I657
assS'kane'
p82749
(dp82750
g10
(lp82751
I542
assS'contradictori'
p82752
(dp82753
g18
(lp82754
I88
assS'straani'
p82755
(dp82756
g48
(lp82757
I2265
assS'cotman'
p82758
(dp82759
g106
(lp82760
I2700
assS'paravermi'
p82761
(dp82762
g350
(lp82763
I1208
assS'made'
p82764
(dp82765
g68
(lp82766
sg26
(lp82767
sg277
(lp82768
sg72
(lp82769
sg287
(lp82770
sg176
(lp82771
sg145
(lp82772
sg76
(lp82773
sg293
(lp82774
sg460
(lp82775
sg183
(lp82776
sg484
(lp82777
sg38
(lp82778
sg85
(lp82779
sg42
(lp82780
I2151
asg306
(lp82781
sg87
(lp82782
sg89
(lp82783
sg91
(lp82784
sg20
(lp82785
sg18
(lp82786
sg223
(lp82787
sg116
(lp82788
sg102
(lp82789
sg106
(lp82790
sg96
(lp82791
sg52
(lp82792
sg22
(lp82793
sg230
(lp82794
sg438
(lp82795
sg440
(lp82796
sg332
(lp82797
sg178
(lp82798
sg4
(lp82799
sg8
(lp82800
sg34
(lp82801
sg384
(lp82802
sg124
(lp82803
sg126
(lp82804
sg10
(lp82805
sg40
(lp82806
sg344
(lp82807
sg48
(lp82808
sg78
(lp82809
sg138
(lp82810
sg140
(lp82811
ssS'puddl'
p82812
(dp82813
g89
(lp82814
I1163
assS'whether'
p82815
(dp82816
g70
(lp82817
sg277
(lp82818
sg287
(lp82819
sg145
(lp82820
sg80
(lp82821
sg262
(lp82822
sg344
(lp82823
sg183
(lp82824
sg303
(lp82825
sg42
(lp82826
I2503
asg18
(lp82827
sg99
(lp82828
sg223
(lp82829
sg293
(lp82830
sg106
(lp82831
sg110
(lp82832
sg52
(lp82833
sg114
(lp82834
sg32
(lp82835
sg332
(lp82836
sg4
(lp82837
sg384
(lp82838
sg68
(lp82839
sg72
(lp82840
sg341
(lp82841
sg128
(lp82842
sg78
(lp82843
sg354
(lp82844
ssS'distract'
p82845
(dp82846
g332
(lp82847
sg4
(lp82848
I1214
asg293
(lp82849
ssS'record'
p82850
(dp82851
g283
(lp82852
sg277
(lp82853
sg74
(lp82854
sg176
(lp82855
sg80
(lp82856
sg183
(lp82857
sg42
(lp82858
I963
asg306
(lp82859
sg91
(lp82860
sg245
(lp82861
sg48
(lp82862
sg99
(lp82863
sg535
(lp82864
sg350
(lp82865
sg116
(lp82866
sg174
(lp82867
sg106
(lp82868
sg114
(lp82869
sg216
(lp82870
sg438
(lp82871
sg318
(lp82872
sg6
(lp82873
sg181
(lp82874
sg78
(lp82875
sg135
(lp82876
ssS'below'
p82877
(dp82878
g70
(lp82879
sg26
(lp82880
sg277
(lp82881
sg72
(lp82882
sg287
(lp82883
sg74
(lp82884
sg256
(lp82885
sg76
(lp82886
sg262
(lp82887
sg295
(lp82888
sg183
(lp82889
sg59
(lp82890
sg89
(lp82891
sg46
(lp82892
sg313
(lp82893
sg44
(lp82894
sg116
(lp82895
sg118
(lp82896
sg94
(lp82897
sg102
(lp82898
sg108
(lp82899
sg52
(lp82900
sg114
(lp82901
sg216
(lp82902
sg329
(lp82903
sg178
(lp82904
sg22
(lp82905
sg124
(lp82906
sg126
(lp82907
sg10
(lp82908
sg40
(lp82909
sg132
(lp82910
sg14
(lp82911
sg16
(lp82912
sg354
(lp82913
I1726
assS'bstitu'
p82914
(dp82915
g42
(lp82916
I2376
assS'ez'
p82917
(dp82918
g68
(lp82919
I2907
assS'meaningless'
p82920
(dp82921
g63
(lp82922
I329
asg85
(lp82923
ssS'seika'
p82924
(dp82925
g18
(lp82926
I41
assS'fregnac'
p82927
(dp82928
g106
(lp82929
I2406
assS'operand'
p82930
(dp82931
g10
(lp82932
I746
assS'abbot'
p82933
(dp82934
g87
(lp82935
I144
assS'wavelength'
p82936
(dp82937
g256
(lp82938
sg149
(lp82939
I556
assS'kusumoto'
p82940
(dp82941
g135
(lp82942
I503
assS'ew'
p82943
(dp82944
g34
(lp82945
I1222
asg116
(lp82946
sg460
(lp82947
sg72
(lp82948
ssS'cmos'
p82949
(dp82950
g22
(lp82951
sg10
(lp82952
sg245
(lp82953
sg14
(lp82954
sg20
(lp82955
sg135
(lp82956
I182
asg63
(lp82957
ssS'mutual'
p82958
(dp82959
g102
(lp82960
I85
asg484
(lp82961
sg318
(lp82962
sg535
(lp82963
sg163
(lp82964
ssS'boor'
p82965
(dp82966
g295
(lp82967
I3865
asg183
(lp82968
ssS'percent'
p82969
(dp82970
g484
(lp82971
sg83
(lp82972
sg303
(lp82973
sg94
(lp82974
I2170
asg96
(lp82975
sg221
(lp82976
ssS'portion'
p82977
(dp82978
g176
(lp82979
sg96
(lp82980
sg68
(lp82981
sg10
(lp82982
sg429
(lp82983
sg46
(lp82984
sg14
(lp82985
sg16
(lp82986
I1717
asg313
(lp82987
sg44
(lp82988
ssS'book'
p82989
(dp82990
g287
(lp82991
sg99
(lp82992
I3377
asg181
(lp82993
sg223
(lp82994
sg76
(lp82995
ssS'branch'
p82996
(dp82997
g70
(lp82998
sg183
(lp82999
sg59
(lp83000
sg83
(lp83001
sg42
(lp83002
I1580
asg48
(lp83003
ssS'booo'
p83004
(dp83005
g38
(lp83006
I1479
assS'theor'
p83007
(dp83008
g295
(lp83009
sg183
(lp83010
sg106
(lp83011
I2953
assS'roiloutcost'
p83012
(dp83013
g89
(lp83014
I2472
assS'june'
p83015
(dp83016
g256
(lp83017
sg484
(lp83018
sg40
(lp83019
sg429
(lp83020
sg108
(lp83021
I2499
asg110
(lp83022
sg535
(lp83023
ssS'hausdorff'
p83024
(dp83025
g32
(lp83026
I1235
assS'wlyl'
p83027
(dp83028
g163
(lp83029
I1629
assS'kindl'
p83030
(dp83031
g106
(lp83032
I2865
assS'ofdata'
p83033
(dp83034
g85
(lp83035
I4371
assS'cliff'
p83036
(dp83037
g102
(lp83038
I3609
asg46
(lp83039
ssS'experienc'
p83040
(dp83041
g438
(lp83042
I996
asg295
(lp83043
sg183
(lp83044
sg83
(lp83045
sg132
(lp83046
sg223
(lp83047
ssS'scientif'
p83048
(dp83049
g230
(lp83050
sg118
(lp83051
sg178
(lp83052
sg6
(lp83053
sg344
(lp83054
sg83
(lp83055
sg306
(lp83056
sg70
(lp83057
sg20
(lp83058
sg149
(lp83059
I3178
assS'reliabl'
p83060
(dp83061
g174
(lp83062
sg283
(lp83063
sg70
(lp83064
sg4
(lp83065
sg181
(lp83066
sg329
(lp83067
sg295
(lp83068
sg183
(lp83069
sg384
(lp83070
sg80
(lp83071
sg277
(lp83072
sg287
(lp83073
sg89
(lp83074
sg102
(lp83075
sg20
(lp83076
sg163
(lp83077
sg59
(lp83078
sg140
(lp83079
I41
assS'erlor'
p83080
(dp83081
g135
(lp83082
I1214
assS'emerg'
p83083
(dp83084
g116
(lp83085
sg74
(lp83086
sg48
(lp83087
sg178
(lp83088
sg4
(lp83089
sg181
(lp83090
sg384
(lp83091
sg38
(lp83092
sg85
(lp83093
sg40
(lp83094
sg128
(lp83095
I94
asg18
(lp83096
sg149
(lp83097
ssS'auxiliari'
p83098
(dp83099
g104
(lp83100
I2743
assS'tubach'
p83101
(dp83102
g174
(lp83103
I2854
assS'invari'
p83104
(dp83105
g32
(lp83106
sg48
(lp83107
sg178
(lp83108
sg76
(lp83109
sg181
(lp83110
sg8
(lp83111
sg183
(lp83112
sg85
(lp83113
sg52
(lp83114
sg429
(lp83115
sg223
(lp83116
sg132
(lp83117
sg14
(lp83118
sg16
(lp83119
sg108
(lp83120
sg138
(lp83121
I760
asg44
(lp83122
ssS'schechtman'
p83123
(dp83124
g87
(lp83125
I171
assa(dp83126
g80
S'Modeling Interactions of the Rat\'s Place and\nHead Direction Systems\n\nA. David Redish and David S. Touretzky\nComputer Science Department & Center for the Neural Basis of Cognition\nCarnegie Mellon University, Pittsburgh PA 15213-3891\nInternet: {dredi sh, ds t}@es . emu. edu\n\nAbstract\nWe have developed a computational theory of rodent navigation that\nincludes analogs of the place cell system, the head direction system, and\npath integration. In this paper we present simulation results showing how\ninteractions between the place and head direction systems can account for\nrecent observations about hippocampal place cell responses to doubling\nand/or rotation of cue cards in a cylindrical arena (Sharp et at., 1990).\nRodents have multiple internal representations of their relationship to their environment.\nThey have, for example, a representation of their location (place cells in the hippocampal\nformation, see Muller et at., 1991), and a location-independent representation of their\nheading (head direction cells in the postsubiculum and the anterior thalamic nuclei, see\nTaube et at., 1990; Taube, 1995).\nIf these representations are to be used for navigation, they must be aligned consistently\nwhenever the animal reenters a familiar environment. This process was examined in a set\nof experiments by Sharp et at. (1990).\n\n1\n\nThe Sharp et al., 1990 experiment\n\nRats spent multiple sessions finding food scattered randomly on the floor of a black cylindrical arena with a white cue card along the wall subtending 90? of arc. The animals were\nnot disoriented before entering the arena, and they always entered at the same location: the\nnorthwest corner. See Figure 3a. Hippocampal place fields were mapped by single-cell\nrecording. A variety of probe trials were then introduced. When an identical second cue\n\n\x0cA. D. REDISH, D. S. TOURETZKY\n\n62\n\nr-----~\n\nHead\nDirection\n~\n\nPath\nr-----\'--~ Integral_.I.o.........J\n(xp,Y,,>\n\nLocal\n\nView\n\'I\' .;)\n\n(T~\n\nPlace\n\nCOde\nA (It)\n\nGoal\n\nMemory\n\nFigure 1: Organization of the rodent navigation model.\ncard was added opposite the first (Figure 3c), most place fields did not double. J Instead,\nthe cells continued to fire at their original locations. However, if the rat was introduced into\nthe double-card environment at the southeast corner (Figure 3d), the place fields rotated\nby 1800 ? But rotation did not occur in single-card probe trials with a southeast entry point\n(Figure 3b). When tested with cue cards rotated by ?30?, Sharp et al. observed that place\nfield locations were controlled by an interaction of the choice of entry point with the cue\ncard positions (Figure 3f.)\n\n2 The CRAWL model\nIn earlier work (Wan et al., 1994a; Wan et al., 1994b; Redish and Touretzky, 1996) we\ndescribed a model of rodent navigation that includes analogs of both place cells and the head\ndirection system. This model also includes a local view module representing egocentric\nspatial information about landmarks, and a separate metric representation of location which\nserves as a substrate for path integration. The existence of a path integration faculty in\nrodents is strongly supported by behavioral data; see Maurer and Seguinot (1995) for\na discussion. Hypotheses about the underyling neural mechanismss are presently being\nexplored by several researchers, including us.\nThe structure of our model is shown in Figure 1. Visual inputs are represented as triples of\nform (Ti, \'i, (Ji), each denoting the type, distance, and egocentric bearing ofa landmark. The\nexperiments reported here used two point-type landmarks representing the left and right\nedges of the cue card, and one surface-type landmark representing the arena wall. For the\nlatter, \'i and (Ji define the normal vector between the rat and the surface. In the local view\nmodule, egocentric bearings (Ji are converted to allocentric form <Pi by adding the current\nvalue represented in the head direction system, denoted as tP h . The visual angle CYij between\npairs of landmarks is also part of the local view, and can be used to help localize the animal\nwhen its head direction is unknown. See Figure 2.\nI Five of the 18 cells recorded by Sharp et al. changed their place fields over the various recording\nsessions. Our model does not reproduce these effects, since it does not address changes in place cell\ntuning. Such changes could occur due to variations in the animal\'s mental state from one trial to the\nnext, or as a result of learning across trials .\n\n\x0cModeling Interactions of the Rat\'s Place and Head Direction Systems\n\n(T.,}\n\nr.,\n1\n\n63\n\n4>.)\n1\n\nFigure 2: Spatial variables used in tuning a place cell to two landmarks i and j when the\nanimal is at path integrator coordinates (xl" Yl\') .\n\nOur simulated place units are radial basis functions tuned to combinations of individual\nlandmark bearings and distances, visual angles between landmark pairs, and path integrator\ncoordinates. Place units can be driven by visual input alone when the animal is trying\nto localize itself upon initial entry at a random spot in the environment, or by the path\nintegrator alone when navigating in the dark. But normally they are driven by both\nsources simultaneously. A key role of the place system is to maintain associations between\nthe two representations, so that either can be reconstructed from the other. The place\nsystem also maintains a record of allocentric bearings of landmarks when viewed from the\ncurrent position; this enables the local view module to compare perceived with remembered\nlandmark bearings, so that drift in the head direction system can be detected and corrected.\nIn computer simulations using a single parameter set, the model reproduces a variety\nof behavioral and neurophysiological results including control of place fields by visual\nlandmarks, persistence of place fields in the dark, and place fields drifting in synchrony\nwith drift in the head direction system. Its predictions for open-field landmark-based\nnavigation behavior match many of the experimental results of Collett et al. (1986) for\ngerbils.\n\n2.1\n\nEntering a familiar environment\n\nUpon entering a familiar environment, the model\'s four spatial representations (local view,\nhead direction, place code, and path integrator coordinates) must be aligned with the\ncurrent sensory input and with each other. Note that local view information is completely\ndetermined given the visual input and head direction, and place cell activity is completely\ndetermined given the local view and path integrator representations. Thus, the alignment\nprocess manipulates just two variables: head direction and path integrator coordinates.\nWhen the animal enters the environment with initial estimates for them, the alignment\nprocess can produce four possible outcomes: (1) Retain the initial values of both variables,\n(2) Reset the head direction, (3) Reset the path integrator, or (4) Reset both head direction\nand the path integrator.\n\n2.2\n\nPrioritizing the outcomes\n\nWhen the animal was placed at the northwest entry point and there were two cue cards\n(Figure 3c), we note that the orientation of the wall segment adjacent to the place field\nis identical with that in the training case. This suggests that the animal\'s head direction\n\n\x0cA. D. REDISH, D. S. TOURETZKY\n\n64\n\ndid not change. The spatial relationship between the entry point and place field was also\nunchanged: notice that the distance from the entry point to the center of the field is the\nsame as in Figure 3a. Therefore, we conclude that the initially estimated path integrator\ncoordinates were retained. Alternatively, the animal could have changed both its head\ndirection (by 180?) and its path integrator coordinates (to those of the southeast comer) and\nproduced consistent results, but to the experimenter the place field would appear to have\nflipped to the other card. Because no flip was observed, the first outcome must have priority\nover the fourth.\nIn panel d, where the place field has flipped to the northwest comer, the orientation of the\nsegment of wall adjacent to the field has changed, but the spatial relationship between the\nentry point and field center has not. Resetting the path integrator and not the head direction\nwould also give a solution consistent with this local view, but with the place field unflipped\n(as in panel b). We conclude that the second outcome (reset head direction) must have\npriority over the third (reset the path integrator).\nThe third and fourth outcomes are demonstrated in Figures 3b and 3f. In panel b, the\norientation of the wall adjacent to the place field is unchanged from panel a, but the spatial\nrelationship between the entry point and the place field center is different, as evidenced by\nthe fact that the distance between them is much reduced. This is outcome 3. In panel f,\nboth variables have changed (outcome 4).\nFinally, the fact that place fields are stable over an entire session, even when there are\nmultiple cue cards (and therefore multiple consistent pairings of head directions and path\nintegrator coordinates) implies that animals do not reset their head direction or path integrator in visually ambiguous environments as long as the current values are reasonably\nconsistent with the local view. We therefore assume that outcome 1 is preferred over the\nothers.\nThis analysis establishes a partial ordering over the four outcomes: 1 is preferred over 4 by\nFigure 3c, and over the others by the stability of place fields, and outcome 2 is preferred\nover 3 by Figure 3d. This leaves open the question of whether outcome 3 or 4 has priority\nover the other. In this experiment, after resetting the path integrator it\'s always safe for the\nanimal to attempt to reset its head direction. If the head direction does not change by more\nthan a few degrees, as in panel b, we observe outcome 3; if it does change substantially, as\nin panel f, we observe outcome 4.\n\n2.3\n\nConsistency\n\nThe viability of an outcome is a function of the consistency between the local view and\npath integrator representations. The place system maintains the association between the\ntwo representations and mediates the comparison between them.\nThe activity A(u) of a place unit is the product of a local view term LV(u) and a path\nintegrator term C(u). LV(u) is in turn a product of five Gaussians: two tuned to bearings\nand two to distances (for the same\' pair of landmarks), and one tuned to the retinal angle\nbetween a pair of landmarks. C(u) is a Gaussian tuned to the path integrator coordinates of\nthe center of the place field.\nIf the two representations agree, then the place units activated by path integrator input will\nbe the same as those activated by the local view module, so the product A(u) computed\nby those units will be significantly greater than zero. The consistency K, of the association\n\n\x0cModeling Interactions of the Rat\'s Place and Head Direction Systems\n\n65\n\nbetween path integrator and local view representations is given by: K, = Lu A(u)/ Lu C(u).\nBecause A(u) < C(u) for all place units, K, ranges between 0 and 1. When the current local\nview is compatible with that predicted by the current path integrator coordinates, K, will be\nhigh; when the two are not compatible, K, will be low.\nEarlier we showed that the navigation system should choose the highest priority viable\noutcome. If the consistency of an outcome is more than K, * better than all higher-priority\noutcomes, that outcome is a viable choice and higher-priority ones are not. K,* is an\nempirically derived constant that we have set equal to 0.04.\n\n3\n\nDiscussion\n\nOur results match all of the cases already discussed. (See Figure 3, panels a through d\nas well as f and h.) Sharp et al. (1990) did not actually test the rotated cue cards with a\nnorthwest entry point, so our result in panel e is a prediction.\nWhen the animals entered from the northwest, but only one cue card was available at 1800 ,\nSharp et al. report that the place field did not rotate. In our model the place field does\nrotate, as a result of outcome 4. This discrepancy can be explained by the fact that this\nparticular manipulation was the last one in the sequence done by Sharp et at. McNaughton\net al. (1994) and Knierim et al. (1995) have shown that if rats experience the cue card\nmoving over a number of sessions, they eventually come to ignore it and it loses control\nover place fields . When we tested our model without a cue card (equivalent to a card being\npresent but ignored), the resulting place field was more diffuse than normal but showed no\nrotation; see Figure 3g. We thus predict that if this experiment had been done before the\nother manipulations rather than after, the place field would have foIlowed the cue card .\n\nIn the Sharp et al. experiment, the animals were always placed in the environment at the\nsame location during training. Therefore, they could reliably estimate their initial path\nintegrator coordinates. They also had a reliable head direction estimate because they were\nnot disoriented. We predict that were the rats trained with a variety of entry points instead\nof just one, using an environment with a single cue card at 0 0 (the training environment\nused by Sharp et al.), and then tested with two cue cards at 0 0 and 1800 , the place field\nwould not rotate no matter what entry point was used. This is because when trained with a\nvariable entry point, the animal would not learn to anticipate its path integrator coordinates\nupon entry; a path integratorreset would have to be done every time in order to establish the\nanimal\'s coordinates. The reset mechanism uses allocentric bearing information derived\nfrom the head direction estimate, and in this task the resulting path integrator coordinates\nwill be consistent with the initial head direction estimate. Hence, outcome 3 will always\nprevail.\nIf the animal is disoriented, however, then both the path integrator and the head direction\n\nsystem must be reset upon entry (because consistency will be low with a faulty head\ndirection), and the animal must choose one cue card or the other to match against its\nmemory. So with disorientation and a variable entry point, the place field will be controlled\nby one or the other cue card with a 50/50 probability. This was found to be true in a related\nbehavioral experiment by Cheng (1986).\nOur model shows how interactions between the place and head direction systems handle the\nvarious combinations of entry point, number of cue cards, and amount of cue card rotation .\nIt predicts that head direction reset will be observed in certain tasks and not in others. In\n\n\x0c66\n\nA. D. REDISH, D. S. TOURETZKY\n\nexperiments such as the single cue card task with an entry in the southeast, it predicts the\nplace code will shift from an initial value corresponding to the northwest entry point to the\nvalue for the southeast entry point, but the head direction will not change. This could be\ntested by recording simultaneously from place cells and head direction cells.\n\nReferences\nCheng, K. (1986). A purely geometric module in the rat\'s spatial representation. Cognition, 23: 149-178.\nCollett, T., Cartwright, B. A., and Smith, B. A. (1986). Landmark learning and visuospatial memories in gerbils. Journal of Comparative Physiology A, 158:835-851.\nKnierim, J. J., Kudrimoti, H. 5., and McNaughton, B. L. (1995). Place cells, head\ndirection cells, and the learning of landmark stability. Journal ofNeuroscience, 15: 164859.\nMaurer, R. and Seguinot, V. (1995). What is modelling for? A critical review of the\nmodels of path integration. Journal of Theoretical Biology, 175:457-475.\nMcNaughton, B . L., Mizumori, S. J. Y., Barnes, C. A., Leonard, B. 1., Marquis, M.,\nand Green, E. J. (1994). Cortical rpresentation of motion during unrestrained spatial\nnavigation in the rat. Cerebral Cortex, 4(1):27-39.\nMuller, R. U., Kubie, 1. L., Bostock, E. M., Taube, J. 5., and Quirk, G. 1. (1991).\nSpatial firing correlates of neurons in the hippocampal formation of freely moving rats.\nIn Paillard, J., editor, Brain and Space, chapter 17, pages 296-333. Oxford University\nPress, New York.\nRedish, A. D. and Touretzky, D. s. (1996). Navigating with landmarks: Computing\ngoal locations from place codes. In Ikeuchi, K. and Veloso, M., editors, Symbolic Visual\nLearning. Oxford University Press. In press.\nSharp, P. E., Kubie, J. L., and Muller, R. U. (1990). Firing properties of hippocampal\nneurons in a visually symmetrical environment: Contributions of multiple sensory cues\nand mnemonic processes. Journal of Neuroscience, 10(9):3093-3105.\nTaube, 1. s. (1995). Head direction cells recorded in the anterior thalamic nuclei of freely\nmoving rats. Journal of Neuroscience, 15(1): 1953-1971.\nTaube, J. 5., Muller, R. I., and Ranck, Jr., J. B . (1990). Head direction cells recorded\nfrom the postsubiculum in freely moving rats. I. Description and quantitative analysis.\nJournal of Neuroscience, 10:420-435.\nWan, H . 5., Touretzky, D. 5., and Redish, A. D. (1994a). Computing goal locations\nfrom place codes. In Proceedings of the 16th annual conference of the Cognitive Science\nsociety, pages 922-927. Lawrence Earlbaum Associates, Hillsdale N1.\nWan, H. 5., Touretzky, D. 5., and Redish, A. D. (1994b). Towards a computational\ntheory of rat navigation. In Mozer, M., Smolen sky, P., Touretzky, D., Elman, J., and\nWeigend, A., editors, Proceedings of the 1993 Connectionist Models Summer School,\npages 11-19. Lawrence Earlbaum Associates, Hillsdale NJ.\n\n\x0cModeling Interactions of the Rat\'s Place and Head Direction Systems\n\n(a) 1 cue card at 0? (East)\nentry in Northwest comer\nangle of rotation (Sharp et al.)\nprecession of HD system = 0 0\n\n= 2.7?\n\n(c) 2 cue cards at 0 0 (East) & 180 0 (West)\nentry in Northwest comer\nangle of rotation (Sharp et al.) = -2.3?\nprecession of HD system = 0 0\n\n67\n\n(b) 1 cue card at 0 0\nentry in Southeast comer\nangle of rotation (Sharp et al.) = -6.0 0\nprecession of HD system = 2?\n\n(d) 2 cue cards at 0 0 & 180 0\nentry in Southeast comer\nangle of rotation (Sharp et al.) = 182.5 0\nprecession of HD system::: 178 0\n\n(e) 2 cue cards at 330 0 & 150 0\nentry in Northwest comer\nnot done by Sharp et al.\nprecession of HD system = 331 0\n\n(f) 2 cue cards at 330 0 & 150 0\n\nentry in Southeast comer\nangle of rotation (Sharp et al.) =158.3?\nprecession of HD system = 151 ?\n\n(g) I cue card at 180 0 (West)\nentry in Northwest comer\nangle of rotation (Sharp et al.) ::: -5.5 0\nprecession of HD system =0 0\n\n(h) 1 cue card at 180 0\nentry in Southeast comer\nangle of rotation (Sharp et al.) = 182.2?\nprecession of HD system = 179 0\n\nFigure 3: Computer simulations of the Sharp et al. (1990) experiment showing that place\nfields are controlled by both cue cards (thick arcs) and entry point (arrowhead). "Angle of\nrotation" is the angle at which the correlation between the probe and training case place\nfields is maximal. Because head direction and place code are tightly coupled in our model,\nprecession of HD is an equivalent measure in our model.\n\n\x0c'
p83127
sg293
S'Active Gesture Recognition using\nLearned Visual Attention\nTrevor Darrell and Alex Pentland\nPerceptual Computing Group\nMIT Media Lab\n20 Ames Street, Cambridge MA, 02138\ntrevor,sandy~media.mit.edu\n\nAbstract\nWe have developed a foveated gesture recognition system that runs\nin an unconstrained office environment with an active camera. Using vision routines previously implemented for an interactive environment, we determine the spatial location of salient body parts\nof a user and guide an active camera to obtain images of gestures\nor expressions. A hidden-state reinforcement learning paradigm is\nused to implement visual attention. The attention module selects\ntargets to foveate based on the goal of successful recognition, and\nuses a new multiple-model Q-Iearning formulation. Given a set\nof target and distractor gestures, our system can learn where to\nfoveate to maximally discriminate a particular gesture.\n\n1\n\nINTRODUCTION\n\nVision has numerous uses in the natural world. It is used by many organisms in\nnavigation and object recognition tasks, for finding resources or avoiding predators.\nOften overlooked in computational models of vision, however, and particularly relevant for humans, is the use of vision for communication and interaction. In these\ndomains visual perception is an important communication modality, either in addition to language or when language cannot be used. In general, people place\nconsiderable weight on visual signals from another individual, such as facial expression, hand gestures, and body language. We have been developing neurally-inspired\nmethods which combine low-level vision and learning to model these visual abilities.\nPreviously, we presented a method for view-based recognition of spatia-temporal\nhand gestures [2] and a similar mechanism for the analysis/real-time tracking of\nfacial expressions [4]. These methods offered real-time performance and a relatively\nhigh level of accuracy, but required foveated images of the object performing the\n\n\x0cActive Gesture Recognition Using Learned Visual Attention\n\n859\n\ngesture. There are many domains/tasks for which these are not unreasonable assumptions, such as interaction with a single user workstation or an automobile with\na single driver. However the method had limited usefulness in unconstrained domains, such as "intelligent rooms" or interactive virtual environments, when the\nidentity and location of the user are unknown.\nIn this paper, we expand our gesture recognition method to include an active component, utilizing a foveated image sensor that can selectively track a person\'s hand\nor face as they walk through a room. The camera tracking and model selection\nroutines are guided by an action-selection system that implements visual attention\nbased on reinforcement learning. Using on a simple reward schedule, this attention\nsystem learns the appropriate object (hand, head) to foveate in order to maximize\nrecognition performance.\n\n2\n\nFOVEATED GESTURE ANALYSIS\n\nOur system for foveated gesture recognition combines person tracking routines,\nan active, high-resolution camera, and view-based normalized correlation analysis.\nFirst we will briefly describe the person tracking module and view-based analysis,\nthen discuss their use with an active camera.\nWe have implemented vision routines to track a user in in an office setting as part\nof our ALIVE system, an Artificial Life Interactive Video Environment[3]. This\nsystem can track people and identify head/hand locations as they walk about a\nroom, and provides the contextual environment within which view-based gesture\nanalysis methods can be successfully applied. The ALIVE system assumed little\nprior knowledge of the user, and operated on coarse-scale images. 1 ALIVE allows\na user to interact with virtual artificial life creatures, through the use of a "magicmirror" metaphor in which user sees him/herself presented in a video display along\nwith virtual creatures. A wide field-of-view video camera acquires an image of the\nuser, which is then combined with computer graphics imagery and projected on a\nlarge screen in front of the user. Vision routines in ALIVE compute figure/ground\nsegmentation and analyze the user\'s silhouette to determine the location of head,\nhands, and other salient body features. We use only a single, calibrated, wide fieldof-view camera to determine the 3-D position of these features. 2 For details of our\nperson tracking method see [14].\nIn our approach to real-time expression matching/tracking, a set of view-based\ncorrelation models is used to represent spatio-temporal gesture patterns. We take\na sequence of images representing the gesture to be trained, and build a set of\nview models that are sufficient to track the object as it performs the gesture. Our\nview models are normalized correlation templates, and can either be intensity-based\nor based on band-pass or wavelet-based signal representations. 3 We applied our\nmodel to the problem of hand gesture recognition [2] as well as for tracking facial\nexpressions [4]. For facial tracking, we implemented an interpolation paradigm to\nmap view-based correlation scores to facial motor controls. We used the Radial Basis\nFunction (RBF) method[7]; interpolation was performed using a set of exemplars\nconsisting of pairs of real faces and model faces in different expressions, which were\n1 A simple mechanism for recognition of hand gestures was implemented in the original\nALIVE system but made no use of high-resolution view models, and could only recognize\npointing and waving motions defined by the motion of the centroid of the hand.\n2By assuming the the user is sitting or standing on the ground plane, we use the imaging\nand ground plane geometry to compute the location of the user in 3-D.\n3The latter have the advantage of being less dependent on illumination direction.\n\n\x0c860\n\nT.DARRELL,A.PENTLAND\n\nanimation / rendering\n\n~VideoWall\n\nVIEW-BASED\nGESTURE\nANALYSIS\n\nFigure 1: Overview of system for person tracking and active gesture recognition.\nStatic, wide-field-of-view, camera tracks user\'s head and hands, which drives gaze\ncontrol of active narrow-field-of-view camera. Foveated images are used for viewbased gesture analysis and recognition. Graphical objects are rendered on video\nwall and can react to user\'s position, pose, and gestures.\nobtained by generating a 3-D model face and asking the user to match it. With this\nsimple formalism, we were able to track expressions of a real user and interpolate\nequivalent 3-D model faces in real-time .\nThis view-based analysis requires detailed imagery, which cannot be obtained from\na single, fixed camera as the user walks about a room. To provide high resolution\nimages for gesture recognition, we augment the wide field-of-view camera in our\ninteractive environment with an active, narrow-field-of-view camera, as shown in\nFigure 1. Information about head/hand location from the existing ALIVE routines\nis used to drive the motor control parameters of the narrow field camera. Currently\nthe camera can be directed to autonomously track head or hands . Using a highly\nsimplified, two expression model offacial expression (neutral and surprised), we have\nbeen able to track facial expressions as users move about the room and the narrow\nangle camera followed the face. For details on this foveated gesture recognition see\n\n[5]\n\n3\n\nVISUAL ATTENTION FOR RECOGNITION\n\nThe visual routines in the ALIVE system can be used to track the head and hands\nof a user, and the active camera can provide foveated images for gesture recognition.\nIf we know a priori which body part will produce the gesture of interest, or if we\nhave a sufficient number of active cameras to track all body parts, then we have\nsolved the problem. Of course, in practice there are more possible loci of gesture\nperformance than there are active cameras, and we have to address the problem of\naction selection for visual routines, i.e. , attention. In our active gesture recognition\nsystem, we have adopted an action selection model based on reinforcement learning.\n\n\x0cActive Gesture Recognition Using Learned Visual Attention\n\n3.1\n\n861\n\nTHE ACTIVE GESTURE RECOGNITION PROBLEM\n\nWe define an Active Gesture Recognition (AGR) task as follows . First, we assume\nprimitive routines exist to provide the continuous valued control and tracking of the\ndifferent body parts that perform gestures. Second, we assume that body pose and\nhand/face state is represented as a feature set, based on the representation produced\nby our body tracker and view-based recognition system, and we define a gesture\nto be a configuration of the user\'s body pose and hand/face expression. Third, we\nassume that, in addition to there being actions for foveating all the relevant body\nparts, there is also a special action labeled accept, and that the execution of this\naction by the AG R system signifies detection of the gesture. Finally, the goal of\nthe AGR task is to execute the accept action whenever the user is in the target\ngesture state, and not to perform that action when the user is in any other (e .g.\ndistract or) state. The AGR system should use the foveation actions to optimally\ndiscriminate the target pattern frqm distractor patterns, even when no single view\nof the user is sufficient to decide what gesture the user is performing.\nAn important problem in applying reinforcement learning to this task is that our\nperceptual observations may not provide a complete description of the user\'s state.\nIndeed, because we have a foveated image sensor we know that the user\'s true\ngestural state will be hidden whenever the user is performing a gesture and the\ncamera is not foveated on the appropriate body part. By definition, a system for\nperceptual action selection must not assume a full observation of state is available,\notherwise there would be no meaningful perception taking place.\nThe AG R task can be considered as a Partially Observable Markov Decision Process\n(POMDP), which is essentially a Markov Decision Process without direct access to\nstate[ll, 9]. Rather than attempt to solve them explicitly, we look to techniques\nfor hidden state reinforcement learning to find a solution [10, 8, 6, 1]. A POMDP\nconsists of a set of states in the world S, a set of observations 0, a set of actions\nA, a reward function R. After executing an action a, the likelihood of transitioning\nbetween two states s, s\' is given by T(s, a, a\'), an observation 0 is generated with\nprobability O(s, a, 0). In practice, T and 0 are not easily obtainable, and we use\nreinforcement learning methods which do not require them a priori.\nOur state is defined by the users pose, facial expression, and hand configurations, expressed in nine variables. Three are boolean and are provided directly by the person\ntracker: person-present, left-arm-extended, and right-arm-extended. Three\nmore are provided by the foveated gesture recognition system, (face, left-hand,\nright-hand), and take on an integer number of values according to the number\nof view-based expressions/hand-poses: in our first experiments face can be one of\nneutral, smile, or surprise, and the hands can each be one of neutral, point, or\ngrab. In addition, three boolean features represent the internal state of the vision\nsystem: head-foveated, left-hand-foveated, right-hand-foveated. At each\ntime step, the world is defined by a state s E S, which is defined by these features .\nAn observation, 0 E 0, consists of the same feature variables, except that those\nprovided by the foveated gesture system (e.g., head and hands) are only observable\nwhen foveated. Thus the face variable is hidden unless the head-foveated variable\nis set, the left-hand variable hidden unless the left-hand-foveated variable set,\nand similarly with the right hand. Hidden variables are set to a undefined value.\nThe set of actions, A, available to the AGR system are 4 foveation commands:\nlook-body, look-head, look-left-hand, and look-right-hand plus the special\naccept action. Each foveation command causes the active camera to follow the\nrespective body part, and sets the internal foveation feature bits accordingly.\n\n\x0c862\n\nT. DARRELL, A. PENTLAND\n\nThe reward function provides a unit positive reward whenever the accept action\nis performed and the user is in the target state (as defined by an oracle, external\nto the AGR system), and a fixed negative reward of magnitude a when performed\nand the user is in a distractor (non-target) state. Zero reward is given whenever a\nfoveation action is performed.\n\n3.2\n\nHIDDEN-STATE REINFORCEMENT LEARNING\n\nWe have implemented a instance-based method for hidden state reinforcement learning, based on earlier work by McCallum [10]. The instance-based approach to reinforcement learning replaces the absolute state with a distributed memory-based\nstate representation. Given a history of action, reward, and observation tuples,\n(a[t], r[t], o[t]) , 0 :::; t :::; T, a Q-value is also stored with each time step, q[t], and\nQ-Iearning[12, 13] is performed by evaluating the similarity of recently observed tuples with sequences farther back in the history chain. Q-values are computed, and\nthe Q-Iearning update rule applied, maintaining this distributed, memory-based\nrepresentation of Q-values.\nAs in traditional Q-Iearning, at each time step the utility of each action in the\ncurrent state is evaluated. If full access to the state was available and a table\nused to represent Q values, this would simply be a table look-up operation, but in a\nPOMDP we do not have full access to state. Using a variation on the instance based\napproach employed by McAllum\'s Nearest Sequence Memory (NSM) algorithm, we\ninstead find the I< nearest neighbors in the history list relative to the current time\npoint, and compute their average Q value. For each element on the history list, we\ncompute the sequence match criteria with the current time point, M(i, T), where\nM(i,j) = S(i,j)\n\n+ M(i -l,j -1)\n\no\n\nif S(i,j)\n\n> 0 and i> 0 and j > 0\n\notherwise.\n\nWe define Sci, j) to be 1 if o[i] = o[j] or a[i] = a(j], 2 if both are equal, and\no otherwise. Using a superscript in parentheses to denote the action index of a\nQ-value, we then compute\nT\n\nQ(a)[T]\n\n= (1/ I<) L v(a)[i]q[t]\n\n,\n\n(1)\n\ni=O\n\nwhere v(a*)[i] indicates whether the history tuple at time step i votes when computing the Q-value of a new action a"\': v(a*)[i] is set to 1 when a[i] = a\'" and M( i-I, T)\nis among the I< largest match values for all k which have a[k] = a"\', otherwise it is\nset to O. Given Q values for each action the optimal policy is simply\nlI"[T] = arg maxQ(a)[T] .\naEA\n\n(2)\n\nThe new action a[T + 1] is chosen either according to this policy or based on an\nexploration strategy. In either case, the action is executed yielding an observation\nand reward, and a new tuple added to the history. The new Q-value is set to be\nthe Q value of the chosen action, q[T + 1] = Q(a[T+1]) [T]. The update step of Q\nlearning is then computed, evaluating\nU[T + 1]\nq[i]\n\n+-\n\n= maxQ(a)[T\n+ 1] ,\naEA\n\n(1 - fJ)q[i]\n\nfor each i such that v(a[T+l])[i]\n\n= l.\n\n+ fJ(r[i] + \')\'U[T + 1]) ,\n\n(3)\n(4)\n\n\x0c863\n\nActive Gesture Recognition Using Learned Visual Attention\n\n60\n\n50\n40\n%error\n\n30\n20\n10\n0.84%\n\n0.44"10\n\n0.48%\n\n0L---------~---.8----~\n\n2\n\nK\n\n(a)\n\n4\n\n16\n\n(\\3={).5, "(=0.5, a.= 10, 2500 trialS)\n\nFigure 2: (a) Multiple model Q-learning: one Q-learning agent for each target\ngesture to be recognized, with coupled observation and action but separate reward\nand Q-value. (b) Results on recognition task with 8 gesture targets; graph shows\nerror rate after convergence plotted as a function of number of nearest neighbors\nused in learning algorithm.\n\n4\n\nMULTIPLE MODEL Q-LEARNING\n\nIn general, we have found the simple, instance-based hidden state reinforcement\nlearning described above to be an effective way to perform action selection for\nfoveation when the task is recognition of a single object from a set of distractors .\nHowever, we did not find that this type of system performed well when the AG R\ntask was extended to include more than one target gesture. When multiple accept\nactions were added to enumerate the different targets, we were not able to find\nexploration strategies that would converge in reasonable time.\nThis is not unexpected, since the addition of multiple causes of positive reward\nmakes the Q-value space considerably more complex. To remedy this problem, we\npropose a multiple model Q-learning system. In a multiple model approach to the\nAG R problem, separate learning agents model the task from each targets perspective. Conceptually, a separate Q-learning agent exists for each target, maintains it\'s\nown Q-value and history structure, and is coupled to the other agents via shared\nobservations. Since we can interpret the Q-value of an individual AGR agent as a\nconfidence value that its target is present, we can mediate among the actions predicted by the different agents by selecting the action from the agent with highest\nQ-value (Figure 2).\nFormally, in our multiple model Q-learning system all agents share the same observation and selected action , but have different reward and Q-values. Thus they\ncan be considered a single Q-learning system, but with vector reward and Q-values.\nOur multiple model learning system is thus obtained by rewriting Eqs. (1)-(4) with\nvector q[t] and r[t]. Using a subscript j to indicate the target index, we have\nT\n\nQ;a)[T]\n\n= (1/ K) L v(a)[i]qj [t]\n\n, 1T[11\n\ni=O\n\n= arg max\n(maxQ;a)[T])\naEA\nJ\n\n.\n\nRewards are computed with: if a[T] = accept then rj [T] = R(j, T) else rj [T]\n= 1 if gesture j was present at time T, else R(j, T) = -(Y. Further,\n\n(5)\n\n= 0;\n\nR(j, T)\n\nUj [T + 1] = maxQ(a)[T + 1] ,\naEA\n\n]\n\n(6)\n\n\x0cT.DARRELL,A.PENTLAND\n\n864\n\n(1- ,8)qj[i] + ,8(rj[i] + /\'Uj[T+ 1]) Vi s.t. v(a[T+1])[i] = 1 .\n(7)\nNote that our sequence match criteria, unlike that in [10], does not depend on r[t];\nthis allows considerable computational savings in the multiple model system since\nv(a) need not depend on j.\n\nqj[i]\n\nf-\n\nWe ran the multiple model learning system on the AGR task using 8 targets, with\n,8 = 0.5, /\' = 0.5, Q; = 10. Results summed over 2500 trials are shown in Figure 2(b),\nwith classification error plotted against the number of nearest neighbors used in the\nNSM algorithm. The error rate shown is after convergence; we ran the algorithm\nwith a period of deterministic exploration before following the optimal policy. (The\nsystem deterministically explored each action/accept pair.) As can be seen from\nthe graph, for any non-degenerate value of K reasonable performance was obtained;\nfor K > 2, the system performed almost perfectly.\n\nReferences\n[1] A. Cassandra, L. P. Kaelbling, and M. Littman. Acting optimally in partially\nobservable stochastic domains. In Proc. AAAI-94, pages 1023-1028. Morgan\nKaufmann, 1994.\n[2] T. Darrell and A. P. Pentland. Classification of Hand Gestures using a ViewBased Distributed Representation In Advances in Neural Information Processing Systems 6, Morgan Kauffman, 1994.\n[3] T. Darrell, P. Maes, B. Blumberg, and A. P. Pentland, A Novel Environment\nfor Situated Vision and Behavior, Proc. IEEE Workshop for Visual Behaviors,\nIEEE Compo Soc. Press, Los Alamitos, CA, 1994\n[4] T. Darrell, I. Essa, and A. P. Pentland, Correlation and Interpolation Networks\nfor Real-time Expression Analysis/Synthesis, In Advances in Neural Information Processing Systems 7, MIT Press, 1995.\n[5] T. Darrell and A. Pentland, A., Attention-driven Expression and Gesture Analysis in an Interactive Environment, in Proc. Inti. Workshop on A utomatic Face\nand Gesture Recognition (IWAFGR \'95), Zurich, Switzerland, 1995.\n[6] T. Jaakkola, S. Singh, and M. Jordan. Reinforcement Learning Algorithm\nfor Partially Observable Markov Decision Problems. In Advances In Neural\nInformation Processing Systems 7, MIT Press, 1995.\n[7] T. Poggio and F. Girosi, A Theory of Networks for Approximation and Learning. MIT AI Lab TR-1140, 1989.\n[8] 1. Lin and T. Michell. Reinforcement learning with hidden states. In Proc.\nAAAI-92. Morgan Kaufmann, 1992.\n[9] W. Lovejoy. A survey of algorithmic methods of partially observed markov\ndecision processes. Annals of Operation Reserach, 28:47-66, 1991.\n[10] R. A. McCallum. Instance-based State Identification for Reinforcement Learning. In Advances In Neural Information Processing Systems 7, MIT Press, 1995.\n[11] Edward J. Sondik. The optimal control of partially observable markov processes\nover the infinite horizon: Discounted costs. Operations Reserach, 26(2):282304, 1978.\n[12] R. S. Sutton. Learning to predict by the method of temporal differences.\nMachine Learning, 3:9-44, 1988.\n[13] C. Watkins and P. Dayan. Q-learning. Machine Learning, 8:279-292, 1992.\n[14] C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland, Pfinder: Real-Time\nTracking of the Human Body, Media Lab Per. Compo TR-353, 1994\n\n\x0c'
p83128
sg344
S'Learning Sparse Perceptrons\n\nJeffrey C. Jackson\nMathematics & Computer Science Dept.\nDuquesne University\n600 Forbes Ave\nPittsburgh, PA 15282\njackson@mathcs.duq.edu\n\nMark W. Craven\nComputer Sciences Dept.\nUniversity of Wisconsin-Madison\n1210 West Dayton St.\nMadison, WI 53706\ncraven@cs.wisc.edu\n\nAbstract\nWe introduce a new algorithm designed to learn sparse perceptrons over input representations which include high-order features.\nOur algorithm, which is based on a hypothesis-boosting method,\nis able to PAC-learn a relatively natural class of target concepts.\nMoreover, the algorithm appears to work well in practice: on a set\nof three problem domains, the algorithm produces classifiers that\nutilize small numbers of features yet exhibit good generalization\nperformance. Perhaps most importantly, our algorithm generates\nconcept descriptions that are easy for humans to understand.\n\n1\n\nIntrod uction\n\nMulti-layer perceptron (MLP) learning is a powerful method for tasks such as concept classification. However, in many applications, such as those that may involve\nscientific discovery, it is crucial to be able to explain predictions. Multi-layer perceptrons are limited in this regard, since their representations are notoriously difficult\nfor humans to understand. We present an approach to learning understandable,\nyet accurate, classifiers. Specifically, our algorithm constructs sparse perceptrons,\ni.e., single-layer perceptrons that have relatively few non-zero weights. Our algorithm for learning sparse perceptrons is based on a new hypothesis boosting algorithm (Freund & Schapire, 1995). Although our algorithm was initially developed\nfrom a learning-theoretic point of view and retains certain theoretical guarantees (it\nPAC-learns the class of sparse perceptrons), it also works well in practice. Our experiments in a number of real-world domains indicate that our algorithm produces\nperceptrons that are relatively comprehensible, and that exhibit generalization performance comparable to that of backprop-trained MLP\'s (Rumelhart et al., 1986)\nand better than decision trees learned using C4.5 (Quinlan, 1993).\n\n\x0cLearning Sparse Perceptrons\n\n655\n\nWe contend that sparse perceptrons, unlike MLP\'s, are comprehensible because they\nhave relatively few parameters, and each parameter describes a simple (Le. linear)\nrelationship. As evidence that sparse perceptrons are comprehensible, consider that\nsuch linear functions are commonly used to express domain knowledge in fields such\nas medicine (Spackman, 1988) and molecular biology (Stormo, 1987).\n\n2\n\nSparse Perceptrons\n\nA perceptron is a weighted threshold over the set of input features and over higherorder features consisting of functions operating on only a limited number of the\ninput features. Informally, a sparse perceptron is any perceptron that has relatively\nfew non-zero weights. For our later theoretical results we will need a more precise\ndefinition of sparseness which we develop now. Consider a Boolean function I :\n{O, 1}n -t { -1, + 1}. Let Ck be the set of all conjunctions of at most k of the inputs\nto I. C k includes the "conjunction" of 0 inputs, which we take as the identically\n1 function. All of the functions in C k map to {-1,+1}, and every conjunction in\nC k occurs in both a positive sense (+1 represents true) and a negated sense (-1\nrepresents true). Then the function I is a k-perceptron if there is some integer s\nsuch that I(x) = sign(L::=1 hi(x)), where for all i, hi E Ck, and sign(y) is undefined\nif y = 0 and is y/lyl otherwise. Note that while we have not explicitly shown any\nweights in our definition of a k-perceptron I, integer weights are implicitly present\nin that we allow a particular hi E Ck to appear more than once in the sum defining\nI. In fact, it is often convenient to think of a k-perceptron as a simple linear\ndiscriminant function with integer weights defined over a feature space with O(nk)\nfeatures, one feature for each element of Ck ?\nWe call a given collection of s conjunctions hi E C k a k-perceptron representation of\nthe corresponding function I, and we call s the size of the representation. We define\nthe size of a given k-perceptron function I as the minimal size of any k-perceptron\nrepresentation of I. An s-sparse k-perceptron is a k-perceptron I such that the size\nof I is at most s. We denote by PI: the set of Boolean functions over {O, 1}n which\ncan be represented as k-perceptrons, and we define Pk = Un Pi:. The subclass of\ns-sparse k-perceptrons is denoted by Pk,/l" We are also interested in the class P~\nof k-perceptrons with real-valued weights, at most r of which are non-zero.\n\n3\n\nThe Learning Algorithm\n\nIn this section we develop our learning algorithm and prove certain performance\nguarantees. Our algorithm is based on a recent "hypothesis boosting" algorithm\nthat we describe after reviewing some basic learning-theory terminology.\n3.1\n\nPAC Learning and Hypothesis Boosting\n\nFollowing Valiant (1984), we say that a function class :F (such as Pk for fixed k)\nis (strongly) PAC-learnable if there is an algorithm A and a polynomial function\nPI such that for any positive f and 8, any I E :F (the target junction), and any\nprobability distribution D over the domain of I, with probability at least 1 8, algorithm A(EX(f, D), f, 8) produces a function h (the hypothesis) such that\nPr[PrD[/(x) I- hex)] > f] < 8. The outermost probability is over the random choices\nmade by the EX oracle and any random choices made by A. Here EX(f, D) denotes\nan oracle that, when queried, chooses a vector of input values x with probability\nD and returns the pair (x,/(x)) to A. The learning algorithm A must run in time\nPI (n, s, c 1 , 8- 1 ), where n is the length of the input vector to I and s is the size of\n\n\x0cJ. C. JACKSON, M. W. CRAVEN\n\n656\n\nAdaBoost\nInput: training set S of m examples of function\nis (~ - \'Y)-approximate, l\'\nAlgorithm:\n\nf, weak learning algorithm WL that\n\n1. T +-- ~ In(m)\n2. for all xES, w(x) +-- l/m\n3. for i = 1 to T do\n4.\nfor all XES, Di(X) +-- w(x)/\nw(x).\n5.\ninvoke WL on S and distribution Di, producing weak hypothesis hi\n\nL:l=l\n\n6.\n\n+-- L:z .h;(z);oI:/(z) Di(X)\n(3i +-- ?i/ (1 - ?i)\n?i\n\n7.\n8.\nfor all XES, if h(x) = f(x) then w(x) +-- w(x) . (3i\n9. enddo\nOutput: h(x) == sign\n\n(L::=l -In((3i) . hi{x))\nFigure 1: The AdaBoost algorithm.\n\nf; the algorithm is charged one unit of time for each call to EX. We sometimes\ncall the function h output by A an ?-approximator (or strong approximator) to f\nwith respect to D. If F is PAC-learnable by an algorithm A that outputs only\nhypotheses in class 1? then we say that F is PAC-learnable by 1?. If F is PAClearnable for ? = 1/2 - 1/\'P2(n, s), where\'P2 is a polynomial function, then :F is\nweakly PA C-learnable, and the output hypothesis h in this case is called a weak\napproximator.\n\nOur algorithm for finding sparse perceptrons is, as indicated earlier, based on the\nnotion of hypothesis boosting. The specific boosting algorithm we use (Figure 1)\nis a version of the recent AdaBoost algorithm (Freund & Schapire, 1995). In the\nnext section we apply AdaBoost to "boost" a weak learning algorithm for Pk,8 into\na strong learner for Pk,8\' AdaBoost is given a set S of m examples of a function\nf : {O,1}n ---+ {-1, +1} and a weak learning algorithm WL which takes ? = ! - l\'\nfor a given l\' b must be bounded by an inverse polynomial in nand s). Adaf300st\nruns for T = In(m)/(2\'Y2) stages. At each stage it creates a probability distribution\nDi over the training set and invokes WL to find a weak hypothesis hi with respect\nto Di (note that an example oracle EX(j, Di) can be simulated given Di and S).\nAt the end of the T stages a final hypothesis h is output; this is just a weighted\nthreshold over the weak hypotheses {hi I 1 ~ i ~ T}. If the weak learner succeeds\nin producing a (~-\'Y)-approximator at each stage then AdaBoost\'s final hypothesis\nis guaranteed to be consistent with the training set (Freund & Schapire, 1995).\n3.2\n\nPAC-Learning Sparse k-Perceptrons\n\nWe now show that sparse k-perceptrons are PAC learnable by real-weighted kperceptrons having relatively few nonzero weights. Specifically, ignoring log factors,\nPk,8 is learnable by P~O(82) for any constant k. We first show that, given a training\nset for any f E Pk,8\' we can efficiently find a consistent h E p~( 8 2 )\' This consistency algorithm is the basis of the algorithm we later apply to empirical learning\nproblems. We then show how to turn the consistency algorithm into a PAC learning\nalgorithm. Our proof is implicit in somewhat more general work by Freund (1993),\nalthough he did not actually present a learning algorithm for this class or analyze\n\n\x0cLearning Sparse Perceptrons\n\n657\n\nthe sample size needed to ensure f-approximation, as we do. Following Freund, we\nbegin our development with the following lemma (Goldmann et al., 1992):\nLemma 1 (Goldmann Hastad Razhorov) For I: {0,1}n -+ {-1,+1} and H,\n\nany set 01 functions with the same domain and range, il I can be represented as\nI(x) = sign(L::=l hi(X?, where hi E H, then lor any probability distribution D\nover {O, 1}n there is some hi such that PrD[f(x) ?- hi(x)] ~ ~ - 218 \'\n\nIf we specialize this lemma by taking H = Ck (recall that Ck is the set of conjunctions of at most k input features of f) then this implies that for any I E Pk,8 and\nany probability distribution D over the input features of I there is some hi E Ck\nthat weakly approximates I with respect to D. Therefore, given a training set S\nand distribution D that has nonzero weight only on instances in S, the following\nsimple algorithm is a weak learning algorithm for Pk: exhaustively test each of the\nO(nk) possible conjunctions of at most k features until we find a conjunction that\n218 )-approximates I with respect to D (we can efficiently compute the approximation of a conjunction hi by summing the values of D over those inputs where hi\n\na-\n\nand I agree). Any such conjunction can be returned as the weak hypothesis. The\nabove lemma proves that if I is a k-perceptron then this exhaustive search must\nsucceed at finding such a hypothesis. Therefore, given a training set of m examples\nof any s-sparse k-perceptron I, AdaBoost run with the above weak learner will, after 2s2In(m) stages, produce a hypothesis consistent with the training set. Because\neach stage adds one weak hypothesis to the output hypothesis, the final hypothesis\nwill be a real-weighted k-perceptron with at most 2s2In(m) nonzero weights.\nWe can convert this consistency algorithm to a PAC learning algorithm as follows.\nFirst, given a finite set of functions F, it is straightforward to show the following\n(see, e.g., Haussler, 1988):\nLemma 2 Let F be a finite set ollunctions over a domain X. For any function\nlover X, any probability distribution D over X, and any positive f and ~, given a\nset S ofm examples drawn consecutively from EX(f, D), where m ~ f-1(ln~-1 +\nIn IFI), then Pr[3h E F I "Ix E S f(x) = h(x) & Prv[/(x) ?- h(x)] > f] < ~, where\nthe outer probability is over the random choices made by EX(f,D).\nThe consistency algorithm above finds a consistent hypothesis in P~, where r\n\n=\n\n2s2 In(m). Also, based on a result of Bruck (1990), it can be shown that In IP~I =\no (r2 + kr log n). Therefore, ignoring log factors, a randomly-generated training set\nof size O(kS4 If) is sufficient to guarantee that, with high probability, our algorithm\nwill produce an f-approximator for any s-sparse k-perceptron target. In other words,\nthe following is a PAC algorithm for Pk,8: compute sufficiently large (but polynomial\nin the PAC parameters) m, draw m examples from EX(f, D) to create a training\nset, and run the consistency algorithm on this training set.\nSo far we have shown that sparse k-perceptrons are learnable by sparse perceptron\nhypotheses (with potentially polynomially-many more weights). In practice, of\ncourse, we expect that many real-world classification tasks cannot be performed\nexactly by sparse perceptrons. In fact, it can be shown that for certain (reasonable)\ndefinitions of "noisy" sparse perceptrons (loosely, functions that are approximated\nreasonably well by sparse perceptrons), the class of noisy sparse k-perceptrons is\nstill PAC-learnable. This claim is based on results of Aslam and Decatur (1993),\nwho present a noise-tolerant boosting algorithm. In fact, several different boosting\nalgorithms could be used to learn Pk,s (e.g., Freund, 1993). We have chosen to use\nAdaBoost because it seems to offer significant practical advantages, particularly in\nterms of efficiency. Also, our empirical results to date indicate that our algorithm\n\n\x0cJ. C. JACKSON, M. W. CRAVEN\n\n658\n\nworks very well on difficult (presumably "noisy") real-world problems. However,\none potential advantage of basing the algorithm on one of these earlier boosters\ninstead of AdaBoost is that the algorithm would then produce a perceptron with\ninteger weights while still maintaining the sparseness guarantee of the AdaBoostbased algorithm.\n3.3\n\nPractical Considerations\n\nWe turn now to the practical details of our algorithm, which is based on the consistency algorithm above. First, it should be noted that the theory developed above\nworks over discrete input domains (Boolean or nominal-valued features). Thus, in\nthis paper, we consider only tasks with discrete input features. Also, because the\nalgorithm uses exhaustive search over all conjunctions of size k, learning time depends exponentially on the choice of k. In this study we to use k = 2 throughout,\nsince this choice results in reasonable learning times.\nAnother implementation concern involves deciding when the learning algorithm\nshould terminate. The consistency algorithm uses the size of the target function\nin calculating the number of boosting stages. Of course, such size information is\nnot available in real-world applications, and in fact, the target function may not be\nexactly representable as a sparse perceptron. In practice, we use cross validation\nto determine an appropriate termination point. To facilitate comprehensibility, we\nalso limit the number of boosting stages to at most the number of weights that\nwould occur in an ordinary perceptron for the task. For similar reasons, we also\nmodify the criteria used to select the weak hypothesis at each stage so that simple\nfeatures are preferred over conjunctive features. In particular, given distribution\nD at some stage j, for each hi E Ck we compute a correlation Ev[/ . hi]. We\nthen mUltiply each high-order feature\'s correlation by i. The hi with the largest\nresulting correlation serves as the weak hypothesis for stage j.\n\n4\n\nEmpirical Evaluation\n\nIn our experiments, we are interested in assessing both the generalization ability\nand the complexity of the hypotheses produced by our algorithm. We compare our\nalgorithm to ordinary perceptrons trained using backpropagation (Rumelhart et al.,\n1986), multi-layer perceptrons trained using backpropagation, and decision trees\ninduced using the C4.5 system (Quinlan, 1993). We use C4.5 in our experiments as\na representative of "symbolic" learning algorithms. Symbolic algorithms are widely\nbelieved to learn hypotheses that are more comprehensible than neural networks.\nAdditionally, to test the hypothesis that the performance of our algorithm can be\nexplained solely by its use of second-order features, we train ordinary perceptrons\nusing feature sets that include all pairwise conjunctions, as well as the ordinary\nfeatures. To test the hypothesis that the performance of our algorithm can be\nexplained by its use of relatively few weights, we consider ordinary perceptrons\nwhich have been pruned using a variant of the Optimal Brain Damage (OBD)\nalgorithm (Le Cun et al., 1989). In our version of OBD, we train a perceptron until\nthe stopping criteria are met, prune the weight with the smallest salience, and then\niterate the process. We use a validation set to decide when to stop pruning weights.\nFor each training set, we use cross-validation to select the number of hidden units\n(5, 10, 20, 40 or 80) for the MLP\'s, and the pruning confidence level for the C4.5\ntrees. We use a validation set to decide when to stop training for the MLP\'s.\nWe evaluate our algorithm using three real-world domains: the voting data set from\nthe UC-Irvine database; a promoter data set which is a more complex superset of\n\n\x0cLearning Sparse Perceptrons\n\ndomain\nvoting\npromoter\ncoding\n\nboosting\n91.5%\n92.7\n72.9\n\n659\n\nTa ble 1: 11est -set accuracy.\nperceptrons\nC4.5\nmulti-layer ordinary 2nd-order\n90.8%\n89.2%\n89.2% * 92.2%\n*\n90.6\n90.0\n88.7\n84.4\n*\n*\n*\n71.6\n69.8\n70.7\n62.6\n\n*\n\n*\n\n*\n\nTable 2: Hypothesis complexity (# weights).\nperceptrons\ndomain\nboosting multi-layer ordinary 2nd-order\nvoting\n450\n12\n651\n30\npromoters\n41\n2267\n228\n25764\nprotein coding\n52\n4270\n60\n1740\n\n*\n\npruned\n87.6% *\n88.2\n*\n70.3\n\n*\n\npruned\n12\n59\n37\n\nUC-Irvine one; and a data set in which the task is to recognize protein-coding\nregions in DNA (Craven & Shavlik, 1993). We remove the physician-fee-freeze\nfeature from the voting data set to make the problem more difficult. We conduct\nour experiments using a lO-fold cross validation methodology, except for in the\nprotein-coding domain. Because of certain domain-specific characteristics of this\ndata set, we use 4-fold cross-validation for our experiments with it.\nTable 1 reports test-set accuracy for each method on all three domains. We measure the statistical significance of accuracy differences using a paired, two-tailed\nt-test. The symbol \'*\' marks results in cases where another algorithm is less accurate than our boosting algorithm at the p ::; 0.05 level of significance. No other\nalgorithm is significantly better than our boosting method in any of the domains.\nFrom these results we conclude that (1) our algorithm exhibits good generalization\nperformance on number of interesting real-world problems, and (2) the generalization performance of our algorithm is not explained solely by its use of second-order\nfeatures, nor is it solely explained by the sparseness of the perceptrons it produces.\nAn interesting open question is whether perceptrons trained with both pruning and\nsecond-order features are able to match the accuracy of our algorithm; we plan to\ninvestigate this question in future work.\nTable 2 reports the average number of weights for all of the perceptrons. For all\nthree problems, our algorithm produces perceptrons with fewer weights than the\nMLP\'s, the ordinary perceptrons, and the perceptrons with second-order features.\nThe sizes of the OBD-pruned perceptrons and those produced by our algorithm\nare comparable for all three domains. Recall, however, that for all three tasks,\nthe perceptrons learned by our algorithm had significantly better generalization\nperformance than their similar-sized OBD-pruned counterparts. We contend that\nthe sizes of the perceptrons produced by our algorithm are within the bounds of\nwhat humans can readily understand. In the biological literature, for example, linear\ndiscriminant functions are frequently used to communicate domain knowledge about\nsequences of interest. These functions frequently involve more weights than the\nperceptrons produced by our algorithm. We conclude, therefore, that our algorithm\nproduces hypotheses that are not only accurate, but also comprehensible.\nWe believe that the results on the protein-coding domain are especially interesting.\nThe input representation for this problem consists of 15 nominal features representing 15 consecutive bases in a DNA sequence. In the regions of DNA that encode\nproteins (the positive examples in our task), non-overlapping triplets of consecu-\n\n\x0c660\n\nJ. C. JACKSON, M. W. eRA VEN\n\ntive bases represent meaningful "words" called codons. In previous work (Craven\n& Shavlik, 1993), it has been found that a feature set that explicitly represents\ncodons results in better generalization than a representation of just bases. However, we used the bases representation in our experiments in order to investigate the\nability of our algorithm to select the "right" second-order features. Interestingly,\nnearly all of the second-order features included in our sparse perceptrons represent\nconjunctions of bases that are in the same codon. This result suggests that our\nalgorithm is especially good at selecting relevant features from large feature sets.\n\n5\n\nFuture Work\n\nOur present algorithm has a number of limitations which we plan to address. Two\nareas of current research are generalizing the algorithm for application to problems\nwith real-valued features and developing methods for automatically suggesting highorder features to be included in our algorithm\'s feature set.\nAcknowledgements\n\nMark Craven was partially supported by ONR grant N00014-93-1-0998. Jeff Jackson\nwas partially supported by NSF grant CCR-9119319.\n\nReferences\nAslam, J. A. & Decatur, S. E. (1993). General bounds on statistical query learning and\nPAC learning with noise via hypothesis boosting. In Proc. of the 34th Annual Annual\nSymposium on Foundations of Computer Science, (pp. 282-291).\nBruck, J . (1990). Harmonic analysis of polynomial threshold functions. SIAM Journal\nof Discrete Mathematics, 3(2):168-177.\nCraven, M . W. & Shavlik, J. W. (1993) . Learning to represent codons: A challenge\nproblem for constructive induction. In Proc. of the 13th International Joint Conf. on\nArtificial Intelligence, (pp. 1319-1324), Chambery, France.\nFreund, Y. (1993). Data Filtering and Distribution Modeling Algorithms for Machine\nLearning. PhD thesis, University of California at Santa Cruz.\nFreund, Y. & Schapire, R. E. (1995). A decision-theoretic generalization of on-line learning and an application to boosting. In Proc. of the ~nd Annual European Conf. on\nComputational Learning Theory.\nGoldmann, M., Hastad, J., & Razborov, A. (1992). Majority gates vs. general weighted\nthreshold gates. In Proc. of the 7th IEEE Conf. on Structure in Complexity Theory.\nHaussler, D. (1988). Quantifying inductive bias: AI learning algorithms and Valiant\'s\nlearning framework. Artificial Intelligence, (pp. 177-221).\nLe Cun, Y., Denker, J. S., & Solla, S. A. (1989). Optimal brain damage. In Touretzky,\nD., editor, Advances in Neural Information Processing Systems (volume ~).\nQuinlan, J. R. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann.\nRumelhart, D., Hinton, G., & Williams, R. (1986). Learning internal representations\nby error propagation. In Rumelhart, D. & McClelland, J., editors, Parallel Distributed\nProcessing: Explorations in the microstructure of cognition. Volume 1. MIT Press.\nSpackman, K. A. (1988). Learning categorical decision criteria. In Proc. of the 5th\nInternational Conf. on Machine Learning, (pp. 36-46), Ann Arbor, MI.\nStormo, G. (1987). Identifying coding sequences. In Bishop, M. J. & Rawlings, C. J.,\neditors, Nucleic Acid and Protein Sequence Analysis: A Practical Approach. IRL Press.\nValiant,1. G. (1984). A theory of the learnable. Comm. of the ACM, 27(11):1134-1142.\n\n\x0c'
p83129
sg78
S'A Neural Network Autoassociator for\nInduction Motor Failure Prediction\nThomas Petsche, Angelo Marcantonio, Christian Darken,\nStephen J. Hanson, Gary M. Kuhn and Iwan Santoso\n[PETSCHE, ANGELO, DARKEN, JOSE, GMK, NIS]@SCR.SIEMENS.COM\n\nSiemens Corporate Research, Inc.\n755 College Road East\nPrinceton, NJ 08853\n\nAbstract\nWe present results on the use of neural network based autoassociators\nwhich act as novelty or anomaly detectors to detect imminent motor\nfailures. The autoassociator is trained to reconstruct spectra obtained\nfrom the healthy motor. In laboratory tests, we have demonstrated that the\ntrained autoassociator has a small reconstruction error on measurements\nrecorded from healthy motors but a larger error on those recorded from a\nmotor with a fault. We have designed and built a motor monitoring system\nusing an autoassociator for anomaly detection and are in the process of\ntesting the system at three industrial and commercial sites.\n\n1 Introduction\nAn unexpected breakdown of an electric induction motor can cause financial loss significantly in excess of the cost of the motor. For example, the breakdown of a motor in a\nproduction line during a production run can cause the loss of work in progress as well as\nloss of production time.\nWhen a motor does fail, it is not uncommon to replace it with an oversized motor based on\nthe assumption that if a motor is not running at its design limit then it will survive longer.\nWhile this is frequently effective, this leads to significantly lower operating efficiencies and\nhigher initial and operating costs.\nThe primary motivation behind this project is the observation that if a motor breakdown and\nbe predicted before the actual breakdown occurs, then the motor can be replaced in a more\norderly way, with minimal interruption of the process in which it is involved. The goal is\nto produce a system that is conceptually similar to a fuel gauge on an automobile. When\nthe system detects conditions that indicate that the motor is approaching its end-of-life, the\noperators are notified that a replacement is necessary in the near future.\n\n\x0cA Neural Network Autoassociator for Induction Motor Failure Prediction\n\n925\n\n2 Background\nAt present, motors in critical operations that are subject to mechanical failures - for example,\nfire pump motors on US Navy vessels - are typically monitored by a human expert who\nperiodically listens to the vibrations of the motor and, based on experience, determines\nwhether the motor sounds healthy or sounds like a problem is developing. Since mechanical\nprobiems in motors typically lead to increased or changed vibrations, this technique can\nwerk well. Unfortunately, it depends on a competent and expensive expert.\nIn an attempt to automate motor monitoring, several vendors have "automated motor monitoring" equipment available. For mechanical failure monitoring, such systems typically rely\non several accelerometers to measure the vibration of the motor at various points and along\nvarious axes. The systems then display information, primarily about the vibration spectrum,\nto an operator who determines whether the motor is functioning properly. These systems\nare expensive since they rely on several accelerometers, each of which is itself expensive,\nas well as data collection hardware and a computer. Further, the systems require an expert\noperator and frequently require that the motor be tested only when it is driving a known load.\nNeither the human motor expert nor the existing motor monitoring systems provide an\naffordable solution for continuous on-line mechanical failure monitoring. However, the\nsuccess of the human expert and existing vibration monitors does demonstrate that in fact,\nthere is sufficient information in the vibration of an electric induction motor to detect\nimminent mechanical failures.\nSiemens Energy and Automation has proposed a new product, the Siemens Advanced Motor\nMaster System II (SAMMS II), that will continuously monitor and protect an electric induction motor while it is operating on-line. Like the presently available SAMMS, the SAMMS\nII is designed to provide protection against thermal and electrical overload an, in addition,\nit will provide detection of insulation deterioration and mechanical fault monitoring.\nIn contrast to existing systems and techniques, the SAMMS II is designed to (1) require\nno human expert to determine if a motor is developing problems; (2) be inexpensive; and\n(3) provide continuous, on-line monitoring of the motor in normal operation.\nThe requirements for the SAMMS II, in partiCUlar the cost constraint, require that several\nissues be resolved. First, in order to produce a low cost system, it is necessary to eliminate\nthe need for expensive accelerometers. Second, wiring should be limited to the motor control\ncenter, i.e., it should not be necessary to run new signal wires from the motor control center\nto the motor. Third, the SAMMS II is to provide continuous on-line monitoring, so the\nsystem must adapt to or factor out the effect of changing loads on the motor. Finally since\nthe SAMMS II would not necessarily be bundled with a motor and so might be used to\ncontrol and monitor an arbitrary motor from an arbitrary manufacturer, the design can not\nassume that a full description of the motor construction is available.\n\n3 Approach\nThe first task was to determine how to eliminate the accelerometers. Based on work done\nelsewhere (Schoen, Habetler & Bartheld, 1994), SE&A determined that it might be possible\nto use measurements of the current on a single phase of the power supply to estimate the\nvibration of the motor. This depends on the assumption that any vibration of the motor will\ncause the rotor to move radially relative to the stator which will cause changes in the airgap\nwhich, in tum, will induce changes in the current.\nExperiments were done at the Georgia Institute of Technology to determine the feasibility\nof this idea using the same sort of data collection system described later. Early experiments\nindicated that, for a single motor driving a variety of loads, it is possible to distinguish\n\n\x0c926\n\nT. PETSCHE, A. MARCANTONIO, C. DARKEN, S. J. HANSON, G. M. KUHN, I. SANTOSO\n\nTable 1: Loads for motors #1 and #2.\n\nLoad type\nconstant\nsinusoidal oscillation at rotating frequency\nsinusoidal oscillation at twice the rotating frequency\nswitching load (50% duty cycle) at rotating frequency\nsinusoidal oscillation 28 Hz\nsinusoidal oscillation at 30 Hz\nswitching load (50% duty cycle) at 30 Hz\n\nLoad Magnitude\nhalf and full rated\nhalf and full rated\nfull rated\nfull rated\nhalf and full rated\nfull rated\nfull rated\n\nTable 2: Neural network classifier experiment.\n\nFeatures (N)\nPerformance on motor #1\nPerformance on motor #2\n\n48\n100%\n\n63\n100%\n30%\n\n64\n92%\n25%\n\n110\n100%\n55%\n\n320\n100%\n37%\n\nbetween a current spectrum obtained from the motor while it is healthy and another obtained\nwhen the motor contains a fault. Moreover, it is also possible to automatically generate a\nclassifiers that correctly determine the presence or absence of a fault in the motor.\nThe first, obvious approach to this monitoring task would seem to be to build a classifier\nthat would be used to distinguish between a healthy motor and one that has developed a\nfault that is likely to lead to a breakdown. Unfortunately, this approach does not work.\nAs described above, we have successfully built classifiers of various sorts using manual and\nautomatic techniques to distinguish between current spectra obtained from a motor when it\nis healthy and those obtained when it contains a fault.\nHowever, since the SAMMS II will be connected to a motor before it fails and will be asked\nto identify a failure without ever seeing a labeled example of a failure from that motor, a\nclassifier can only be used if it can be trained on data collected from one or more motors\nand then used to monitor the motor of interest. Unfortunately, experiments indicate that\nthis will not work.\nOne of these experiments is illustrated in table 2. Several feedforward neural network classifiers were trained using examples from a single motor under four conditions: (1) healthy,\n(2) unbalanced, (3) containing a broken rotor bar and (4) containing a hole in the outer\nbearing race. The ten different loads listed in table 1 were applied to the motor for each of\nthese conditions.\nThe networks contained N inputs (where N is given in table 2); 9 hidden units and 4\noutputs. There were 40 training examples where each example is the average of 50 distinct\nmagnitude scaled FFrs obtained from motor #1 from a single load/fault combination. The\ntest data for which the results are reported in the table consisted of 40 averaged FFfs from\nmotor #1 and 20 averaged FFfs (balanced and unbalanced only) from motor #2. The test\nset for motor #1 is completely distinct from the training set.\nIn the case where n = 110, the FFf components were selected to include the frequencies\nidentified by the theory of motor physics as interesting for the three fault conditions and\nexclude all other components. This led to an improvement over the other cases where a\nsingle contiguous set of components was chosen, but the performance still degrades to about\nrandom chance instead of 100%.\nThis experiment clearly illustrates that is is possible to distinguish between healthy and\nfaulty spectra obtained from the same motor. However, it also clearly illustrates that a\n\n\x0cA Neural Network Autoassociator for Induction Motor Failure Prediction\n\nMeasurements\n\nNovelty\ndetection\n\nNovelty\n\nDecision\n\n927\nDiagnosis\n\nAdaptation\nAlgOrithm\n\nFigure 1: The basic form of an anomaly detection system.\nclassifier trained on one motor does not perform well on another motor since the error rates\nincrease immensely. Based on results such as these, we have concluded that it is not feasible\nto build a single classifier that would be trained once and then placed in the field to monitor\na motor. Instead we are pursuing an alternative based on anomaly detection which adapts\na monitor to the particular motor for which it is responsible.\n\n4\n\nAnomaly detection\n\nThe basic notion of anomaly detection for monitoring is illustrated in figure 1. Statistical\nanomaly detection centers around a model of the data that was seen while the motor was\noperating normally. This model is produced by collecting spectra from the motor while\nit is operating normally. Once trained, the system compares each new spectrum to the\nmodel to determine how similar to or different from the training set it is. This similarity\nis described by an "anomaly metric" which, in the simplest case, can be thresholded to\ndetermine whether the motor is still normal or has developed a fault. Once the "anomaly\nmetric" has been generated, various statistical techniques can be used to determine if there\nhas been a change in the distribution of values.\n\n5 A Neural Network-based Anomaly Detector\nThe core of the most successful monitoring system we have built to date is a neural network\ndesigned to function as an autoassociator (Rumelhart, Hinton & Williams, 1986, called it\nan "encoder"). We use a simple three layer feedforward network with N inputs, N outputs\nand K < N hidden units. The input layer is fully connected to the hidden layer which is\nfully connected to the output layer. Each unit in the hidden and output layers computes\n\n= (J ( 2::;0 Wi,jXj) , where Xi is the output of neuron i which receives inputs from Mi other\nneurons and Wi,j is the weight on the connection from neuron} to neuron i. The network is\ntrained using the backpropagation algorithm to reconstruct the input vector on the output\nunits. Specifically, if Xi is one of n input vectors and Xi is the corresponding output vector,\nthe network is trained to minimize the sum of squared errors E = 2::~1 Ilxi - xdl 2. Once\ntraining is complete, the anomaly metric is mi = IIXi - xi11 2 .\n\nXi\n\n6\n\nAnomaly Detection Test\n\nWe have tested the effectiveness of the neural network autoassociator as an anomaly detector\non several motors. For all these tests, the autoasociator had 20 hidden units. The hidden\nlayer size was chosen after some experimentation and data analysis on motor #1 , but no\nattempt was made to tune the\' hidden layer size for motor #2 or motor #3.\nMotor #1 was tested using the ten different loads listed in table 1 and four different\n\n\x0cT. PETSCHE, A. MARCANTONIO, C. DARKEN, S. 1. HANSON, O. M. KUHN, I: SANTOSO\n\n928\n\nq\n\n,---------------------------~\n\n. .......\n\n,.;- ,\n\n,_.-\n\n<Xl\n\n<Xl\n\no\n\no\n\n.. ..-...\n\n..-\n\n... :\n\nC\\I\n\n"!\n\no\n\no\n\n0.0\n\no.oooos\n\n0.0001\nThreshold\n\n0.00015\n\n0.0002\n\nbalanced\nunbalanced\n\n...-\n\n0.0\n\n0.00002 0.00004 0.00006 0.00008 0.00010 0.00012\nThreshold\n\nFigure 2: Probability of error as a function of threshold using individual FFfs on (a) motor #1 with 319 inputs and (b) motor #2 with 320 inputs.\nhealth/fault conditions: healthy (balanced); unbalanced; broken rotor bar; and a hole in\nthe outer bearing race. Motor #2 was tested while driving the same ten loads, but for one\nhealthy and one faulty condition: healthy (balanced) and unbalanced.\nFor both motors #1 and #2, recordings of a single current phase were made as follows. For\neach fault condition, a load was selected and applied and the motor was run and the current\nsignal recorded for five minutes. Then a new load was introduced and the motor was run\nagain. The load was constant during any five minute recording session.\nMotor #3 was tested using thirteen different loads, but only two fault conditions: healthy\n(balanced) and unbalanced. In this case, however, load changes occurred at random times.\nWe preprocessed this data to to identify where the load changes occurred to generate the\ntraining set and the healthy motor test sets.\n\n6.1\n\nPreprocessing\n\nRecordings were made on a digital audio tape (OAT). The current on a single phase was\nmeasured with a current transformer, amplified, notch filtered to reduce the magnitude of\nthe 60Hz component, amplified again and then applied as input to the OAT. The notch filter\nwas a switched capacitor filter which reduced the magnitude at 60Hz by about 30dB.\nThe time series obtained from the OAT was processed to reduce the sampling rate and then\ndividing the data into non-overlapping blocks and computing the FFT of each block. A\nsubset of the FFf magnitude coefficients was selected and for each FFT, independent of\nany other FFf, the components were linearly scaled and translated to the interval [e, 1 e] (typically e = 0.02). That is, for each FFT consisting of coefficients to, ... .tn-t,\nwe selected a subset, F, (the same for all FFTs) of the components and computed a =\n(l - 2e)(maxiEFh - miniEFh)-t and b = miniEFh. Then the input vector, x, to the\nnetwork is Xj = a(fij - b) + e where, for allj < k: ij, ik E F and ij < ik.\n\n6.2 Experimental Results\nIn figure 2a, we illustrate the results of a typical anomaly detection experiment on motor #1\nusing an autoassociator with 319 inputs and 20 hidden units. This graph illustrates the\nperformance (false alarm and miss rates) of a very simple anomaly detection system which\nthresholds the anomaly metric to determine if the motor is good or bad. The decreasing\ncurve that starts at threshold = 0, P(error) = 1 is the false alarm rate as a function of the\nthreshold. Each increasing curve is the miss rate for a particular fault type.\nIn figure 2b we illustrate the performance of an autoassociator on motor #2 using an\n\n\x0cA Neural Network Autoassociator for Induction Motor Failure Prediction\nq\n\nI\n\n.....\n\n929\n\n-\'\n\n/~\',.....\n\n<Xl\n\nci\n\n%~\nii\n\n,.I\n\n/\n\n/\n\n,,/\n\niL-.:t:\n0\n\n.... -.\n\n......\n\n\'"ci\n\n../ - - - - - - - - - - - - 1\n\n0\n\nci\n\n0.0\n\n0.0001\n\n0.0002\n\n0.0003\n\n0.0004\n\n0.0005\n\nThreshold\n\nFigure 3: Probability of error for motor #3 using individual FFTs and 319 inputs.\nq ,----------------------------,\n\nq\n\n<Xl\n\no\n\n,---------------------------~\n\n<Xl\n\nci\n\n,\n\n....... .\n\n,/\n\n\'"ci\n\nbalanced\nunbalanced\n.,.-\'\n\no\nci ~---.---,r_--.---~--_r--_.~\n\n0.0\n\n0.00005\n\n0.0001\n\n0.00015\n\nThreshold\n\n0.0002\n\n0.0\n\n0.00002 0.00004 0.00006 0.00008 0.00010 0.00012\nThreshold\n\nFigure 4: Probability of error using averaged FFTs for (a) motor #1 and 319 inputs\n(b) motor #2 and 320 inputs.\nautoassociator with 320 inputs and 20 hidden units. Figure 3 shows our results on motor #3\nusing an autoassociator with 319 inputs.\nWe have found significant performance improvements by averaging several consecutive\nFFTs. In figure 4 we show the results for motors #1 and #2 when we averaged 11 FFTs to\nproduce the input features. Compare these curves to those in figure 2. In particular, notice\nthat the probability of error is much lower for the averaged FFTs when the good motor\ncurve crosses anyone of the faulty motor curves.\n\n7\n\nCandor System Design\n\nBased on our experiments with autoassociators, we designed a prototype mechanical motor\ncondition monitoring system. The functional system architecture is shown in figure 5. In\norder to control costs, the system is implemented on a PC. The system is designed so that\neach PC can monitor up to 128 motors using one 16-bit analog to digital converter. The\nsignals are collected, filtered and multiplexed on custom external signal processing cards.\nEach card supports up to eight motors (with up to 16 cards per PC).\nThe system records current measurements from one motor at a time. For each motor,\nmeasurements are collected, four FFTs are computed on non-overlapping time series, and\nthe four FFTs are averaged to produce a vector that is input to the neural network. The system\nreports that a motor is bad only if more than five of the last ten averaged FFTs produced an\nanomaly metric more than five standard deviations greater than the mean metric computed\non the training set. Otherwise the motor is reported to be normal. In addition to monitoring\nthe motors, the prototype systems are designed to record all measurements on tape to support\n\n\x0c930\n\nT. PETSCHE, A. MARCANTONIO, C. DARKEN, S. 1. HANSON, G. M. KUHN, I. SANTOSO\n\nGOOD\nBAD\n\nFigure 5: Functional architecture of Candor.\nfuture experiments with alternative algorithms and tuning to improve performance.\nTo date, three monitoring systems have been installed: in an oil refinery, in a testing\nlaboratory and on an office building ventilation system. The system has correctly detected\nthe only failure it has seen so far: when a filter on the inlet to a water circulation pump\nbecame clogged the spectrum changed so much that the average daily novelty metric jumped\nfrom less than one standard deviation above the training set average to more than twenty\nstandard deviations. We hope to have further test results in a year or so.\n\n8\n\nRelated work\n\nGluck and Myers (1993) proposed a model oflearning in the hippocampus based in part on\nan autoassociator which is used to detect novel stimuli and to compress the representation\nof the stimuli. This model has accurately predicted many of the classical conditioning\nbehaviors that have been observed in normal and hippocampal-damaged animals. Based on\nthis work, Japkowicz, Myers and Gluck (1995) independently derived an autoassociatorbased novelty detector for machine learning tasks similar to that used in our system.\nTogether with Gluck, we have tested an autoassociator based anomaly detector on helicopter\ngearbox failures for the US Navy. In this case, the autoassociator is given 512 inputs\nconsisting of 64 vibration based features from each of 8 accelerometers mounted at different\nlocations on the gearbox. In a blind test, the autoassociator was able to correctly distinguish\nbetween feature vectors taken from a damaged gearbox and other feature vectors taken\nfrom normal gearboxes, all recorded in flight. Our anomaly detector will be included in\ntest flights of a gearbox monitoring system later this year.\n\nReferences\nGluck, M. A. & Myers, C. E. (1993). Hippocampal mediation of stimulus representation:\nA compuational theory. Hippocampus, 3(4), 491-561.\nJapkowicz, N., Myers, c., & Gluck, M. A. (1995). A novelty detection approach to\nclassification. In Proceedings of the Fourteenth International Joint Conference on\nArtificial Intelligence.\nRumelhart, D ., Hinton, G., & Williams, R. (1986). Learning internal representations by\nerror propagation. In D . Rumelhart & J. McClelland (Eds.), Parallel Distributed\nProcessing (pp. 318-362). MIT Press.\nSchoen, R., Habetler, T., & Bartheld, R. (1994) . Motor bearing damage detection using\nstator current monitoring. In Proceedings of the IEEE lAS Annual Meeting.\n\n\x0c'
p83130
sg59
S'Visual gesture-based robot guidance\nwith a modular neural system\n\nE. Littmann,\n\nA. Drees, and H. Ritter\n\nAbt. Neuroinformatik, Fak. f. Informatik\nUniversitat Ulm, D-89069 Ulm, FRG\nenno@neuro.informatik.uni-ulm.de\n\nAG Neuroinformatik, Techn. Fakultat\nUniv. Bielefeld, D-33615 Bielefeld, FRG\nandrea,helge@techfak.uni-bielefeld.de\n\nAbstract\nWe report on the development of the modular neural system "SEEEAGLE" for the visual guidance of robot pick-and-place actions.\nSeveral neural networks are integrated to a single system that visually recognizes human hand pointing gestures from stereo pairs\nof color video images. The output of the hand recognition stage is\nprocessed by a set of color-sensitive neural networks to determine\nthe cartesian location of the target object that is referenced by the\npointing gesture. Finally, this information is used to guide a robot\nto grab the target object and put it at another location that can\nbe specified by a second pointing gesture. The accuracy of the current system allows to identify the location of the referenced target\nobject to an accuracy of 1 cm in a workspace area of 50x50 cm. In\nour current environment, this is sufficient to pick and place arbitrarily positioned target objects within the workspace. The system\nconsists of neural networks that perform the tasks of image segmentation, estimation of hand location, estimation of 3D-pointing\ndirection, object recognition, and necessary coordinate transforms.\nDrawing heavily on the use of learning algorithms, the functions of\nall network modules were created from data examples only.\n\n1\n\nIntroduction\n\nThe rapidly developing technology in the fields of robotics and virtual reality requires the development of new and more powerful interfaces for configuration and\ncontrol of such devices. These interfaces should be intuitive for the human advisor\nand comfortable to use. Practical solutions so far require the human to wear a\ndevice that can transfer the necessary information. One typical example is the data\nglove [14, 12]. Clearly, in the long run solutions that are contactless will be much\nmore desirable, and vision is one of the major modalities that appears especially\nsuited for the realization of such solutions.\nIn the present paper, we focus on a still restricted but very important task in robot\ncontrol, the guidance of robot pick-and-place actions by unconstrained human pointing gestures in a realistic laboratory environment. The input of target locations by\n\n\x0c904\n\nE. LITTMANN, A. DREES, H. RITTER\n\npointing gestures provides a powerful, very intuitive and comfortable functionality\nfor a vision-based man-machine interface for guiding robots and extends previous\nwork that focused on the detection of hand location or the discrimination of a small,\ndiscrete number of hand gestures only [10, 1, 2, 8]. Besides two color cameras, no\nspecial device is necessary to evaluate the gesture of the human operator.\nA second goal of our approach is to investigate how to build a neural system for\nsuch a complex task from several neural modules. The development of advanced\nartificial neural systems challenges us with the task of finding architect.ures for the\ncooperat.ion of multiple functional modules such that. part of the structure of the\noverall system can be designed at a useful level of abstraction, but at the same t.ime\nlearning can be used to create or fine-tune the functionality of parts of t.he system\non the basis of suit.able training examples.\nTo approach this goal requires to shift the focus from exploring t.he properties of\nsingle networks to exploring the propert.ies of entire systems of neural networks.\nThe work on "mixtures of experts" [3, 4] is one important contribution along these\nlines. While this is a widely applicable and powerful approach, there clearly is\na need to go beyond the exploration of strictly hierarchical systems and to gain\nexperience with architectures t.hat admit more complex types of information flow\nas required e.g. by the inclusion of feat.ures such as control of focal attention or\nreent.rant processing branches. The need for such features arose very naturally in\nthe context of the task described above, and in the following sect.ion we will report\nour results wit.h a system architecture that is crucially based on the exploitation of\nsuch elements.\n\n2\n\nSystem architecture\n\nOur system, described in fig. 1, is situated in a complex laboratory environment. A\nrobot arm with manipulator is mounted at one side of a table with several objects\nof different color placed on it. A human operator is positioned at the next side to\nthe right of the robot. This scenery is watched by two cameras from the other two\nsides from high above. The cameras yield a stereo color image of t.he scene (images\n10). The operator points with one hand at one of the objects on the table. On the\nbasis of the image information, the object is located and the robot grabs it. Then,\nthe operator points at another location, where the robot releases the object. 1\nThe syst.em consists of several hardware components: a PUMA 560 robot arm with\nsix axes and a three-fingered manipulator 2; two single-chip PULNIX color cameras;\ntwo ANDRox vision boards with software for data acquisition and processing; a\nwork space consisting of a table with a black grid on a yellow surface. Robot and\nperson refer to the same work space. Bot.h cameras must show both the human\nhand and the table with the objects. Within this constraint, the position of the\ncameras can be chosen freely as long as they yield significantly different views.\nAn important prerequisite for the recognition of the pointing direction is the segmentation of the human hand from the background scenery. This task is solved by\na LLM network (Sl) trained to yield a probability value for each image pixel to\nbelong to the hand region. The training is based on t.he local color information.\nThis procedure has been investigated in [7].\nAn important feature of the chosen method is the great reliability and robustness\nof both the classification performance and the localization accuracy of the searched\nobject. Furthermore, the performance is quite constant over a wide range of image\nresolutions. This allows a fast two-step procedure: First, the images are segmented\nin low resolution (Sl: 11 -+ A1) and the hand position is extracted. Then, a small\n1 In analogy to the sea eagle who watches its prey from high above, shoots down to grab\nthe prey, and then flies to a safe place to feed, we nicknamed our system "SEE-EAGLE".\n2Development by Prof. Pfeiffer, TV Munich\n\n\x0cVisual Gesture-based Robot Guidance with a Modular Neural System\n\n905\n\nFig. 1: System architecture. From two color camera images 10 we extract the hand position\n(11 I> Sl I> A1 (pixel coord.) I> P1 I> cartesian hand coord.). In a subframe centered on\nthe hand location (12) we determine the pointing direction (12 I> S2 I> A2 (pixel coord.) I>\nG I> D I> pointing angles). Pointing direction and hand location define a cartesian target\nlocation that is mapped to image coord. that define the centers of object subframes (10 I>\nP2 I> 13). There we determine the target object (13 I> S3 I> A3) and map the pixel coord.\nof its centers to world coord. (A3 I> P3 I> world target loc.). These coordinates are used\nto guide the robot R to the target object.\n\n\x0c906\n\nE. LITTMANN. A. DREES. H. RlTIER\n\nsubframe (12) around the estimated hand position is processed in high resolution\nby another dedicated LLM network (S2: 12 - t A2). For details of the segmentation\nprocess, refer to [6].\nThe extraction of hand information by LLMs on the basis of Gabor masks has\nalready been studied for hand posture [9] and orientation [5]. The method is based\non a segmented image containing the hand only (A2). This image is filtered by 36\nGabor masks that are arranged on a 3x3 grid with 4 directions per grid position\nand centered on the hand. The filter kernels have a radius of 10 pixels, the distance\nbetween the grid points is 20 pixels. The 36 filter responses (G) form the input\nvector for a LLM network (D). Further details of the processing are reported in [6].\nThe network yields the pointing direction of the hand (D: 12 - t G - t pointing\ndirection). Together with the hand position which is computed by a parametrized\nself-organizing map ("PSOM", see below and [11, 13]) (P1: Al - t cartesian hand\nposition), a (cartesian) target location in the workspace can be calculated. This\nlocation can be retransformed by the PSOM into pixel coordinates (P2: cartesian\ntarget location - t target pixel coordinates). These coordinates define the center of\nan "attention region" (13) that is searched for a set of predefined target objects.\nThis object recognition is performed by a set of LLM color segmentation networks\n(S3: 13 - t A3), each previously trained for one of the defined targets. A ranking\nprocedure is used to determine the target object. The pixel coordinates ofthe target\nin the segmented image are mapped by the PSOM to world coordinates (P3: A3 - t\ncartesian target position). The robot R now moves to above these world coordinates,\nmoves vertically down, grabs whatever is there, and moves upward again. Now, the\nsystem evaluates a second pointing gesture that specifies the place where to place\nthe object. This time, the world coordinates calculated on the basis of the pointing\ndirection from network D and the cartesian hand location from PSOM PI serve\ndirectly as target location for the robot.\nFor our processing we must map corresponding pixels in the stereo images to cartesian world coordinates. For these transformations, training data was generated\nwith aid of the robot on a precise sampling grid. We automatically extract the\npixel coordinates of a LED at the tip of the robot manipulator from both images.\nThe seven-dimensional feature vector serves as training input for an PSOM network [11]. By virtue of its capability to represent a transformation in a symmetric,\n"multiway" -fashion, this offers the additional benefit that both the camera-to-world\nmapping and its inverse can be obtained with a single network trained only once on\na data set of 27 calibration positions of the robot. A detailed description for such\na procedure can be found in [13].\n\n3\n\nResults\n\n3.1 System performance\nThe accuracy of the current system allows to estimate the pointing target to an\naccuracy of 1 ? 0.4 cm (average over N = 7 objects at randomly chosen locations\nin the workspace) in a workspace area of 50x50 cm. In our current environment,\nthis is sufficient to pick and place any of the seven defined target objects at any\nlocation in the workspace. This accuracy can only be achieved if we use the object\nrecognition module described in sec. 2. The output of the pointing direction module\napproximates the target location with an considerably lower accuracy of 3.6? 1.6 cm.\n3.2 Image segmentation\nThe problem to evaluate these preprocessing steps has been discussed previously [7],\nespecially the relation of specifity and sensitivity of the network for the given task.\nAs the pointing recognition is based on a subframe centered on the hand center, it\nis very sensitive to deviations from this center so that a good localization accuracy\n\n\x0cVisual Gesture-based Robot Guidance with a Modular Neural System\n\n907\n\nis even more important than the classification rate. The localization accuracy is\ncalculated by measuring the pixel distance between the centers determined manually on the original image and as the center of mass in the image obtained after\napplication of the neural network. Table 1 provides quantitative results.\nOn the whole) the two-step cascade of LLM networks yields for 399 out of 400 images\nan activity image precisely centered on the human hand. Only in one image) the\nfirst LLM net missed the hand completely) due to a second hand in the image that\ncould be clearly seen in this view. This image was excluded from further processing\nand from the evaluation of the localization accuracy.\n\nPerson A\nPerson H\n\nCamera A\nPixel deviatIOn\nNRMSE\n0.8 ? 1.2\n0.03 ? 0.06\n1.3 ? 1.4\n0.06 ? 0.11\n\nCamera B\nPixel deViatIOn\nNRMSE\n0.8 ? 2.2\n0.03 ? 0.09\n2.2 ? 2.8\n0.11 ? 0.21\n\nTable 1: Estimation error of the hand localization on the test set. Absolute error in pixels\nand normalized error for both persons and both camera images.\n\n3.3 Recognition performance\nOne major problem in recognizing human pointing gestures is the variability of these\ngestures and their measurement for the acquisition of reliable training information.\nDifferent persons follow different strategies where and how to point (fig. 2 (center)\nand (right?. Therefore) we calculate this information indirectly. The person is\ntold to point at a certain grid position with known world coordinates. From the\ncamera images we extract the pixel positions of the hand center and map them to\nworld coordinates using the PSOM net (PI in fig . 1). Given these coordinates the\nangles of the intended pointing vector with the basis vectors of the world coordinate\nsystem can be calculated trigonometrically. These angles form the target vector for\nthe supervised training of a LLM network (D in fig. 1).\n\nAfter training) the output of the net is used to calculate the point where the pointing\nvector intersects the table surface. For evaluation of the network performance we\nmeasure the Euclidian distance between this point and the actual grid point where\nthe person intended to point at. Fig. 3 (left) shows the mean euclidean error MEE\nof the estimated target position as a function of the number of learning steps. The\nerror on the training set can be considerably reduced) whereas on the test set the\nimprovement stagnates after some 500 training steps. If we perform even more\ntraining steps the performance might actually suffer from overfitting. The graph\ncompares training and test results achieved on images obtained by two different\nways of determining the hand center. The "manual" curves show the performance\nthat can be achieved if the Gabor masks are manually centered on the hand. For\nthe "neuronal)) curves) the center of mass calculated in the fine-segmented and postprocessed subframe was used. This allows us to study the influence of the error of\nthe segmentation and localization steps on the pointing recognition. This influence\nis rather small. The MEE increases from 17 mm for the optimal method to 19 mm\nfor the neural method) which is hardly visible in practice.\nThe curves in fig. 3 (center) are obtained if we apply the networks to images of\nanother person. The MEE is considerably larger but a detailed analysis\' shows\nthat part of this deviation is due to systematic differences in the pointing strategy\nas shown in fig. 2 (right). Over a wide range, the number of nodes used for the\nLLM network has only minor influence on the performance. While obviously the\nperformance on the training set can be arbitrarily improved by spending more nodes,\nthe differences in the MEE on the test set are negligible in a range of 5 to 15 nodes.\nUsing more nodes is problematic as the training data consists of 50 examples only.\nIf not indicated otherwise) we use LLM networks with 10 nodes. Further results)\n\n\x0c908\n\nE. LIITMANN. A. DREES. H. RIITER\n\nFig. 2: The table grid points can be reconstructed according to the network output. The\ntarget grid is dotted . Reconstruction of training grid (left) and test grid (center) for one\nperson, and of the test grid for another person (right).\nMEB on test oet of unknown perron\n\nMER\n30\n\n20\n\ne?\n\nI~\n\n10\n\n~\n\n---- ~--.---\n\n~\n~-\n\n0\n\nn\n\nm..... aI,trainneuronal, train manual, test -\n\n:l~\n\n100\n\n250\n\nsao\n\n1000 2SOO SOOO\n\ntrain.., itHabonr\n\ne\n?\n\n70\n68\n66\n64\n62\n60\n58\n56\n\n4\n\n-~.\n\n100\n\n:l~\n\nsao\n\n1000\n\n2SOO SOOO\n\nFig. 3: The euclidean error of\nestimated target point calculated using the network output depends on the preprocessing (left), and the person\n(center).\n\ntrairq IteratioN\n\ncomparing the pointing recognition based on only one of the camera images, indicate\nthat the method works better if the camera takes a lateral view rather than a frontal\nview . All evaluations were done for both persons. The performance was always very\nsimilar.\n\n4\n\nDiscussion\n\nWhile we begin to understand many properties of neural networks at the single\nnetwork level, our insight into principled ways of how to build neural systems is\nstill rather limited . Due to the complexity of this task, theoretical progress is\n(and probably will continue to be) very slow. What we can do in the mean time,\nhowever, is to experiment with different design strategies for neural systems and\ntry to "evolve" useful approaches by carefully chosen case studies.\nThe current work is an effort along these lines. It is focused on a challenging,\npractically important vision task with a number of generic features that are shared\nwith vision tasks for which biological vision systems were evolved.\nOne important issue is how to achieve robustness at the different processing levels\nof the system. There are only very limited possibilities to study this issue in simulations, since practically nothing is known about the statistical properties of the\nvarious sources of error that occur when dealing with real world data. Thus, a real\nimplementation that works with actual data is practically the only way to study\nthe robustness issue in a realistic fashion. Therefore, the demonstrated integration\nof several functional modules that we had developed previously in more restricted\nsettings [7, 6] was a non-trivial test of the feasability of having these functions\ncooperate in a larger, modular system. It also gives confidence that the scaling\nproblem can be dealt with successfully if we apply modular neural nets.\nA related and equally important issue was the use of a processing strategy in which\nearlier processing stages incrementally restrict the search space for the subsequent\nstages. Thus, the responsibility for achieving the goal is not centralized in any single\nmodule and subsequent modules have always the chance to compensate for limited\nerrors of earlier stages. This appears to be a generally useful strategy for achieving\n\n\x0cVisual Gesture-based Robot Guidance with a Modular Neural System\n\n909\n\nrobustness and for cutting computational costs that is related to the use of "focal\nattention" , which is clearly an important element of many biological vision systems.\nA third important point is the extensive use of learning to build the essential constituent functions of the system from data examples. We are not yet able to train\nthe assembled system as a whole. Instead, different modules are trained separately\nand are integrated only later. Still, the experience gained with assembling a complex system via this "engineering-type" of approach will be extremely valuable for\ngradually developing the capability of crafting larger functional building blocks by\nlearning methods.\nWe conclude that carefully designed experiments with modular neural systems that\nare based on the use of real world data and that focus on similar tasks for which\nalso biological neural systems were evolved can make a significant contribution in\ntackling the challenge that lies ahead of us: to develop a reliable technology for the\nconstruction of large-scale artificial neural systems that can solve complex tasks in\nreal world environments.\nAcknowledgements\nWe want to thank Th. Wengerek (robot control), J. Walter (PSOM implementation), and\nP. Ziemeck (image acquisition software). This work was supported by BMFT Grant No.\nITN9104AO.\n\nReferences\n[1] T. J. Darell and A. P. Pentland. Classifying hand gestures with a view-based distributed representation. In J . D. Cowan, G. Tesauro, and J. Alspector, editors, Neural\nInformation Processing Systems 6, pages 945-952. Morgan Kaufman, 1994.\n[2] J. Davis and M. Shah. Recognizing hand gestures. In J.-O. Eklundh, editor, Computer\nVision - ECCV \'94, volume 800 of Lecture Notes in Computer Science, pages 331340. Springer-Verlag, Berlin Heidelberg New York, 1994.\n[3] R.A. Jacobs, M.1. Jordan, S.J. Nowlan, and G.E. Hinton. Adaptive mixtures of local\nexperts. Neural Computation, 3:79- 87, 1991.\n[4] M.1. Jordan and R.A. Jacobs. Hierarchical mixtures of experts and the EM algorithm.\nNeural Computation, 6(2):181-214, 1994.\n[5] F. Kummert, E. Littmann, A. Meyering, S. Posch, H. Ritter, and G. Sagerer. A\nhybrid approach to signal interpretation using neural and semantic networks. In\nMustererkennung 1993, pages 245-252. Springer, 1993.\n[6] E. Littmann, A. Drees, and H. Ritter. Neural recognition of human pointing gestures\nin real images. Submitted to Neural Processing Letters, 1996.\n[7] E. Littmann and H. Ritter. Neural and statistical methods for adaptive color segmentation - a comparison. In G. Sagerer, S. Posch, and F. Kummert, editors,\nMustererkennung 1995, pages 84-93. Springer-Verlag, Heidelberg, 1995.\n[8] C. Maggioni. A novel device for using the hand as a human-computer interface. In\nProceedings HC1\'93 - Human Control Interface, Loughborough, Great Britain, 1993.\n[9] A. Meyering and H. Ritter. Learning 3D shape perception with local linear maps. In\nProc. of the lJCNN, volume IV, pages 432-436, Baltimore, MD, 1992.\n[10] Steven J. Nowlan and John C. Platt. A convolutional neural network hand tracker.\nIn Neural Information Processing Systems 7. Morgan Kaufman Publishers, 1995.\n[11] H. Ritter. Parametrized self-organizing maps for vision learning tasks. In P. Morasso,\neditor, ICANN \'94. Springer-Verlag, Berlin Heidelberg New York, 1994.\n[12] K. Viiiina.nen and K. Bohm. Gesture driven interaction as a human factor in virtual\nenvironments - an approach with neural networks. In R. Earnshaw, M. Gigante, and\nH. Jones, editors, Virtual reality systems, pages 93-106. Academic Press, 1993.\n[13] J. Walter and H. Ritter. Rapid learning with parametrized self-organizing maps.\nNeural Computing, 1995. Submitted.\n[14] T. G. Zimmermann, J. Lanier, C. Blanchard, S. Bryson, and Y. Harvill. A hand\ngesture interface device. In Proc. CHI+GI, pages 189-192, 1987.\n\n\x0c'
p83131
sg484
S'Improving Committee Diagnosis with\nResampling Techniques\n\nBambang Parmanto\nDepartment of Information Science\nUniversity of Pittsburgh\nPittsburgh, PA 15260\nparmanto@li6.pitt. edu\n\nPaul W. Munro\nDepartment of Information Science\nUniversity of Pittsburgh\nPittsburgh, PA 15260\nmunro@li6.pitt. edu\n\nHoward R. Doyle\nPittsburgh Transplantation Institute\n3601 Fifth Ave, Pittsburgh, PA 15213\ndoyle@vesaliw. tu. med. pitt. edu\n\nAbstract\nCentral to the performance improvement of a committee relative to\nindividual networks is the error correlation between networks in the\ncommittee. We investigated methods of achieving error independence between the networks by training the networks with different\nresampling sets from the original training set. The methods were\ntested on the sinwave artificial task and the real-world problems of\nhepatoma (liver cancer) and breast cancer diagnoses.\n\n1\n\nINTRODUCTION\n\nThe idea of a neural net committee is to combine several neural net predictors\nto perform collective decision making, instead of using a single network (Perrone,\n1993). The potential of a committee in improving classification performance has\nbeen well documented. Central to this improvement is the extent to which the\nerrors tend to coincide. Committee errors occur where the misclassification sets of\nindividual networks overlap. On the one hand, if all errors of committee members\ncoincide, using a committee does not improve performance. On the other hand, if\nerrors do not coincide, performance of the committee dramatically increases and\nasymptotically approaches perfect performance. Therefore, it is beneficial to make\nthe errors among the networks in the committee less correlated in order to improve\nthe committee performance.\n\n\x0cImproving Committee Diagnosis with Resampling Techniques\n\n883\n\nOne way of making the networks less correlated is to train them with different sets\nof data. Decreasing the error correlation by training members of the committee\nusing different sets of data is intuitively appealing. Networks trained with different\ndata sets have a higher probability of generalizing differently and tend to make\nerrors in different places in the problem space.\nThe idea is to split the data used in the training into several sets. The sets are\nnot necessarily mutually exclusive, they may share part of the set (overlap). This\nidea resembles resampling methods such as cross-validation and bootstrap known\nin statistics for estimating the error of a predictor from limited sets of available\ndata. In the committee framework, these techniques are recast to construct different\ntraining sets from the original training set. David Wolpert (1992) has put forward\na general framework of training the committee using different partitions of the\ndata known as stacked generalization. This approach has been adopted to the\nregression environment and is called stacked regression (Breiman, 1992). Stacked\nregression uses cross-validation to construct different sets of regression functions.\nA similar idea of using a bootstrap method to construct different training sets has\nbeen proposed by Breiman (1994) for classification and regression trees predictors.\n\n2\n2.1\n\nTHE ALGORITHMS\nBOOTSTRAP COMMITTEE (BOOTC)\n\nConsider a total of N items are available for training. The approach is to generate\nK replicates from the original set, each containing the same number of item as the\noriginal set. The replicates are obtained from the original set by drawing at random\nwith replacement. See Efron & Tibshirani (1993) for background on bootstrapping.\nUse each replicate to train each network in the committee.\nUsing this bootstrap procedure, each replicate is expected to include roughly 36\n% duplicates (due to replacement during sampling). Only the distinct fraction is\nused for training and the leftover fraction for early stopping, if necessary (notice\nslight difference from the standard bootstrapping and from Breiman\'s bagging).\nEarly stopping usually requires a fraction of the data to be taken from the original\ntraining set, which might degrade the performance of the neural network. The\nadvantage of a BOOTC is that the leftover sample is already available.\nAlgorithm:\n1. Generate bootstrap replicates Ll, ... , LK from the original set.\n\n2. For each bootstrap replicate, collect unsampled items into leftover sample\n..\nl*l , ... , l*K .\nset s, gIVIng:\n3. For each Lk, train a network. Use the leftover set l*k as validation stopping\ncriteria if necessary. Giving K neural net predictors: f(~i Lk)\n4. Build a committee from the bootstrap networks using a simple averaging\nprocedure: fcom(~) =\n~~=l f(~i Lk)\n\nic\n\nThere is no rule as to how many bootstrap replicates should be used to achieve a\ngood performance. In error estimation, the number ranges from 20 to 200. It is\nbeneficial to keep the number of replicates, hence the number of networks, small to\nreduce training time. Unless the networks are trained on a parallel machine, training\ntime increases proportionally to the number of networks in the committee. In this\nexperiment, 20 bootstrap training replicates were constructed for 20 networks in\n\n\x0c884\n\nB. PARMANTO, P. W. MUNRO, H. R. DOYLE\n\nthe committee. Twenty replicates were chosen since beyond this number there is\nno significant improvement on the performance.\n\n2.2\n\nCROSS-VALIDATION COMMITTEE (CVC)\n\nThe algorithm is quite similar to the procedure used in prediction error estimation.\nFirst, generate replicates from the original training set by removing a fraction of\nthe data. Let D denote the original data, and D- V denote the data with subset\nv removed. The procedure revolves so that each item is in the removed fraction\nat least once. Generate replicates D11Jl , ??? Di/Ie and train each network in the\ncommittee with one replicate.\nAn important issue in the eve is the degree of data overlap between the replicates.\nThe degree of overlap depends on the number of replicates and the size of a removed\nfraction from the original sample. For example, if the committee consists of 5\nnetworks and 0.5 of the data are removed for each replicate, the minimum fraction\nof overlap is 0 (calculation: (v x 2) - 1.0) and the maximum is ~ (calculation:\n1.0 -\n\nk)\'\n\nAlgorithm:\n\n1. Divide data into v-fractions db . . . , dv\n2. Leave one fraction die and train network fie with the rest of the data (D-d le ).\n3. Use die as a validation stopping criteria, if necessary.\n4. Build a committee from the networks using a simple averaging procedure.\nThe fraction of data overlap determines the trade-off between the individual network\nperformance and error correlation between the networks. Lower correlation can be\nexpected if the networks train with less overlapped data, which means a larger\nremoved fraction and smaller fraction for training. The smaller the training set\nsize, the lower the individual network performance that can be expected.\nWe investigated the effect of data overlap on the error correlations between the\nnetworks and the committee performance. We also studied the effect of training\nsize on the individual performance. The goal was to find an optimal combination\nof data overlap and individual training size.\n\n3\n\nTHE BASELINE & PERFORMANCE EVALUATION\n\nTo evaluate the improvement of the proposed methods on the committee performance, they should be compared with existing methods as the baseline. The common method for constructing a committee is to train an ensemble of networks\nindependently. The networks in the committee are initialized with different sets\nof weights. This type of committee has been reported as achieving significant improvement over individual network performances in regression (Hashem, 1993) and\nclassification tasks (Perrone, 1993; Parmanto et al., 1994).\nThe baseline, BOOTe, and eve were compared using exactly the same architecture\nand using the same pair of training-test sets. Performance evaluation was conducted\nusing 4-fold exhaustive cross-validation where 0.25 fraction of the original data is\nused for the test set and the remainder of the data is used for the training set. The\nprocedure was repeated 4 times so that all items were once on the test set. The\nperformance was calculated by averaging the results of 4 test sets. The simulations\n\n\x0cImproving Committee Diagnosis with Resampling Techniques\n\n885\n\nwere conducted several times using different initial weights to exclude the possibility\nthat the improvement was caused by chance.\n\n4\n4.1\n\nEXPERIMENTS\nSYNTHETIC DATA: SINWAVE CLASSIFICATION\n\nThe sinwave task is a classification problem with two classes, a negative class represented as 0 and a positive class represented as 1. The data consist of two input\nvariables, x = (Xli X2). The entire space is divided equally into two classes with\nthe separation line determined by the curve X2 = sin( 2: Xl). The upper half of the\nrectangle is the positive class, while the lower half is the negative one (see Fig. 1).\nGaussian noise along the perfect boundary with variance of 0.1 is introduced to\nthe clean data and is presented in Fig. 1 (middle). Let z be a vector drawn from\nthe Gaussian distribution with variance TI, then the classification rule is given by\nequation:\n(1)\nA similar artificial problem is used to analyze the bias-variance trade-offs by Geman\net al. (1992).\n\nFigure 1: Complete and clean data/without noise (top), complete data with noise\n(middle), and a small fraction used for training (bottom).\nThe population contains 3030 data items, since a grid of 0.1 is used for both Xl and\nX2 . In the real world, we usually have no access to the entire population. To mimic\nthis situation, the training set contained only a small fraction of the population.\nFig. 1 (bottom) visualizes a training set that contains 200 items with 100 items for\neach class. The training set is constructed by randomly sampling the population.\nThe performance of the predictor is measured with respect to the test set. The\npopulation (3030 items) is used as the test set.\n\n4.2\n\nHEPATOMA DETECTION\n\nHepatoma is a very important clinical problem in patients who are being considered\nfor liver transplantation for its high probability of recurrence. Early hepatoma\ndetection may improve the ultimate outlook of the patients since special treatment\ncan be carried out. Unfortunately, early detection using non-invasive procedures\n\n\x0c886\n\nB. PARMANTO, P. W. MUNRO, H. R. DOYLE\n\ncan be difficult, especially in the presence of cirrhosis. We have been developing\nneural network classifiers as a detection system with minimum imaging or invasive\nstudies (Parmanto et al., 1994).\nThe task is to detect the presence or absence (binary output) of a hepatoma given\nvariables taken from an individual patient. Each data item consists of 16 variables,\n7 of which are continuous variables and the rest are binary variables, primarily\nblood measurements.\nFor this experiment, 1172 data items with their associated diagnoses are available.\nOut of 1172 itmes, 693 items are free from missing values, 309 items contain missing\nvalues only on the categorical variables, and 170 items contain missing values on\nboth types of variables. For this experiment, only the fraction without missing\nvalues and the fraction with missing values on the categorical variables were used,\ngiving the total item of 1002. Out of the 1002 items, 874 have negative diagnoses\nand the remaining 128 have positive diagnoses.\n\n4.3\n\nBREAST CANCER\n\nThe task is to diagnose if a breast cytology is benign or malignant based on cytological characteristics. Nine input variables have been established to differentiate\nbetween the benign and malignant samples which include clump thickness, marginal\nadhesion, the uniformity of cell size and shape, etc.\nThe data set was originally obtained from the University of Wisconsin Hospitals\nand currently stored at the UCI repository for machine learning (Murphy & Aha,\n1994). The current size of the data set is 699 examples.\n\n5\n\nTHE RESULTS\nCommittee Performance\n\nIndiv. Performance\n\n~ ~.::.:.:-:~~~?:\n.: : ..::.::---.-.-.........---.. . .\n\n?\n? :!\n\n---.... _---\n\n,I; N\n\n~ .....\n\no\n\n4\n\n6\n\n10\n\n12\n\n14\n\n16\n\n4\n\n8\n\nbas.an.\n\n0\n\n-\n\n?\n\n::-:::.\n\n&10...,,,,\n\n10\n\n12\n\n14\n\n/I hidden units\n\n/I hidden units\n\nCorrelation\n\nPercent Improvement\n\n16\n\n0r------------------.\n\n.... -... -\n\n-~-------~-------~\n......-. ....-.........\n-- ...._-. --_._..._---.\n" , --.\n\nQ\n\no\n\no~\n\n4\n\n-~. . . .\n\n& :::: li&>mr",\n________________\n6\n\n10\n\n12\n\nII hidden units\n\n14\n\n~\n\n16\n\n8\n\n10\n\n/I hidden\n\n12\n\n14\n\n16\n\n...,its\n\nFigure 2: Results on the sinwave classif. task. Performances of individual nets\nand the committee (top); error correlation and committee improvement (bottom).\nFigure 2. (top) and Table 1. show that the performance of the committee is always\nbetter than the average performance of individual networks in all three committees.\n\n\x0cImproving Committee Diagnosis with Resampling Techniques\n\nTask\n\nMethods\n\nSmwave\n(2 vars )\n\nBaseline\nBOOTC\nCVC\nBaseline\nBOOTC\nCVC\nBaSeline\nBOOTC\nCVC\n\nCancer\n(9 vars)\nHepatoma\n(16 vars)\n\nIndiv. Nets\n% error\n13.31\n12.85\n15.72\n2.7\n3.14\n3.2\n25.95\n26.00\n26.90\n\nError\nCorr\n.87\n.57\n.33\n.96\n.83\n.80\n.89\n.70\n.55\n\nCommittee\n% error\n11.8\n8.36\n9.79\n2.5\n2.0\n1.63\n23.25\n19.72\n19.05\n\n887\n\nImprov.\nto Indiv.\n11 \'70\n35 %\n38 %\n5%\n34 %\n49 %\n10.5 %\n24 %\n29 %\n\nImprov.\nto baseline\n\n29 %\n17 %\n\n20 %\n35 %\n\n15.2 %\n18 %\n\nTable 1: Error rate, correlation, and performance improvement calculated based on\nthe best architecture for each method. Reduction of misclassification rates compare\nto the baseline committee\nCorrelation vs . Fraction of Data Overlap\n\n0r-----------------------____- .\nm\n\n?\n\n.,.,\nT\n\nN\n\no\n\n!\n\ni~ ,,\nFraction 01data overlap\n\nFigure 3: Error correlation and fraction of overlap in training data (results from\nthe sinwave classification task).\n\nThe CVC and BOOTC are always better than the baseline even when the individual\nnetwork performance is worse. Figure 2 (bottom) and the table show that the\nimprovement of a committee over individual networks is proportional to the error\ncorrelation between the networks in the committee. The CVC consistently produces\nsignificant improvement over its individual network performance due to the low error\ncorrelation, while the baseline committee only produces modest improvement. This\nresult confirms the basic assumption of this research: committee performance can\nbe improved by decorrelating the errors made by the networks.\nThe performance of a committee depends on two factors: individual performance of\nthe networks and error correlation between the networks. The gain of using BOOTC\nor CVC depends on how the algorithms can reduce the error correlations while still\nmaintaining the individual performance as good as the individual performance of the\nbaseline. The BOOTC produced impressive improvement (29 %) over the baseline\non the sinwave task due to the lower correlation and good individual performance.\nThe performances of the BOOTC on the other two tasks were not as impressive\ndue to the modest reduction of error correlation and slight decrease in individual\nperformance. The performances were still significantly better than the baseline\ncommittee. The CVC, on the other hand, consistently reduced the correlation and\n\n\x0c888\n\nB. PARMANTO, P. W. MUNRO, H. R. DOYLE\n\nimproved the committee performance. The improvement on the sinwave task was\nnot as good as the BOOTC due to the low individual performance.\nThe individual performance of the CVC and BOOTC in general are worse than the\nbaseline. The individual performance of CVC is 18 % and 19 % lower than the\nbaseline on the sinwave and cancer tasks respectively, while the BOOTC suffered\nsignificant reduction of individual performance only on the cancer task (16 %). The\ndegradation of individual performance is due to the smaller training set for each\nnetwork on the CVC and the BOOTC. The detrimental effect of a small training\nset, however, is compensated by low correlation between the networks. The effect\nof a smaller training set depends on the size of the original training set. If the data\nsize is large, using a smaller set may not be harmful. On the contrary, if the data set\nis small, using an even smaller data set can significantly degrade the performance.\nAnother interesting finding of this experiment is the relationship between the error\ncorrelation and the overlap fraction in the training set. Figure 3 shows that small\ndata overlap causes the networks to have low correlation to each other.\n\n6\n\nSUMMARY\n\nTraining committees of networks using different set of data resampled from the\noriginal training set can improve committee performance by reducing the error correlation among the networks in the committee. Even when the individual network\nperformances of the BOOTC and CVC degrade from the baseline networks, the\ncommittee performance is still better due to the lower correlation.\nAcknowledgement\n\nThis study is supported in part by Project Grant DK 29961 from the National\nInstitutes of Health, Bethesda, MD. We would like to thank the Pittsburgh Transplantation Institute for providing the data for this study.\nReferences\n\nBreiman, L, (1992) Stacked Regressions, TR 367, Dept. of Statistics., UC. Berkeley.\nBreiman, L, (1994) Bagging Predictors, TR 421, Dept. of Statistics, UC. Berkeley.\nEfron, B., & Tibshirani, R.J. (1993) An Introd. to the Bootstrap. Chapman & Hall.\nHashem, S. (1994). Optimal Linear Combinations of Neural Networks. PhD Thesis,\nPurdue University.\nGeman, S., Bienenstock, E., and Doursat, R. (1992) Neural networks and the\nbias/variance dilemma. Neural Computation, 4(1), 1-58.\nMurphy, P. M., &. Aha, D. W. (1994). UCI Repository of machine learning databases\n[ftp: ics.uci.edu/pub/machine-Iearning-databases/]\nParmanto, B., Munro, P.W., Doyle, H.R., Doria, C., Aldrighetti, 1., Marino, I.R.,\nMitchel, S., and Fung, J.J. (1994) Neural network classifier for hepatoma detectipn.\nProceedings of the World Congress of Neural Networks 1994 San Diego, June 4-9.\nPerrone, M.P. (1993) Improving Regression Estimation: Averaging Methods for\nVariance Reduction with Eztension to General Convez Measure Optimization. PhD\nThesis, Department of Physics, Brown University.\nWolpert, D. (1992). Stacked generalization, Neural Networks, 5, 241-259.\n\n\x0c'
p83132
sg38
S'Dynamics of On-Line Gradient Descent\nLearning for Multilayer Neural Networks\n\nDavid Saad"\nDept. of Comp o Sci. & App. Math.\nAston University\nBirmingham B4 7ET, UK\n\nSara A. Solla t\nCONNECT, The Niels Bohr Institute\nBlegdamsdvej 17\nCopenhagen 2100, Denmark\n\nAbstract\nWe consider the problem of on-line gradient descent learning for\ngeneral two-layer neural networks. An analytic solution is presented and used to investigate the role of the learning rate in controlling the evolution and convergence of the learning process.\nLearning in layered neural networks refers to the modification of internal parameters\n\n{J} which specify the strength of the interneuron couplings, so as to bring the map\n\n1.\n\nfJ implemented by the network as close as possible to a desired map\nThe\ndegree of success is monitored through the generalization error, a measure of the\ndissimilarity between fJ and\n\n1.\n\ne\n\nConsider maps from an N-dimensional input space onto a scalar (, as arise in\nthe formulation of classification and regression tasks. Two-layer networks with an\narbitrary number of hidden units have been shown to be universal approximators\n[1] for such N-to-one dimensional maps. Information about the desired map i is\nprovided through independent examples (e, (1\'), with (I\' = i(e) for all p . The\nexamples are used to train a student network with N input units, K hidden units,\nand a single linear output unit; the target map\nis defined through a teacher\nnetwork of similar architecture except for the number M of hidden units. We\ninvestigate the emergence of generalization ability in an on-line learning scenario\n[2], in which the couplings are modified after the presentation of each example so\nas to minimize the corresponding error. The resulting changes in {J} are described\nas a dynamical evolution; the number of examples plays the role of time .\nIn this paper we limit our discussion to the case of the soft-committee machine\n[2], in which all the hidden units are connected to the output unit with positive\ncouplings of unit strength, and only the input-to-hidden couplings are adaptive.\n\ni\n\n*D.Saad@aston.ac.uk\ntOn leave from AT&T Bell Laboratories, Holmdel, NJ 07733, USA\n\n\x0cDynamics of On-line Gradient Descent Learning for Multilayer Neural Networks\n\n303\n\nConsider the student network: hidden unit i receives information from input unit\nr through the weight hr, and its activation under presentation of an input pattern\n~ = (6,? .. ,~N) is Xi = J i .~, with J i = (hl, ... ,JiN) defined as the vector of\nincoming weights onto the i-th hidden unit. The output of the student network is\na(J,~) = L:~l 9 (Ji . ~), where 9 is the activation function of the hidden units,\ntaken here to be the error function g(x) == erf(x/V2), and J == {Jdl<i<K is the set\nof input-to-hidden adaptive weights.\n- Training examples are of the form (e, (Il) . The components of the independently\ndrawn input vectors ~Il are un correlated random variables with zero mean and\nunit variance. The corresponding output (Il is given by a deterministic teacher\nwhose internal structure is the same as for the student network but may differ in\nthe number of hidden units. Hidden unit n in the teacher network receives input\ninformation through the weight vector Bn = (B nl , ... , BnN), and its activation\nis Y~ = Bn .\nThe corresponding\nunder presentation of the input pattern\noutput is (Il = L:~=l 9 (Bn ?e). We will use indices i,j,k,l ... to refer to units\nin the student network, and n, m, ... for units in the teacher network.\n\ne\n\ne.\n\nThe error made by a student with weights J on a given input\nquadratic deviation\n\n~\n\nis given by the\n\n(1)\nPerformance on a typical input defines the generalization error Eg(J)\n< E(J ,~) >{O through an average over all possible input vectors ~, to be performed implicitly through averages over the activations x = (Xl"\'" XK) and\nY = (YI\nYM). Note that both < Xi >=< Yn >= 0; second order correlations are\ngiven by the overlaps among the weight vectors associated with the various hidden\nunits: < Xi Xk > = J i . Jk == Qikl < Xi Yn > = J i . Bn == Rin, and < Yn Ym > =\nBn . Bm == Tnm. Averages over x and yare performed using the resulting multivariate Gaussian probability distribution, and yield an expression for the generalization\nerror in terms of the parameters Qik l Rin, and Tnm [3]. For g(x) == erf(x/V2) the\nresult is:\nI \' ?? I\n\n1 {,""\n\n-\n\nL...J arCSlll\n\n7r\' k\n\nz\n\n- 2 ,""\nL...J arCSlll\n\n.\nsn\n\nQik\nV1+Qii V1+Qu\nRin\n\nV1 + Qi; V1 + Tnn\n\nTnm\n+ ,""\nL...J arCSlll --;:;=:=;:;;;=---;:::=:::::::;:;;;:==\nnm\n} .\n\nV1+Tnn V1+Tmm\n\n(2)\n\nThe parameters Tnm are characteristic of the task to be learned and remain fixed.\nThe overlaps Qik and Rin, which characterize the correlations among the various\nstudent units and their degree of specialization towards the implementation of the\ndesired task, are determined by the student weights J and evolve during training.\nA gradient descent rule for the update of the student weights results in Jf+l =\nJf + bf\nwhere the learning rate TJ has been scaled with the input size N, and\n\nN e,\n\nor "g\'(xf) [~g(y~) - ~g(xj\')l\n\n(3)\n\nis defined in terms of both the activation function 9 and its derivative g\'. The time\nevolution of the overlaps Rin and Qik can be explicitly written in terms of similar\n\n\x0cD. SAAD. S. A. SOLLA\n\n304\n\ndifference equations. In the large N limit the normalized number of examples\nQ\' = piN can be interpreted as a continuous time variable, leading to the equations\nof motion\n\n(4)\nto be averaged over all possible ways in which an example can be chosen at a given\ntime st.ep. The dependence on the current input is only through the activations\nx and y; the corresponding averages can be performed analytically for g(x) =\nerf( x I v\'2), resulting in a set of coupled first-order differential equations [3]. These\ndynamical equations are exact, and provide a novel tool used here to analyze the\nlearning process for a general soft-committee machine with an arbitrary number ]{\nof hidden units, trained to implement a task defined through a teacher of similar\narchitecture except for the number M of hidden units. In what follows we focus on\nuncorrelated teacher vectors of unit length, Tnm = onm.\n\ne\n\nThe time evolution of the overlaps Rin and Qik follows from integrating the equations of motion (4) from initial conditions determined by a random initialization of\nthe student vectors {Jdl<i<K. Random initial norms Qii for the student vectors\nare taken here from a unIform distribution in the [0,0.5] interval. Overlaps Qik\nbetween independently chosen student vectors Ji and Jk, or ~n between J i and\nan unknown teacher vector Bn are small numbers, of order 1/VN for N ~ ]{, M,\nand taken here from a uniform distribution in the [0,10- 12] interval.\nWe show in Fig. 1a-c the evolution of the overlaps and generalization error for a\nrealizable case: ]{ = M = 3 and "I = 0.1. This example illustrates the successive regimes of the learning process. The system quickly evolves into a symmetric\nsubspace controlled by an unstable suboptimal solution which exhibits no differentiation among the various student hidden units. Trapping in the symmetric subspace\nprevents the specialization needed to achieve the optimal solution, and the generalization error remains finite, as shown by the plateau in Fig. 1c. The symmetric\nsolution is unstable, and the perturbation introduced through the random initialization of the overlaps ~n eventually takes over: the student units become specialized\nand the matrix R of student-teacher overlaps tends towards the matrix T, except\nfor a permutational symmetry associated with the arbitrary labeling of the student\nhidden units. The generalization error plateau is followed by a monotonic decrease\ntowards zero once the specialization begins and the system evolves towards the\noptimal solution. The evolution of the overlaps and generalization error for the unrealizable case ]{ < M is characterized by qualitatively similar stages, except that\nthe asymptotic behavior is controlled by a suboptimal solution which reflects the\ndifferences between student and teacher architectures.\nCurves for the time evolution of the generalization error for different values of "I\nshown in Fig. 1d for ]{ = M = 3 identify trapping in the symmetric subspace\nas a small "I phenomenon. We therefore consider the equations of motion (4) in\nthe small "I regime. The term proportional to "12 is neglected and the resulting\ntruncated equations of motion are used to investigate a phase characterized by\nstudents of similar norms: Qii = Q for all 1 ~ i ~ ]{, similar correlations among\nthemselves: Qik = C for all i 1= k, and similar correlations with the teacher vectors:\nR in = R for all 1 ~ i ~ ]{, 1 ~ n ~ M. The resulting dynamical equations exhibit\na fixed point solution at\nM M -\n\nQ"\n\n= C" = ]{2\n\n]{2\n\n+ ..j]{4 2M _ 1\n\n]{2\n\n+ M2\nand\n\nR"\n\nrcr\n\n= VM\n\n(5)\n\n\x0c305\n\nDynamics of On-line Gradient Descent Learning for Multilayer Neural Networks\n\nr-\n\n(a)\nLOO.B-\n\n(b)\n... ...... R" --R\'2\n.. R 2 ,\nR, J\n---- R2 2 ........ R 2 ,\n- - . Rl1 - .-- RJ 2\n---- R"\n\nO.B-\n\n1\n\na\n\n0.4-\n\n~~~~-\n\n-\n\n~.\n0 .2\n\nt\n\n0., --0\'2\n---- 0, J\n......... O2 J\n\n.!:I::\n..... 0.6-\n\n~---\n\ng:: J\n\n~\n\n0.4-\n\n2000\n\n6000\n\n(c)\n\nR\n\n6\n\n~\n\n0.0\n2000\n\n0\n\nBOOO\n\n4000\n\nBOOO\n\nex\nO.OB-\n\n11 0.1\n11 0.3\n-?- ? ?110.5\n--110 .7\n\nO.OB-\n\n0 .06bO\n\n0.06-\n\n6000\n\n(d)\n\n0.1-\n\n~\n\n5\n\nI\n\n?\n\n0.2-\n\n"\n\n4000\n\nex\n\nbO\n\n,\n\ni\n\n"j\'\n\n--\'\n\n0.0\n0\n\n~ 0.6.....\n\nt\n\nr-/\n\n1.0-\n\nW\n\n0.04- f-----,\n\n0 .04\n.. ......\n\n0.02-\n\n0.02\n0.0\n\n\\\n\nI\n\n0\n\n4000\n\n2000\n\n6000\n\n\'r--"-\'\n..... ,\\\n\n..\n\ni\n\nI\n\ni\n0.0\nBOOO\n\n0\n\n2000\n\nex\n\n4000\n\n6000\n\nex\n\nFigure 1: Dependence of the overlaps and the generalization error on the normalized number of examples Q\' for a three-node student learning a three-node teacher\ncharacterized by Tnm = onm. Results for TJ = 0.1 are shown for (a) student-student\noverlaps Qik and (b) student-teacher overlaps Rin . The generalization error is shown\nin (c) , and again in (d) for different values of the learning rate.\nfor the general case , which reduces to\n\nQ*\n\n-\n\nC*\n\n1\n- 2K-1\n\nin the realizable case (K\ngiven by\n\n= M),\n\nand\n\nR* _\n\nrcr _\n\n- VK -\n\n1\nVK(2K -1)\n\n(6)\n\nwhere the corresponding generalization error is\n\nE; = ~ {i - K arcsin (2~{ )}\n\n.\n\n(7)\n\nA simple geometrical picture explains the relation Q* = C* = M(R*)2 at the\nsymmetric fixed point . The learning process confines the student vectors {Jd to\nthe subspace SB spanned by the set of teacher vectors {Bn} . For Tnm = onm\nthe teacher vectors form an orthonormal set: Bn = en , with en . em = Onm for\n1 :::; n , m :::; M , and provide an expansion for the weight vectors of the trained\nstudent: Ji = Ln R inen . The student-teacher overlaps Rin are independent of i in\nthe symmetric phase and independent of n for an isotropic teacher: Rin = R" for\nall 1 :::; i :::; K and 1 :::; n :::; M. The expansion Ji = R* Ln en for all i results in\nQ* = C* = M(R*)2.\n\n\x0cD. SAAD, S. A. SOLLA\n\n306\n\nThe length of the symmetric plateau is controlled by the degree of asymmetry in the\ninitial conditions [2] and by the learning rate "I . The small "I analysis predicts trapping times inversely proportional to "I, in quantitative agreement with the shrinking\nplateau of Fig. 1d. The increase in the height of the plateau with decreasing "I is\na second order effect, as the truncated equations of motion predict a unique value:\nf; = 0.0203 for K = M = 3. The mechanism for the second order effect is revealed by an examination of Fig. 1a: the student-student overlaps do agree with\nthe prediction C" = 0.2 of the small "I analysis for K = M = 3, but the norms of\nthe student vectors remain larger, at Q = Q" +~ . The gap ~ between diagonal\nand off-diagonal elements is observed numerically to increase with increasing "I, and\nis responsible for the excess generalization error. A first order expansion in ~ at\nR = R", C = C .. , and Q = Q" + ~ yields\nt\n\ng\n\n= -K{7r\n+\n7r -6- l\\.,arcsm. (1)\n2K\n\n2K -1 }\n2K + 1 ~ ,\n\n(8)\n\nin agreement with the trend observed in Fig. 1d for the realizable case.\nThe excess norm\n\n~\n\nof the student vectors corresponds to a residual component in\n\nJ i not confined to the subspace SB. The weight vectors of the trained student can\nbe written as Ji\nR" Ln en +\nwith\nen\n0 for all 1 n\nM. Student\n\n=\n\nJt,\n\nJt .\n\n=\n\n:s :s\n\nweight vectors are not constrained to be identical; they differ through orthogonal\ncomponents Jt which are typically uncorrelated: Jt?J t = 0 for i =1= k. Correlations\nQik = C do satisfy C = C .. = M(R .. )2, but norms Qii = Q are given by Q = Q"+~,\nwith ~ =11 J.L 112. Learning at very small "I tends to eliminate J.L and confine the\nstudent vectors to SB.\nEscape from the symmetric subspace signals the onset of hidden unit specialization.\nAs shown in Fig. 1b, the process is driven by a breaking of the uniformity of the\nstudent-teacher correlations: each student node becomes increasingly specialized to\na specific teacher node, while its overlap with the remaining teacher nodes decreases\nand eventually decays to its asymptotic value. In the realizable case this asymptotic\nvalue is zero, while in the unrealizable case two different non-zero asymptotic values\ndistinguish weak overlaps with teacher nodes imitated by other student vectors from\nmore significant overlaps with those teacher nodes not specifically imitated by any\nof the student vectors.\nThe matrix of student-teacher overlaps can no longer be characterized by a unique\nparameter, as we need to distinguish between a dominant overlap R between a\ngiven student node and the teacher node it begins to imitate, secondary overlaps S\nbetween the same student node and the teacher nodes to which other student nodes\nare being assigned, and residual overlaps U with the remaining teacher nodes. The\nstudent hidden nodes can be relabeled so as to bring the matrix of student-teacher\noverlaps to the form Rin = RDin + S(l - Din)8(K - n) + U(l - 8(K - n)), where\nthe step function 8 is 0 for negative arguments and 1 otherwise. The emerging\ndifferentiation among student vectors results in a decrease of the overlaps Qik = C\nfor i =1= k, while their norms Qii = Q increase. The matrix of student-student\noverlaps takes the form Qik = QDik + C(l - Dik).\nHere we limit our description of the onset of specialization to the realizable case, for\nwhich Rin = RDin +S(l-Din). The small "I analysis is extended to allow for S =1= R in\norder to describe the escape from the symmetric subspace. The resulting dynamical\nC ..\n1/(2K - 1)\nequations are linearized around the fixed point solution at Q"\nand R" = S .. = 1/ K (2K - 1), and the generalization error is expanded around its\nfixed point value (7) to first order in the corresponding deviations q, c, r, and s. The\nanalysis identifies a relevant perturbation with q = c = 0 and s = -r /(K -1), which\n\nJ\n\n=\n\n=\n\n\x0c307\n\nDynamics of On-line Gradient Descent Learning for Multilayer Neural Networks\n\n0.3-.--- - - - - - - - - - - - - ,\n0.2\n\nFigure 2: Dependence of the\ntwo leading decay eigenvalues on\nthe learning rate \'fJ in the realizable case: A1 (curved line) and\nA2 (straight line) are shown for\nM = J{ = 3.\n\nt<.\n\n0.1\n\n-0.1\n\n0.0\n\n0.2\n\n0 .4\n\n0.6\n\n0.8\n\n1.0\n\n1.2\n\n1.4\n\n1.6\n\n1.8\n\nleaves the generalization error unchanged and explains the behavior illustrated in\nFig . la-b . It is the differentiation between Rand S which signals the escape from the\nsymmetric subspace; the differentiation between Q and C occurs for larger values\nof Q\'. The relevant perturbation corresponds to an enhancement of the overlap\nR = R* + r between a given student node and the teacher node it is learning to\nimitate, while the overlap S = S* + 5 between the same student node and the\nremaining teacher nodes is weakened. The time constant associated with this mode\nis T = (7r/2J{)(2J{ - 1)1/2(2J{ + 1)3/2, with T""" 27rJ{ in the large J{ limit.\nIt is in the subsequent convergence to an asymptotic solution that the realizable\nand unrealizable cases exhibit fundamental differences . We examine first the realizable scenario, in which the system converges to an optimal solution with perfect\ngeneralization .\nAs the specialization continues, the dominant overlaps R grow, and the secondary\noverlaps S decay to zero. Further specialization involves the decay to zero of the\nstudent-student correlations C and the growth of the norms Q of the student vectors.\nTo investigate the convergence to the optimal solution we linearize the equations\nof motion around the asymptotic fixed point at S* = C" = 0, R* = Q* = 1,\nwith f; = o. We describe convergence to the optimal solution by applying the full\nequations of motion (4) to a phase characterized by R in = Rbin + S(l - bin) and\nQik = Qbik\n\n+ C(l -\n\nbin).\n\nLinearization of the full equations of motion around the asymptotic fixed point\nresults in four eigenvalues; the dependence of the two largest eigenvalues on \'fJ is\nshown in Fig. 2 for M = J{ = 3. An initially slow mode corresponds to the\neigenvalue A2, which remains negative for all values of \'fJ, while the eigenvalue A1\nfor the initially fast mode becomes positive as \'fJ exceeds \'fJmax, given by\n7r\n\n75 - 42V3\n\n\'fJmax = J{ 25V3 - 42\n\n(9)\n\nto first order in 1/ J{. The optimal solution with f* = 0 is not accessible for\n> \'fJmax? Exponential convergence of R, S, C, ana Q to their optimal values\nis guaranteed for all learning rates in the range (0, \'fJmax) ; in this regime the gener0, with a rate controlled by the slowest\nalization error decays exponentially to f;\ndecay mode. An expansion of fg in terms of r = 1 - R, 5 , c, and q = 1 - Q reveals\nthat of the leading modes whose eigenvalues are shown in Fig. 2 only the mode associated with A1 contributes to the decay of the linear term , while the decay of the\nsecond order term is controlled by the mode associated with A2 and dominates the\nconvergence if 2A2 < A1. The learning rate \'fJopt which guarantees fastest asymptotic\ndecay for the generalization error follows from A1(\'fJopt) = 2A2(\'fJopt).\n\'fJ\n\n=\n\nThe asymptotic convergence of unrealizable learning is an intrinsically more complicated process that cannot be described in closed analytic form. The asymptotic\n\n\x0cD. SAAD. S. A. SOLLA\n\n308\n\nvalues of the order parameters and the generalization error depend on the learning rate TJ; convergence to an optimal solution with minimal generalization error\nrequires TJ --+ 0 as a --+ 00. Optimal values for the order parameters follow from a\nsmall TJ analysis, equivalent to neglecting J.L and assuming student vectors confined\nto SB. The resulting expansion J i = 2:~=1 Hinen, with Rii = R, Hin = S for\n1 :::; n :::; J{, n 1= i, and Hin = U for J{ + 1 :s n :::; M, leads to\nQ = R2 + (I< -1)S2 + (M - J{)U 2 , C = 2RS + (I< - 2)S2 = (M - J{)U 2 . (10)\nThe equations of motion for the remaining parameters R, S, and U exhibit a fixed\npoint solution which controls the asymptotic behavior. This solution cannot be\nobtained analytically, but numerical results are well approximated to order (1/ J{3)\nby\n6\\1\'3 - 3 L (\n1)\n8\nJ{2 1 - J{\n\nR*\n\n1-\n\nS*\n\n(1 --:) :3\'\n\nw=\n\n,\n\n~ ( 1-2~\' )\n\n(11)\n\nwhere L == M - J{. The corresponding fixed point values Q" and C" follow from\nEq. (10). Note that R" is lower than for the realizable case, and that correlations\nU" (significant) and S .. (weaker) between student vectors and the teacher vectors\nthey do not imitate are not eliminated . The asymptotic generalization error is given\nby\n(12)\nto order (1/ J{2). Note its proportionality to the mismatch L between teacher and\nstudent architectures.\nLearning at fixed and sufficiently small TJ results in exponential convergence to\nan asymptotic solution whose fixed point coordinates are shifted from the values\ndiscussed above. The solution is suboptimal; the resulting increase in f; from its\noptimal value (12) is easily obtained to first order in TJ , and it is also proportional\nto L. We have investigated convergence to the optimal solution (12) for schedules\nof the form TJ(a) = TJo/(a-ao)Z for the decay of the learning rate. A constant rate\nTJo is used for a :::; 0\'0; the monotonic decrease of TJ for a > 0\'0 is switched on after\nspecialization begins. Asymptotic convergence requires 0 < z :::; 1; fastest decay of\nthe generalization error is achieved for z = 1/2.\nSpecialization as described here and illustrated in Fig.l is a simultaneous process in\nwhich each student node acquires a strong correlation with a specific teacher node\nwhile correlations to other teacher nodes decrease. Such synchronous escape from\nthe symmetric phase is characteristic of learning scenarios where the target task is\ndefined through an isotropic teacher. In the case of a graded teacher we find that\nspecialization occurs through a sequence of escapes from the symmetric subspace,\nordered according to the relevance of the corresponding teacher nodes [3].\nAcknowledgement The work was supported by the EU grant CHRX-CT92-0063.\n\nReferences\n[1] G. Cybenko, Math . Control Signals and Systems 2, 303 (1989) .\n[2] M. Biehl and H. Schwarze, J. Phys. A 28, 643 (1995).\n[3] D. Saad and S. A. Solla, Phys. Rev. E, 52,4225 (1995) .\n\n\x0c'
p83133
sg83
S'Improving Elevator Performance Using\nReinforcement Learning\n\nRobert H. Crites\nComputer Science Department\nUniversity of Massachusetts\nAmherst, MA 01003-4610\ncritesGcs.umass.edu\n\nAndrew G. Barto\nComputer Science Department\nUniversity of Massachusetts\nAmherst, MA 01003-4610\nbartoGcs.umass.edu\n\nAbstract\nThis paper describes the application of reinforcement learning (RL)\nto the difficult real world problem of elevator dispatching. The elevator domain poses a combination of challenges not seen in most\nRL research to date. Elevator systems operate in continuous state\nspaces and in continuous time as discrete event dynamic systems.\nTheir states are not fully observable and they are nonstationary\ndue to changing passenger arrival rates. In addition, we use a team\nof RL agents, each of which is responsible for controlling one elevator car. The team receives a global reinforcement signal which\nappears noisy to each agent due to the effects of the actions of the\nother agents, the random nature of the arrivals and the incomplete\nobservation of the state. In spite of these complications, we show\nresults that in simulation surpass the best of the heuristic elevator\ncontrol algorithms of which we are aware. These results demonstrate the power of RL on a very large scale stochastic dynamic\noptimization problem of practical utility.\n\n1\n\nINTRODUCTION\n\nRecent algorithmic and theoretical advances in reinforcement learning (RL) have\nattracted widespread interest. RL algorithms have appeared that approximate dynamic programming (DP) on an incremental basis. Unlike traditional DP algorithms, these algorithms can perform with or without models of the system, and\nthey can be used online as well as offline, focusing computation on areas of state\nspace that are likely to be visited during actual control. On very large problems,\nthey can provide computationally tractable ways of approximating DP. An example of this is Tesauro\'s TD-Gammon system (Tesauro, 1992j 1994; 1995), which\nused RL techniques to learn to play strong masters level backgammon. Even the\n\n\x0cR. H . CR~.A.G. BARTO\n\n1018\n\nbest human experts make poor teachers for this class of problems since they do not\nalways know the best actions. Even if they did, the state space is so large that\nit would be difficult for experts to provide sufficient training data. RL algorithms\nare naturally suited to this class of problems, since they learn on the basis of their\nown experience. This paper describes the application of RL to elevator dispatching,\nanother problem where classical DP is completely intractable. The elevator domain\nposes a number of difficulties that were not present in backgammon. In spite of\nthese complications, we show results that surpass the best of the heuristic elevator\ncontrol algorithms of which we are aware. The following sections describe the elevator dispatching domain, the RL algorithm and neural network architectures that\nwere used, the results, and some conclusions.\n\n2\n\nTHE ELEVATOR SYSTEM\n\nThe particular elevator system we examine is a simulated 10-story building with\n4 elevator cars (Lewis, 1991; Bao et al, 1994). Passenger arrivals at each floor are\nassumed to be Poisson, with arrival rates that vary during the course of the day.\nOur simulations use a traffic profile (Bao et al, 1994) which dictates arrival rates for\nevery 5-minute interval during a typical afternoon down-peak rush hour. Table 1\nshows the mean number of passengers arriving at each floor (2-10) during each\n5-minute interval who are headed for the lobby. In addition, there is inter-floor\ntraffic which varies from 0% to 10% of the traffic to the lobby.\n\nTable 1: The Down-Peak Traffic Profile\nThe system dynamics are approximated by the following parameters:\n? Floor time (the time to move one floor at the maximum speed): 1.45 secs.\n? Stop time (the time needed to decelerate, open and close the doors, and\naccelerate again): 7.19 secs.\n? Turn time (the time needed for a stopped car to change direction): 1 sec.\n? Load time (the time for one passenger to enter or exit a car): random\nvariable from a 20th order truncated Erlang distribution with a range from\n0.6 to 6.0 secs and a mean of 1 sec.\n? Car capacity: 20 passengers.\nThe state space is continuous because it includes the elapsed times since any hall\ncalls were registered. Even if these real values are approximated as binary values,\nthe size of the state space is still immense. Its components include 218 possible\ncombinations of the 18 hall call buttons (up and down buttons at each landing\nexcept the top and bottom), 240 possible combinations of the 40 car buttons, and\n184 possible combinations of the positions and directions of the cars (rounding off\nto the nearest floor). Other parts of the state are not fully observable, for example,\nthe desired destinations of the passengers waiting at each floor. Ignoring everything\nexcept the configuration of the hall and car call buttons and the approximate position and direction of the cars, we obtain an extremely conservative estimate of the\nsize of a discrete approximation to the continuous state space:\n\n\x0cImproving Elevator Performance Using Reinforcement Learning\n\n1019\n\nEach car has a small set of primitive actions. Ifit is stopped at a floor, it must either\n"move up" or "move down". If it is in motion between floors, it must either "stop\nat the next floor" or "continue past the next floor". Due to passenger expectations,\nthere are two constraints on these actions: a car cannot pass a floor if a passenger\nwants to get off there and cannot turn until it has serviced all the car buttons in its\npresent direction. We have added three additional action constraints in an attempt\nto build in some primitive prior knowledge: a car cannot stop at a floor unless\nsomeone wants to get on or off there, it cannot stop to pick up passengers at a floor\nif another car is already stopped there, and given a choice between moving up and\ndown, it should prefer to move up (since the down-peak traffic tends to push the\ncars toward the bottom of the building). Because of this last constraint, the only\nreal choices left to each car are the stop and continue actions. The actions of the\nelevator cars are executed asynchronously since they may take different amounts of\ntime to complete.\nThe performance objectives of an elevator system can be defined in many ways. One\npossible objective is to minimize the average wait time, which is the time between\nthe arrival of a passenger and his entry into a car. Another possible objective is\nto minimize the average 6y6tem time, which is the sum of the wait time and the\ntravel time. A third possible objective is to minimize the percentage of passengers\nthat wait longer than some dissatisfaction threshold (usually 60 seconds). Another\ncommon objective is to minimize the sum of 6quared wait times. We chose this\nlatter performance objective since it tends to keep the wait times low while also\nencouraging fair service.\n\n3\n\nTHE ALGORITHM AND NETWORK\nARCHITECTURE\n\nElevator systems can be modeled as ducrete event systems, where significant events\n(such as passenger arrivals) occur at discrete times, but the amount oftime between\nevents is a real-valued variable. In such systems, the constant discount factor \'Y\nused in most discrete-time reinforcement learning algorithms is inadequate. This\nproblem can be approached using a variable discount factor that depends on the\namount of time between events (Bradtke & Duff, 1995). In this case, returns are\ndefined as integrals rather than as infinite sums, as follows:\nbecomes\nwhere rt is the immediate cost at discrete time t, r.,. is the instantaneous cost at\ncontinuous time T (e.g., the sum of the squared wait times of all waiting passengers),\nand {3 controls the rate of exponential decay.\nCalculating reinforcements here poses a problem in that it seems to require knowledge of the waiting times of all waiting passengers. There are two ways of dealing\nwith this problem. The simulator knows how long each passenger has been waiting.\nIt could use this information to determine what could be called omnucient reinforcements. The other possibility is to use only information that would be available\nto a real system online. Such online reinforcements assume only that the waiting\ntime of the first passenger in each queue is known (which is the elapsed button\ntime). If the Poisson arrival rate A for each queue is estimated as the reciprocal of\nthe last inter-button time for that queue, the Gamma distribution can be used to\nestimate the arrival times of subsequent passengers. The time until the nth. subsequent arrival follows the Gamma distribution r(n, f). For each queue, subsequent\n\n\x0cR. H. CRITES, A. G. BARTO\n\n1020\n\narrivals will generate the following expected penalties during the first b seconds after\nthe hall button has been pressed:\n\nL Jor\n\nb\n\n00\n\nn=l\n\n(prob\n\nnth\n\narrival occurs at time r) . (penalty given arrival at time r) dr\n\n0\n\nThis integral can be solved by parts to yield expected penalties. We found that\nusing online reinforcements actually produced somewhat better results than using\nomniscient reinforcements, presumably because the algorithm was trying to learn\naverage values anyway.\nBecause elevator system events occur randomly in continuous time, the branching\nfactor is effectively infinite, which complicates the use of algorithms that require\nexplicit lookahead. Therefore, we employed a team of discrete-event Q-Iearning\nagents, where each agent is responsible for controlling one elevator car. Q(:z:, a)\nis defined as the expected infinite discounted return obtained by taking action a\nin state :z: and then following an optimal policy (Watkins, 1989). Because of the\nvast number of states, the Q-values are stored in feedforward neural networks. The\nnetworks receive some state information as input, and produce Q-value estimates\nas output. We have tested two architectures. In the parallel architecture, the agents\nshare a single network, allowing them to learn from each other\'s experiences and\nforcing them to learn identical policies. In the fully decentralized architecture, the\nagents have their own networks, allowing them to specialize their control policies.\nIn either case, none of the agents have explicit access to the actions of the other\nagents. Cooperation has to be learned indirectly via the global reinforcement signal.\nEach agent faces added stochasticity and nonstationarity because its environment\ncontains other learning agents. Other work on team Q-Iearning is described in\n(Markey, 1994).\nThe algorithm calls for each car to select its actions probabilistic ally using the\nBoltzmann distribution over its Q-value estimates, where the temperature is lowered gradually during training. After every decision, error backpropagation is used\nto train the car\'s estimate of Q(:z:, a) toward the following target output:\n\nwhere action a is taken by the car from state :z: at time t x , the next decision by\nthat car is required from state y at time ty, and TT and (3 are defined as above.\ne-tJ(tv-t.) acts as a variable discount factor that depends on the amount of time\nbetween events. The learning rate parameter was set to 0.01 or 0.001 and {3 was set\nto 0.01 in the experiments described in this paper.\nAfter considerable experimentation, our best results were obtained using networks\nfor pure down traffic with 47 input units, 20 hidden sigmoid units, and two linear\noutput units (one for each action value). The input units are as follows:\n? 18 units: Two units encode information about each of the nine down hall\nbuttons. A real-valued unit encodes the elapsed time if the button has\nbeen pushed and a binary unit is on if the button has not been pushed.\n\n\x0cImproving Elevator Performance Using Reinforcement Learning\n\n1021\n\n? 16 units: Each of these units represents a possible location and direction\nfor the car whose decision is required. Exactly one of these units will be on\nat any given time.\n? 10 units: These units each represent one of the 10 floors where the other cars\nmay be located. Each car has a "footprint" that depends on its direction\nand speed. For example, a stopped car causes activation only on the unit\ncorresponding to its current floor, but a moving car causes activation on\nseveral units corresponding to the floors it is approachmg, with the highest\nactivations on the closest floors.\n? 1 unit: This unit is on if the car whose decision is required is at the highest\nfloor with a waiting passenger.\n? 1 unit: This unit is on if the car whose decision is required is at the floor\nwith the passenger that has been waiting for the longest amount of time.\n? 1 unit: The bias unit is always on.\n\n4\n\nRESULTS\n\nSince an optimal policy for the elevator dispatching problem is unknown, we measured the performance of our algorithm against other heuristic algorithms, including\nthe best of which we were aware. The algorithms were: SECTOR, a sector-based\nalgorithm similar to what is used in many actual elevator systems; DLB, Dynamic\nLoad Balancing, attempts to equalize the load of all cars; HUFF, Highest Unanswered Floor First, gives priority to the highest floor with people waiting; LQF,\nLongest Queue First, gives priority to the queue with the person who has been\nwaiting for the longest amount of time; FIM, Finite Intervisit Minimization, a receding horizon controller that searches the space of admissible car assignments to\nminimize a load function; ESA, Empty the System Algorithm, a receding horizon\ncontroller that searches for the fastest way to "empty the system" assuming no new\npassenger arrivals. ESA uses queue length information that would not be available\nin a real elevator system. ESA/nq is a version of ESA that uses arrival rate information to estimate the queue lengths. For more details, see (Bao et al, 1994). These\nreceding horizon controllers are very sophisticated, but also very computationally\nintensive, such that they would be difficult to implement in real time. RLp and\nRLd denote the RL controllers, parallel and decentralized. The RL controllers were\neach trained on 60,000 hours of simulated elevator time, which took four days on a\n100 MIPS workstation. The results are averaged over 30 hours of simulated elevator\ntime. Table 2 shows the results for the traffic profile with down traffic only.\nAlgorithm\nSECTOR\nDLB\nBASIC HUFF\nLQF\nHUFF\nFIM\nESA/nq\nESA\nRLp\nRLd\n\nI AvgWait I SquaredWait I SystemTime I Percent>60 secs I\n21.4\n19.4\n19.9\n19.1\n16.8\n16.0\n15.8\n15.1\n14.8\n14.7\n\n674\n658\n580\n534\n396\n359\n358\n338\n320\n313\n\n47.7\n53.2\n47.2\n46.6\n48.6\n47.9\n47.7\n47.1\n41.8\n41.7\n\n1.12\n2.74\n0.76\n0.89\n0.16\n0.11\n0.12\n0.25\n0.09\n0.07\n\nTable 2: Results for Down-Peak Profile with Down Traffic Only\n\n\x0cR.H.C~.A.G. BARTO\n\n1022\n\nTable 3 shows the results for the down-peak traffic profile with up and down traffic,\nincluding an average of 2 up passengers per minute at the lobby. The algorithm\nwas trained on down-only traffic, yet it generalizes well when up traffic is added\nand upward moving cars are forced to stop for any upward hall calls.\nAlgorithm\nSECTOR\nDLB\nBASIC HUFF\nLQF\nHU ...?F\nESA\nFIM\nRLp\nRLd\n\nI AvgWait I Squared wait I SystemTime I Percent>60 secs I\n27.3\n21.7\n22.0\n21.9\n19.6\n18.0\n17.9\n16.9\n16.9\n\n1252\n826\n756\n732\n608\n524\n476\n476\n468\n\n54.8\n54.4\n51.1\n50.7\n50.5\n50.0\n48.9\n42.7\n42.7\n\n9.24\n4.74\n3.46\n2.87\n1.99\n1.56\n0.50\n1.53\n1.40\n\nTable 3: Results for Down-Peak Profile with Up and Down Traffic\nTable 4 shows the results for the down-peak traffic profile with up and down traffic,\nincluding an average of 4 up passengers per minute at the lobby. This time there is\ntwice as much up traffic, and the RL agents generalize extremely well to this new\nsituation.\nAlgorithm\nSECTOR\nHUFF\nDLB\nLQF\nBASIC HUFF\nFIM\nESA\nRLd\nRLp\n\nI AvgWait I SquaredWait I SystemTime I Percent>60 secs I\n30.3\n22.8\n22.6\n23.5\n23.2\n20.8\n20.1\n18.8\n18.6\n\n1643\n884\n880\n877\n875\n685\n667\n593\n585\n\n59.5\n55.3\n55.8\n53.5\n54.7\n53.4\n52.3\n45.4\n45.7\n\n13.50\n5.10\n5.18\n4.92\n4.94\n3.10\n3.12\n2.40\n2.49\n\nTable 4: Results for Down-Peak Profile with Twice as Much Up Traffic\nOne can see that both the RL systems achieved very good performance, most notably as measured by system time (the sum of the wait and travel time), a measure\nthat was not directly being minimized. Surprisingly, the decentralized RL system\nwas able to achieve as good a level of performance as the parallel RL system. Better performance with nonstationary traffic profiles may be obtainable by providing\nthe agents with information about the current traffic context as part of their input\nrepresentation. We expect that an additional advantage of RL over heuristic controllers may be in buildings with less homogeneous arrival rates at each floor, where\nRL can adapt to idiosyncracies in their individual traffic patterns.\n\n5\n\nCONCLUSIONS\n\nThese results demonstrate the utility of RL on a very large scale dynamic optimization problem. By focusing computation onto the states visited during simulated\ntrajectories, RL avoids the need of conventional DP algorithms to exhaustively\n\n\x0cImproving Elevator Performance Using Reinforcement Learning\n\n1023\n\nsweep the state set. By storing information in artificial neural networks, it avoids\nthe need to maintain large lookup tables. To achieve the above results, each RL\nsystem experienced 60,000 hours of simulated elevator time, which took four days\nof computer time on a 100 MIPS processor. Although this is a considerable amount\nof computation, it is negligible compared to what any conventional DP algorithm\nwould require. The results also suggest that approaches to decentralized control\nusing RL have considerable promise. Future research on the elevator dispatching\nproblem will investigate other traffic profiles and further explore the parallel and\ndecentralized RL architectures.\nAcknowledgements\nWe thank John McNulty, Christ os Cassandras, Asif Gandhi, Dave Pepyne, Kevin\nMarkey, Victor Lesser, Rod Grupen, Rich Sutton, Steve Bradtke, and the ANW\ngroup for assistance with the simulator and for helpful discussions. This research\nwas supported by the Air Force Office of Scientific Research under grant F4962093-1-0269.\nReferences\nG. Bao, C. G. Cassandras, T. E. Djaferis, A. D. Gandhi, and D. P. Looze. (1994)\nElevator Di,patcher, for Down Peale Traffic. Technical Report, ECE Department,\nUniversity of Massachusetts, Amherst, MA.\nS. J. Bradtke and M. O. Duff. (1995) Reinforcement Learning Methods for\nContinuous-Time Markov Decision Problems. In: G. Tesauro, D. S. Touretzky\nand T. K. Leen, eds., Advance, in Neural Information Procelling Sy,tem, 7, MIT\nPress, Cambridge, MA.\nJ. Lewis. (1991) A Dynamic Load Balancing Approach to the Control of Multuerver\nPolling Sy,tem, with Applicationl to Elevator Syltem Dupatching. PhD thesis,\nUniversity of Massachusetts, Amherst, MA.\nK. L. Markey. (1994) Efficient Learning of Multiple Degree-of-Freedom Control\nProblems with Quasi-independent Q-agents. In: M. C. Mozer, P. Smolensky,\nD. S. Touretzky, J. L. Elman and A. S. Weigend, eds., Proceeding\' of the 1993\nConnectionilt Modell Summer SchooL Erlbaum Associates, Hillsdale, NJ.\nG. Tesauro. (1992) Practical Issues in Temporal Difference Learning. Machine\nLearning 8:257-277.\nG. Tesauro. (1994) TO-Gammon, a Self-Teaching Backgammon Program, Achieves\nMaster-Level Play. Neural Computation 6:215-219 .\nG. Tesauro. (1995) Temporal Difference Learning and TD-Gammon. Communication, of the ACM 38:58-68.\nC. J. C. H. Watkins. (1989) Learning from Delayed Reward,. PhD thesis, Cambridge University.\n\n\x0c'
p83134
sg85
S'A Bound on the Error of Cross Validation Using\nthe Approximation and Estimation Rates, with\nConsequences for the Training-Test Split\nMichael Kearns\nAT&T Research\n\n1 INTRODUCTION\nWe analyze the performance of cross validation 1 in the context of model selection and\ncomplexity regularization. We work in a setting in which we must choose the right number\nof parameters for a hypothesis function in response to a finite training sample, with the goal\nof minimizing the resulting generalization error. There is a large and interesting literature\non cross validation methods, which often emphasizes asymptotic statistical properties, or\nthe exact calculation of the generalization error for simple models. Our approach here is\nsomewhat different, and is pri mari Iy inspired by two sources. The first is the work of Barron\nand Cover [2], who introduced the idea of bounding the error of a model selection method\n(in their case, the Minimum Description Length Principle) in terms of a quantity known as\nthe index of resolvability. The second is the work of Vapnik [5], who provided extremely\npowerful and general tools for uniformly bounding the deviations between training and\ngeneralization errors.\nWe combine these methods to give a new and general analysis of cross validation performance. In the first and more formal part of the paper, we give a rigorous bound on the error\nof cross validation in terms of two parameters of the underlying model selection problem:\nthe approximation rate and the estimation rate. In the second and more experimental part\nof the paper, we investigate the implications of our bound for choosing \'Y, the fraction of\ndata withheld for testing in cross validation. The most interesting aspect of this analysis is\nthe identification of several qualitative properties of the optimal \'Y that appear to be invariant\nover a wide class of model selection problems:\n? When the target function complexity is small compared to the sample size, the\nperformance of cross validation is relatively insensitive to the choice of \'Y.\n? The importance of choosing \'Y optimally increases, and the optimal value for \'Y\ndecreases, as the target function becomes more complex relative to the sample\nsize.\n? There is nevertheless a single fixed value for\'Y that works nearly optimally for a\nwide range of target function complexity.\n\n2 THE FORMALISM\nWe consider model selection as a two-part problem: choosing the appropriate number of\nparameters for the hypothesis function, and tuning these parameters. The training sample\nis used in both steps of this process. In many settings, the tuning of the parameters is\ndetermined by a fixed learning algorithm such as backpropagation, and then model selection\nreduces to the problem of choosing the architecture. Here we adopt an idealized version of\nthis division of labor. We assume a nested sequence of function classes Hl C ... C H d ??? ,\ncalled the structure [5], where Hd is a class of boolean functions of d parameters, each\nIPerhaps in conflict with accepted usage in statistics, here we use the term "cross validation" to\nmean the simple method of saving out an independent test set to perform model selection. Precise\ndefinitions will be stated shortly.\n\n\x0c184\n\nM.KEARNS\n\nfunction being a mapping from some input space X into {O, I}. For simplicity, in this\npaper we assume that the Vapnik-Chervonenkis (VC) dimension [6, 5] of the class Hd is\nO(d). To remove this assumption, one simply replaces all occurrences of d in our bounds by\nthe VC dimension of H d ? We assume that we have in our possession a learning algorithm\nL that on input any training sample 8 and any value d will output a hypothesis function\nhd E H d that minimizes the training error over H d - that is, ?t ( hd) = minhE H" { ?t (h)},\nwhere EtCh) is the fraction of the examples in 8 on which h disagrees with the given label.\nIn many situations, training error minimization is known to be computationally intractable,\nleading researchers to investigate heuristics such as backpropagation. The extent to which\nthe theory presented here applies to such heuristics will depend in part on the extent to\nwhich they approximate training error minimization for the problem under consideration.\nModel selection is thus the problem of choosing the best value of d. More precisely, we\nassume an arbitrary target function I (which mayor may not reside in one of the function\nclasses in the structure H1 C ... C H d ???), and an input distribution P; I and P together\ndefine the generalization error function ?g(h)\nPrzEP[h(x) =f I(x)]. We are given a\ntraining sample 8 of I, consisting of m random examples drawn according to P and labeled\nby I (with the labels possibly corrupted by a noise process that randomly complements each\nlabel independently with probability TJ < 1/2). The goal is to minimize the generalization\nerror of the hypothesis selected.\n\n=\n\nIn this paper, we will make the rather mild but very useful assumption that the structure has\nthe property that for any sample size m, there is a value dm.u:(m) such that ?t(hdm.u:(m)) =\no for any labeled sample 8 of m examples. We call the function dmaz(m) the fitting\nnumber of the structure. The fitting number formalizes the simple notion that with enough\nparameters, we can always fit the training data perfectly, a property held by most sufficiently\npowerful function classes (including multilayer neural networks). We typically expect the\nfitting number to be a linear function of m, or at worst a polynomial in m. The significance\nof the fitting number for us is that no reasonable model selection method should choose hd\nfor d ~ dmaz(m), since doing so simply adds complexity without reducing the training\nerror.\nIn this paper we concentrate on the simplest version of cross validation. We choose a\nparameter "( E [0, 1], which determines the split between training and test data. Given the\ninput sample 8 of m examples, let 8\' be the subsample consisting of the first (1 - "()m\nexamples in 8, and 8" the subsample consisting of the last "(mexamples. In cross validation,\nrather than giving the entire sample 8 to L, we give only the smaller sample 8\', resulting in\nthe sequence h1\' ... , h dmaz ((1-"I)m) of increasingly complex hypotheses. Each hypothesis\nis now obtained by training on only (I - "()m examples, which implies that we will only\nconsider values of d smaller than the corresponding fitting number dmaz((1 - "()m); let\nus introduce the shorthand d"!naz for dmaz((1 - "()m). Cross validation chooses the hd\nsatisfying hd mini E{1, .. . ,d~az} {?~\' (~)} where ?~\' (~) is the error of hi on the subsample\n8". Notice that we are not considering multifold cross validation, or other variants that\nmake more efficient use of the sample, because our analyses will require the independence\nof the test set. However, we believe that many of the themes that emerge here may apply to\nthese more sophisticated variants as well.\n\n=\n\nWe use ?ClI ( m) to denote the generalization error ?g( hd) ofthe hypothesis hd chosen by cross\nvalidation when given as input a sample 8 of m random examples of the target function.\nObviously, ?clI(m) depends on 8, the structure, I, P, and the noise rate. When bounding\n?cv (m), we will use the expression "with high probability" to mean with probability 1 - ~\nover the sample 8, for some small .fixed constant ~ > O. All of our results can also be\nstated with ~ as a parameter at the cost of a loge 1/~) factor in the bounds, or in terms of the\nexpected value of ?clI(m).\n\n3\n\nTHE APPROXIMATION RATE\n\nIt is apparent that any nontrivial bound on ?cv (m) must take account of some measure of the\n"complexity" of the unknown target function I. The correct measure of this complexity is\nless obvious. Following the example of Barron and Cover\'s analysis of MDL performance\n\n\x0cA Bound on the Error of Cross Validation\n\n185\n\nin the context of density estimation [2], we propose the approximation rate as a natural\nmeasure of the complexity of I and P in relation to the chosen structure HI C ... C H d ????\nThus we define the approximation rate function Eg(d) to be Eg(d) = minhEH.. {Eg(h)}. The\nfunction E9 (d) tells us the best generalization error that can be achieved in the class Hd,\nand it is a nonincreasing function of d. If Eg(S) = 0 for some sufficiently large s, this\nmeans that the target function I, at least with respect to the input distribution, is realizable\nin the class H., and thus S is a coarse measure of how complex I is. More generally, even\nif Eg(d) > 0 for all d, the rate of decay of Eg(d) still gives a nice indication of how much\nrepresentational power we gain with respect to I and P by increasing the complexity of\nour models. StilI missing, of course, is some means of determining the extent to which this\nrepresentational power can be realized by training on a finite sample of a given size, but\nthis will be added shortly. First we give examples of the approximation rate that we will\nexamine following the general bound on E ClI ( m).\nThe Intervals Problem. In this problem, the input space X is the real interval [0,1], and\nthe class Hd of the structure consists of all boolean step functions over [0,1] of at most\nd steps; thus, each function partitions the interval [0, 1] into at most d disjoint segments\n(not necessarily of equal width), and assigns alternating positive and negative labels to\nthese segments. The input space is one-dimensional, but the structure contains arbitrarily\ncomplex functions over [0, 1]. It is easily verified that our assumption that the VC dimension\nof Hd is Oed) holds here, and that the fitting number obeys dmllZ(m) S m. Now suppose\nthat the input density P is uniform, and suppose that the target function I is the function\nof S alternating segments of equal width 1/ s, for some s (thUS, I lies in the class H.).\nWe will refer to these settings as the intervals problem. Then the approximation rate is\nEg(d) = (1/2)(1 - dis) for 1 S d < sand Eg(d) = 0 for d ~ s (see Figure 1).\nThe Perceptron Problem. In this problem, the input space X is RN for some large\nnatural number N. The class Hd consists of all perceptrons over the N inputs in which\nat most d weights are nonzero. If the input density is spherically symmetric (for instance,\nthe uniform density on the unit ball in R N ), and the target function is the function in H.\nwith all s nonzero weights equal to 1, then it can be shown that the approximation rate\nis Eg(d) = (1/11") cos-I(..jd/s) for d < s [4], and of course Eg(d) = 0 for d ~ s (see\nFigure 1).\nPower Law Decay. In addition to the specific examples just given, we would also like\nto study reasonably natural parametric forms of Eg( d), to determine the sensitivity of our\ntheory to a plausible range of behaviors for the approximation rate. This is important,\nbecause in practice we do not expect to have precise knowledge of Eg(d), since it depends\non the target function and input distribution. Following the work of Barron [1], who shows\na c/dbound on Eg(d) for the case of neural networks with one hidden layer under a squared\nerror generalization measure (where c is a measure of target function complexity in terms\nof a Fourier transform integrability condition) 2, we can consider approximation rates of\nthe form Eg(d) = (c/d)a + Emin, where Emin ~ 0 is a parameter representing the "degree\nof unreal izability" of I with respect to the structure, and c, a > 0 are parameters capturing\nthe rate of decay to Emin (see Figure 1).\n\n4 THE ESTIMATION RATE\nFor a fixed I, P and HI C .. . C Hd? .., we say that a function p( d, m) is an estimation rate\nboundifforall dand m, with high probability over the sampleSwehave IEt(hd)-Eg(hct)1 S\np(d, m), where as usual hd is the result of training error minimization on S within H d.\nThus p( d, m) simply bounds the deviation between the training error and the generalization\nerror of h d ? Note that the best such bound may depend in a complicated way on all of\nthe elements of the problem: I, P and the structure. Indeed, much of the recent work\non the statistical physics theory of learning curves has documented the wide variety of\nbehaviors that such deviations may assume [4, 3]. However, for many natural problems\n2Since the bounds we will give have straightforward generalizations to real-valued function learning under squared error, examining behavior for Eg( d) in this setting seems reasonable.\n\n\x0cM. KEARNS\n\n186\n\nit is both convenient and accurate to rely on a universal estimation rate bound provided\nby the powerful theory of unifonn convergence: Namely, for any I, P and any structure,\nthe function p(d, m) = ..j(d/m) log(m/d) is an estimation rate bound [5]. Depending\nupon the details of the problem, it is sometimes appropriate to omit the loge m/ d) factor,\nand often appropriate to refine the J dim behavior to a function that interpolates smoothly\nbetween dim behavior for small Et to Jd/m for large Et. Although such refinements are\nboth interesting and important, many of the qualitative claims and predictions we will make\nare invariant to them as long as the deviation kt(hd) - Eg(hd)1 is well-approximated by a\npower law (d/m)a (0 > 0); it will be more important to recognize and model the cases in\nwhich power law behavior is grossly violated.\nNote that this universal estimation rate bound holds only under the assumption that the\ntraining sample is noise-free, but straightforward generalizations exist. For instance, if the\ntraining data is corrupted by random label noise at rate 0 ~ TJ < 1/2, then p( d, m)\n..j(d/(1 - 2TJ)2m)log(m/d) is again a universal estimation rate bound.\n\n5 THE BOUND\nTheorem 1 Let HI C ... C H d ? .. be any structure, where the VC dimension 0/ Hd is\nOed). Let I and P be any target function and input distribution, let Eg(d) be the approximation rate/unction/or the structure with respect to I and P, and let p(d, m) be an\nestimation rate bound/or the structure with respect to I and P. Then/or any m, with high\nprobability\n\nEcv(m)\n\n~\n\nmin\n\nI~d~di...\n\n{Eg(d)\n\n+ p(d, (1\n\n- ,)m)}\n\n+0\n\n(1)\n\n(\n\nwhere, is the/raction o/the training sample used/or testing, and lfYmax is thefitting number\ndmax ( (1 -,)m). Using the universal estimation bound rate and the rather weak assumption\nthat dmax(m) is polynomial in m, we obtain that with high probability\n10g?I-,)m)) .\n\n,m\n\n(2)\nStraightforward generalizations 0/ these bounds/or the case where the data is corrupted\nby classification noise can be obtained, using the modified estimation rate bound given in\nSection 4 3.\nWe delay the proof of this theorem to the full paper due to space considerations. However,\nthe central idea is to appeal twice to uniform convergence arguments: once within each class\nHd to bound the generalization error of the resulting training error minimizer hd E Hd, and\na second time to bound the generalization error of the hd minimizing the error on the test\nset of ,m examples.\nIn the bounds given by (1) and (2), themin{? } expression is analogous to Barron and Cover\'s\nindex of resolvability [2]; the final tenn in the bounds represents the error introduced by\nthe testing phase of cross validation. These bounds exhibit tradeoff behavior with respect\nto the parameter,: as we let, approach 0, we are devoting more of the sample to training\nthe hd, and the estimation rate bound tenn p(d, (1 - ,)m) is decreasing. However, the\ntest error tenn O( Jlog(~,u:)/(Tm)) is increasing, since we have less data to accurately\nestimate the Eg(hd). The reverse phenomenon occurs as we let, approach 1.\nWhile we believe Theorem 1 to be enlightening and potentially useful in its own right,\nwe would now like to take its interpretation a step further. More precisely, suppose we\n~e main effect of classification noise at rate \'1 is the replacement of occurrences in the bound of\nthe sample size m by the smaller "effective" sample size (1 - \'1)2m.\n\n\x0cA Bound on the Error of Cross Validation\n\n187\n\nassume that the bound is an approximation to the actual behavior of EClI(m). Then in\nprinciple we can optimize the bound to obtain the best value for "Y. Of course, in addition\nto the assumptions involved (the main one being that p(d, m) is a good approximation to\nthe training-generalization error deviations of the hd), this analysis can only be carried out\ngiven information that we should not expect to have in practice (at least in exact form)in particular, the approximation rate function Eg(d), which depends on f and P. However.\n\nwe argue in the coming sections that several interesting qualitative phenomena regarding\nthe choice of"Y are largely invariant to a wide range of natural behaviors for Eg (d).\n\n6\n\nA CASE STUDY: THE INTERVALS PROBLEM\n\nWe begin by performing the suggested optimization of"Y for the intervals problem. Recall\nthat the approximation rate here is Eg(d) = (1/2)(1 - d/8) for d < 8 and Ey(d) = 0 for\nd ~ 8, where 8 is the complexity of the target function. Here we analyze the behavior\nobtained by assuming that the estimation rate p(d, m) actually behaves as p(d, m) =\nJd/(l - "Y)m (so we are omitting the log factor from the universal bound), and to simplify\nthe formal analysis a bit (but without changing the qualitative behavior) we replace the\nterm Jlog?1 - "Y)m)/bm) by the weaker Jlog(m)/m. Thus, if we define the function\nF(d, m, "Y) = Ey(d) + Jd/(1 - "Y)m + Jlog(m)/bm) then following Equation (1), we\nare approximating EclI (m) by EclI (m) ~ min1<d<d"\n_ _ maa: {F(d, m, "Yn 4.\nThe first step of the analysis is to fix a value for"Y and differentiate F( d, m, "Y) with respect\nto d to discover the minimizing value of d; the second step is to differentiate with respect to\n"Y. It can be shown (details omitted) that the optimal choice of"Y under the assumptions is\n"Yopt (log (m)/ 8)1/3/(1 + (Iog(m)/ 8)1/3). It is importantto remember at this point that\ndespite the fact that we have derived a precise expression for "Yopt. due to the assumptions\nand approximations we have made in the various constants, any quantitative interpretation\nof this expression is meaningless. However, we can reasonably expect that this expression\ncaptures the qualitative way in which the optimal "Y changes as the amount of data m\nchanges in relation to the target function complexity 8. On this score the situation initially\nappears rather bleak, as the function (log( m)/ 8)1/3 /(1 + (log(m)/ 8)1/3) is quite sensitive\nto the ratio log(m)/8, which is something we do not expect to have the luxury of knowing\nin practice. However, it is both fortunate and interesting that "Yopt does not tell the entire\nstory. In Figure 2, we plot the function F ( 8, m, "Y) as a function of"Y for m = 10000 and for\nseveral different values of 8 (note that for consistency with the later experimental plots, the\nz axis of the plot is actually the training fraction 1 - "Y). Here we can observe four important\nqualitative phenomena, which we list in order of increasing subtlety: (A) When 8 is small\ncompared to m, the predicted error is relatively insensitive to the choice of "Y: as a function\nof "Y, F( 8, m, "Y) has a wide, flat bowl, indicating a wide range of "Y yielding essentially the\nsame near-optimal error. (B) As s becomes larger in comparison to the fixed sample size\nm, the relative superiority of "Yopt over other values for"Y becomes more pronounced. In\nparticular, large values for"Y become progressively worse as s increases. For example, the\n10 (again, m\n10000), even though "Yopt 0.524 ... the choice\nplots indicate that for s\n"Y = 0.75 will result in error quite near that achieved using "Yopt. However, for s = 500,\n"Y = 0.75 is predicted to yield greatly suboptimal error. Note that for very large s, the bound\npredicts vacuously large error for all values of "Y, so that the choice of "Y again becomes\nirrelevant. (C) Because of the insensitivity to "Y for s small compared to m, there is a fixed\nvalue of "Y which seems to yield reasonably good performance for a wide range of values\nfor s. This value is essentially the value of "Yopt for the case where 8 is large but nontrivial\ngeneralization is still possible, since choosing the best value for "Y is more important there\nthan for the small 8 case. (D) The value of "Yopt is decreasing as 8 increases. This is slightly\ndifficult to confirm from the plot, but can be seen clearly from the precise expression for\n\n=\n\n=\n\n=\n\n=\n\n"Yopt.\n4 Although there are hidden constants in the 0(.) notation of the bounds. it is the relative weights\nof the estimation and test error terms that is important. and choosing both constants equal to 1 is a\nreasonable choice (since both terms have the same Chernoff bound origins).\n\n\x0c188\n\nM.KEARNS\n\nIn Figure 3, we plot the results of experiments in which labeled random samples of size\nm = 5000 were generated for a target function of s equal width intervals, for s = 10,100\nand 500. The samples were corrupted by random label noise at rate TJ = 0.3. For each\nvalue of \'Y and each value of d, (1 - \'Y)m of the sample was given to a program performing\ntraining error minimization within Hd.; the remaining \'Ym examples were used to select the\nbest hd. according to cross validation. The plots show the true generalization error of the\nhd. selected by cross validation as a function of\'Y (the generalization error can be computed\nexactly for this problem). Each point in the plots represents an average over 10 trials.\nWhile there are obvious and significant quantitative differences between these experimental\nplots and the theoretical predictions of Figure 2, the properties (A), (B) and (C) are rather\nclearly borne out by the data: (A) In Figure 3, when s is small compared to m, there\nis a wide range of acceptable \'Y; it appears that any choice of\'Y between 0.10 and 0.50\nyields nearly optimal generalization error. (B) By the time s = 100, the sensitivity to\'Y is\nconsiderably more pronounced. For example, the choice \'Y = 0.50 now results in clearly\nsuboptimal performance, and it is more important to have \'Y close to 0.10. (C) Despite these\ncomplexities, there does indeed appear to be single value of\'Y - approximately 0.10that performs nearly optimally for the entire range of s examined.\nThe property (D) - namely, that the optimal \'Y decreases as the target function complexity\nis increased relative to a fixed m - is certainly not refuted by the experimental results,\nbut any such effect is simply too small to be verified. It would be interesting to verify\nthis prediction experimentally, perhaps on a different problem where the predicted effect is\nmore pronounced.\n\n7 CONCLUSIONS\nFor the cases where the approximation rate Eg(d) obeys either power law decay or is that\nderived for the perceptron problem discussed in Section 3, the behavior of EClI(m) as a\nfunction of \'Y predicted by our theory is largely the same (for example, see Figure 4). In the\nfull paper, we describe some more realistic experiments in which cross validation is used\nto determine the number of backpropogation training epochs. Figures similar to Figures 2\nthrough 4 are obtained, again in rough accordance with the theory.\nIn summary, our theory predicts that although significant quantitative differences in the\nbehavior of cross validation may arise for different model selection problems, the properties\n(A), (B), (C) and (D) should be present in a wide range of problems. At the very least,\nthe behavior of our bounds exhibits these properties for a wide range of problems. It\nwould be interesting to try to identify natural problems for which one or more of these\nproperties is strongly violated; a potential source for such problems may be those for which\nthe underlying learning curve deviates from classical power law behavior [4, 3].\nAcknowledgements: I give warm thanks to Yishay Mansour, Andrew Ng and Dana Ron\nfor many enlightening conversations on cross validation and model selection. Additional\nthanks to Andrew Ng for his help in conducting the experiments.\n\nReferences\n[1] A. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE\nTransaclions on Information Theory. 19:930-944. 1991.\n[2] A. R. Barron and T. M. Cover. Minimum complexity density estimation. IEEE Transaclions on\nInformation Theory, 37:1034-1054, 1991.\n[3] D. Haussler, M. Kearns. H.S. Seung, and N. Tishby. Rigourous learning curve bounds from\nstatistical mechanics. In Proceedings of the Seventh Annual ACM Confernce on Compulalional\nLearning Theory. pages 76-87. 1991l.\n[4] H. S. Seung, H. Sompolinsky. and N. Tishby. Statistical mechanics of learning from examples.\nPhysical Review, A45:6056-6091, 1992.\n[5] V. N. Vapnik:. Estimalion ofDependences Based on Empirical Dala. Springer-Verlag, New York,\n1982.\n[6] V. N. Vapnik: and A. Y. Chervonenkis. On the uniform convergence of relative frequencies of\nevents to their probabilities. Theory of Probability and ils Applicalions. 16(2):264-280, 1971.\n\n\x0c189\n\nA Bound on the Error of Cross Validation\nrgg\n\npprox mat on\n\na as\n\nFigure 1: Plots of three approximation rates:\nfor the intervals problem with target complexity\nII = 250 intervals (linear plot intersecting d-axis at\n250), for the perceptron problem with target complexity II = 150 nonzero weights (nonlinear plot\nintersecting d-axis at 150), and for power law decay asymptoting at E",," = 0.05.\n\nFigure 2: Plot of the predicted generalization error\nof cross validation for the intervals model selection\nproblem, as a function of the fraction 1 - "\'( of\ndata used for training. (In the plot, the fraction of\ntraining data is 0 on the left (-y = 1) and 1 on the\nright ("\'( 0?. The fixed sample size m = 10,000\nwas used, and the 6 plots show the error predicted\nby the theory for target function complexity values\nII = 10 (bottom plot), 50, 100, 250, 500, and 1000\n(top plot) .\n\nv grror\n\n=\n\n..\n..,\nrr vs\n\nra n sat s za, s-\n\nno\n\nSQ,\n\nm-\n\nFigure 3: Experimental plots of cross validation\ngeneralization error in the intervals problem as a\nfunction of training set size (1-"\'() m. Experiments\nwith the three target complexity values II = 10,100\nand 500 (bottom plot to top plot) are shown. Each\npoint represents performance averaged over 10 trials .\n\n..\n.~\n\n.~\n\n.. ..,J ....\n\nCV\n\nDun.\n\n(c\n\n)\n\nor c\n\nrom\n\n.\n\nto\n\n. , m-\n\n?i.\n\nFigure 4: Plot of the predicted generalization error\n\nof cross validation for the power law case E,( d) =\n(c/d), as a function of the fraction 1-",(ofdata used\nfor training. The fixed sample size m = 25,000\nwas used, and the 6 plots show the error predicted\nby the theory for target function complexity values\nc = 1 (bottom plot), 25,50,75, 100. and 150 (top\nplot).\n\n\' .1\n\n...\n\n\x0c'
p83135
sg303
S'A Model of Spatial Representations in\nParietal Cortex Explains Hemineglect\n\nAlexandre Pouget\nDept of Neurobiology\nUCLA\nLos Angeles, CA 90095-1763\nalex@salk.edu\n\nTerrence J. Sejnowski\nHoward Hughes Medical Institute\nThe Salk Institute\nLa Jolla, CA 92037\nterry@salk.edu\n\nAbstract\nWe have recently developed a theory of spatial representations in\nwhich the position of an object is not encoded in a particular frame\nof reference but, instead, involves neurons computing basis functions of their sensory inputs. This type of representation is able\nto perform nonlinear sensorimotor transformations and is consistent with the response properties of parietal neurons. We now ask\nwhether the same theory could account for the behavior of human\npatients with parietal lesions. These lesions induce a deficit known\nas hemineglect that is characterized by a lack of reaction to stimuli\nlocated in the hemispace contralateral to the lesion. A simulated\nlesion in a basis function representation was found to replicate three\nof the most important aspects of hemineglect: i) The models failed\nto cross the leftmost lines in line cancellation experiments, ii) the\ndeficit affected multiple frames of reference and, iii) it could be\nobject centered. These results strongly support the basis function\nhypothesis for spatial representations and provide a computational\ntheory of hemineglect at the single cell level.\n\n1\n\nIntroduction\n\nAccording to current theories of spatial representations, the positions of objects\nare represented in multiple modules throughout the brain, each module being specialized for a particular sensorimotor transformation and using its own frame of\nreference. For instance, the lateral intraparietal area (LIP) appears to encode the\nlocation of objects in oculocentric coordinates, presumably for the control of saccadic eye movements. The ventral intraparietal cortex (VIP) and the premotor\ncortex, on the other hand, seem to use head-centered coordinates and might be\n\n\x0c11\n\nA Model of Spatial Representations in Parietal Cortex Explains Hemineglect\n\nB\n\nA\n. . .-----.a...\n\nRight\nStimulus\n\n~ Left\n.-----Stimulus\n\nCl\n\nC2\n\nit~\nCl\n\nFP\n\n!\n\nC2\n\nTarget\n\nC3\n\nDistractors\n\n\\~\n\n+ ??? "\n+\n??? "\n\nCl\nC2\nC3\n\nFigure 1: A. Retinotopic neglect modulated by egocentric position. B. Stimuluscentered neglect\n\ninvolved in the control of hand movements toward the face.\nThis modular theory of spatial representations is not fully consistent with the behavior of patients with parietal or frontal lesions. Such lesions causes a syndrome\nknown as hemineglect which is characterized by a lack of response to sensory stimuli appearing in the hemispace contralateral to the lesion [3]. According to the\nmodular view, the deficit should be behavior dependent, e.g., oculocentric for eye\nmovements, head-centered for reaching. However, experimental and clinical studies\nshow that this is not the case. Instead, neglect affects multiple frames of reference\nsimultaneously, and to a first approximation, independently of the task.\nThis point is particularly clear in an experiment by Karnath et al (1993) (Figure 1A). Subjects were asked to identify a stimulus that can appear on either side\nof the fixation point. In order to test whether the position of the stimuli with\nrespect to the body affects performance, two conditions were tested: a control condition with head straight ahead (C1), and a second condition with head rotated\n20 degrees on the right (or equivalently, with the trunk rotated 20 degrees on the\nleft, see figure) (C2). In C2, both stimuli appeared further to the right ofthe trunk\nwhile being at the same location with respect to the head and retina than in Cl.\nMoreover, the trunk-centered position of the left stimulus in C2 was the same than\nthe trunk-centered position of the right stimulus in C1.\nAs expected, subjects with right parietal lesions performed better on the right\nstimulus in the control condition, a result consistent with both, retinotopic and\ntrunk-centered neglect. To distinguish between the two frames of reference, one\nneeds to compare performance across conditions.\nIf the deficit is purely retinocentric, the results should be identical in both conditions, since the retinotopic location of the stimuli does not vary. If, on the other\nhand, the deficit is purely trunk-centered, the performance on the left stimulus\nshould improve when the head is turned right since the stimulus now appears further toward the right of the trunk-centered hemispace. Furthermore, performance\non the right stimulus in the control condition should be the same as performance on\nthe left stimulus in the rotated condition, since they share the same trunk-centered\nposition in both cases.\n\n\x0cA. POUGET, T. J. SEJNOWSKI\n\n12\n\nNeither of these hypotheses can fully account for the data. As expected from a\nretinotopic neglect, subjects always performed better on the right stimulus in both\nconditions. However, performance on the left stimulus improved when the head\nwas turned right (C2), though not sufficiently to match the level of performance on\nthe right stimulus in the control condition (C1). Therefore, these results suggest a\nretinotopic neglect modulated by trunk-centered factors.\nIn addition, Karnath et al (1991) tested patients on a similar experiment in which\nsubjects were asked to generate a saccade toward the target. The analysis of reaction\ntime revealed the same type of results than the one found in the identification\ntask, thereby demonstrating that the spatial deficit is, to a first approximation,\nindependent of the task .\nAn experiment by Arguin and Bub (1993) suggests that neglect can be objectcentered as well. As shown in figure 1B, they found that reaction times were faster\nwhen the target appeared on the right of a set of distractors (C2), as opposed\nto the left (C1), even though the target is at the same retinotopic location in\nboth conditions. Interestingly, moving the target further to the right leads to even\nfaster reaction times (C3), showing that hemineglect is not only object-centered but\nretinotopic as well in this task.\nThese results strongly support the existence of spatial representations using multiple\nframes of reference simultaneously shared by several behaviors. We have recently\ndeveloped a theory [6] which has precisely these properties and we ask here whether\na simulated lesion would lead to a deficit similar to hemineglect. Our theory posits\nthat parietal neurons computes basis function (BF) of sensory signals, such as visual, or auditory inputs, and posture signals, such as eye or head position. The\nresulting representation, which we called a basis function map, can be used for performing nonlinear transformations of the sensory inputs, the type of transformations\nrequired for sensorimotor coordination.\n\n2\n\nModel Organization\n\nThe model contains two distinct parts: a network for performing sensorimotor transformations and a selection mechanism.\n2.1\n\nNetwork Architecture\n\nWe implemented a network using basis function units in the intermediate layer\nto perform a transformation from a visual retinotopic map to two motor maps\nin, respectively, head-centered and oculocentric coordinates (Figure 2). The input\ncontains a retinotopic visual map analog to the one found in the early stages of\nvisual processing, and a set of units encoding eye position, similar to the neurons\nfound in the intralaminar nucleus of the thalamus. These input units project to a\nset of intermediate units shared by both transformations. Each intermediate unit\ncomputes a gaussian of the retinal location of object, r x , multiplied by a sigmoid of\neye position, ex:\n\nOi=-----\n\n1 + e- e:Z:-/;rj\n\n(1)\n\nThese units are organized in a map covering all possible combinations of retinal\nand eye position selectivities. As we have shown elsewhere [6], this type of response\nfunction is consistent with the response of single parietal neurons found in area 7a.\n\n\x0c13\n\nA Model of Spatial Representations in Parietal Cortex Explains Hemineglect\n\nA\n\nB\n\nSaccadic Eye Movements\n\nt\n\nReaching\n\nt\n\n(00000000000000)\n\n(0000000000009)\n\nRetinotopic map\n(Superior CollicuJus)\n\nHead-centered map\n(Premotor Cortex)\n\n,.\n~\n.~\n\n0\n\n00000000000000\n\n& ., ::::::::::::::\nQ)\n\n-,0\n\n~ .1\'\n\nRetinal position (0)\n\n00000000000000\n00000000000000\n00000000000000\n\n] . ::::::::::::::\n,8\n\nt&:-6 .. t .,\', J0,\\~\n\nHead-centered position (0)\n\n?\nc::\nBFmap\n(7a)\n\n00000000000000\n\n:::::::::::::0\n\n,g\n\nBFmap\n(7a)\n\nl\n\n~\n\n\'"\n\n"\n\nRetinal position (0)\n\n7~i1i\n\ntJ\\,L\\.\n_20\n\n000000000\n\nRetinotopic map\n(VI)\n\nEye position cells\n(Thalamus)\n\n_10\n\n0\n\n10\n\n..\n\nRetinal position (0)\n\n~~\n.\n\nFigure 2: A. Network architecture B. Typical pattern of activity\nThe resulting map forms a basis function map which encodes the location of objects\nin head-centered and retinotopic coordinates simultaneously.\nThe activity of the unit in the output maps is computed by a simple linear combination of the BF unit activities. Appropriate values of the weights were found by\nusing linear regression techniques.\nThis architecture mimics the pattern of projections of the parietal area 7a. 7a is\nknown to project to, both , the superior colliculus and the premotor cortex (via the\nventral parietal area, VIP) , in which neurons have, respectively, retinotopic and\nhead-centered visual receptive fields. Figure 2B shows a typical pattern of activity\nin the network when two stimuli are presented simultaneously while the eye fixated\n10 degrees toward the right.\n\n2.2\n\nHemispheric Biases and Lesion Model\n\nNeurophysiological data indicate that both hemispheres contain neurons with all\npossible combinations of retinal and eye position selectivities, but with a contralateral bias. Hence , most neurons in the right parietal cortex (resp . left) have their\nretinal receptive field on the left hemiretina (resp . right) . The bias for eye position\nis much weaker but a trend has been reported in several studies [1] .\nTherefore, spatial representations in a patient with a right parietal lesions are biased\ntoward the right side of space. We modeled such a lesion by using a similar bias in\nthe intermediate layer of our network . The BF map simply has more neurons tuned\nto right retinal and eye positions. We found that the exact profile of the neuronal\ngradient across the basis function maps did not matter as long as it was monotonic\nand contralateral for both eye position and retinal location .\n\n2.3\n\nSelection model\n\nWe also developed a selection mechanism to model the behavior of patients when\npresented with several stimuli simultaneously. The simultaneous presentation of\n\n\x0c14\n\nA. POUGET, T. J. SEJNOWSKI\n\nstimuli induces multiple hills of activity in the network (see for instance the pattern\nof activity shown in figure IB for two visual stimuli). Our selection mechanism\noperates on the peak values of these hills.\nAt each time step, the most active stimulus is selected according to a winner-takeall and its corresponding activity is set to zero (inhibition of return). At the next\ntime step, the second highest stimuli is selected while the previously selected item\nis allowed to recover slowly. This procedure ensures that the most active item is\nnot selected twice in a row, but because of the recovery process, stimulus with high\nactivity might be selected again if displayed long enough.\nThis mechanism is such that the probability of selecting an item is proportional\nto two factors : the absolute amount of activity associated with the item, and the\nrelative activity with respect to other competing items.\n2.4\n\nEvaluating network performance\n\nWe used this model to simulate several experiments in which patient performance\nwas evaluated according to reaction time or percent of correct response.\nReaction time in the model was taken to be proportional to the number of time\nsteps required by our selection mechanism to select a particular target. Performance\non identification task was assumed to be proportional to the strength of the activity\ngenerated by the stimuli in the BF map.\n\n3\n3.1\n\nResults\nLine cancellation\n\nWe first tested the network on the line cancellation test, a test in which patients are\nasked to cross out short line segments uniformly spread over a page. To simulate\nthis test, we presented the display shown in figure 3A and we ran the selection\nmechanism to determine which lines get selected by the network . As illustrated in\nfigure 3A, the network crosses out only the lines located in the right half of the\ndisplay, just as left neglect patients do in the same task. The rightward gradient\nintroduced by the lesion biases the selection mechanism in favor of the most active\nlines, i.e., the ones on the right. As a result, the rightmost lines win the competition\nover and over, preventing the network from selecting the left lines.\n3.2\n\nMixture of frames of reference\n\nNext, we sought to determine the frame of reference of neglect in the model. Since\nKarnath et al (1993) manipulated head position, we simulated their experiment\nby using a BF map integrating visual inputs with head position, rather than eye\nposition. We show in figure 3B the pattern of activity obtained in the retinotopic\noutput layer of the network in the various experimental conditions (the other maps\nbehaved in a similar way). In both conditions, head straight ahead (dotted lines) or\nturned on the side (solid lines), the right stimulus is associated with more activity\nthan the left stimulus. This is the consequence of the larger number of cells in\nthe basis function map for rightward position. In addition, the activity for the left\nstimulus increases when the head is turned to the right. This effect is related to the\nlarger number of cells in the basis function maps tuned to right head positions.\nSince network performance is proportional to activity strength, the overall pattern\nof performance was found to be similar to what has been reported in human patients\n\n\x0c15\n\nA Model of Spatial Representations in Parietal Cortex Explains Hemineglect\n\nc\n\nA\n\na1\n\n___________ _\n\n,.\nI\n\n+\n\n,.,,\n\n,.,\n!\n\n!,\n\n"\n\nI\n\n,\n\n\',I?\n\'_.\'\n.1\n\n\'\\\'\\\n\n)( ? ? ?\nFP\n\nTarget\n\nC1\n\nDistractors\n\na21-----:--;-~-~\n.".... \'.\' ...\n\nB\n\n+\n\na3\n\n,.\n\n~\n\n? ? ? )(\n\nC2\n\n-----------------------\n\n,\',\n\n1".\n,...\n\n",.. ." .\n\nI?\n\nLeft\nStimulus\n\nRight\nStimulus\n\n+\n\n\'./\n\n\'\n\n???\n\n)(\n\nC3\n\nFigure 3: Network behavior in line cancellation task (A). Activity patterns in the\nretinotopic output layer when simulating the experiments by Karnath et al (1993)\n(B) and Arguin et al (1993) (C)\n(figure lA), namely: the right stimulus was better processed than the left stimulus\nand performance on the left stimulus increases when the head is rotated toward the\nright. Therefore, just like in human, neglect in the model is neither retinocentric\nnor trunk-centered alone, but both at the same time.\n3.3\n\nObject-centered effect\n\nWhen simulating Arguin et al (1993) experiments, the network reaction times were\nfound to follow the same trends than for human patients. Figure 3C illustrates the\npatterns of activity in the retinotopic output layer of the network when simulating\nthe three conditions of Arguin experiments. Notice that the absolute activity associated with the target (solid lines) in conditions 1 and 2 is the same, but the activity\nof the distractors (dotted lines) differs in the two conditions. In condition 1, they\nhave higher relative activity and thereby strongly delay the detection of the target\nby the selection mechanism. In condition 2, the distractors are now less active than\nthe target and do not delay target processing as much as they do in condition 1.\nThe reaction time decreases even more in condition 3, due to a higher absolute\nactivity associated with the target . Therefore, the network exhibits retinocentric\nand object-centered neglect, just like parietal patients [2].\n\n4\n\nDiscussion\n\nThe model of parietal cortex presented here was originally developed by considering the response properties of parietal neurons and the computational constraints\ninherent in sensorimotor transformations. It was not designed to model neglect, so\nits ability to account for a wide range of deficits is additional evidence in favor of\nthe basis function hypothesis .\nAs we have shown, our model captures three essential aspects of the neglect syndrome: 1) It reproduces the pattern of line crossing reported in patients in linecancellation experiments, 2) the deficit coexists in multiple frames of reference simultaneously, and 3) the model accounts for some of the object-based effects.\n\n\x0c16\n\nA. POUGET, T. J. SEJNOWSKI\n\nWe can account for a very large number of studies beyond the ones we have considered here, using very similar computational principles. We can reproduce, in\nparticular, the behavior of patients in line-bisection experiments and we can explain why neglect affects multiple cartesian frames of reference such as retinotopic,\nhead-centered, trunk-centered, environment-centered (i.e. with respect to gravity),\nand object-centered.\nIt must be emphasized that these results have been obtained without using explicit representations of these various cartesian frames of reference (except for the\nretinotopy of the BF map). In fact, this is precisely because the lesion affected\nnoncartesian representations that we have been able to reproduce these results. We\nhave assumed that the lesion affects the functional space in which the basis functions\nare defined. This functional space shares common dimensions with cartesian spaces,\nbut cannot be reduced to the latter. Hence, a basis function map integrating retinal\nlocation and head position is retinotopic, but not solely retinotopic. Consequently,\nany attempts to determine the cartesian space in which hemineglect operates is\nbound to lead to inconclusive results in which cartesian frames of reference appear\nto be mixed.\nThis study and previous research [6] suggests that the parietal cortex represents\nthe position of objects by computing basis functions of the sensory and posture\ninputs. It would now be interesting to see if this hypothesis could also account for\nsensorimotor adaptation, such as learning to reach properly when wearing visual\nprisms. We predict that adaptation takes place in several frames of reference simultaneously, a prediction that is testable and would provide further support for the\nbasis function framework.\n\nReferences\n[1] R.A. Andersen, C. Asanuma, G. Essick, and R.M. Siegel. Corticocortical connections of anatomically and physiologically defined subdivisions within the inferior\nparietal lobule. Journal of Comparative Neurology, 296(1):65-113,1990.\n[2] M. Arguin and D.N. Bub. Evidence for an independent stimulus-centered reference frame from a case of visual hemineglect. Cortex, 29:349-357, 1993.\n[3] K.M. Heilman, R.T. Watson, and E. Valenstein. Neglect and related disorders.\nIn K.M. Heilman and E. Valenstein, editors, Clinical Neuropsychology, pages\n243-294. Oxford University Press, New York, 1985.\n[4] H.O. Karnath, K. Christ, and W. Hartje. Decrease of contralateral neglect by\nneck muscle vibration and spatial orientation of trunk midline. Brain, 116:383396, 1993.\n[5] H.O. Karnath, P. Schenkel, and B. Fischer. Trunk orientation as the determining factor of the \'contralateral\' deficit in the neglect syndrome and as the physical anchor of the internal representation of body orientation in space. Brain,\n114:1997-2014, 1991.\n[6] A. Pouget and T.J. Sejnowski. Spatial representations in the parietal cortex\nmay use basis functions. In G. Tesauro, D.S. Touretzky, and T.K. Leen, editors, Advances in Neural Information Processing Systems, volume 7. MIT Press,\nCambridge, MA, 1995.\n\n\x0c'
p83136
sg438
S'683\n\nA MEAN FIELD THEORY OF LAYER IV OF VISUAL CORTEX\nAND ITS APPLICATION TO ARTIFICIAL NEURAL NETWORKS*\nChristopher L. Scofield\nCenter for Neural Science and Physics Department\nBrown University\nProvidence, Rhode Island 02912\nand\nNestor, Inc., 1 Richmond Square, Providence, Rhode Island,\n02906.\nABSTRACT\nA single cell theory for the development of selectivity and\nocular dominance in visual cortex has been presented previously\nby Bienenstock, Cooper and Munrol. This has been extended to a\nnetwork applicable to layer IV of visual cortex 2 . In this paper\nwe present a mean field approximation that captures in a fairly\ntransparent manner the qualitative, and many of the\nquantitative, results of the network theory. Finally, we consider\nthe application of this theory to artificial neural networks and\nshow that a significant reduction in architectural complexity is\npossible.\nA SINGLE LAYER NETWORK AND THE MEAN FIELD\nAPPROXIMATION\nWe consider a single layer network of ideal neurons which\nreceive signals from outside of the layer and from cells within\nthe layer (Figure 1). The activity of the ith cell in the network is\nc\'1 -- m\'1 d + ""\'\n~ T .. c\'\n~J J\'\n\nJ\n\n(1)\n\nEach cell\nd is a vector of afferent signals to the network.\nreceives input from n fibers outside of the cortical network\nthrough the matrix of synapses mi\' Intra-layer input to each cell\nis then transmitted through the matrix of cortico-cortical\nsynapses L.\n? American Institute of Physics 1988\n\n\x0c684\n\nAfferent\nSignals\n\n>\n\n... ..\n\nm2\n\nm1\n\nmn\n\n~\n\nr;.\n\n",...-\n\nd\n\n.L\n:\n\n1\n\n,~\n\n2\n\n... ..\n\n, ...c.. ,\n\n~\n\n~\n\nFigure 1: The general single layer recurrent\nnetwork.\nLight circles are the LGN -cortical\nsynapses.\nDark circles are the (nonmodifiable) cortico-cortical synapses.\nWe now expand the response of the i th cell into individual\nterms describing the number of cortical synapses traversed by\nthe signal d before arriving through synapses Lij at cell i.\nExpanding Cj in (1), the response of cell i becomes\nci\n\n=mi d + l: ~j mj d + l: ~jL Ljk mk d + 2: ~j 2Ljk L Lkn mn d +... (2)\nJ\n\nJ\n\nK\n\nJ\n\nK\' n\n\nNote that each term contains a factor of the form\n\nThis factor describes the first order effect, on cell q, of the\ncortical transformation of the signal d.\nThe mean field\napproximation consists of estimating this factor to be a constant,\nindependant of cell location\n(3)\n\n\x0c685\n\nThis assumption does not imply that each cell in the network is\nselective to the same pattern, (and thus that mi = mj). Rather,\nthe assumption is that the vector sum is a constant\n\nThis amounts to assuming that each cell in the network is\nsurrounded by a population of cells which represent, on average,\nall possible pattern preferences.\nThus the vector sum of the\nafferent synaptic states describing these pattern preferences is a\nconstant independent of location.\nFinally, if we assume that the lateral connection strengths are\na function only of i-j then Lij becomes a circular matrix so that\n\nr. Lij ::: ~J Lji = Lo = constan t.\n1\n\nThen the response of the cell i becomes\n(4)\n\nfor I\n\n~\n\nI <1\n\nwhere we define the spatial average of cortical cell activity C = in\nd, and N is the average number of intracortical synapses.\nHere, in a manner similar to that in the theory of magnetism,\nwe have replaced the effect of individual cortical cells by their\naverage effect (as though all other cortical cells can be replaced\nby an \'effective\' cell, figure 2). Note that we have retained all\norders of synaptic traversal of the signal d.\nThus, we now focus on the activity of the layer after\n\'relaxation\' to equilibrium. In the mean field approximation we\ncan therefore write\n(5)\n\nwhere the mean field\n\na\nwith\n\n=am\n\n\x0c686\n\nand we asume that\ninhibitory).\n\nAfferent\nSignals\nd\n\nLo < 0 (the network is,\n\non\n\naverage,\n\n>\n\nFigure 2: The single layer mean field network.\nDetailed connectivity between all cells of the\nnetwork is replaced with a single (nonmodifiable) synapse from an \'effective\' cell.\nLEARNING IN THE CORTICAL NETWORK\n\nWe will first consider evolution of the network according to a\nsynaptic modification rule that has been studied in detail, for\nsingle cells, elsewhere!? 3.\nWe consider the LGN - cortical\nsynapses to be the site of plasticity and assume for maximum\nsimplicity that there is no modification of cortico-cortical\nsynapses. Then\n(6)\n\n.\n\nLij = O.\nIn what follows c denotes the spatial average over cortical cells,\nwhile Cj denotes the time averaged activity of the i th cortical cell.\nThe function cj> has been discussed extensively elsewhere.\nHere\nwe note that cj> describes a function of the cell response that has\nboth hebbian and anti-hebbian regions.\n\n\x0c687\n\nThis leads to a very complex set of non-linear stochastic\nequations that have been analyzed partially elsewhere 2 . In\ngeneral, the afferent synaptic state has fixed points that are\nstable and selective and unstable fixed points that are nonselective!, 2. These arguments may now be generalized for the\nnetwork. In the mean field approximation\n(7)\n\nThe mean field, a has a time dependent component m. This\nvaries as the average over all of the network modifiable\nsynapses and, in most environmental situations, should change\nslowly compared to the change of the modifiable synapses to a\nsingle cell. Then in this approximation we can write\n\n?\n\n(mi(a)-a) = cj>[mi(a) - a] d.\n\n(8)\n\nWe see that there is a mapping\nmi\' <-> mica) - a\n\n(9)\n\nsuch that for every mj(a) there exists a corresponding (mapped)\npoint mj\' which satisfies\n\nthe original equation for the mean field zero theory. It can be\nshown 2, 4 that for every fixed point of mj( a = 0), there exists a\ncorresponding fixed point mj( a) with the same selectivity and\nstability properties.\nThe fixed points are available to the\nneurons if there is sufficient inhibition in the network (ILo I is\nsufficiently large).\nAPPLICATION OF THE MEAN FIELD NETWORK TO\nLAYER IV OF VISUAL CORTEX\nNeurons in the primary visual cortex of normal adult cats are\nsharply tuned for the orientation of an elongated slit of light and\nmost are activated by stimulation of either eye. Both of these\nproperties--orientation selectivity and binocularity--depend on\nthe type of visual environment experienced during a critical\n\n\x0c688\n\nperiod of early postnatal development. For example, deprivation\nof patterned input during this critical period leads to loss of\norientation selectivity while monocular deprivation (MD) results\nin a dramatic shift in the ocular dominance of cortical neurons\nsuch that most will be responsive exclusively to the open eye.\nThe ocular dominance shift after MD is the best known and most\nintensively studied type of visual cortical plasticity.\nThe behavior of visual cortical cells in various rearing\nconditions suggests that some cells respond more rapidly to\nenvironmental changes than others.\nIn monocular deprivation,\nfor example, some cells remain responsive to the closed eye in\nspite of the very large shift of most cells to the open eye- Singer\net. al. 5 found, using intracellular recording, that geniculo-cortical\nsynapses on inhibitory interneurons are more resistant to\nmonocular deprivation than are synapses on pyramidal cell\ndendrites. Recent work suggests that the density of inhibitory\nGABAergic synapses in kitten striate cortex is also unaffected by\nMD during the cortical period 6, 7.\nThese results suggest that some LGN -cortical synapses modify\nrapidly, while others modify relatively slowly, with slow\nmodification of some cortico-cortical synapses. Excitatory LGNcortical synapses into excitatory cells may be those that modify\nprimarily.\nTo embody these facts we introduce two types of\nLGN -cortical synapses:\nthose (mj) that modify and those (Zk)\nthat remain relatively constant. In a simple limit we have\n\nand\n\n(10)\n\nWe assume for simplicity and consistent with the above\nphysiological interpretation that these two types of synapses are\nconfined to two different classes of cells and that both left and\nright eye have similar synapses (both m i or both Zk) on a given\ncell. Then, for binocular cells, in the mean field approximation\n(where binocular terms are in italics)\n\n\x0c689\n\nwhere dl(r) are the explicit left (right) eye time averaged signals\narriving form the LGN.\nNote that a1(r) contain terms from\nmodifiable and non-modifiable synapses:\nal(r) =\n\na (ml(r) + zl(r?).\n\nUnder conditions of monocular deprivation, the animal is reared\nwith one eye closed. For the sake of analysis assume that the\nright eye is closed and that only noise-like signals arrive at\ncortex from the right eye. Then the environment of the cortical\ncells is:\nd = (di, n)\n\n(12)\n\nFurther, assume that the left eye synapses have reached their\n1\n\nr\n\nselective fixed point, selective to pattern d 1 ? Then (mi\' m i )\n(m:*, xi) with IXil ?lm!*1.\nlinear analysis of the\nthe closed eye\n\n<I> -\n\n=\n\nFollowing the methods of BCM, a local\nfunction is employed to show that for\n\nXi =\n\na (1 - }..a)-li.r.\n\n(13)\n\nwhere A. = NmIN is the ratio of the number modifiable cells to the\ntotal number of cells in the network. That is, the asymptotic\nstate of the closed eye synapses is a scaled function of the meanfield due to non-modifiable (inhibitory) cortical cells. The scale\nof this state is set not only by the proportion of non-modifiable\ncells, but in addition, by the averaged intracortical synaptic\nstrength Lo.\nThus contrasted with the mean field zero theory the deprived\neye LGN-cortical synapses do not go to zero.\nRather they\napproach the constant value dependent on the average inhibition\nproduced by the non-modifiable cells in such a way that the\nasymptotic output of the cortical cell is zero (it cannot be driven\nby the deprived eye). However lessening the effect of inhibitory\nsynapses (e.g. by application of an inhibitory blocking agent such\nas bicuculine) reduces the magnitude of a so that one could once\nmore obtain a response from the deprived eye.\n\n\x0c690\n\nWe find, consistent with previous theory and experiment,\nthat most learning can occur in the LGN-cortical synapse, for\ninhibitory (cortico-cortical) synapses need not modify.\nSome\nnon-modifiable LGN-cortical synapses are required.\nTHE MEAN FIELD APPROXIMATION AND\nARTIFICIAL NEURAL NETWORKS\nThe mean field approximation may be applied to networks in\nwhich the cortico-cortical feedback is a general function of cell\nactivity. In particular, the feedback may measure the difference\nbetween the network activity and memories of network activity.\nIn this way, a network may be used as a content addressable\nmemory.\nWe have been discussing the properties of a mean\nfield network after equilibrium has been reached. We now focus\non the detailed time dependence of the relaxation of the cell\nactivity to a state of equilibrium.\nHopfield8 introduced a simple formalism for the analysis of\nthe time dependence of network activity.\nIn this model,\nnetwork activity is mapped onto a physical system in which the\nstate of neuron activity is considered as a \'particle\' on a potential\nenergy surface.\nIdentification of the pattern occurs when the\nactivity \'relaxes\' to a nearby minima of the energy.\nThus\nmlmma are employed as the sites of memories. For a Hopfield\nnetwork of N neurons, the intra-layer connectivity required is of\norder N2. This connectivity is a significant constraint on the\npractical implementation of such systems for large scale\nproblems. Further, the Hopfield model allows a storage capacity\nwhich is limited to m < N memories 8, 9. This is a result of the\nproliferation of unwanted local minima in the \'energy\' surface.\nRecently, Bachmann et al. l 0, have proposed a model for the\nrelaxation of network activity in which memories of activity\npatterns are the sites of negative \'charges\', and the activity\ncaused by a test pattern is a positive test \'charge\'. Then in this\nmodel, the energy function is the electrostatic energy of the\n(unit) test charge with the collection of charges at the memory\nsites\n\nE = -IlL ~ Qj I J-l- Xj I - L,\nJ\n\n(14)\n\n\x0c691\n\nwhere Jl (0) is a vector describing the initial network activity\ncaused by a test pattern, and Xj\' the site of the jth memory. L is\na parameter related to the network size.\nThis model has the advantage that storage density is not\nrestricted by the the network size as it is in the Hopfield model,\nand in addition, the architecture employs a connectivity of order\nm x N.\nNote that at each stage in the settling of Jl (t) to a memory\n(of network activity) Xj\' the only feedback from the network to\neach cell is the scalar\n~\n\nJ\n\nQ. I Jl- X? I - L\nJ\n\nJ\n\n(15)\n\nThis quantity is an integrated measure of the distance of the\ncurrent network state from stored memories.\nImportantly, this\nmeasure is the same for all cells; it is as if a single virtual cell\nwas computing the distance in activity space between the\ncurrent state and stored states. The result of the computation is\nThis is a\nthen broadcast to all of the cells in the network.\ngeneralization of the idea that the detailed activity of each cell in\nthe network need not be fed back to each cell.\nRather some\nglobal measure, performed by a single \'effective\' cell is all that is\nsufficient in the feedback.\nDISCUSSION\n\nWe have been discussing a formalism for the analysis of\nnetworks of ideal neurons based on a mean field approximation\nof the detailed activity of the cells in the network. We find that\na simple assumption concerning the spatial distribution of the\npattern preferences of the cells allows a great simplification of\nthe analysis. In particular, the detailed activity of the cells of\nthe network may be replaced with a mean field that in effect is\ncomputed by a single \'effective\' cell.\nFurther, the application of this formalism to the cortical layer\nIV of visual cortex allows the prediction that much of learning in\ncortex may be localized to the LGN-cortical synaptic states, and\nthat cortico-cortical plasticity is relatively unimportant. We find,\nin agreement with experiment, that monocular deprivation of\nthe cortical cells will drive closed-eye responses to zero, but\nchemical blockage of the cortical inhibitory pathways would\nreveal non-zero closed-eye synaptic states.\n\n\x0c692\n\nFinally, the mean field approximation allows the development\nof single layer models of memory storage that are unrestricted\nin storage density, but require a connectivity of order mxN. This\nis significant for the fabrication of practical content addressable\nmemories.\nACKNOWLEOOEMENTS\nI would like to thank Leon Cooper for many helpful discussions\nand the contributions he made to this work.\n\n*This work was supported by the Office of Naval Research and\nthe Army Research Office under contracts #NOOOI4-86-K-0041\nand #DAAG-29-84-K-0202.\n\nREFERENCES\n[1] Bienenstock, E. L., Cooper, L. N & Munro, P. W. (1982) 1.\nNeuroscience 2, 32-48.\n[2] Scofield, C. L. (I984) Unpublished Dissertation.\n[3] Cooper, L. N, Munro, P. W. & Scofield, C. L. (1985) in Synaptic\nModification, Neuron Selectivity and Nervous System\nOrganization, ed. C. Levy, J. A. Anderson & S. Lehmkuhle,\n(Erlbaum Assoc., N. J.).\n[4] Cooper, L. N & Scofield, C. L. (to be published) Proc. Natl. Acad.\nSci. USA ..\n[5] Singer, W. (1977) Brain Res. 134, 508-000.\n[6] Bear, M. F., Schmechel D. M., & Ebner, F. F. (1985) 1. Neurosci.\n5, 1262-0000.\n[7] Mower, G. D., White, W. F., & Rustad, R. (1986) Brain Res. 380,\n253-000.\n[8] Hopfield, J. J. (1982) Proc. Natl. A cad. Sci. USA 79, 2554-2558.\n[9] Hopfield, J. J., Feinstein, D. 1., & Palmer, R. O. (1983) Nature\n304, 158-159.\n[10] Bachmann, C. M., Cooper, L. N, Dembo, A. & Zeitouni, O. (to be\npublished) Proc. Natl. Acad. Sci. USA.\n\n\x0c'
p83137
sg116
S'How Perception Guides Production\nBirdsong Learning\n\n?\n\nIn\n\nChristopher L. Fry\ncfry@cogsci.ucsd.edu\nDepartment of Cognitive Science\nUniversity of California at San Diego\nLa Jolla, CA 92093-0515\n\nAbstract\nA c.:omputational model of song learning in the song sparrow\n(M elospiza melodia) learns to categorize the different syllables of\na song sparrow song and uses this categorization to train itself to\nreproduce song. The model fills a crucial gap in the computational\nexplanation of birdsong learning by exploring the organization of\nperception in songbirds. It shows how competitive learning may\nlead to the organization of a specific nucleus in the bird brain,\nreplicates the song production results of a previous model (Doya\nand Sejnowski, 1995), and demonstrates how perceptual learning\ncan guide production through reinforcement learning.\n\n1\n\nINTRODUCTION\n\nThe passeriformes or songbirds make up more than half of all bird species and\nare divided into two groups: the os cines which learn their songs and sub-oscines\nwhich do not. Oscines raised in isolation sing degraded species typical songs similar\nto wild song. Deafened oscines sing completely degraded songs (Konishi, 1965) ,\nwhile deafened sub-oscines develop normal songs (Kroodsma and Konishi, 1991)\nindicating that auditory feedback is crucial in oscine song learning.\nInnate structures in the bird brain regulate song learning. For example, song sparrows show innate preferences for their own species\' songs and song structure (Marler, 1991). Innate preferences are thought to be encoded in an auditory template\nwhich limits the sounds young birds may copy. According to the auditory template hypothesis birds go through two phases during song learning , a memorization phase and a motor phase. In the memorization phase, which lasts\nfrom approximately 20 to 50 days after birth in the song sparrow, the bird selects\nwhich sounds to copy based on an innate template and refines the template based\n\n\x0c111\n\nHow Perception Guides Production in Birdsong Learning\nPOSTERIOR\n\nANTERIOR\n\ning\n--+ ......\nPall..,.\n~l\'IodUclcn\n\n10 ?? chea ?\n\nSJrIn I\n\nPall ...,\n\nFigure 1: A simplified sketch of a saggital section of the songbird brain . Field L (Field\nL) receives auditory input and projects to the production pathway: HVc (formerly the\ncaudal nucleus of the hyperstriatum), RA (robust nucleus of archistriatum), nXIIts (hypoglossal nerve), the syrinx (vocal organ) and the learning pathway: X (area X), DLM\n(medial nucleus of the dorsolateral thalamus), LMAN (lateral magnocellular nucleus of\nthe anterior neostriatum), RA (Konishi, 1989; Vicario, 1994). V is the lateral ventricle.\non the sounds it hears . In the motor phase (from approximately 272 to 334 days\nafter birth) the template provides feedback during singing. Learning to sing the\nmemorized, template song is a gradual process of refining the produced song to\nmatch memory (Marler, 1991).\nA song is made up of phrases, phrases of syllables and syllables of notes. Syllables,\nusually separated by periods of silence, are the main units of analysis. Notes typically last from 10-100 msecs and are used to construct syllables (100-200 msecs)\nwhich are reused to produce trills and other phrases.\n\n2\n\nNEUROBIOLOGY OF SONG\n\nThe two main neural pathways that govern song are the motor and learning pathways seen in figure 1 (Konishi , 1989). Lesions to the motor pathway interrupt\nsinging throughout life while lesions to the learning pathway disrupt early song\nlearning. Although these pathways seem to have segregated functions , recordings\nof neurons during song playback have shown that cells throughout the song system\nrespond to song (Konishi, 1989).\nStudies of song perception have shown the best auditory stimulus that will evoke a\nresponse in the song system is the bird\'s own song (Margoliash , 1986) . The song\nspecific neurons in HV c of the white-crowned sparrow often require a sequence of\ntwo syllables to respond (Margoliash , 1986 ; Margoliash and Fortune , 1992) and are\nmade up of two main types in HV c . One type is sensitive to temporal combinations\nof stimuli while the other is sensitive to harmonic characteristics (Margoliash and\nFortune, 1992) .\n\n3\n\nCOMPUTATION\n\nPrevious computational work on birdsong learning predicted individual neural responses using back-propagation (Margoliash and Bankes , 1993) and modelled motor\nmappings for song production (Doya and Sejnowski, 1995). The current work de-\n\n\x0cC.L.FRY\n\n112\n1\n\n2\n\n00\n\n8\n\nKohonen Neuron\n\nInpulLayer\n\nSliding\nWindo"",.\n\n~\n\n..\n\n_ _ _ _ __\n\n~\n\n1000\n\nFigure 2: Perceptual network input encoding. The song is converted into frequency bins\nwhich are presented to the Kohonen layer over four time steps.\nvelops a model of birdsong syllable perception which extends Doya and Sejnowski\'s\n(1995) model of birdsong learning. Birdsong syllable segmentation is accomplished\nusing an unsupervised system and this system is used to train the network to reproduce its input using reinforcement learning.\nThe model implements the two phases of the auditory template hypothesis, memorization and motor. In the first phase the template song is segmented into\nsyllables by an unsupervised Kohonen network (Kohonen, 1984). In the second\nphase the syllables are reproduced using a reinforcement learning paradigm based\non Doya and Sejnowski (1995).\nThe model extends previous work in three ways: 1) a self-organizing network picks\nout syllables in the song; 2) the self-organizing network provides feedback during\nsong production; and 3) a more biologically plausible model of the syrinx is used to\ngenerate song.\n3.1\n\nPerception\n\nRecognizing a syllable involves identifying a short sequence of notes. Kohonen\nnetworks use an unsupervised learning method to categorize an input space based\non similar neural responses. Thus a Kohonen network is a natural candidate for\nidentifying the syllables in a song.\nOne song from the repertoire of a song sparrow was chosen as the training song\nfor the network . The song was encoded by passing a sliding window across the\ntraining waveform (sampled at 22 .255 kHz) of the selected song. At each time step,\na non-overlapping 256 point (~ .011 sec) fast fourier transform (FFT) was used to\ngenerate a power spectrum (figure 2). The power spectrum was divided into 8 bins.\nEach bin was mapped to a real number using a gaussian summation procedure with\nthe peak of the gaussian at the center of each frequency bin. Four time-steps were\npassed to each Kohonen neuron.\nThe network\'s task was to identify similar syllables in the input song. The input\nsong was broken down into syllables by looking for points where the power at all\n\n\x0c113\n\nHow Perception Guides Production in Birdsong Learning\n\n10\n\n>.\nu\n.:\n\ng."\n"..."\nkH<\n\n5\n\nt\n-/>,\n\na\ns\n\n0.0\n\n0.5\n\n1.0\n\n20\n\ntime\n\n"ii:..\nCD\n\n.\n\nS\n:I\nCD\n\nz\n\nn1\nn2\nn3\nn4\n\nn5\nn6\nn7\nn8\n\nFigure 3: Categorization of song syllables by a Kohonen network. The power-spectrum of\nthe training song is at the top. The responses of the Kohonen neurons are at the bottom.\nFor each time-step the winning neuron is shown with a vertical bar. The shaded areas\nindicate the neuron that fired the most during the presentation of the syllable.\n\nfrequencies dropped below a threshold . A syllable was defined as sound of duration\ngreater than .011 seconds bounded by two low-power points . The network was not\ntrained on the noise between syllables. The song was played for the network ten\ntimes (1050 training vectors), long enough for a stable response pattern to emerge.\n\n=\n\nThe activation of a neuron was : N etj\n\'ExiWij\' Where: N etj = output of neuron\nj , Wij = the weight connecting inputi to n eu ronj , Xi = inputi. The Kohonen network was trained by initializing the connection weights to 1/Jnumber of neurons\n+ small random component (r S; .01) , normalizing the inputs , and updating the\nweights to the winning neuron by the following rule : W n ew = W old + a(x - W old)\nwhere : a = training rat e = .20 . If the same neuron won twice in a row the training rate was decreased by 1/2. Only the winning neuron was reinforced resulting\nin a non-localized feature map .\n3.1.1\n\nPerceptual Results\n\nThe Kohonen network was able to assign a unique neuron to each type of syllable\n(figure 3) . Of the eight neurons in the network. the one that fired the most frequently\nduring the presentation of a syllable uniquely identified the type of syllable. The\nfirst four syllables of the input song sound alike, contain similar frequencies , and\nare coded by the first neuron (N1). The last three syllables sound alike, contain\nsimilar frequencies , and are coded by the fourth neuron (N4). Syllable five was\ncoded by neuron six (N6) , syllable six by neuron two (N2) and syllable seven by\nneuron eight (N8).\nFigure 4 shows the frequency sensitivity of each neuron (1-8, figure 3) plotted against\neach time step (1-4). This plot shows the harmonic and temporally sensitive neurons that developed during the learning phase of the Kohonen network. Neuron 2\nis sensitive to only one frequency at approximately 6-7 kHz , indicated by the solid\nwhite band across the 6-7 kHz frequency range in figure 4. Neuron 4 is sensitive\nto mid-range frequencies of short duration . Note that in figure 4 N4 responds\n\n\x0cC. L. FRY\n\n114\n\no\n\n1\n\n2\n\n3\n\n4\n\n01\n\nN6\n\nN5\n\n23\nN7\n\n4\nN8\n\nTime S t e p\n\nFigure 4: The values of the weights mapping frequency bins and time steps to Kohonen\nneurons. White is maximum , Black is minimum .\nmaximally to mid-range frequencies only in the first two time steps. It uses this\ntemporal sensitivity to distinguish between the last three syllables and the fifth syllable (figure 3) by keying off the length of time mid-range frequencies are present.\nContrast this early response sensitivity with neuron 6, which is sensitive to midrange frequencies of long duration , but responds only after one time step . It uses\nthis temporal sensitivity to respond to the long sustained frequency of syllable four .\nConsidered together, neurons 2,4,6 and 8 illustrate the two types of neurons (temporal and harmonic) found in HVc by Margoliash and Fortune (1993). Competitive\nlearning may underly the formation of these neurons in HV c.\n3.2\n\nProduction\n\nAfter competitive learning trains the perceptual part of the network to categorize\nthe song into syllables , the perceptual network can be used to train the production\nside of the network to sing.\nThe first step in modelling song production is to create a model of the avian vocal apparatus , the syrinx. In the syrinx sounds arise when air flows through the\nsyringeal passage and causes the tympanic membrane to vibrate. The frequency is\ncontrolled by the tension of the membrane controlled by the syringeal musculature.\nThe amplitude is dependent on the area of the syringeal orifice which is dependent\non the tension of the labium. The interactions of this system were modelled by\nmodulated sine waves. Four parameters governed the fundamental frequency(p) ,\nfrequency modulation(tm) , amplitude (ex) and frequency of amplitude modulation(I). The range of the parameters was set according to calculations in Greenwalt\n(1968). The parameters were combined in the following equation (based on Greenwalt, 1968), f(ex , l,p, tm , t) excos(21l"t 1) cos(21l"t p + cos(21l"t tm)) .\n\n=\n\nUsing this equation song can be generated over time by making assumptions about\nthe response properties of neurons in RA . Following Doya and Sejnowski (1995) it\nwas assumed that pools of RA neurons have different temporal response profiles.\nSyllable like temporal responses can be generated by modifying the weights from\nthe Kohonell layer (HV c) to the production layer (RA) .\n\n\x0cHow Perception Guides Production in Birdsong Learning\nTnnni~\n\n115\n\nSong\n\n,\n\n. .. .\n\n.f . ?~:if>vr.\n\n...\ni\n\ni\n\nI\n\n\'0\n\n15\n\n20\n\nTim.\n\nJ\'etworl< Song trained with Spectmgmm Target\n\nTi\'llll!\n\nJ\'etworl< Song trained with J\'euraJ. Activation Target\n10\n\nkHz\n\nS\n\naa\n\nas\n\n10\n\n15\n\n20\n\nFigure 5: Training song and two songs produced with different representations of the\ntraining song.\nThe production side of the network was trained using the reinforcement learning\nparadigm described in Doya and Sejnowski (1995). Each syllable was presented in\nthe order it occurred in the training song to the Kohonen layer, which turned on a\nsingle neuron. A random vector was added to the weights from the Kohonen layer to\nthe output layer and a syllable was produced. The produced syllable was compared\nto the stored representation of the template song which was used to generate an\nerror signal and an estimate of the gradient. If the evaluation of the produced\nsyllable was better than a threshold the weights were kept, otherwise they were\ndiscarded .\nTwo experiments were done using different representations of the template song.\nIn the first experiment the template song was the stored power spectrum of each\nsyllable and the error signal was the cosine of the angle between the power spectrum\nof the produced syllable and the template syllable. In the second experiment the\ntemplate song was the stored neural responses to song (recorded during the memorization phase) and the error signal was the Euclidean distance between neural\nresponses to the produced syllable and the neural responses to the template song.\n3.2.1\n\nProduction Results\n\nFigure 5 shows the output of the production network after training with different\nrepresentations of the training song. The network was able to replicate the major\nfrequency components of the training song to a high degree of accuracy. The song\ntrained with the spectrogram target was learned to a 90% average cosine between\nthe spectrograms of the produced song and the training song on each syllable with\nthe best syllable learned to 100% accuracy and the worst to 85% after 1000 trials. A\ncrucial aspect to achieving performance was smoothing the template spectrogram.\nThe third song shows that the network was able to learn the template song using the\nneural responses of the perceptual system to generate the reinforcement signal. The\naverage distance between the initial randomly produced syllables and the training\n\n\x0cC. L.FRY\n\n116\n\nsong was reduced by 50%.\n\n4\n\nDISCUSSION\n\nThis work fills a crucial gap in the computational explanation of song learning left\nby prior work . Doya and Sejnowski (1995) showed how song could be produced\nbut left unanswered the questions of how song is perceived and how the perceptual\nsystem provides feedback during song production. This study shows a time-delay\nKohonen network can learn to categorize the syllables of a sample song and this\nnetwork can train song production with no external teacher. The Kohonen network\nexplains how neurons sensitive to temporal and harmonic structure could arise in\nthe songbird brain through competitive learning. Taken as a whole , the model\npresents a concrete proposal of the computational principles governing the Auditory Template Hypothesis and how a song is memorized and used to train song\nproduction. Future work will flesh out the effects of innate structure on learning by\nexamining how the settings of the initial weights on the network affect song learning\nand predict experimental effects of deafening and isolation .\nAcknowledgements\nThanks to S. Vehrencamp for providing the song data, J . Batali, J. Elman, J. Bradbury and T. Sejnowski for helpful comments , and K. Doya for advice on replicating\nhis model.\nReferences\nDoya, K . and Sejnowski, T .J. (1995). A novel reinforcement model of bird song vocalization\nlearning. In Tesauro, G ., Touretzky, D. S. and Leen , T.K., editors, Advances in Neural\nInformation Processing Systems 7. MIT Press, Cambridge, MA.\nGreenwalt, C.H. (1968). Bird Song: Acoustics and Physiology. Smithsonian Institution\nPress. Wash., D.C.\nKohonen, T . (1984). Self-organization and Associative Memory, Vol. 8. Springer-Verlag,\nBerlin.\nKonishi, M. (1965). The role of auditory feedback in the control of vocalization in the\nwhite-crowned sparrow. Zeitschrijt fur Tierpsychogie , 22,770-783.\nKonishi, M. (1989). Birdsong for Neurobiologists. Neuron , 3, 541-549.\nKroodsma, D.E . and Konishi , M. (1991) . A suboscine bird (eastern phoebe, Sayonoris\nphoebe) develops normal song without auditory feedback. Animal Behavior, 42, 477-487.\nMarler , P. (1991). The instinct to learn. In The Epigenesis of Mind: Essays on Biology\nand Cognition, eds. S. Carey and R. Gelman. Lawrence Erlbaum Associates.\nMargoliash , D . (1986). Preference for autogenous song by auditory neurons in a song\nsystem nucleus of the white-crowned sparrow . Journal of Neuroscience, 6,1643-1661.\nMargoliash, D . and Bankes, S.C. (1993) . Computations in the Ascending Auditory Pathway in Songbirds Related to Song Learning. American Zoologist, 33 , 94-103.\nMargoliash , D. and Fortune, E . (1992). Temporal and Harmonic Combination-Sensitive\nNeurons in the Zebra Finch\'s HVc. Journal of Neuroscien ce, 12, 4309-4326.\nVicario , D. (1994). Motor Mechanisms Relevant to Auditory-Vocal Interactions in Songbirds. Bra in, Behavior and Evolution,44, 265-278 .\n\n\x0c'
p83138
sg118
S'A Neural Network Model of 3-D\nLightness Perception\n\nLuiz Pessoa\nFederal Univ. of Rio de Janeiro\nRio de Janeiro, RJ, Brazil\npessoa@cos.ufrj.br\n\nWilliam D. Ross\nBoston University\nBoston, MA 02215\nbill@cns.bu.edu\n\nAbstract\nA neural network model of 3-D lightness perception is presented\nwhich builds upon the FACADE Theory Boundary Contour System/Feature Contour System of Grossberg and colleagues. Early\nratio encoding by retinal ganglion neurons as well as psychophysical results on constancy across different backgrounds (background\nconstancy) are used to provide functional constraints to the theory\nand suggest a contrast negation hypothesis which states that ratio\nmeasures between coplanar regions are given more weight in the\ndetermination of lightness of the respective regions. Simulations\nof the model address data on lightness perception, including the\ncoplanar ratio hypothesis, the Benary cross, and White\'s illusion.\n\n1\n\nINTRODUCTION\n\nOur everyday visual experience includes surface color constancy. That is, despite 1)\nvariations in scene lighting and 2) movement or displacement across visual contexts,\nthe color of an object appears to a large extent to be the same. Color constancy\nrefers, then, to the fact that surface color remains largely constant despite changes\nin the intensity and composition of the light reflected to the eyes from both the\nobject itself and from surrounding objects. This paper discusses a neural network\nmodel of 3D lightness perception - i.e., only the achromatic or black to white\ndimension of surface color perception is addressed. More specifically, the problem\nof background constancy (see 2 above) is addressed and mechanisms to accomplish\nit in a system exhibiting illumination constancy (see 1 above) are proposed.\nA landmark result in the study of lightness was an experiment reported by Wallach (1948) who showed that for a disk-annulus pattern, lightness is given by the\nratio of disk and annulus luminances (i.e., independent of overall illumination); the\n\n\x0cA Neural Network Model of 3-D Lightness Perception\n\n845\n\nso-called ratio principle. In another study, Whittle and Challands (1969) had subjects perform brightness matches in a haploscopic display paradigm. A striking\nresult was that subjects always matched decrements to decrements , or increments\nto increments, but never increments to decrements. Whittle and Challands\' (1969)\nresults provide psychophysical support to the notion that the early visual system\ncodes luminance ratios and not absolute luminance. These psychophysical results\nare in line with results from neurophysiology indicating that cells at early stages\nof the visual system encode local luminance contrast (Shapley and Enroth-Cugell,\n1984). Note that lateral inhibition mechanisms are sensitive to local ratios and can\nbe used as part of the explanation of illumination constancy.\nDespite the explanatory power of the ratio principle, and the fact that the early\nstages of the visual system likely code contrast, several experiments have shown that,\nin general, ratios are insufficient to account for surface color perception. Studies\nof background constancy (Whittle and Challands, 1969; Land and McCann, 1971;\nArend and Spehar, 1993), of the role of 3-D spatial layout and illumination arrangement on lightness perception (e.g. , Gilchrist, 1977) as well as many other effects,\nargue against the sufficiency of local contrast measures (e.g., Benary cross, White \'s,\n1979 illusion). The neural network model presented here addresses these data using\nseveral fields of neurally plausible mechanisms of lateral inhibition and excitation.\n\n2\n\nFROM LUMINANCE RATIOS TO LIGHTNESS\n\nThe coplanar ratio hypothesis (Gilchrist, 1977) states that the lightness of a given\nregion is determined predominantly in relation to other coplanar surfaces, and not\nby equally weighted relations to all retinally adjacent regions. We propose that in\nthe determination of lightness, contrast measures between non-coplanar adjacent\nsurfaces are partially negated in order to preserve background constancy.\nConsider the Benary Cross pattern (input stimulus in Fig. 2). If the gray patch on\nthe cross is considered to be at the same depth as the cross , while the other gray\npatch is taken to be at the same depth as the background (which is below the cross),\nthe gray patch on the cross should look lighter (since its lightness is determined\nin relation to the black cross), and the other patch darker (since its lightness is\ndetermined in relation to the white background) . White\'s (1979) illusion can be\ndiscussed in similar terms (see the input stimulus in Fig. 3).\nThe mechanisms presented below implement a process of partial contrast negation in\nwhich the initial retinal contrast code is modulated by depth information such that\nthe retinal contrast consistent with the depth interpretation is maintained while the\nretinal contrast not supported by depth is negated or attenuated.\n\n3\n\nA FILLING-IN MODEL OF 3-D LIGHTNESS\n\nContrast/Filling-in models propose that initial measures of boundary contrast followed by spreading of neural activity within filling-in compartments produce a response profile isomorphic with the percept (Gerrits & Vendrik, 1970; Cohen &\nGrossberg, 1984; Grossberg & Todorovic, 1988; Pessoa, Mingolla, & Neumann,\n1995). In this paper we develop a neural network model of lightness perception in\nthe tradition of contrast/filling-in theories. The neural network developed here is an\nextension of the Boundary Contour System/Feature Contour System (BCS/FCS)\nproposed by Cohen and Grossberg (1984) and Grossberg and Mingolla (1985) to\nexplain 3- D lightness data.\n\n\x0cL. PESSOA. W. D. ROSS\n\n846\n\nA fundamental idea of the BCS/FCS theory is that lateral inhibition achieves illumination constancy but requires the recovery of lightness by the filling-in, or diffusion ,\nof featural quality ("lightness" in our case) . The final diffused activities correspond\nto lightness, which is the outcome of interactions between boundaries and featural\nquality, whereby boundaries control the process of filling-in by forming gates of\nvariable resistance to diffusion .\n\nHow can the visual system construct 3-D lightness percepts from contrast measures\nobtained by retinotopic lateral inhibition? A mechanism that is easily instantiated in\na neural model and provides a straightforward modification to the contrast/fillingin proposal of Grossberg and Todorovic (1988) is the use of depth-gated filling-in.\nThis can be accomplished through a pathway that modulates boundary strength\nfor boundaries between surfaces or objects across depth. The use of permeable\nor "leaky" boundaries was also used by Grossberg and Todorovic (1988) for 2-D\nstimuli. In the current usage, permeability is actively increased at depth boundaries\nto partially negate the contrast effect - since filling-in proceeds more freely - and\nthus preserve lightness constancy across backgrounds. Figure 1 describes the four\ncomputational stages of the system.\n\nI\n\nBOUNDARIES\n\n,...---------,\n~\n\n~-\n\n~\n\nON/OFF\nFILTERING\n\nj\n~I\n\nRLLlNG-IN\n\nI\n\nI\n\n\'"\n\nI\n\nDEPTH\nMAP\n\nI\n\nFigure 1: Model components.\n\nStage 1: Contrast Measurement. At this stage both ON and OFF neural fields\nwith lateral inhibitory connectivity measure the strength of contrast at image regions - in uniform regions a contrast measurement of zero results . Formally, the\nON field is given by\n\ndyi; -_ -aYij+ + ((3 dt\n\nct\n\n+ ) C + - (+\nYij\n\nYij\n\nij\n\n+ \'Y ) Eij+\n\n(1)\n\nyi;\n\nEt;\n\nwhere a , (3 and \'Yare constants;\nis the total excitatory input to\nand\nis the\ntotal inhibitory input to\nThese terms denote discrete convolutions of the input\nIij with Gaussian weighting functions, or kernels. An analogous equation specifies\nYi; for the OFF field . Figure 2 shows the ON-contrast minus the OFF-contrast.\n\nyi; .\n\nStage 2: 2-D Boundary Detection. At Stage 2, oriented odd-symmetric boundary detection cells are excited by the oriented sampling of the ON and OFF Stage 1\ncells. Responses are maximal when ON activation is strong on one side of a cell\'s\nreceptive field and OFF activation is strong on the opposite side. In other words,\nthe cells are tuned to ON/OFF contrast co-occurrence, or juxtaposition (see Pessoa\net aI., 1995). The output at this stage is the sum of the activations of such cells at\neach location for all orientations. The output responses are sharpened and localized\nthrough lateral inhibition across space; an equation similar to Equation 1 is used .\nThe final output of Stage 2 is given by the signals Zij (see Fig. 2, Boundaries).\nStage 3: Depth Map. In the current implementation a simple scheme was employed for the determination of the depth configuration. Initially, four types of\n\n\x0c847\n\nA Neural Network Model of 3-D Lightness Perception\n\nT-junction cells detect such configurations in the image. For example,\nIij\n\n=\n\nZi-d ,j\n\nx\n\nZi+d ,j\n\nx\n\nZi ,j+d,\n\n(2)\n\nwhere d is a constant, detects T-junctions, where left , right, and top positions of the\nboundary stage are active; similar cells detect T-junctions of different orientations.\nThe activities of the T-junction cells are then used in conjunction with boundary\nsignals to define complete boundaries. Filling-in within these depth boundaries\nresults in a depth map (see Fig. 2, Depth Map).\n\nStage 4: Depth-modulated Filling-in . In Stage 4, the ON and OFF contrast\nmeasures are allowed to diffuse across space within respective filling-in regions . Diffusion is blocked by boundary activations from Stage 2 (see Grossberg & Todorovic,\n1988, for details). The diffusion process is further modulated by depth information.\nThe depth map provides this information; different activities code different depths .\nIn a full blown implementation of the model, depth information would be obtained\nby the depth segmentation of the image supported by both binocular disparity and\nmonocular depth cues.\nDepth-modulated filling-in is such that boundaries across depths are reduced in\nstrength. This allows a small percentage of the contrast on either side ofthe boundary to leak across it resulting in partial contrast negation, or reduction, at these\nboundaries. ON and OFF filling-in domains are used which receive the corresponding\nON and OFF contrast activities from Stage 1 as inputs (see Fig. 2, Filled-in).\n\n4\n\nSIMULATIONS\n\nThe present model can account for several important phenomena, including 2 - D\neffects of lightness constancy and contrast (see Grossberg and Todorovic, 1988).\nThe simulations that follow address 3 -D lightness effects.\n4.1\n\nBenary Cross\n\nFigure 2 shows the simulation for the Benary Cross . The plotted gray level values\nfor filling-in reflect the activities of the ON filling-in domain minus the OFF domain.\nThe model correctly predicts that the patch on the cross appears lighter than the\npatch on the background. This result is a direct consequence of contrast negation.\nThe depth relationships are such that the patch on the cross is at the same depth as\nthe cross and the patch on the background is at the same depth as the background\n(see Fig. 2, Depth Map) . Therefore, the ratio of the background to the patch on\nthe cross (across a depth boundary) and the ratio of the cross to the patch on\nthe background (also across a depth boundary), are given a smaller weight in the\nlightness computation. Thus, the background will have a stronger effect on the\nappearance of the patch on the background, which will appear darker. At the same\ntime, the cross will have a greater effect on the appearance of the patch on the\ncross , which will appear lighter.\n4.2\n\nWhite\'s lllusion\n\nWhite \'s (1979) illusion (Fig. 3) is such that the gray patches on the black stripes\nappear lighter than the gray patches on the white stripes. This effect is considered\na puzzling violation of simultaneous contrast since the contour length of the gray\npatches is larger for the stripes they do not lie on . Simultaneous contrast would\npredict that the gray patches on the black stripes appear lighter than the ones on\nwhite.\n\n\x0c848\n\nL. PESSOA, W. D. ROSS\n\nI\n\nL~\n\nI\nI\n\n- -- I\n\nStimulus\n\nBoundaries\n\nDepth Map\n\nON-OFF Contrast\n\nFilled-in\n\nFigure 2: Benary Cross. The filled-in values of the gray patch on the cross are higher\nthan the ones for the gray patch on the background. Gray levels code intensity;\ndarker grays code lower values, lighter grays code higher values.\nFigure 3 shows the result of the model for White\'s effect . The T-junction information in the stimulus determines that the gray patches are coplanar with the\npatches they lie on. Therefore, their appearance will be determined in relation to\nthe contrast of their respective backgrounds. This is obtained, again, through contrast modulation, where the contrast of, say, the gray patch on a black stripe is\npreserved, while the contrast of the same patch with the white is partially negated\n(due to the depth arrangement).\n4.3\n\nCoplanar Hypothesis\n\nGilchrist (1977) showed that the perception of lightness is not determined by retinal\nadjacency, and that depth configuration and spatial layout help specify lightness.\nMore specifically, it was proposed that the ratio of coplanar surfaces, not necessarily\nretinally adjacent, determines lightness, the so-called coplanar ratio hypothesis.\nGilchrist was able to convincingly demonstrate this by comparing the perception of\nlightness in two equivalent displays (in terms of luminance values), aside from the\nperceived depth relationships in the displays.\nFigure 4 shows computer simulations of the coplanar ratio effect. The same stimulus\nis given as input in two simulations with different depth specifications. In one\n(Depth Map 1), the depth map specifies that the rightmost patch is at a different\ndepth than the two leftmost patches which are coplanar. In the other (Depth Map\n2), the two rightmost patches are coplanar and at a different depth than the leftmost\npatch. In all, the depth organization alters the lightness of the central region, which\nshould appear darker in the configuration of Depth Map 1 than the one for Depth\nMap 2. For Depth Map 1, since the middle patch is coplanar with a white patch, this\npatch is darkened by simultaneous contrast. For Depth Map 2, the middle patch\nwill be lightened by contrast since it is coplanar with a black patch. It should be\nnoted that the depth maps for the simulations shown in Fig . 4 were given as input.\n\n\x0cA Neural Network Model of 3-D Lightness Perception\n\n-\n\n- --\n\n- 1\n\n-,\n\n849\n\n1\n\nBoundaries\n\nStimulus\n\nON-OFF Contrast\n\nFilled-in\n\nFigure 3: White\'s effect. The filled-in values of the gray patches on the black stripes\nare higher than the ones for the gray patches on white stripes.\nThe current implementation cannot recover depth trough binocular disparity and\nonly employs monocular cues as in the previous simulations.\n\n5\n\nCONCLUSIONS\n\nIn this paper, data from experiments on lightness perception were used to extend\nthe BCSjFCS theory of Grossberg and colleagues to account for several challenging\nphenomena. The model is an initial step towards providing an account that can\ntake into consideration the complex factors involved in 3-D vision - see Grossberg\n(1994) for a comprehensive account of 3-D vision.\nAcknowledgements\nThe authors would like to than Alan Gilchrist and Fred Bonato for their suggestions\nconcerning this work. L. P. was supported in part by Air Force Office of Scientific\nResearch (AFOSR F49620-92-J-0334) and Office of Naval Research (ONR N0001491-J-4100); W. R. was supported in part by HNC SC-94-001.\n\nReference\nArend , L., & Spehar, B. (1993) Lightness, brightness, and brightness contrast : 2.\nReflectance variation. Perception {3 Psychophysics 54 :4576-468.\nCohen, M., & Grossberg, S. (1984) Neural dynamics of brightness perception:\nFeatures, boundaries, diffusion, and resonance. Perception {3 Psychophysics\n36:428-456.\nGerrits, H. & Vendrik, A. (1970) Simultaneous contrast, filling-in process and information processing in man\'s visual system. Experimental Brain Research\n11:411-430.\n\n\x0c850\n\nL. PESSOA, W. D. ROSS\n\nFilled-in 2\n\nStimulus\n\nDepth Map 1\n\nFilled-in 1\n\nFigure 4: Gilchrist\'s coplanarity. The Filled-in values for the middle patch on top\nare higher than on bottom.\nGilchrist, A. (1977) Perceived lightness depends on perceived spatial arrangement.\nScience 195:185-187.\n\nGrossberg, S. (1994) 3-D vision and figure-ground separation by visual cortex. Perception & Psychophysics 55:48-120 .\nGrossberg, S., & Mingolla, E . (1985) Neural dynamics of form perception: Boundary\ncompletion, illusory figures, and neon color spreading. Psychological Review\n92:173-211.\nGrossberg, S., & Todorovic. D. (1988). Neural dynamics of 1-D and 2-D brightness\nperception: A unified model of classical and recent phenomena. Perception &\nPsychophysics 43:241-277 .\n\nLand, E., & McCann, J . (1971). Lightness and retinex theory. Journal of the Optical\nSociety of America 61:1-11.\n\nPessoa, L., Mingolla, E., & Neumann, H. (1995) A contrast- and luminance-driven\nmultiscale network model of brightness perception. Vision Research 35:22012223.\nShapley, R., & Enroth-Cugell, C. (1984) Visual adaptation and retinal gain controls.\nIn N. Osborne and G. Chader (eds.), Progress in Retinal Research, pp. 263346. Oxford: Pergamon Press.\nWallach, H. (1948) Brightness constancy and the nature of achromatic colors. Journal of Experimental Psychology 38: 310-324.\nWhite, M. (1979) A new effect of pattern on perceived lightness. Perception 8:413416 .\nWhittle, P., & Challands, P. (1969) The effect of background luminance on the\nbrightness of flashes . Vision Research 9:1095-1110.\n\n\x0c'
p83139
sg34
S'Stable Dynamic Parameter Adaptation\n\nStefan M. Riiger\nFachbereich Informatik, Technische Universitat Berlin\nSekr. FR 5-9, Franklinstr. 28/29\n10587 Berlin, Germany\nasync~cs. tu-berlin.de\n\nAbstract\nA stability criterion for dynamic parameter adaptation is given. In\nthe case of the learning rate of backpropagation, a class of stable\nalgorithms is presented and studied, including a convergence proof.\n\n1\n\nINTRODUCTION\n\nAll but a few learning algorithms employ one or more parameters that control the\nquality of learning. Backpropagation has its learning rate and momentum parameter; Boltzmann learning uses a simulated annealing schedule; Kohonen learning\na learning rate and a decay parameter; genetic algorithms probabilities, etc. The\ninvestigator always has to set the parameters to specific values when trying to solve\na certain problem. Traditionally, the metaproblem of adjusting the parameters is\nsolved by relying on a set of well-tested values of other problems or an intensive\nsearch for good parameter regions by restarting the experiment with different values. In this situation, a great deal of expertise and/or time for experiment design\nis required (as well as a huge amount of computing time).\n\n1.1\n\nDYNAMIC PARAMETER ADAPTATION\n\nIn order to achieve dynamic parameter adaptation, it is necessary to modify the\nlearning algorithm under consideration: evaluate the performance of the parameters\nin use from time to time, compare them with the performance of nearby values, and\n(if necessary) change the parameter setting on the fly. This requires that there\nexist a measure of the quality of a parameter setting, called performance, with the\nfollowing properties: the performance depends continuously on the parameter set\nunder consideration, and it is possible to evaluate the performance locally, i. e., at\na certain point within an inner loop of the algorithm (as opposed to once only at\nthe end of the algorithm). This is what dynamic parameter adaptation is all about.\n\n\x0c226\n\nS.M.RUOER\n\nDynamic parameter adaptation has several virtues. It is automatic; and there is no\nneed for an extra schedule to find what parameters suit the problem best. When\nthe notion of what the good values of a parameter set are changes during learning,\ndynamic parameter adaptation keeps track of these changes.\n\n1.2\n\nEXAMPLE: LEARNING RATE OF BACKPROPAGATION\n\nBackpropagation is an algorithm that implements gradient descent in an error\nfunction E: IRn ~ llt Given WO E IRn and a fixed \'" > 0, the iteration rule is\nW H1 = w t - ",V E(wt). The learning rate", is a local parameter in the sense that\nat different stages of the algorithm different learning rates would be optimal. This\nproperty and the following theorem make", especially interesting.\nTrade-off theorem for backpropagation. Let E: JR1l ~ IR be the error function of\na neural net with a regular minimum at w? E IRn , i. e., E is expansible into a\nTaylor series about w? with vanishing gradient V E( w?) and positive definite Hessian\nmatrix H(w?) . Let A denote the largest eigenvalue of H(w?). Then, in general,\nbackpropagation with a fixed learning rate", > 2/ A cannot converge to w? .\nProof. Let U be an orthogonal matrix that diagonalizes H(w?), i. e., D :=\nUT H (w?) U is diagonal. Using the coordinate transformation x = UT (w - w?)\n\nand Taylor expansion, E(w) - E(w?) can be approximated by F(x) := x T Dx/2.\nSince gradient descent does not refer to the coordinate system, the asymptotic behavior of backpropagation for E near w? is the same as for F near O. In the latter\ncase, backpropagation calculates the weight components x~ = x~(I- Dii",)t at time\nstep t. The diagonal elements Dii are the eigenvalues of H(w?); convergence for all\ngeometric sequences t 1-7 x~ thus requires", < 2/ A.\nI\nThe trade-off theorem states that, given "\', a large class of minima cannot be found,\nnamely, those whose largest eigenvalue of the corresponding Hessian matrix is larger\nthan 2/",. Fewer minima might be overlooked by using a smaller "\', but then the\nalgorithm becomes intolerably slow. Dynamic learning-rate adaptation is urgently\nneeded for backpropagation!\n\n2\n\nSTABLE DYNAMIC PARAMETER ADAPTATION\n\nTransforming the equation for gradient descent, wt+l = w t - ",VE(wt), into a\ndifferential equation, one arrives at awt fat = -",V E(wt). Gradient descent with\nconstant step size", can then be viewed as Euler\'s method for solving the differential\nequation. One serious drawback of Euler\'s method is that it is unstable: each finite\nstep leaves the trajectory of a solution without trying to get back to it. Virtually\nany other differential-equation solver surpasses Euler\'s method, and there are even\nsome featuring dynamic parameter adaptation [5].\nHowever, in the context of function minimization, this notion of stability ("do not\ndrift away too far from a trajectory") would appear to be too strong. Indeed,\ndifferential-equation solvers put much effort into a good estimation of points that\nare as close as possible to the trajectory under consideration. What is really needed\nfor minimization is asymptotic stability: ensuring that the performance of the parameter set does not decrease at the end of learning. This weaker stability criterion\nallows for greedy steps in the initial phase of learning.\nThere are several successful examples of dynamic learning-rate adaptation for backpropagation: Newton and quasi-Newton methods [2] as an adaptive ",-tensor; individual learning rates for the weights [3, 8]; conjugate gradient as a one-dimensional\n",-estimation [4]; or straightforward ",-adaptation [1, 7].\n\n\x0cStable Dynamic Parameter Adaptation\n\n227\n\nA particularly good example of dynamic parameter adaptation was proposed by\nSalomon [6, 7]: let ( > 1; at every step t of the backpropagation algorithm test two\nvalues for 17, a somewhat smaller one, 17d(, and a somewhat larger one, 17t(; use as\n17HI the value with the better performance, i. e., the smaller error:\n\nThe setting of the new parameter (proves to be uncritical (all values work, especially\nsensible ones being those between 1.2 and 2.1). This method outperforms many\nother gradient-based algorithms, but it is nonetheless unstable.\n\nb)\nFigure 1: Unstable Parameter Adaptation\nThe problem arises from a rapidly changing length and direction of the gradient,\nwhich can result in a huge leap away from a minimum, although the latter may have\nbeen almost reached. Figure 1a shows the niveau lines of a simple quadratic error\nfunction E: 1R2 -+ IR along with the weight vectors wo, WI , . .. (bold dots) resulting\nfrom the above algorithm. This effect was probably the reason why Salomon suggested using the normalized gradient instead of the gradient, thus getting rid of the\nchanges in the length of the gradient. Although this works much better, Figure 1b\nshows the instability of this algorithm due to the change in the gradient\'s direction.\nThere is enough evidence that these algorithms converge for a purely quadratic\nerror function [6, 7]. Why bother with stability? One would like to prove that an\nalgorithm asymptotically finds the minimum, rather than occasionally leaping far\naway from it and thus leaving the region where the quadratic Hessian term of a\nglobally nonquadratic error function dominates.\n\n3\n\nA CLASS OF STABLE ALGORITHMS\n\nIn this section, a class of algorithms is derived from the above ones by adding\nstability. This class provides not only a proof of asymptotic convergence, but also\na significant improvement in speed.\nLet E: IRn -+ IR be an error function of a neural net with random weight vector\nW O E IRn. Let ( > 1, 170 > 0, 0 < c ~ 1, and 0 < a ~ 1 ~ b. At step t of the algorithm, choose a vector gt restricted only by the conditions gtV E(wt)/Igtllv Ew t I ~ c\nand that it either holds for all t that 1/1gtl E [a, b) or that it holds for all t that\nIVE(wt)I/lgtl E [a, b), i. e., the vectors g have a minimal positive projection onto\nthe gradient and either have a uniformly bounded length or are uniformly bounded\nby the length of the gradient. Note that this is always possible by choosing gt as the\ngradient or the normalized gradient.\nLet e: 17 t-t E (wt - 17gt) denote a one-dimensional error function given by E, w t and\ngt. Repeat (until the gradient vanishes or an upper limit of t or a lower limit Emin\n\n\x0cS.M.ROOER\n\n228\n\nof E is reached) the iteration\n\'T/* ..\'T/Hl\n\n=\n\'T/d(\n\'T/t(\n\nW H1\n\n= w t - \'T/tHg t with\n\n\'T/t(/2\n1 + e(\'T/t() - e(O)\n\'T/t(gt\\1 E(wt)\n\nif e(O) < e(\'T/t()\n(1)\n\nif e(\'T/d() ::; e(\'T/t() ::; e(O)\notherwise.\n\nThe first case for \'T/Hl is a stabilizing term \'T/*, which definitely decreases the error\nwhen the error surface is quadratic, i. e., near a minimum. \'T/* is put into effect\nwhen the errOr e(T}t() , which would occur in the next step if\'T/t+l = \'T/t( was chosen,\nexceeds the error e(O) produced by the present weight vector w t . By construction,\n\'T/* results in a value less than \'T/t(/2 if e(\'T/t() > e(O); hence, given ( < 2, the learning\nrate is decreased as expected, no matter what E looks like. Typically, (if the values\nfor ( are not extremely high) the other two cases apply, where \'T/t( and \'T/d ( compete\nfor a lower error.\nNote that, instead of gradient descent, this class of algorithms proposes a "gt descent," and the vectors gt may differ from the gradient. A particular algorithm is\ngiven by a specification of how to choose gt.\n\n4\n\nPROOF OF ASYMPTOTIC CONVERGENCE\n\nAsymptotic convergence. Let E: w f-t 2:~=1 AiW; /2 with Ai > O. For all ( > 1,\n1, 0 < a ::; 1 ::; b, \'T/o > 0, and WO E IRn , every algorithm from Section :1\nproduces a sequence t f-t wt that converges to the minimum 0 of E with an at least\nexponential decay of t f-t E(wt).\n\no < c ::;\n\nProof. This statement follows if a constant q < 1 exists with E(W H1 ) ::; qE(wt) for\nall t. Then, limt~oo w t = 0, since w f-t ..jE(w) is a norm in IRn.\nFix a w t , \'T/t, and a gt according to the premise. Since E is a positive definite\nquadratic form, e: \'T/ f-t E( wt - \'T/g t ) is a one-dimensional quadratic function with\na minimum at, say, \'T/*. Note that e(O) = E(wt) and e(\'T/tH) = E(wt+l). e is\ncompletely determined by e(O), e\'(O) = -gt\\1 E(wt), \'T/te and e(\'T/t(). Omitting the\nalgebra, it follows that \'T/* can be identified with the stabilizing term of (1).\n\ne(O)\n.A\'-~--I\n-...-...J\'----+I\n\ne"----r-++--+j\n\nqe( 0)\n(1 - q11)e(0) + q11e(\'T/*)\nqee(O)\n\n__~<-+--+I 11t+~:11? e(O)\n\ne(\'T/*)\n\n1--_ _ _ _---""\'......-- --A~-_+_--+t\n\n+ (1 -\n\n11t?~:11? )e(\'T/*)\n\ne(\'T/tH)\n\no\nFigure 2: Steps in Estimating a Bound q for the Improvement of E.\n\n\x0c229\n\nStable Dynamic Parameter Adaptation\n\nIf e(17t() > e(O), by (1) 17t+l will be set to 17?; hence, Wt+l has the smallest possible\nerror e(17?) along the line given by l. Otherwise, the three values 0, 17t!(, and 17t(\ncannot have the same error e, as e is quadratic; e(17t() or e(17t!() must be less than\ne(O), and the argument with the better performance is used as 17tH\' The sequence\nt I-t E(wt) is strictly decreasing; hence, a q ~ 1 exists. The rest of the proof shows\nthe existence of a q < 1.\n\nAssume there are two constants 0\n\n< qe, qT/ < 1 with\nE\n~\n\nLet 17tH\n\n~\n\n(2)\n(3)\n\n[qT/,2 - qT/]\nqee(O).\n\n17?; using first the convexity of e, then (2), and (3), one obtains\ne(17tH -17? 2 ?\n17.\n17\n\n+ (1- 17t+l -17?)\n17.\n\n17\n\n.)\n\n< 17t+l -17? e(O) + (1- 17tH -17? )e(17.)\n<\n<\n\n17?\n(1 - qT/)e(O) + qf/e(17?)\n(1- qT/(1 - qe))e(O).\n\n17?\n\nFigure 2 shows how the estimations work. The symmetric case 0\nthe same result E(wt+l) ~ qE(wt) with q := 1 - qT/(1 - qe) < 1.\n\n< 17tH\n\n~\n\n17? has\n\nLet ,X < := minPi} and ,X> := max{\'xi}. A straightforward estimation for qe yields\n,X<\n\nqe := 1 - c2 ,X> < 1.\nNote that 17? depends on w t and gt. A careful analysis of the recursive dependence\nof 17t+l /17? (w t , gt) on 17t /17?( wt - 1 ,l-l) uncovers an estimation\n\n( <)\n\n._ min _2_ ~ ca ~\nqT/ .{(2 + l\' (2 + 1 b\'x>\n\n5\n\n3/2\n\n<\n\n17o (,X\n, bmax{1, J2\'x> E(WO)}}\n\n>0\n\n.\n\n?\n\nNON-GRADIENT DIRECTIONS CAN IMPROVE\nCONVERGENCE\n\nIt is well known that the sign-changed gradient of a function is not necessarily the\nbest direction to look for a minimum. The momentum term of a modified backpropagation version uses old gradient directions; Newton or quasi-Newton methods\nexplicitly or implicitly exploit second-order derivatives for a change of direction;\nanother choice of direction is given by conjugate gradient methods [5].\nThe algorithms from Section 3 allow almost any direction, as long as it is not nearly\nperpendicular to the gradient. Since they estimate a good step size, these algorithms\ncan be regarded as a sort of "trial-and-error" line search without bothering to find\nan exact minimum in the given direction, but utilizing any progress made so far.\nOne could incorporate the Polak-Ribiere rule, ctt H\ngate directions with dO = \\1 E (WO), a = 1, and\n(3\n\n=\n\n= \\1 E(Wt+l) + a(3ctt, for\n\n(\\1E(Wt+l) - \\1E(wt))\\1E(wt+l)\n(\\1 E(Wt))2\n\nconju-\n\n\x0cS.M. RUOER\n\n230\n\nto propose vectors gt := ett /Iettl for an explicit algorithm from Section 3. As in\nthe conjugate gradient method, one should reset the direction ett after each n (the\nnumber of weights) updates to the gradient direction. Another reason for resetting\nthe direction arises when gt does not have the minimal positive projection c onto\nthe normalized gradient.\n\na = 0 sets the descent direction gt to the normalized gradient "V E(wt)/I"V E(wt)lj\nthis algorithm proves to exhibit a behavior very similar to Salomon\'s algorithm with\nnormalized gradients. The difference lies in the occurrence of some stabilization\nsteps from time to time, which, in general, improve the convergence.\nSince comparisons of Salomon\'s algorithm to many other methods have been published [7], this paper confines itself to show that significant improvements are\nbrought about by non-gradient directions, e. g., by Polak-Ribiere directions (a = 1).\nTable 1: Average Learning Time for Some Problems\n\nPROBLEM\n\nEmin\n\na = 0\n\na = 1\n\n(a) 3-2-4 regression\n(b) 3-2-4 approximation\n(c) Pure square (n = 76)\n(d) Power 1.8 (n = 76)\n(e) Power 3.8 (n = 76)\n(f) 8-3-8 encoder\n\n10?\n10- 4\n10- 16\n10- 4\n10- 16\n10- 4\n\n195? 95%\n1070 ? 140%\n464? 17%\n486? 29%\n28 ? 10%\n1380? 60%\n\n58 ? 70%\n189? 115%\n118? 9%\n84? 23%\n37? 14%\n300? 60%\n\nTable 1 shows the average number of epochs of two algorithms for some problems.\nThe average was taken over many initial random weight vectors and over values of\n( E [1.7,2.1]j the root mean square error of the averaging process is shown as a\npercentage. Note that, owing to the two test steps for ",t/( and "\'t(, one epoch has\nan overhead of around 50% compared to a corresponding epoch of backpropagation.\na f:. 0 helps: it could be chosen by dynamic parameter adaptation.\nProblems (a) and (b) represent the approximation of a function known only from\nsome example data. A neural net with 3 input, 2 hidden, and 4 output nodes was\nused to generate the example dataj artificial noise was added for problem (a). The\nsame net with random initial weights was then used to learn an approximation.\nThese problems for feedforward nets are expected to have regular minima.\nProblem (c) uses a pure square error function E: w rt L:~1 ilwil P /2 with p = 2\nand n = 76. Note that conjugate gradient needs exactly n epochs to arrive at the\nminimum [5]. However, the few additional epochs that are needed by the a = 1\nalgorithm to reach a fairly small error (here 118 as opposed to 76) must be compared\nto the overhead of conjugate gradient (one line search per epoch).\nPowers other than 2, as used in (d) or (e), work well as long as, say, p > 1.5. A power\n< 1 will (if n ~ 2) produce a "trap" for the weight vector at a location near a\ncoordinate axis, where, owing to an infinite gradient component, no gradient-based\nalgorithm can escape1 . Problems are expected even for p near 1: the algorithms of\nSection 3 exploit the fact that the gradient vanishes at a minimum, which in turn\nis numerically questionable for a power like 1.1. Typical minima, however, employ\npowers 2,4, ... Even better convergence is expected and found for large powers.\n\np\n\nIDynamic parameter adaptation as in (1) can cope with the square-root singularity\n(p = 1/2) in one dimension, because the adaptation rule allows a fast enough decay of\nthe learning rate; the ability to minimize this one-dimensional square-root singularity is\nsomewhat overemphasized in [7].\n\n\x0cStable Dynamic Parameter Adaptation\n\n231\n\nThe 8-3-8 encoder (f) was studied, because the error function has global minima\nat the boundary of the domain (one or more weights with infinite length). These\nminima, though not covered in Section 4, are quickly found. Indeed, the ability\nto increase the learning rate geometrically helps these algorithms to approach the\nboundary in a few steps.\n\n6\n\nCONCLUSIONS\n\nIt has been shown that implementing asymptotic stability does help in the case of the\nbackpropagation learning rate: the theoretical analysis has been simplified, and the\nspeed of convergence has been improved. Moreover, the presented framework allows\ndescent directions to be chosen flexibly, e. g., by the Polak-Ribiere rule. Future work\nincludes studies of how to apply the stability criterion to other parametric learning\nproblems.\n\nReferences\n[1] R. Battiti. Accelerated backpropagation learning: Two optimization methods.\nComplex Systems, 3:331-342, 1989.\n[2] S. Becker and Y. Ie Cun. Improving the convergence of back-propagation learning with second order methods. In D. Touretzky, G. Hinton, and T. Sejnowski,\neditors, Proceedings of the 1988 Connectionist Models Summer School, pages\n29-37. Morgan Kaufmann, San Mateo, 1989.\n[3] R. Jacobs. Increased rates of convergence through learning rate adaptation.\nNeural Networks, 1:295-307, 1988.\n[4] A. Kramer and A. Sangiovanni-Vincentelli. Efficient parallel learning algorithms\nfor neural networks. In D. Touretzky, editor, Advances in Neural Information\nProcessing Systems 1, pages 40-48. Morgan Kaufmann, San Mateo, 1989.\n[5] W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. Numerical\nRecipes in C. Cambridge University Press, 1988.\n[6] R. Salomon. Verbesserung konnektionistischer Lernverfahren, die nach der Gradientenmethode arbeiten. PhD thesis, TU Berlin, October 1991.\n[7] R. Salomon and J. L. van Hemmen. Accelerating backpropagation through\ndynamic self-adaptation. Neural Networks, 1996 (in press).\n[8] F. M. Silva and L. B. Almeida. Speeding up backpropagation. In Proceedings of\nNSMS - International Symposium on Neural Networks for Sensory and Motor\nSystems, Amsterdam, 1990. Elsevier.\n\n\x0c'
p83140
sg36
S'Statistical Theory of Overtraining - Is\nCross-Validation Asymptotically\nEffective?\ns. Amari, N. Murata, K.-R. Miiller*\nDept. of Math. Engineering and Inf. Physics, University of Tokyo\nHongo 7-3-1, Bunkyo-ku, Tokyo 113, Japan\nM. Finke\nInst. f. Logik , University of Karlsruhe\n76128 Karlsruhe, Germany\n\nH. Yang\nLab . f. Inf. Representation, RIKEN,\nWakoshi, Saitama, 351-01, Japan\n\nAbstract\nA statistical theory for overtraining is proposed. The analysis\ntreats realizable stochastic neural networks, trained with KullbackLeibler loss in the asymptotic case. It is shown that the asymptotic\ngain in the generalization error is small if we perform early stopping, even if we have access to the optimal stopping time. Considering cross-validation stopping we answer the question: In what ratio\nthe examples should be divided into training and testing sets in order to obtain the optimum performance. In the non-asymptotic\nregion cross-validated early stopping always decreases the generalization error. Our large scale simulations done on a CM5 are in\nnice agreement with our analytical findings.\n\n1\n\nIntroduction\n\nTraining multilayer neural feed-forward networks, there is a folklore that the generalization error decreases in an early period of training, reaches the minimum and\nthen increases as training goes on, while the training error monotonically decreases.\nTherefore, it is considered advantageous to stop training at an adequate time or to\nuse regularizers (Hecht-Nielsen [1989), Hassoun [1995), Wang et al. [1994)\' Poggio\nand Girosi [1990), Moody [1992)\' LeCun et al. [1990] and others). To avoid overtraining, the following stopping rule has been proposed based on cross-validation:\n*Permanent address: GMD FIRST, Rudower Chaussee 5, 12489 Berlin, Germany.\nE-mail: Klaus@first .gmd.de\n\n\x0cStatistical Theory of Overtraining-Is Cross-Validation Asymptotically Effective?\n\n177\n\nDivide all the available examples into two disjoint sets. One set is used for training. The other set is used for testing such that the behavior of the trained network\nis evaluated by using the test examples and training is stopped at the point that\nminimizes the testing error.\nThe present paper gives a mathematical analysis of the so-called overtraining phenomena to elucidate the folklore. We analyze the asymptotic case where the number\nt of examples are very large. Our analysis treats 1) a realizable stochastic machine,\n2) Kullback-Leibler loss (negative ofthe log likelihood loss), 3) asymptotic behavior\nwhere the number t of examples is sufficiently large (compared with the number m\nof parameters). We firstly show that asymptotically the gain of the generalization\nerror is small even if we could find the optimal stopping time. We then answer the\nquestion: In what ratio, the examples should be divided into training and testing\nsets in order to obtain the optimum performance. We give a definite answer to this\nproblem. When the number m of network parameters is large, the best strategy is\nto use almost all t examples in the training set and to use only l/v2m examples\nin the testing set, e.g. when m = 100, this means that only 7% of the training\npatterns are to be used in the set determining the point for early stopping.\nOur analytic results were confirmed by large-scale computer simulations of threelayer continuous feedforward networks where the number m of modifiable parameters are m = 100. When t > 30m, the theory fits well with simulations, showing\ncross-validation is not necessary, because the generalization error becomes worse\nby using test examples to obtain an adaequate stopping time. For an intermediate\nrange, where t < 30m overtraining occurs surely and the cross-validation stopping\nimproves the generalization ability strongly.\n\n2\n\nStochastic feedforward networks\n\nLet us consider a stochastic network which receives input vector x and emits\noutput vector y. The network includes a modifiable vector parameter w =\n(WI,"\', w m ) and is denoted by N(w). The input-output relation of the network N(w) is specified by the conditional probability p(Ylx; w). We assume (a)\nthat there exists a teacher network N(wo) which generates training examples\nfor the student N(w). And (b) that the Fisher information matrix Gij(w) =\nE\n\n[a~. logp(x, y; w) a~j logp(x, y; w)] exists, is non-degenerate and is smooth in\n\nw, where E denotes the expectation with respect to p(x, Y; w) = q(x)p(Ylx; w).\nThe training set Dt = {(Xl, YI), ... , (Xt, Yt)} consists of t independent examples\ngenerated by the distribution p(x, Y; wo) of N(wo). The maximum likelihood estimator (m.l.e.) Vi is the one that maximizes the likelihood of producing D t , or\nequivalently minimizes the training error or empirical risk function\n1\n\nRtrain(w) =\n\nt\n\n-i I:logp(xi,Yi;w).\n\n(2.1)\n\ni=l\n\nThe generalization error or risk function R(w) of network N(w) is the expectation\nwith respect to the true distribution,\n\nR(w)\n\n= -Eo[logp(x, Y; w)] = Ho+D(wo II w) = Ho+Eo\n\n[log p~x, Y; wojJ, (2.2)\np x,y;w\n\nwhere Eo denotes the expectation with respect to p(x, Y; wo), Ho is the entropy\nof the teacher network and D(wo II w) is the Kullback-Leibler divergence from\nprobability distribution p(x,y;wo) to p(x,y;w) or the divergence of N(w) from\nN(wo). Hence, minimizing R(w) is equivalent to minimizing D(wo II w), and the\n\n\x0cS. AMARI, N. MURATA, K. R. MULLER, M. FINKE, H. YANG\n\n178\n\nminimum is attained at w = Wo. The asymptotic theory of statistics proves that the\nm.l.e. Wt is asymptotically subject to the normal distribution with mean Wo and\nvariance G-1 It, where G- 1 is the inverse of the Fisher information matrix G. We\ncan expand for example the risk R(w) = Ho+ t(w -wo)TG(wo)(w -wo) + 0 (/2)\nto obtain\n(Rgen(w)) = Ho\n\n+ ~ +0\n\nC~\n\n),\n\n(Rtrain(w)) = Ho -\n\n~ + 0 C~),\n\n(2.3)\n\nas asymptotic result for training and test error (see Murata et al. [1993] and Amari\nand Murata [1990)) . An extension of (2.3) including higher order corrections was\nrecently obtained by Mliller et al. [1995].\nLet us consider the gradient descent learning rule (Amari [1967], Rumelhart et al.\n[1986], and many others), where the parameter w(n) at the nth step is modified by\nw(n\n\n+ 1) =\n\nw(n) _\n\n?\n\nf)Rtr~~(wn) ,\n\n(2.4)\n\nand where ? is a small positive constant. This is batch learning where all the\ntraining examples are used for each iteration of modifying w( n).l The batch process\nis deterministic and w( n) converges to W, provided the initial w(O) is included in\nits basin of attraction. For large n we can argue, that w(n) is approaching w\nisotropically and the learning trajectory follows a linear ray towards w (for details\nsee Amari et al. [1995]).\n\n3\n\nVirtual optimal stopping rule\n\nDuring learning as the parameter w(n) approaches W, the generalization behavior\nof network N {w(n)} is evalulated by the sequence R(n) = R{w(n)}, n = 1,2, . ..\nThe folklore says that R(n) decreases in an early period oflearning but it increases\nlater. Therefore, there exists an optimal stopping time n at which R(n) is minimized. The stopping time nopt is a random variable depending on wand the initial\nw(O) . We now evaluate the ensemble average of (R(nopd).\nThe true Wo and the m.l.e. ware in general different, and they are apart of order\n1/Vt. Let us compose a sphere S of which the center is at (1/2)(wo+w) and which\npasses through both Wo and W, as shown in Fig.1b. Its diameter is denoted by d,\nwhere d2 = Iw - Wo 12 and\nEo [d 2 ]\n\nEo[(w - wo? G- 1(w - wo)] =\n\n~tr(G-1G) =\nt\n\nm.\n\nt\n\n(3 .1)\n\nLet A be the ray, that is the trajectory w(n) starting at w(O) which is not in the\nneighborhood of Wo . The optimal stopping point w" that minimizes\nR(n)\n\n= Ho + ~Iw(n) -\n\nwol 2\n\n(3.2)\n\nis given by the first intersection of the ray A and the sphere S.\nSince w" is the point on A such that Wo - w" is orthogonal to A, it lies on the\nsphere S (Fig.1b). When ray A\' is approaching w from the opposite side ofwo (the\nright-hand side in the figure), the first intersection point is w itself. In this case,\nthe optimal stopping never occurs until it converges to W.\nLet () be the angle between the ray A and the diameter Wo - w of the sphere S.\nWe now calculate the distribution of () when the rays are isotropically distributed.\nlWe can alternatively use on-line learning, studied by Amari [1967], Heskes and Kappen\n[1991] , and recently by Barkai et al. [1994] and SolI a and Saard [1995].\n\n\x0cStatistical Theory of Overtraining-Is Cross-Validation Asymptotically Effective?\n\n179\n\nLemma 1. When ray A is approaching V. from the side in which Wo is included, the\nprobability density of 0, 0 :::; 0 :::; 7r /2, is given by\n\nreO)\n\n= -1- sinm- 2 0,\n\nwhere\n\n1m\n\n= 17r/2 sinm OdO.\n\n(3.3)\n\n0\n\n1m-2\n\nThe det,ailed proof of this lemma can be found in Amari et aI. [1995]. Using the\ndensity of 0 given by Eq.(3.3) and we arrive at the following theorem.\nTheorem 1. The average generalization error at the optimal stopping point is\ngiven by\n\n(3.4)\n\nProof When ray A is at angle 0, 0 :::; 0 < 7r /2, the optimal stopping point w* is on\nthe sphere S. It is easily shown that Iw* - wol = dsinO. This is the case where A\nis from the same side as Wo (from the left-hand side in Fig.l b), which occurs with\nprobability 0.5, and the average of (d sin 0)2 is\nEo[(dsinO?]\n\nEo[d 2 ] r/\\in 2 Osinm- 2 OdO\n\nJo\n\n= m ~ = m (1- ~).\n\nt 1m-2\nt\nm\nWhen 0 is 7r/2 :::; 0 :::; 7r, that is A approaches V. from the opposite side, it does\nnot stop until it reaches V., so that Iw* - Wo 12 = IV. - Wo I = d 2 ? This occurs with\nprobability 0.5. Hence, we proved the theorem.\n1m - 2\n\nThe theorem shows that, if we could know the optimal stopping time nopt for\neach trajectory, the generalization error decreases by 1/2t, which has an effect of\ndecreasing the effective dimensions by 1/2. This effect is neglegible when m is large.\nThe optimal stopping time is of the order logt. However, it is impossible to know\nthe optimal stopping time. If we stop learning at an estimated optimal time nopt,\nwe have a small gain when the ray A is from the same side as Wo but we have\nsome loss when ray A is from the opposite direction. This shows that the gain is\neven smaller if we use a common stopping time iiopt independent of V. and w(O) as\nproposed by Wang et aI. [1994]. However, the point is that there is neither direct\nmeans to estimate nopt nor iiopt rather than for example cross-validation. Hence,\nwe analyze cross-validation stopping in the following .\n\n4\n\nOptimal stopping by cross-validation\n\nThe present section studies asymptotically two fundamental problems: 1) Given t\nexamples , how many examples should be used in the training set and how many\nin the testing set? 2) How much gain can one expect by the above cross-validated\nstopping?\nLet us divide t examples into rt examples of the training set and r\'t examples of the\ntesting set, where r + r\' = 1. Let V. be the m.I.e. from rt training examples, and let\nw be the m .I.e. from the other r\'t testing examples. Since the training examples\nand testing examples are independent, V. and ware subject to independent normal distributions with mean Wo and covariance matrices G-1/(rt) and G-l/(r\'t),\nrespecti vely.\nLet us compose the triangle with vertices Wo, V. and w. The trajectory A starting\nat w(O) enters V. linearly in the neighborhood . The point w" on the trajectory A\nwhich minimizes the testing error is the point on A that is closest to W, since the\ntesting error defined by\n1\nRtest(w) = r\'t ~{-logp(xi\'Yi; w)},\n(4.1)\nt\n\n\x0cS. AMARI, N. MURATA, K. R. MULLER, M. FINKE, H. YANG\n\n180\n\nwhere summation is taken over r\'t testing examples, can be expanded as\n\nRtest(w) == Ho -\n\n~Iw - wol 2 + ~Iw - w1 2 .\n\n(4.2)\n\nLet S be the sphere centered at (w + w)/2 and passing through both wand w.\nIt \'s diameter is given by d == Iw - wi. Then, the optimal stopping point w* is\ngiven by the intersection of the trajectory A and sphere S . When the trajectory\ncomes from the opposite side of W, it does not intersect S until it converges to w,\nso that the optimal point is w* == w in this case. Omitting the detailed proof, the\ngeneralization error of w* is given by Eq.(??) , so that we calculate the expectation\nE[lw*\n\n-woI 2 ] == m _ ~ (~_~).\ntr\n\n2t\n\n1"\n\nl\'\n\nLemma 2. The average generalization error by the optimal cross-validated stopping\nIS\n\n*\n\n(R(w ,1\')) = Ho\n\n+\n\n2m - 1\n\n4rt\n\n1\n\n+ 4r\'t\n\n(4.3)\n\nWe can then calculate the optimal division rate\n\nJ2m -1-1\nropt = 1 -\n\n2(m _ 1)\n\nand\n\n1\nropt = 1 - J2m\n\n(large m limit).\n\n(4.4)\n\nof examples, which minimizes the generalization error. So for large m only\n(1/J2m) x 100% of examples should be used for testing and all others for training.\nFor example, when m = 100 , this shows that 93% of examples are to be used for\ntraining and only 7% are to be kept for testing. From Eq.( 4.4) we obtain as optimal\ngeneralization error for large m\n\n(R(w\', ropt? = Ho\n\n+; (1 +If) .\n\n( 4.5)\n\nThis shows that the generalization error asymptotically increases slightly by crossvalidation compared with non-stopped learning which is using all the examples for\ntraining.\n\n5\n\nSimulations\n\nWe use standard feed-forward classifier networks with N inputs, H sigmoid hidden\nunits and M softmax outputs (classes). The output activity 0/ of the lth output\nunit is calculated via the softmax squashing function\n_\n.\n_\n_\np(y?-GI!x,w)-O/-l\n\nexp(h/)\n\n+ 2: k exp (h k )\'\n\n/=l ,?? ?,M,\n\nwg\n\nwhere h? = Lj\nSj - \'19? is the local field potential. Each output 0/ codes the aposteriori probability of being in class G/, 0 0 denotes a zero class for normalization\npurposes. The m network parameters consist of biases \'19 and weights w . When x\nis input, the activity of the j-th hidden unit is\nN\n\nSj\n\n= [1\n\n+ exp( -\n\nL\n\nWf{:Xk -\n\n\'I9.f)]-I ,\n\nj = 1, .. " H .\n\nk=1\n\nThe input layer is connected to the hidden layer via w H , the hidden layer is connected to the output layer via wo, but no short-cut connections are present . Although the network is completely deterministic, it is constructed to approximate\n\n\x0cStatistical Theory of Overtraining-Is Cross-Validation Asymptotically Effective?\n\n181\n\nclass conditional probabilities (Finke and Miiller [1994]) .\nThe examples {(x}, yd, .. " (Xt , Yt)} are produced randomly, by drawing Xi, i =\n1, .. . , t, from a uniform distribution independently and producing the labels Yi\nstochastically from the t eacher classifier. Conjugate gradient learning with linesearch on the empirical risk function Eq.(2.1) is applied, starting from some random initial vector . The generalization ability is measured using Eq. (2.2) on a large\ntest set (50000 patterns). Note that we use Eq. (2.1) on the cross-validation set ,\nbecause only the empirical risk is available on the cross-validation set in a practical\nsituation. We compare the generalisation error for the settings: exhaustive training\n(no stopping), early stopping (controlled by the cross-validation set) and optimal\nstopping (controlled by the large testset) . The simulations were performed on a\nparallel computer (CM5). Every curve in the figures takes about 8h of computing\ntime on a 128 respectively 256 partition of the CM5, i.e. we perform 128-256 parallel trials . This setting enabled us to do extensive statistics (cf. Amari et al. [1995]) .\nFig. la shows the results of simulations, where N = 8, H = 8, M = 4, so that\nthe number m of modifiable parameters is m = (N + I)H + (H + I)M = 108. We\nobserve clearly, that saturated learning without early stopping is the best in the\nasymptotic range of t > 30m , a range which is due to the limited size of the data\nsets often unaccessible in practical applications . Cross-validated early stopping does\nnot improve the generalization error here, so that no overtraining is observed on\nthe average in this range. In the asymptotic area (figure 1) we observe that the\nsmaller the percentage of the training set , which is used to determine the point of\nearly stopping, the better the performance of the generalization ability. When we\nuse cross-validation, the optimal size of the test set is about 7% of all the examples ,\nas the theory predicts.\nClearly, early stopping does improve the generalization ability to a large extent in\nan intermediate range for t < 30m (see Miiller et al. [1995]) . Note , that our theory also gives a good estimate of the optimal size of the early stopping set in this\nintermediate range.\n0.05\n\n"\n\nopt, 4 -\n\n20.% -+---\n\n0.045\n\n,3:l% ..\n/ 42% ...... .. n9,gtopping -.. >E} ...\n\n0.04\n\n;\'\n\n/ "". >:i<:;_=-~;:~~::::~::~~~:------\n\n0.035\n\n\'i\n\nCit\n\n0.03\n\n.~~;;?;;;;?/\n\n0.025\n0.02\n0.015\n\nom\n0.005\n\nA\n\n~.\n\n~A.~~.::.../\n" ,\n\nL....I..._--\'-_-\'-_-\'--_-\'------\'_--\'-_--\'-_-\'-----l\n\n5e-5\n\nle-4 1.5e-4 2e-4 2.5e-4 3e-4 3.5e-4 4e-4 4.5e-4 5e-4\nlIt\n\n(a)\n\n(b)\n\nFigure 1: (a) R(w) plotted as a function of lit for different sizes r\' of the early\nstopping set for an 8-8-4 classifier network. opt. denotes the use of a very large\ncross-validation set (50000) and no stopping adresses the case where 100% of the\ntraining set is used for exhaustive learning. (b) Geometrical picture to determine\nthe optimal stopping point w* .\n\n\x0cs. AMARI. N. MURATA. K. R. MOLLER. M. FINKE. H. YANG\n\n182\n\n6\n\nConclusion\n\nWe proposed an asymptotic theory for overtraining. The analysis treats realizable\nstochastic neural networks, trained with Kullback-Leibler loss.\nIt is demonstrated both theoretically and in simulations that asymptotically the gain\nin the generalization error is small if we perform early stopping, even if we have\naccess to the optimal stopping time. For cross-validation stopping we showed for\nlarge m that optimally only r~pt = 1/ J2m examples should be used to determine\nthe point of early stopping in order to obtain the best performance. For example,\nif m = 100 this corresponds to using 93% of the t training patterns for training and\nonly 7% for testing where to stop. Yet, even if we use rapt for cross-validated stopping the generalization error is always increased comparing to exhaustive training.\nNevertheless note, that this range is due to the limited size of the data sets often\nunaccessible in practical applications.\nIn the non-asymptotic region simulations show that cross-validated early stopping\nalways helps to enhance the performance since it decreases the generalization error.\nIn this intermediate range our theory also gives a good estimate of the optimal size\nof the early stopping set. In future we will consider higher order correction terms\nto extend our theory to give also a quantitative description of the non-asymptotic\nregIOn.\nAcknowledgements: We would like to thank Y. LeCun, S. Bos and K Schulten\nfor valuable discussions. K -R. M. thanks K Schulten for warm hospitality during\nhis stay at the Beckman Inst. in Urbana, Illinois. We acknowledge computing time\non the CM5 in Urbana (NCSA) and in Bonn, supported by the National Institutes\nof Health (P41RRO 5969) and the EC S & T fellowship (FTJ3-004, K . -R. M.).\nReferences\nAmari , S. [1967], IEEE Trans., EC-16, 299- 307.\nAmari, S., Murata, N. [1993], Neural Computation 5, 140\nAmari, S., Murata, N., Muller, K-R., Finke, M., Yang, H. [1995], Statistical Theory\nof Overtraining and Overfitting, Univ. of Tokyo Tech. Report 95-06, submitted\nBarkai, N. and Seung, H. S. and Sompolinski, H. [1994], On-line learning of dichotomies, NIPS\'94\nFinke, M. and Muller, K-R. [1994] in Proc. of the 1993 Connectionist Models summer school, Mozer, M., Smolensky, P., Touretzky, D.S ., Elman, J.L. and Weigend,\nA.S. (Eds.), Hillsdale, NJ: Erlenbaum Associates, 324\nHassoun, M. H. [1995], Fundamentals of Artificial Neural Networks, MIT Press.\nHecht-Nielsen, R. [1989], Neurocomputing, Addison-Wesley.\nHeskes, T. and Kappen, B. [1991]\' Physical Review, A44, 2718- 2762.\nLeCun, Y., Denker, J .S., Solla, S. [1990], Optimal brain damage, NIPS\'89\nMoody, J . E. [1992]\' The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems, NIPS 4\nMurata, N., Yoshizawa, S., Amari , S. [1994], IEEE Trans ., NN5, 865-872.\nMuller, K-R., Finke, M., Murata, N., Schulten, K and Amari, S. [1995] A numerical study on learning curves in stochastic multilayer feed-forward networks, Univ.\nof Tokyo Tech. Report METR 95-03 and Neural Computation in Press\nPoggio, T. and Girosi, F. [1990], Science, 247, 978- 982.\nRissanen, J. [1986], Ann, Statist., 14, 1080- 1100.\nRumelhart, D., Hinton, G. E., Williams, R. J. [1986], in PDP, Vol.1, MIT Press.\nSaad, D., Solla, S. A. [1995], PRL, 74,4337 and Phys. Rev. E, 52,4225\nWang, Ch., Venkatesh, S. S., Judd, J. S. [1994], Optimal stopping and effective machine complexity in learning, to appear, (revised and extended version of NIPS\'93).\n\n\x0c'
p83141
sg460
S'Learning Fine Motion by Markov\nMixtures of Experts\n\nMarina Meilii\nDept. of Elec. Eng . and Computer Sci.\nMassachussetts Inst . of Technology\nCambridge, MA 02139\nmmp@ai .mit.edu\n\nMichael I. J Ol\'dan\nDept.of Brain and Cognitive Sciences\nMassachussetts Inst. of Technology\nCambridge, MA 02139\njordan@psyche.mit .edu\n\nAbstract\nCompliant control is a standard method for performing fine manipulation tasks, like grasping and assembly, but it requires estimation\nof the state of contact (s.o.c.) between the robot arm and the objects involved. Here we present a method to learn a model of the\nmovement from measured data. The method requires little or no\nprior knowledge and the resulting model explicitly estimates the\ns.o.c. The current s.o.c. is viewed as the hidden state variable of\na discrete HMM. The control dependent transition probabilities\nbetween states are modeled as parametrized functions of the measurement. We show that their parameters can be estimated from\nmeasurements at the same time as the parameters of the movement\nin each s.o.c. The learning algorithm is a variant of the EM procedure. The E step is computed exactly ; solving the M step exactly\nis not possible in general. Here, gradient ascent is used to produce\nan increase in likelihood .\n\n1\n\nINTRODUCTION\n\nFor a large class of robotics tasks , such as assembly tasks or manipulation of relatively light-weight objects, under appropriate damping of the manipulator the\ndynamics of the objects can be neglected . For these tasks the main difficulty is in\nhaving the robot achieve its goal despite uncertainty in its position relative to the\nsurrounding objects. Uncertainty is due to inaccurate knowledge of the geometric\nshapes and positions of the objects, of their physical properties (surface friction\ncoefficients), or to positioning errors in the manipulator. The standard solution\nto this problem is controlled compliance first introduced in (Mason, 1981). Under\ncompliant motion , the task is performed in stages; in each stage the robot arm\n\n\x0cM. MElLA, M. I. JORDAN\n\n1004\n\nmaintains contact with a selected surface or feature of the environment; the stage\nends when contact with the feature corresponding to the next stage is made.\nDecomposing the given task into subtasks and specifying each goal or subgoal in\nterms of contact constraints has proven to be a particularly fertile idea, from which\na fair number of approaches have evolved. But each of them have to face and solve\nthe problem of estimating the state of contact (i .e. checking if the contact with\nthe correct surface is achieved) , a direct consequence of dealing with noisy measurements . Additionally, most approaches assume prior geometrical and physical\nknowledge of the environment .\nIn this paper we present a method to learn a model of the environment which will\nserve to estimate the s.o.c. and to predict future positions from noisy measurements.\nIt associates to each state of contact the coresponding movement model (m.m.); that\nis: a relationship between positions, nominal and actual velocities that holds over a\ndomain of the position-nominal velocity space. The current m.m. is viewed as the\nhidden state variable of a discrete Hidden Markov Model (HMM) with transition\nprobabilities that are parametrized functions of the measurement. We call this\nmodel Markov Mixture of Experts (MME) and show how its parameters can be\nestimated. In section 2 the problem is defined, section 3 introduces the learning\nalgorithm, section 4 presents a simulated example and 5 discusses other aspects\nrelevant to the implementation.\n\n2\n\nREACHABILITY GRAPHS AND MARKOV\nMIXTURES OF EXPERTS\n\nFor any ensemble of objects, the space of all the relative degrees of freedom of the\nobjects in the ensemble is called the configuration space (C-space). Every possible configuration of the ensemble is represented by a unique point in the C-space\nand movement in the real space maps into continuous trajectories in the C-space\n(Lozano-Perez, 1983). The sets of points corresponding to each state of contact\ncreate a partition over the C-space. Because trajectories are continuous, a point\ncan move from a s.o.c. only to a neighboring s.o.c. This can be depicted by a directed graph with vertices representing states of contact and arcs for the possible\ntransitions between them , called the reach ability graph . If no constraints on the\nvelocities are imposed, then in the reachability graph each s.o.c. is connected to all\nits neighbours. But if the range of velocities is restricted, the connectivity of the\ngraph decreases and the connections are generally non-symmetric. Figure 1 shows\nan example of a C-space and its reachability graph for velocities with only positive\ncomponents.\nIdeally, in the absence of noise, the states of contact can be perfectly observed\nand every transition through the graph is thus deterministic. To deal with the\nuncertainty in the measurements, we will attach probabilities to the arcs of the graph\nin the following way: Let us denote by Qi the set of configurations corresponding\nto s.o.c. i and let the movement of a point x with uniform nominal velocity v for a\ntime aT be given by x( t + aT) =\n(x, v, aT); both x and v are vectors of same\ndimension as the C-space. Now, let x\', v\' be the noisy measurements of the true\nvalues x, v, x E Qj and P[x, vlx\', v\',j] the posterior distribution of (x , v) given the\nmeasurements and the s.o.c. Then, the probability of transition to a state i from a\ngiven state j in time T3 can be expressed as:\n\nr\n\nP[ilx\',v\',j] =\n\nr P[x,vlx\',v\',j]dxdv =\n\naij(x\',V\')\n\n(1)\n\nJ{x ,vIXEQj ,rex ,v ,T.)EQ.}\n\nDefining the transition probability matrix A = [aji]rj=l and assuming measurement\n\n\x0cLearning Fine Motion by Markov Mixtures of Experts\n\n1005\n\ny\n\nx\n\n(a)\n\n(b)\n\nFigure 1: A configuration space (a) and its reachability graph (b). The nodes\nrepresent movement models: C is the free space, A and B are surfaces with static\nand dynamic friction, G represents jamming in the corner. The velocity V has\npositive components.\nnoise P[x\'lq = i, x E Qd leads to an HMM with output x having a continuous\nemission probability distribution and where the s.o.c. plays the role of a hidden\nstate variable. Our main goal is to estimate this model from observed data.\nTo give a general statement of the problem we will assume that all the position,\nvelocity and force measurements are represented by the input vector u; the output\nvector y of dimensionality ny contains the future position (which our model will\nlearn to predict). Observations are made at moments which are integer multiples\nof T$\' indexed by t = 0,1, .. , T. If T$ is a constant sampling time the dependency of\nthe transition probability on Ts can be ignored. For the purpose of the parameter\nestimation, the possible dependence between u(t) and yet + 1) will also be ignored,\nbut it should be considered when the trained model is used for prediction.\nThroughout the following section we will also assume that the input-output dependence is described by a Gaussian conditional density p(y(t)lu(t), q(t) = k) with\nmean f(u(t),(h:) and variance E = (1\'21. This is equivalent to assuming that given\nthe S.O.c . all noise is additive Gaussian output noise, which is obviously an approximation. But this approximation will allow us to derive certain quantities in closed\nform in an effective way.\nThe function feu, (he) is the m.m. associated with state of contact k (with Ok its\nparameter vector) and q is the selector variable representing it . Sometimes we will\nfind it useful to partition the domain of a m.m. into subdomains and to represent\nit by a different function (i .e. a different set of parameters Ok) on each of the\nsubdomains; then, the name movement model will be extended to them.\nThe evolution of q is controlled by a Markov chain which depends on u and of a set\nof parameters W:\n\naij(u(t), W)\nwith\n\n= Pr[q(t + 1) = ilq(t) = j, u(t)]\n\nL aij(u, W) = 1\n\nt\n\n= 0, 1, ...\n\n\\:Iu, W, j = 1, . .. , m.\n\n(2)\n\n\x0cM. MElLA, M. I. JORDAN\n\n1006\n\ny\n\nu\n\n!\n\n,,------.1&\n\'d\n\nq\n1-------\'\n\nt .......................... .................. ............. _????????????? ~ . . .............................. _...................... .\n\nFigure 2: The Markov Mixture of Experts architecture\nFig. 2 depicts this architecture. It can be easily seen that this model generalizes the\nmixture of experts (ME) architecture (Jacobs, et al., 1991), to which it reduces in\nthe case where aij are independent of j (the columns of A are all equal). It becomes\nthe model of (Bengio and Frasconi, 1995) when A and f are neural networks.\n\n3\n\nAN EM ALGORITHM FOR MME\n\nTo estimate the values of the unknown parameters (J"2, Wk, Ok, k = 1, ... ,m given\nthe sequence of observations {(u(t), y(t))};=o, T> 0 the Expectation Maximization\n(EM) algorithm will be used. The states {q(t)};=o play the role of the unobserved\nvariables. More about EM can be found in (Dempster et al., 1977) while aspects\nspecific to this algorithm are in (Meila and Jordan, 1994).\nThe E step computes the probability of each state and of every transition to occur\nat t E {O, ... , T} given the observations and an initial parameter set. This can be\ndone efficiently by the forward-backward algorithm (Rabiner and Juang, 1986).\n\nI {(u(t), y(t))};=o, W, 0, (J"2]\nPr[q(t) = j, q(t + 1) = i I {(u(t), y(t))};=o , W,\nPr[q(t) = k\n\n(3)\n0,\n\n(J"2]\n\nIn the M step the new estimates of the parameters are found by maximizing the\naverage complete log-likelihood J, which in our case has the form\nT-l\n\nJ(O, (J"2, W)\n\n=\n\nm\n\nL L eij(t) lnaij(u(t), W)t=o i,j=l\n\nSince each parameter appears in only one term of J the maximization is equivalent\nto:\nT\n\n0l:ew = argmin\nIh\n\nL \'n(t) lIy(t) t=o\n\nf( u(t), Ok)11 2\n\n(5)\n\n\x0cLearning Fine Motion by Markov Mixtures of Experts\n\n1007\n\nT-l\n\nw new\n\n= argmax L L~ij(t) In (aij(u(t), w))\nW\nt=o ij\n\n(6)\n\n1\nny(T + 1)\n\n(7)\n\nT\n\nm\n\n~ ~ \'\'}\'k(t) Ily(t) - I(u(t), Ok )11 2\n\nThere is no general closed form solution to (5) and (6). Their difficulty depends on\nthe form of I and aij. The complexity of the m.m . is determined by the geometrical\nshape of the objects\' surfaces. For planar surfaces and no rotational degrees of\nfreedom I is linear in Ok. Then, (5) becomes a weighted least squares problem\nwhich can be solved in closed form.\nThe functions in A depend both on the movement and of the noise models. Because\nthe noise is propagated through non-linearities to the output, an exact form as in\n(1) may be hard to compute analytically. Moreover, a correct noise model for\neach of the possible uncertainties is rarely available (Eberman, 1995). A common\npractical approach is to trade accuracy for computability and to parametrize A in\na form which is easy to update but deprived of physical meaning. In all the cases\nwhere maximization cannot be performed exactly, one can resort to Generalized\nEM by merely increasing J. In particular, gradient ascent in parameter space is\na technique which can replace maximization. This modification will not affect the\noverall convergence of the EM iteration but can significantly reduce its speed.\nBecause EM only finds local maxima of the likelihood, the initialization is important.\nIf I( u, Ok) correspond to physical movement models , good initial estimates for their\nparameters can be available . The same applies to those components of W which\nbear physical significance. A complementary approach is to reduce the number of\nparameters by explicitly setting the probabilities of impossible transitions to O.\n\nSIMULATION RESULTS\n\n4\n\nSimulations have been run on the C-space shown in fig . 1. The inputs were the\n4-dimensional vectors of position (x, y) and nominal velocity (Vx , Vy); the output\nwas the predicted position. The coordinate range was [0, 10] and the admissible\nvelocities were confined to the upper right quadrant (Vmax 2: Vx , Vy 2: Vmin > 0).\nThe restriction in direction implied that the trajectories remain in the coordinate\ndomain; it also appeared in the topology of the reachability graph, which has no\ntransition to the free space from another state.\nThis model was implemented by a MME. The m.m. are linear in the parameters,\ncorresponding to the piecewise linearity of the true model. To implement the transition matrix A we used a bank of gating net-works, one for each s.o.c., consisting\nof 2 layer perceptrons with softmax 1 output. There are 230 free parameters in the\ngating networks and 64 in the m.m.\nThe training set included N = 5000 data points, in sequences of length T ~ 6, all\nstarting in free space. The starting position of the sequence and the nominal velocities at each step were picked randomly. We found that a more uniform distribution\nof the data points over the states of contact is necessary for successful learning.\nSince this is not expected to happen in applications (where, e.g., sticking occurs\nless often than sliding) , the obtained models were tested also on a distribution that\n1\n\n()\n\nThe softmax function is given by: softmax. x\n\nvectors of the same dimension.\n\n= Zexp(WTx)\n,i = 1, .. m\njexp(W x)\n!\n\nT\nj\n\nwith Wj , x\n\n\x0cM. MElLA, M. I. JORDAN\n\n1008\n\nTable 1: Performance of MME versus ME\nError (MSE) 1/2\nUmform V distribution\n.1\n0\n.2\n.3\n.4\n.023\n.11\n.219 .327 .437\n.010 .109 .218 .327 .435\n.044 .129 .247 .367 .488\n.034 .126 .245 .366 .488\n\nTest set\nnoise level\nMME,(1\' =.2\nMME,(1\' =0\nME, (1\' = .2\nME, (1\' =0\n\n(a) Model Prediction Standard\nTrammg distributIon\n.1\n.2\n.4\n.3\n0\n.024 .113 .222 .332 .443\n.003 .114 .228 .343 .456\n.052 .133\n.25\n.37\n.493\n.047 .131\n.49\n.25\n.37\n\nTest set\nnoise level\nMME, (1\' =.2\nMME, (1\' =0\nME, (1\' =.2\nME, (1\' =0\n\n(b) State Misclassification\nTrammg distribution\n.1\n.~\n.3\n0\n.4\n5.15 5.2\n5.5\n5.9\n6.4\n1.40 2.35 3.25 4.13\n.78\n6.46 6.60 7.18 7.73 8.13\n6.25 6.45 6.98 7.61 8.15\n\nError [%]\nUmform\n.1\nU\n3.45 3.5\n1.19\n.89\n3.85 3.90\n3.84 3.98\n\nV distribution\n.~\n.3\n.4\n3.8\n4.2\n4.6\n1.70 2.30 2.88\n4.38 4.99 5.65\n4.53 5.05 5.70\n\nwas uniform over velocities (and consequently, highly non-uniform over states of\ncontact). Gaussian noise with (1\'=0.2 or 0 was added to the (x, y) training data.\nIn the M step, the parameters of the gating networks were updated by gradient\nascent. For the m .m.least squares estimation was used. To ensure that models and\ngates are correctly coupled, initial values for () are chosen around the true values.\nAs discussed in the previous section, this is not an unrealistic assumption . W was\ninitialized with small random values. Each simulation was run until convergence.\nWe used two criteria to measure the performance of the learning algorithm: square\nroot of prediction MSE and hidden state misdassificaton. The results are summarized in table 1. The test set size is 50,000 in all cases. Input noise is Gaussian with\nlevels between 0 and 0.4. Comparisons were made with a ME model with the same\nnumber of states.\nThe simulations show that the MME architecture is tolerant to input noise, although\nit is not taking it into account explicitly. The MME consistently outperforms the\nME model in both prediction and state estimation accuracy.\n\n5\n\nDISCUSSION\n\nAn algorithm to estimate the parameters of composite movement models in the\npresence of noisy measurements has been presented. The algorithm exploits the\nphysical decomposability of the problem and the temporal relationship between the\ndata points to produce estimates of both the model\'s parameters and the s.o.c. It\nrequires only imprecise initial knowledge about the geometry and physical properties\nof the system.\n\nPrediction via MME The trained model can be used either as an estimator for\nthe state of contact or as a forward model in predicting the next position. For\nthe former goal the forward part of the forward-backward algorithm can be used\nto implement a recursive estimator or the methods in (Eberman, 1995) can be\nused. The obtained \'Yk(t) , combined with the outputs of the movement models, will\nproduce a predicted output y. An improved posterior estimate of y can be obtained\n\n\x0cLearning Fine Motion by Markov Mixtures of Experts\n\n1009\n\nby combining f) with the current measurement.\nScaling issues. Simulations have shown that relatively large datasets are required\nfor training even for a small number of states. But, since the states represent\nphysical entities, the model will inherit the geometrical locality properties thereof.\nThus, the number of possible transitions from a state will be bounded by a small\nconstant when the number of states grows, keeping the data complexity linear in\nm.\nAs a version of EM, our algorithm is batch. It follows that parameters are not\nadapted on line. In particular, the discretization time T& must be fixed prior to\ntraining. But small changes in Ts can be accounted for by rescaling the velocities\nV. For the other changes, inasmuch as they are local, relearning can be confined to\nthose components of the architecture which are affected.\n\nReferences\nBengio, Y. and Frasconi, P . (1995). An input output HMM architecture. In G.\nTesauro, D. Touretzky, & T. Leen (Eds.), Neural Information Processing Sys.\ntems 7, Cambridge, MA: MIT Press, pp. 427-435.\nDempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood from\nincomplete data via the EM algorithm. Journal of the Royal Statistical Society,\nB, 39:1- 38.\nEberman, B. S. (1995). A sequential decision approach to sensing manipulation\ncontact features. PhD thesis, M.I.T., Dept. of Electrical Engineering.\nJacobs, R. A., Jordan, M. 1., Nowlan, S., & Hinton, G. E. (1991). Adaptive mixtures\nof local experts. Neural Computation, 3, 1-12.\nLozano-Perez, T. (1983). Spatial planning: a configuration space approach. IEEE\nTransactions on Computers.\nMason, M. T. (1981). Compliance and force control for computer controlled manipulation. IEEE Trans. on Systems, Man and Cybernetics.\nMeila, M. and Jordan, M. 1. (1994). Learning the parameters of HMMs with auxilliary input. Technical Report 9401, MIT Computational Cognitive Science,\nCambridge, MA.\nRabiner, R. L. and Juang, B. H. (1986). An introduction to hidden Markov models.\nASSP Magazine, 3(1):4-16.\n\n\x0c'
p83142
sg68
S'Universal Approximation and Learning\nof Trajectories Using Oscillators\n\nPierre Baldi*\nDivision of Biology\nCalifornia Institute of Technology\nPasadena, CA 91125\npfbaldi@juliet.caltech.edu\n\nKurt Hornik\nTechnische Universitat Wien\nWiedner Hauptstra8e 8-10/1071\nA-1040 Wien, Austria\nKurt.Hornik@tuwien.ac.at\n\nAbstract\nNatural and artificial neural circuits must be capable of traversing specific state space trajectories. A natural approach to this\nproblem is to learn the relevant trajectories from examples. Unfortunately, gradient descent learning of complex trajectories in\namorphous networks is unsuccessful. We suggest a possible approach where trajectories are realized by combining simple oscillators, in various modular ways. We contrast two regimes of fast\nand slow oscillations. In all cases, we show that banks of oscillators\nwith bounded frequencies have universal approximation properties.\nOpen questions are also discussed briefly.\n\n1\n\nINTRODUCTION: TRAJECTORY LEARNING\n\nThe design of artificial neural systems, in robotics applications and others, often\nleads to the problem of constructing a recurrent neural network capable of producing\na particular trajectory, in the state space of its visible units. Throughout evolution,\nbiological neural systems, such as central pattern generators, have also been faced\nwith similar challenges. A natural approach to tackle this problem is to try to\n"learn" the desired trajectory, for instance through a process of trial and error\nand subsequent optimization. Unfortunately, gradient descent learning of complex\ntrajectories in amorphous networks is unsuccessful. Here, we suggest a possible\napproach where trajectories are realized, in a modular and hierarchical fashion, by\ncombining simple oscillators. In particular, we show that banks of oscillators have\nuniversal approximation properties.\n\n* Also with the Jet Propulsion Laboratory, California Institute of Technology.\n\n\x0cP. BALDI, K. HORNIK\n\n452\n\nTo begin with, we can restrict ourselves to the simple case of a network with one!\nvisible linear unit and consider the problem of adjusting the network parameters\nin a way that the output unit activity u(t) is equal to a target function I(t), over\nan interval of time [0, T]. The hidden units of the network may be non-linear and\nsatisfy, for instance, one of the usual neural network charging equations such as\ndUi\ndt\n\n~\n= - UiTi + L..JjWij/jUj(t\n-\n\nTij),\n\n(1)\n\nwhere Ti is the time constant of the unit, the Tij represent interaction delays, and\nthe functions Ij are non-linear input/output functions, sigmoidal or other. In the\nnext section, we briefly review three possible approaches for solving this problem,\nand some of their limitations. In particular, we suggest that complex trajectories\ncan be synthesized by proper combination of simple oscillatory components.\n\n2\n2.1\n\nTHREE DIFFERENT APPROACHES TO TRAJECTORY LEARNING\nGRADIENT DESCENT APPROACHES\n\nOne obvious approach is to use a form of gradient descent for recurrent networks\n(see [2] for a review), such as back-propagation through time, in order to modify any adjustable parameters of the networks (time constants, delays, synaptic\nweights and/or gains) to reduce a certain error measure, constructed by comparing\nthe output u(t) with its target I(t). While conceptually simple, gradient descent\napplied to amorphous networks is not a successful approach, except on the most\nsimple trajectories. Although intuitively clear, the exact reasons for this are not\nentirely understood, and overlap in part with the problems that can be encountered\nwith gradient descent in simple feed-forward networks on regression or classification\ntasks.\nThere is an additional set of difficulties with gradient descent learning offixed points\nor trajectories, that is specific to recurrent networks, and that has to do with the\nbifurcations of the system being considered. In the case of a recurrent 2 network, as\nthe parameters are varied, the system mayor may not undergo a series of bifurcations, i.e., of abrupt changes in the structure of its trajectories and, in particular, of\nits at tractors (fixed points, limit cycles, ... ). This in turn may translate into abrupt\ndiscontinuities, oscillations or non-convergence in the corresponding learning curve.\nAt each bifurcation, the error function is usually discontinuous, and therefore the\ngradient is not defined. Learning can be disrupted in two ways: when unwanted\nabrupt changes occur in the flow of the dynamical system, or when desirable bifurcations are prevented from occurring. A classical example of the second type is the\ncase of a neural network with very small initial weights being trained to oscillate,\nin a symmetric and stable fashion, around the origin. With small initial weights,\nthe network in general converges to its unique fixed point at the origin, with a large\nerror. If we slightly perturb the weights, remaining away from any bifurcation, the\nnetwork continues to converge to its unique fixed point which now may be slightly\ndisplaced from the origin, and yield an even greater error, so that learning by gradient descent becomes impossible (the starting configuration of zero weights is a local\nminimum of the error function).\n1 All the results to be derived can be extended immediately to the case of higherdimensional trajectories.\n2In a feed-forward network, where the transfer functions of the units are continuous, the\noutput is a continuous function of the parameters and therefore there are no bifurcations.\n\n\x0cUniversal Approximation and Learning of Trajectories Using Oscillators\n\n453\n\n8\no\nFigure 1: A schematic representation of a 3 layer oscillator network for double figure\neight. Oscillators with period T in a given layer gate the corresponding oscillators,\nwith period T /2, in the previous layer.\n\n2.2\n\nDYNAMICAL SYSTEM APPROACH\n\nIn the dynamical system approach, the function /(t) is approximated in time, over\n[0, T] by a sequence of points Yo, Yl, .... These points are associated with the iterates\nof a dynamical system, i.e., Yn+l\nF(Yn)\nFn(yo), for some function F. Thus\nthe network implementation requires mainly a feed-forward circuit that computes\nthe function F. It has a simple overall recursive structure where, at time n, the\noutput F(Yn) is calculated, and fed back into the input for the next iteration.\nWhile this approach is entirely general, it leaves open the problem of constructing\nthe function F. Of course, F can be learned from examples in a usual feed-forward\nconnectionist network. But, as usual, the complexity and architecture of such a\nnetwork are difficult to determine in general. Another interesting issue in trajectory\nlearning is how time is represented in the network, and whether some sort of clock is\nneeded. Although occasionally in the literature certain authors have advocated the\nintroduction of an input unit whose output is the time t, this explicit representation\nis clearly not a suitable representation, since the problem of trajectory learning\nreduces then entirely to a regression problem. The dynamical system approach\nrelies on one basic clock to calculate F and recycle it to the input layer. In the\nnext approach, an implicit representation of time is provided by the periods of the\noscillators.\n\n=\n\n2.3\n\n=\n\nOSCILLATOR APPROACH\n\nA different approach was suggested in [1] where, loosely speaking, complex trajectories are realized using weakly pre-structured networks, consisting of shallow\nhierarchical combinations of simple oscillatory modules. The oscillatory modules\ncan consist, for instance, of simple oscillator rings of units satisfying Eq. 1, with\ntwo or three high-gain neurons, and an odd number of inhibitory connections ([3]).\nTo fix the ideas, consider the typical test problem of constructing a network capable\nof producing a trajectory associated with a double figure eight curve (i.e., a set\nof four loops joined at one point), see Fig. 1. In this example, the first level of\nthe hierarchy could contain four oscillator rings, one for each loop of the target\ntrajectory. The parameters in each one of these four modules can be adjusted, for\ninstance by gradient descent, to match each of the loops in the target trajectory.\n\n\x0c454\n\nP. BALDI, K. HORNIK\n\nThe second level of the pyramid should contain two control modules. Each of these\nmodules controls a distinct pair of oscillator networks from the first level, so that\neach control network in the second level ends up producing a simple figure eight .\nAgain, the control networks in level two can be oscillator rings and their parameters\ncan be adjusted . In particular, after the learning process is completed, they should\nbe operating in their high-gain regimes and have a period equal to the sum of the\nperiods of the circuits each one controls.\nFinally, the third layer consists of another oscillatory and adjustable module which\ncontrols the two modules in the second level, so as to produce a double figure\neight. The third layer module must also end up operating in its high-gain regime\nwith a period equal to four times the period of the oscillators in the first layer.\nIn general, the final output trajectory is also a limit cycle because it is obtained\nby superposition of limit cycles in the various modules. If the various oscillators\nrelax to their limit cycles independently of one another, it is essential to provide\nfor adjustable delays between the various modules in order to get the proper phase\nadjustments. In this way, a sparse network with 20 units or so can be constructed\nthat can successfully execute a double figure eight.\nThere are actually different possible neural network realizations depending on how\nthe action of the control modules is implemented. For instance, if the control units\nare gating the connections between corresponding layers, this amounts to using\nhigher order units in the network. If one high-gain oscillatory unit, with activity\nc(t) always close to 0 or 1, gates the oscillatory activities of two units Ul(t) and\nU2(t) in the previous layer, then the overall output can be written as\nout(t) = C(t)Ul (t)\n\n+ (1 - C(t))U2(t) .\n\n(2)\n\nThe number of layers in the network then becomes a function of the order of the\nunits one is willing to use. This approach could also be described in terms of a\ndynamic mixture of experts architecture, in its high gain regime. Alternatively,\none could assume the existence of a fast weight dynamics on certain connections\ngoverned by a corresponding set of differential equations. Although we believe that\noscillators with limit cycles present several attractive properties (stability, short\ntransients, biological relevance, . . . ), one can conceivably use completely different\ncircuits as building blocks in each module.\n\n3\n\nGENERALIZATION AND UNIVERSAL APPROXIMATION\n\nWe have just described an approach that combines a modular hierarchical architecture, together with some simple form of learning, enabling the synthesis of a neural\ncircuit suitable for the production of a double figure eight trajectory. It is clear that\nthe same approach can be extended to triple figure eight or, for that matter, to any\ntrajectory curve consisting of an arbitrary number of simple loops with a common\nperiod and one common point. In fact it can be extended to any arbitrary trajectory. To see this, we can subdivide the time interval [0, T] into n equal intervals of\nduration f = Tin . Given a certain level of required precision, we can always find n\noscillator networks with period T (or a fraction of T) and visible trajectory Ui(t),\nsuch that for each i, the i-th portion of the trajectory u(t) with if ~ t ~ (i + l)f\ncan be well approximated by a portion of Ui(t) , the trajectory of the i-th oscillator.\nThe target trajectory can then be approximated as\n\n(3)\n\n\x0cUniversal Approximation and Learning of Trajectories Using Oscillators\n\n455\n\nAs usual, the control coefficient Cj(t) must have also period T and be equal to 1\nfor i{ :5 t :5 (i + 1){, and 0 otherwise. The control can be realized with one large\nhigh-gain oscillator, or as in the case described above, by a hierarchy of control\noscillators arranged, for instance, as a binary tree of depth m if n = 2m , with the\ncorresponding multiple frequencies.\nWe can now turn to a slightly different oscillator approach, where trajectories are to\nbe approximated with linear combinations of oscillators, with constant coefficients.\nWhat we would like to show again is that oscillators are universal approximators\nfor trajectories. In a sense, this is already a well-known result of Fourier theory\nsince, for instance, any reasonable function f with period T can be expanded in the\nform 3\n\nA.k\n\n= kiT.\n\n(4)\n\nFor sufficiently smooth target functions, without high frequencies in their spectrum,\nit is well known that the series in Eq. 4 can be truncated. Notice, however, that both\nEqs. 3 and 4 require having component oscillators with relatively high frequencies,\ncompared to the final trajectory. This is not implausible in biological motor control,\nwhere trajectories have typical time scales of a fraction of a second, and single\ncontrol neurons operate in the millisecond range. A rather different situation arises\nif the component oscillators are "slow" with respect to the final product.\nThe Fourier representation requires in principle oscillations with arbitrarily large\nfrequencies (0, liT, 2IT, .. . , niT, .. .). Most likely, relatively small variations in the\nparameters (for instance gains, delays andlor synaptic weights) of an oscillator\ncircuit can only lead to relatively small but continuous variations of the overall\nfrequency. For instance, in [3] it is shown that the period T of an oscillator ring\nwith n units obeying Eq. 1 must satisfy\n\nThus, we need to show that a decomposition similar in flavor to Eq. 4 is possible,\nbut using oscillators with frequencies in a bounded interval. Notice that by varying\nthe parameters of a basic oscillator, any frequency in the allowable frequency range\ncan be realized, see [3]. Such a linear combination is slightly different in spirit from\nEq. 2, since the coefficients are independent of time, and can be seen as a soft\nmixture of experts. We have the following result.\nTheorem 1 Let a < b be two arbitrary real numbers and let f be a continuous\nfunction on [0, T]. Then for any error level { > 0, there exist n and a function 9n\nof the form\n\nsuch that the uniform distance\n\nIlf -\n\n9n 1100 is less than {.\n\nIn fact, it is not even necessary to vary the frequencies A. over a continuous band\n[a, b]. We have the following.\nTheorem 2 Let {A.k} be an infinite sequence with a finite accumulation point, and\nlet f be a continuous function on [0,7]. Then for any error level { > 0, there exist\nn and a function 9n(t) 2:~=10:\'ke27rjAkt such that Ilf - 9nll00 < {.\n\n=\n\n3In what follows, we use the complex form for notational convenience.\n\n\x0cP. BALDI, K. HORNIK\n\n456\n\nThus, we may even fix the oscillator frequencies as e.g. Ak = l/k without losing\nuniversal approximation capabilities. Similar statements can be made about meansquare approximation or, more generally, approximation in p-norm LP(Il), where\n1 ~ p < 00 and Il is a finite measure on [0, T]:\nTheorem 3 For all p and f in LP(Il) and for all { > 0, we can always find nand\ngn as above such that Ilf - gn IILP{Jl) < {.\nThe proof of these results is surprisingly simple. Following the proofs in [4], if one\nof the above statements was not true, there would exist a nonzero, signed finite\nmeasure (T with support in [0, T] such that hO,T] e21fi >.t d(T(t) = for all "allowed"\n\n?\n\nfrequencies A. Now the function z t-+ !rO,T] e21fizt d(T(t) is clearly analytic on the\nwhole complex plane. Hence, by a well-known result from complex variables, if it\nvanishes along an infinite sequence with a finite accumulation point, it is identically\nzero. But then in particular the Fourier transform of (T vanishes, which in turn\nimplies that (T is identically zero by the uniqueness theorem on Fourier transforms,\ncontradicting the initial assumption.\nNotice that the above results do not imply that f can exactly be represented as\ne.g. f(t) = e 21fi >.t dV(A) for some signed finite measure v-such functions are not\nonly band-limited, but also extremely smooth (they have an analytic extension to\nthe whole complex plane).\n\nf:\n\nHence, one might even conjecture that the above approximations are rather poor\nin the sense that unrealistically many terms are needed for the approximation.\nHowever, this is not true-one can easily show that the rates of approximation\ncannot be worse that those for approximation with polynomials. Let us briefly sketch\nthe argument, because it also shows how bounded-frequency oscillators could be\nconstructed.\nFollowing an idea essentially due to Stinchcombe & White [5], let, more generally,\n9 be an analytic function in a neighborhood of the real line for which no derivative\nvanishes at the origin (above, we had g(t) = e 21fit ). Pick a nonnegative integer n\nand a polynomial p of degree not greater than n - 1 arbitrarily. Let us show that\nfor any { > 0, we can always find a gn of the form gn(t)\nE~=l Cl\'kg(Akt) with Ak\narbitrarily small such that lip - gn 1100 < {. To do so, note that we can write\n\n=\n\np(t) =\n\nL\n\nn- l\n\n1=0\n\nis,t \' ,\n\nwhere rn(At) is of the order of An, as A -t 0, uniformly for t in [0, T] . Hence,\n\nL:=l Cl\'kg(Ak t )\n\nL:=l Cl\'k\n=\n\n(L~=-ol fil (At)l + rn (At))\n\nL~=~l (L:: 1 Cl\'kAi) filt l + L:=l Cl\'krn (Akt).\n\nNow fix n distinct numbers el, ... ,en , let Ak = Ak(p) = pek, and choose the Cl\'k =\nCl\'k(p) such that E;=lCl\'k(p)Ak(p)\' = iSl/fil for I = 0, ... , n - 1. (This is possible\nbecause, by assumption, all fil are non-zero.) It is readily seen that Cl\'k (p) is of\nthe order of pl-n as p -t (in fact, the j-th row of the inverse of the coefficient\nmatrix of the linear system is given by the coefficients of the polynomial nktj (A Ak)/(Aj -Ak)). Hence, as p -t 0, the remainder term EZ=lCl\'k(p)rn(Ak(p)t) is ofthe\norder of p, and thus E~=lCl\'k(p)g(Adp)t) -t E~=-oliS,t\' = p(t) uniformly on [0, T].\n\n?\n\nNote that using the above method, the coefficients in the approximation grow quite\nrapidly when the approximation error tends to 0. In some sense, this was to be\n\n\x0cUniversal Approximation and Learning of Trajectories Using Oscillators\n\n457\n\nexpected from the observation that the classes of small-band-limited functions are\nrather "small". There is a fundamental tradeoff between the size of the frequencies,\nand the size of the mixing coefficients. How exactly the coefficients scale with the\nwidth of the allowed frequency band is currently being investigated.\n\n4\n\nCONCLUSION\n\nThe modular oscillator approach leads to trajectory architectures which are more\nstructured than fully interconnected networks, with a general feed-forward flow of\ninformation and sparse recurrent connections to achieve dynamical effects. The\nsparsity of units and connections are attractive features for hardware design; and\nso is also the modular organization and the fact that learning is much more circumscribed than in fully interconnected systems. We have shown in different ways\nthat such architectures have universal approximation properties. In these architectures, however, some form of learning remains essential, for instance to fine tune\neach one of the modules. This, in itself, is a much easier task than the one a fully\ninterconnected and random network would have been faced with. It can be solved\nby gradient or random descent or other methods. Yet, fundamental open problems\nremain in the overall organization of learning across modules, and in the origin of\nthe decomposition. In particular, can the modular architecture be the outcome of a\nsimple internal organizational process rather than an external imposition and how\nshould learning be coordinated in time and across modules (other than the obvious:\nmodules in the first level learn first, modules in the second level second, .. . )? How\nsuccessful is a global gradient descent strategy applied across modules? How can the\nsame modular architecture be used for different trajectories, with short switching\ntimes between trajectories and proper phases along each trajectory?\nAcknowledgments\n\nThe work of PB is in part supported by grants from the ONR and the AFOSR.\nReferences\n\n[1] Pierre Baldi. A modular hierarchical approach to learning. In Proceedings of the\n2nd International Conference on Fuzzy Logic and Neural Networks, volume II,\npages 985-988, IIzuka, Japan, 1992.\n[2] Pierre F. Baldi. Gradient descent learning algorithm overview: a general dynamic systems perspective. IEEE Transactions on Neural Networks, 6(1}:182195, January 1995.\n[3] Pierre F. Baldi and Amir F. Atiya. How delays affect neural dynamics and\nlearning. IEEE Transactions on Neural Networks, 5(4):612-621, July 1994.\n[4] Kurt Hornik. Some new results on neural network approximation. Neural Networks, 6:1069-1072,1993.\n[5] Maxwell B. Stinchcombe and Halbert White. Approximating and learning unknown mappings using multilayer feedforward networks with bounded weights.\nIn International Joint Conference on Neural Networks, volume III, pages 7-16,\nWashington, 1990. Lawrence Earlbaum, Hillsdale.\n\n\x0c'
p83143
sg72
S'A Unified Learning Scheme:\nBayesian-Kullback Ying-Yang Machine\n\nLei Xu\n1. Computer Science Dept., The Chinese University of HK, Hong Kong\n2. National Machine Perception Lab, Peking University, Beijing\n\nAbstract\nA Bayesian-Kullback learning scheme, called Ying-Yang Machine,\nis proposed based on the two complement but equivalent Bayesian\nrepresentations for joint density and their Kullback divergence.\nNot only the scheme unifies existing major supervised and unsupervised learnings, including the classical maximum likelihood or\nleast square learning, the maximum information preservation, the\nEM & em algorithm and information geometry, the recent popular\nHelmholtz machine, as well as other learning methods with new\nvariants and new results; but also the scheme provides a number\nof new learning models.\n\n1\n\nINTRODUCTION\n\nMany different learning models have been developed in the literature. We may\ncome to an age of searching a unified scheme for them. With a unified scheme,\nwe may understand deeply the existing models and their relationships, which may\ncause cross-fertilization on them to obtain new results and variants; We may also be\nguided to develop new learning models, after we get better understanding on which\ncases we have already studied or missed, which deserve to be further explored.\nRecently, a Baysian-Kullback scheme, called the YING-YANG Machine, has been\nproposed as such an effort(Xu, 1995a). It bases on the Kullback divergence and two\ncomplement but equivalent Baysian representations for the joint distribution of the\ninput space and the representation space, instead of merely using Kullback divergence for matching un-structuralized joint densities in information geometry type\nlearnings (Amari, 1995a&b; Byrne, 1992; Csiszar, 1975). The two representations\nconsist of four different components. The different combinations of choices of each\ncomponent lead the YING-YANG Machine into different learning models. Thus,\nit acts as a general learning scheme for unifying the existing major unsupervised\nand supervised learnings. As shown in Xu(1995a), its one special case reduces to\nthe EM algorithm (Dempster et aI, 1977; Hathaway, 1986; Neal & Hinton , 1993)\n\n\x0c445\n\nA Unified Learning Scheme: Bayesian-Kullback Ying-Yang Machine\n\nand the closely related Information Geometry theory and the em algorithm (Amari,\n1995a&b), to MDL autoencoder with a "bits-back" argument by Hinton & Zemel\n(1994) and its alternative equivalent form that minimizes the bits of uncoded residual errors and the unused bits in the transmission channel\'s capacity (Xu, 1995d),\nas well as to Multisets modeling learning (Xu, 1995e)- a unified learning framework\nfor clustering, PCA-type learnings and self-organizing map. It other special case\nreduces to maximum information preservation (Linsker, 1989; Atick & Redlich,\n1990; Bell & Sejnowski, 1995). More interestingly its another special case reduces\nto Helmholtz machine (Dayan et al,1995 ; Hinton, 1995) with new understandings.\nMoreover , the YING-YANG machine includes also maximum likelihood or least\nsquare learning.\nFurthermore, the YING- YANG Machine has also been extended to temporal patterns with a number of new models for signal modeling. Some of them are the\nextensions of Helmholtz machine or maximum information preservation learning to\ntemporal processing. Some of them include and extend the Hidden Markov Model\n(HMM), AMAR and AR models (Xu, 1995b). In addition, it has also been shown in\nXu(1995a&c, 1996a) that one special case of the YING-YANG machine can provide\nus three variants for clustering or VQ, particularly with criteria and an automatic\nprocedure developed for solving how to select the number of clusters in clustering\nanalysis or Gaussian mixtures - a classical problem that remains open for decades .\nIn this paper, we present a deep and systematical further study. Section 2 redescribes the unified scheme on a more precise and systematical basis via discussing\nthe possible marital status of the two Bayesian representations for joint density.\nSection 3 summarizes and explains those existing models under the unified scheme,\nparticularly we have clarified some confusion made in the previous papers (Xu,\n1995a&b) on maximum information preservation learning. Section 4 proposed and\nsummarizes a number of possible new models suggested by the unified scheme.\n\n2\n\nBAYESIAN-KULLBACK YING-YANG MACHINE\n\nAs argued in Xu (1995a), unsupervised and supervised learning problems can be\nsummarized into the problem of estimating joint density P(x, y) of patterns in\nthe input space X and the representation space Y, as shown in Fig.I. Under the\nBayesian framework, we have two representations for P(x, y). One is PM! (x , y) =\nPM! (ylx)PM! (x), implemented by a model Ml called YANG/(male) part since it\nperforms the task of transferring a pattern/(a real body) into a code/(a seed). The\nother is PM 2(X, y) = PM2(xly)PM 2(Y), implemented by a model M2 called YING\npart since it performs the task of generating a pattern/(a real body) from a code/(a\nseed). They are complement to each other and together implement an entire circle\nx -t y -t x. This compliments to the ancient chinese YING-YANG philosophy.\nHere we have four components PM! (x), PM! (ylx), PM2 (xly) and PM 2(Y). The\nPM! (x) can be fixed at some density estimate on input data, e.g ., we have at least\ntwo choices-Parzen window estimate Ph (x) or empirical estimate Po (x) :\nPh(X)\n\n=\n\nN~d I:~l K( X~XI),\n\nPo(x)\n\n= limh ...OPh(X) = -b I:~l 8(x -\n\nXi).\n\n(1)\n\nFor PM!(ylx), PM2 (xly), each can have three choices: (1) from a parametric family specified by model Ml or M 2 ; (2) free of model with PM!(ylx) = P(ylx) or\nPM 2(xly) = P(xly); (3) broken channel PM! (ylx) = PM!(y) or PM 2(xly) = PM2 (X) .\nFinally, PM 2(y) with its y consistent to PM! (ylx) can also being from a parametric\nfamily or free of model. Any combinations of the choices of the four components\nforms a potential YING-YANG pair. We at least have 2 x 3 x 3 x 2 = 36 pairs.\nA YING-YANG pair has four types of marital status: (a) marry, i.e., YING and\n\n\x0c446\n\nL. XU\n...p.....n. .tIon apace v\nSymbola. Intea __ ? Binary Cod..\n\n\':.2(Y)\n\nDecoding\n\nEncoding\n\no ........trng\n\n".ooannlon\n\nAepr_ _ ntatlon\n\nAeconatruotlon\n\nFigure 1 The joint spaces X, Y and the YING-YANG Machine\n\nYANG match each other; (b) divorce, i.e., YING and YANG go away from each\nother; (c) YING chases YANG, YANG escapes; (d) YANG chases YING, but YING\nescapes. The four types can be described by a combination of minimization (chasing) and maximization (escaping) on one of the two Kullback divergences below:\n,\nf\n(\\\nPM) (ylx) PM) (x)\n(\nR.(MI,M2)\n= x,y PMlyx)PMl(x)logpM2 (I)P\n()dxdy\n2a)\nx Y M2 Y\n\n(\n\n) f x,y PM2 (X\\)\nY PM2 ()\nY log P\n\nK M2,MI =\n\nPM2\n\n(xly) P M2 (y)\n(I)P\n()dxdy\n\nMl Y x\n\nMl x\n\n(\n\n2b\n\n)\n\nWe can replace K(MI\' M 2) by K(M2, MJ) in the table. The 2nd & 3rd columns are for\n(c) (d) respectively, each has two cases depending on who starts the act and the two\nare usually not equivalent. Their results are undefined depending on initial condition for\nM I,M2, except of two special cases: (i) Free PMl(Y\\X) and parametric PM2(X\\Y), with\nminM2 maxMl K being the same as (b) with broken PM l (y\\x), and with maXM2 minMl K\ndefined but useless. (ii) Free PM2 (X\\Y) and parametric PMl(y\\X), with minMl maXM2 K\nthe same as case (a) with broken PM2 (xly), with minMl maxM2 K defined but useless.\n\nTherefore, we will focus on the status marry and divorce. Even so, not all of the\nabove mentioned 2 x 3 x 3 x 2 = 36 YING-YANG pairs provide sensible learning\nmodels although minM l ,M2 K and maxM l ,M2 K are always well defined. Fortunately,\na quite number of them indeed lead us to useful learning models, as will be shown\nin the sequent sections.\nWe can implement minM l ,M2 K(Ml, M 2) by the following Alternative Minimization\n(ALTMIN) procedure:\nStep 1 Fix M2 = M21d, to get Mr ew = arg M inM l K L( M I , M 21d )\nStep 2 Fix MI = Mfld, to get M:;ew = arg MinM2 KL(Mfld, M 2)\nThe ALTMIN iteration will finally converge to a local minimum of K(MI , M 2 ). We can\nhave a similar procedure for maXM l ,M2 K(M I , M2) via replacing Min by Max.\n\nSince the above scheme bases on the two complement YING and YANG Bayesian\nrepresentations and their Kullback divergence for their marital status, we call it\nBayesian-Kullback YING- YANG learning scheme. Furthermore, under this scheme\nwe call each obtained YING-YANG pair that is sensible for learning purpose as a\nBayesian-Kullback YING- YANG Machine or YING- YANG machine shortly.\n\n3\n\nUNIFIED EXISTING LEARNINGS\n\nLet PMl(X) = Po(x) by eq.(l) and put it into eq.(2), through certain mathematics\nwe can get K(M1 , M2) = hMl - haMl - QM l ,2 + D with D independent of M 1 , M2\nand hMll haMl\' QM l ,2 given by Eqs.(El)(E2)&(E4) in Tab.2 respectively. The larger\n\n\x0cA Unified Learning Scheme: Bayesian-Kullback Ying-Yang Machine\n\n447\n\nis the hM l , the more discriminative or separable are the representations in Y for the\ninput data set. The larger is the haMl\' the more concentrated the representations\nin Y . The larger is the qM l ,2\' the better PM2(xIY) fits the input data.\nTherefore, minM l ,M2 K(M1, M 2) consists of (1) best fitting of PM2 (xIY) on input\ndata via maxQM l ,2\' which is desirable, (2) producing more concentrated representations in Y to occupy less resource, which is also desirable and is the behind reason for\nsolving the problem of selecting cluster number in clustering analysis Xu(1995a&c,\n1996a), (3) but with the cost of less discriminative representations in Y for the input\ndata. Inversely, maxM l ,M2 K(M1 , M 2 ) consists of (1) producing best discriminative\nor separable representation PMl (ylx) in Y for the input data set, which is desirable,\nin the cost of (2) producing a more uniform representation in Y to fully occupy the\nresource, and (3) causing PM 2(xly) away from fitting input data.\nShown in Table 2 are the unified existing unsupervised learnings. For the case\nH-f- W, we have hMl = h, haMl =ha , QM l ,2 =QM2, and minMJ?M1 , M2) results in PM 2(y) = PM l (y) =O:y and PM2(xly)PM 2(Y) = PM2(X)PM l (ylx) with\nPM2 (X) =I:~=l PM2 (xly)PM2 (y)? In turn, we get K(M 1 , M 2) =-L M2 + D with\nLM2 being the likelihood given by eq.(E5), i.e., we get maximum likelihood estimation on mixture model. In fact, the ALTMIN given in Tab.2 leads us to exactly the\nEM algorithm by Dempster et al(1977). Also, here PMl(X,y), PM2(X,y) is equivalent to the data submanifold D and model submanifold M in the Information\nGeometry theory (Amari, 1995a&b), with the ALTMIN being the em algorithm.\nAs shown in Xu(95a), the cases also includes the MDL auto-encoder (Hinton &\nZemel, 1994) and Multi-sets modeling (Xu, 1995e).\nFor the case Single-M, the hMl - haMl is actually the information transmitted by\nthe YANG part from x to y. In this case, its minimization produces a non-sensible\nmodel for learning. However, its maximization is exactly the Informax learning\nscheme (Linsker, 1989; Atick & Redlich, 1990; Bell & Sejnowski, 1995). Here, we\nclear up a confusion made in Xu(95a&b) where the minimization was mistakenly\nconsidered.\nFor the case H-m- W, the hMl -haMl -Q Ml,2 isjust the -F(d; B, Q) used by Dayan et\nal (1995) and Hinton et al (1995) for Helmholtz machine. We can set up the detailed\ncorrespondence that (i) here PMl(ylx;) is their Qa; (ii) logPM 2(x,y is their -Ea;\nand (iii) their Pa is PM 2 (ylx) = PM2(xly)PM 2(Y)/ I:y PM 2(xly)PM 2(Y). So, we get\na new perspective for Helmholtz machine . Moreover, we know that K(M1, M 2) becomes a negative likelihood only when PM2(xly)PM 2(Y) = PM2(X)PM l (ylx), which\nis usually not true when the YANG and YING parts are both parametric. So\nHelmholtz machine is not equivalent to maximum likelihood learning in general\nwith a gap depending on PM 2(xly)PM2 (y) - PM2 (X)PMl (ylx). The equivalence is\napproximately acceptable only when the family of PM2(xly) or/and PM l (ylx;) is\nlarge enough or M 2 , Ml are both linear with gaussian density.\nIn Tab.4, the case Single-Munder K(M2, Ml) is the classical maximum likelihood\n(ML) learning for supervised learning which includes the least square learning by\nback propagation (BP) for feedfarward net as a special case. Moreover, its counterpart for a backward net as inverse mapping is the case Single-Funder K(Ml, M 2).\n\n4\n\nNEW LEARNING MODELS\n\nFirst, a number of variants for the above existing models are given in Table 2.\nSecond, a particular new model can be obtained from the case H-m- Wby changing\nminM l ,M2 into maxM l ,M2. That is, we have maXM l ,M2 [hMl - haMl - QM l ,2]\' shortly\n\n\x0c448\n\nL.XU\n\nTable 2: BKC-YY Machine for Unsupervised Learning ( Part I) : K(MI, M 2)\nGiven Data {X;}f:l\' Fix PMl (x) = Po(x) by eq.(l), and thus K(MI\' M2) = Kb + D, with\nD irrelevant to M 1 , M2 and K b given by the following formulae and table:\n\nh\n\n= -N1 ""N,k\nP(ylx;)logP(ylx;) ,\n~t"y\n\nhMl\n\n= -N1 "Ut\n" , y PMl(ylx;)logPMl(ylx;),\n\n(El)\n\nh aMl = 2::yO\'~llogO\'~l,\nO\'~l = 1:i 2::; PMl(ylx;), ha = 2:: y O\'ylogO\'y,\nO\'y = 1:i 2::. P(ylx;),\nP(ylx;) = O\'yPM2 (xily)J 2:: y O\'yPM2 (x;iy),\nqM 1 ,2 = 1:; 2::;,y PM l (Ylx;) log PM2(x;iy),\nqM2 = 1:; 2::i,y P(ylx;) log PM2(Xily),\nL~2 = 1:i 2::; ,y O\'y log PM2(x;iy),\nLM2 = 1:; 2::; log 2:: y O\'yPM2(X.ly)\nMarriage\nStatus\nCondition\n\nKb\n\nH-f-W\n\nPM l (ylx)\nfree, i.e.,\n\nPMl:~YJXJ\n= Pyx\nh-ha-qM2\n= -LM2\n(minl\n~1: t\'IX\nM2, get\n\nP(ylx;)\nO\'y by\n\nSingle-M\nPM2 (y)\n= PM l (y)\nPM2(xly)\n= PM2 (X)\n= Po(xl\n\nSingle-F\n\nH-m-W\n\n(E2)\n(E3)\n(E4)\n(E5)\n\nW-f-H\nUniform\n\nPMl (ylx)\n\nPM2(y),\n\nand\n\nand free\n\nP M2 (xly)\n\nPM~~~I~~\n=\nP xjy\n\nlhMl-haMl\n\nhaMl\n\nP Ml (ylx)\n= PMl(y)\n\nhMl - haMl\n\n-LM2\n\n(max)\n\n(min)\n\n(~~l~)]\nmIn\n\nGet M2\nby\nmax\nL~2\'\n\nt\'IX\nM2, get\nMI by\nmin [hMl\n-haMl - QM l ,2]\n\n(min)\n\n~1:\n\nGet MI\nby max\nhMl-haMl\n\nGet\n\nMI by\n\nmm\n(E3), O\'~l\nhaMl\nALTMIN\nby (E2)\n82: Fix M 1 ,\n82: get\nget M2 by\nM2 by\nmax QM2.\nmax QM l 2\'\nKepeat\nKepeat\nNo\nNo\nNo\n81,82.\nRepeat\nRepeat\n81,82.\nRepeat\n1. ML on\nMixtures\nDupli&EM\ncated\nHelmRelated\n(Dem77)\nInformax,\nto\nMaximum\nmodels\nholtz\n2. Informmutual\nby ML\nmachine\nPCA\nExisting\nation\ngeometry\nInformlearning\n(Hin95)\nEquiv(Amari95)\nation\non\n(Day95)\n-lent\nmodels\n3. MDL\ninput\n~Lin89~\nAutodata.\nAti90\nencoder\n(BeI95)\nin94 )\n4. ulti-sets\nmodeling\n(Xu94 ,95)\n1. t\'or H-f- W type, we have:\nThree VQ variants when PM2(xly) is Gaussian. Also, criteria for\nselecting the correct k for VQ or clustering (Xu95a&c).\nNew\n2. For H-m-W type, we have:\nResults\nRobust PCA + criterion for determining subspace dimension (Xu, 95c).\n1. More smooth PMl_(x)given by Parzen window estimate.\n2. Factorial coding PM2(y) = ~M2(Y;) with binary y = [YI "\', yrn].\nVariants\n3. Factorial coding PM l (ylx) = . PM2(Yi Ix) with binary [YI ... , Yrn].\n4. Replace \'2::11 .\' in all the above items by \'fu ?dy\' for real y.\nNote: H- Husband, W-WIfe, f- follows, M-Male, F-Female, m-matches. X-f-Y stands for\nX part is free. Single-X stands for the other part broken. H-m-W stands for both parts\nbeing parametric. \'(min)\' stands for min Kb and \'(max), stands for max Kb.\n\nW\n\n\x0c449\n\nA Unified Learning Scheme: Bayesian-Kullback Ying-Yang Machine\n\nTable 3: BKC-YY Machine for Unsupervised Learning ( Part II) : J(M2 , Ml)\nGiven Data {Xi}F:I, Fix P MI (x) = Po(x) by eq.(l), and thus J(M2 , Ml) = J(b + D, with\nD irrelevant to M 1 , M2 and J(b given by the following formulae and table:\n\n(E6)\n(E7)\n(ES)\n(E9)\n\n(ElO)\nMarrIage\nStatus\n\nSi ngle-M\n\nH-f-W\n\nSingle-F\n\nH-m-W\n\nW-f-H\n\n+\n\n-La MI\n\nThe same as those m Table 1.\n\nC;ondahon\nlha M2 -\n\nhM2\n\n- L MI ,2]\n\nlhaMI- LMI ]\n\n[h~2\n\nhM2\n\n+ haMI\n\n-qM 2,1]\n\n(if forcing\n\nJ(b\n\n-\n\nPM1(y) =\n\n(max)\nGet M2\nby\nmax\nhM2\nALTMIN\n\n(min)\nS1:\nFix M I ,\nget ai:2\nby (E7) .\nS2:\nupdate\nMI by\nmax LMI ,2\n\nP02?\n(~I)\nmIn\nSI:\nFix M 1 ,\nget ai:l\nby (E2).\nin Tab.l\nS2:\nupdate\nMI by\nmax LMI\n\n(min)\n\' SI:\nFix M 2 ,\nget MI\nby\nmIn\n\n(max)\nGet M2\nby\nmax\nh~2\'\n\n(min)\nGet MI\nby\nmax\n\n[haMI\n- q M 2,1]\n\nL o MI\n\nS2: Fix M 1 ,\nget M2\nby min\nh~?-qM~ I\n\n1\\l0\n\nt;xlstmg\nmodels\nVanants\n\nRepeat\nno\nnew!\n\nttepeat\nSI, S2\nno\nnew!\n\n1\\l0\nttepeat\n.H.epeat\nRepeat\nSI, S2\nSI,S2\nno\nno\nno\nnew!\nnew!\nnew!\nImilar to those m Table 1.\n\n1\\l0\n\nRepeat\nno\nnew!\n\nTable 4: BKC-YY Machine for Supervised Learning\nGiven Data {Xi,y.}F:I , Fix PMI(X) = Po(x) by eq.(l).\nh\'kl\n\n= -kEiPM1(y;JXi)logPMI(Yil x i),\n\nh\'k2 = -kEiPM2(x;JYi)logPM2(x;Jy.),\n\n(Ell)\n\nQ\'kI ,2 = -k Ei PM I (y;JXi) log PM2 (xdYi), Q\'k2 , 1 = -k Ei PM2(X;Jy.) log PM I (Y.lxi) , (El2)\nL\'kl = -k Ei log PMI (y;JX i ), L\'k2 = -k Ei log PM2(XiIYi) ,\n(El3)\nK(MI,\nMarnage\nStatus\nJ(b\n\nFeature\n~\'xastmg\n\nmodels\n\nM2)\n\n= Kb + D\n\nSingle-M\nhMl\n\nSingle-F\n-LM2\n\n(max)\nmIrumum\nentropy (ME)\nF-net\nno\nnew!\n\n(min)\nML\n\nB-net\ntH\' on\n\nB-net\n\nH-m-W\nh MI -QMI2\n\n(min) ,\nMIxed\nF-B\nnet\nno\nnew!\n\nJ((M2 , MI)\n\n= J(b + D\n\nSingle-M\n-LMI\n\nSi ngle-F\nhM2\n\n(min)\n\n(max)\nmlrumum\nentropy\nB-net\nno\nnew!\n\nML\n\nF-net\ntiP on\n\nF-net\n\nH-m-W\nh M2-QM2 1\n\n(min) ,\nMixed\nB-F\nnet\nno\nnew!\n\n\x0c450\n\nL. XU\n\ndenoted by H-m- W-Max. This model is a dual to the Helmholtz machine in order\nto focus on getting best discriminative or separable representations PM l (ylx) in Y\ninstead of best fitting of PM2(xly) on input data.\n\nThird, by replacing K(M 1 , M 2) with K(M2, M 1 ), in Table 3 we can obtain new\nmodels that are the counterparts of those given in Table 2. For the case H-J- W,\nits maxM l ,M2 gives minimum entropy estimate on PM 2 (X) instead of maximum\nlikelihood estimate on PM 2 (X) in Table 2. For the case Single-M, it will function\nsimilarly to the case Single-F in Table 2, but with minimum entropy on PMl (ylx)\nin Table 2 replaced by maximum likelihood on PM l (ylx) here. For the case H-mW, the focus shifts from on getting best fitting of PM 2(xly) on input data to on\ngetting best discriminative representations PM 1 (ylx) in Y, which is similar to the\njust mentioned H-m- W-Max, but with minimum entropy on PMJylx) replaced by\nmaximum likelihood on PM 1 (ylx). The other two cases in Table 3 have been also\nchanged similarly from those in Table 2.\nFourth, several new model have also been proposed in Table 4 for supervised learning . Instead of maximum likelihood, the new models suggest learning by minimum\nentropy or a mix of maximum likelihood and minimum entropy.\nFinally, further studies on the other status in Table 1 are needed. Heuristically,\nwe can also treat the case H-m- W by two separated steps. We first get Ml by\nmax[h Ml - haMl], and then get M2 by maxqM l ,2; or we first get M2 by min[h ha - qM2] and then get Ml by min[hMl - haMl - QMl,2]\' The two algorithms\nattempt to get both a good discriminative representation by PMl (ylx) and a good\nfitting of PM 2(xly) on input data. However whether they work well needs to be\ntested experimentally.\nWe are currently conducting experiments on comparison several of the above new\nmodels against their existing counterparts.\nAcknowledgements The work was Supported by the HK RGG Earmarked Grant\nGUHK250/94E.\nReferences\nAmari , S(1995a) [Amari95] " Information geometry of the EM and em algorithms for neural networks",\nNeural Networks 8, to appear.\nAmari, S(1995b), Neural Computation 7 ppI3-18.\nAtick, J .J. & Redlich, A .N . (1990) [Ati901. Neural Computation Vo1.2, No.3 , pp308-320 .\nBell A . J . & Sejnowski , T . J.(1995) [Be195], Neural Computation Vo1.7 , No.6 , 1129-1159 .\nByrne, W . (1992), IEEE Trans. Neural Networks 3 , pp612-620 .\nCsiszar, I. , 11975), Annals of Probabil.ty 3, ppI46-158.\nDayan, P. , Hinton , G. E ., & Neal, R. N. (1995) [Day95], Neural Computat.on Vo1.7, No.5 , 889-904.\nDempster, A.P., Laird , N .M ., & Rubin, D .B. (1977) [Dem77] , 1. Royal Statist. Soc.ety , 839, 1-38.\nHathaway, R.J.(1986), Statistics & Probability Letters 4, pp53-56 .\nHinton, G . E ., et ai, (1995) [Hin95], Sc.ence 268, pp1158-1160 .\nHinton , G. E . & Zemel, R.S. (19M) [Hin94], Advances in NIPS 6, pp3-10.\nLinsker, R. (1989) [Lin89], Advances in NIPS 1, ppI86-194.\nNeal , R. N .& Hinton, G. E(1993), A new view of the EM algorithm that Jushfies Incremental and other\nvanants , pr~rint.\nXu, L . (1996 , "How Many Clusters? : A YING- YANG Machine Based Theory For A Classical Open\nProblem In attern Recognition" , to appear on Proc. IEEE ICNN96.\nXu , L. (1995a), "YING-YANG Machine: a Bayesian-Kullback scheme for unified learnings and new\nresults on vector quantization" , Keynote talk, Proc. Inti Conf. on Neural Information Processing\n(ICONIP95), Oct 30 - Nov . 3, 1995 , pp977-988 .\nXu , L.(1995b), "YING-YANG Machine for Temporal Signals", Keynote talk, Proc IEEE inti Conf.\nNeural Networks & Signal Processing, Vol.I, pp644-651, Nanjing, 10-13 , 1995.\nXu , L . (1995c) , "New Advances on The YING- YANG Machine", Invited paper, Proc. of 1995 IntI.\nSymposium on Artificial Neural Networks, ppIS07-12 , Dec. 18-20 , Taiwan .\nXu , L . (1995d), "Cluster Number Selection, Adaptive EM Algorithms and Competitive Learnings",\nInvited paper, Proc . Inti Conf. on Neural Information Processing (ICONIP95), Oct 30 - Nov. 3, 1995 ,\nVol. II , ppI499-1502.\nXu , L . (1995e), Invited paper, Proc. WCNN95, Vol.I, pp35-42. Also, Invited paper, Proc . IEEE ICNN\n1994, ppI315-320 .\nXu , L. , & Jordan, M.I . (1993). Proc . of WCNN \'93, Portland, OR, Vol. II, 431-434 .\n\n\x0c'
p83144
sg281
S'Estimating the Bayes Risk from Sample Data\n\nRobert R. Snapp? and Tong Xu\n\nComputer Science and Electrical Engineering Department\nUniversity of Vermont\nBurlington, VT 05405\n\nAbstract\nA new nearest-neighbor method is described for estimating the Bayes risk\nof a multiclass pattern claSSification problem from sample data (e.g., a\nclassified training set). Although it is assumed that the classification problem can be accurately described by sufficiently smooth class-conditional\ndistributions, neither these distributions, nor the corresponding prior probabilities of the classes are required. Thus this method can be applied to\npractical problems where the underlying probabilities are not known. This\nmethod is illustrated using two different pattern recognition problems.\n\n1 INTRODUCTION\nAn important application of artificial neural networks is to obtain accurate solutions to\npattern classification problems. In this setting, each pattern, represented as an n-dimensional\nfeature vector, is associated with a discrete pattern class, or state of nature (Duda and Hart,\n1973). Using available information, (e.g., a statistically representative set of labeled feature\nvectors {(Xi, fin, where Xi E Rn denotes a feature vector and fi E l:::: {Wl,W2, ... ,we},\nits correct pattern class), one desires a function (e.g., a neural network claSSifier) that assigns\nnew feature vectors to pattern classes with the smallest possible misclassification cost.\nIf the classification problem is stationary, such that the patterns from each class are generated\n\naccording to known probability distributions, then it is possible to construct an optimal\nclasSifier that assigns each pattern to a class with minimal expected risk. Although our\nmethod can be generalized to problems in which different types of classification errors\nincur different costs, we shall simplify our discussion by assuming that all errors are equal.\nIn this case, a Bayes claSSifier assigns each feature vector to a class with maximum posterior\nprobability. The expected risk of this classifier, or Bayes risk then reduces to the probability\nof error\nRB =\n? E-mail:snapp<Demba.uvm.edu\n\nr [1 - SUPPCf!X)]\nJCx)dx,\nfEL\n\nJs\n\n(1)\n\n\x0c233\n\nEstimating the Bayes Risk from Sample Data\n\n(Duda and Hart, 1973). Here, P( fix) denotes the posterior probability of class f conditioned\non observing the feature vector x, f(x) denotes the unconditional mixture density of the\nfeature vector x, and S C Rn denotes the probability-one support of f.\nKnowing how to estimate the value of the Bayes risk of a given classification problem with\na specific input representation, may facilitate the design of more accurate classifiers. For\nexample, since the value of RB depends upon the set of features chosen to represent each\npattern (e.g., the significance of the input units in a neural network classifier), one might\ncompare estimates of the Bayes risk for a number of different feature sets, and then select\nthe representation that yields the smallest value. Unfortunately, it is necessary to know the\nexplicit probability distributions to evaluate (1). Thus with the possible exception of trivial\nexamples, the Bayes risk cannot be determined exactly for practical classification problems.\nLacking the means to evaluate the Bayes risk exactly, motivates the development of statistical\nestimators of RB. In this paper, we use a recent asymptotic analysis of the finite-sample\nrisk of the k-nearest-neighbor classifier to obtain a new procedure for estimating the Bayes\nrisk from sample data. Section 2 describes the k-nearest-neighbor algorithm, and briefly\ndescribes how estimates of its finite-sample risk have been used to estimate RB. Section 3\ndescribes how a recent asymptotic analysis of the finite-sample risk can be applied to obtain\nnew statistical estimators of the Bayes risk. In Section 4 the k-nearest-neighbor algorithm\nis used to estimate the Bayes risk of two example problems. Section 5 contains some\nconcluding remarks.\n\n2\n\nTHE k-NEAREST-NEIGHBOR CLASSIFIER\n\nDue to its analytic tractability, and its nearly optimal performance in the large sample limit,\nthe k-nearest-neighbor classifier has served as a useful framework for estimating the Bayes\nrisk from classified samples. Recall, that the k-nearest-neighbor algorithm (Fix and Hodges,\n1951) clasSifies an n-dimensional feature vector x by consulting a reference sample of m\ncorrectly clasSified feature vectors Xm = {(Xi, f i ) : i = 1, ... m}. First, the algorithm\nidentifies the k nearest neighbors of x, Le., the k feature vectors within Xm that lie closest\nto x with respect to a given metric. Then, the classifier assigns x to the most frequent class\nlabel represented by the k nearest neighbors. (A variety of procedures can be used to resolve\nties.) In the following, C denotes the number of pattern classes.\nThe finite-sample risk of this algorithm, R m , equals the probability that the k-nearestneighbor classifier assigns x to an incorrect class, averaged over all input vectors x, and\nall m-samples, X m . The following properties have been shown to be true under weak\nassumptions:\nProperty 1 (Cover and Hart, 1967): For.fixed k,\nRm\n\nwith\nRB\n\n-+\n\nRoo(k),\n\nas m\n\n~ Roo(1) ~ RB (2 -\n\n-+ 00\n\nC ~ 1R B ).\n\n(2)\n\nProperty 2 (Devroye, 1981): If k ~ 5, and C = 2, then there exist universal constants\na =0.3399? .. , and =0.9749 ... such that Roo(k) is bounded by\n\n(3\n\nRB ~ Roo(k) ~ (1 + ak)RB ,\n\nwhere\n\nak\n\n(3)\n\na..(f (\n= k _ 3.25 1 + ~ .\n\nMore generally, ifC = 2, then\nHB \'" Roo(k) \'"\n\n(1 If)\n+\n\nRB-\n\n(3)\n\n\x0cR. R. SNAPP, T. XU\n\n234\n\nBy the latter property, this algorithm is said to be Bayes consistent in that for any c > 0, it\nis possible to construct a k-nearest-neighbor classifier such that IRm - RBI < c if m and\nk are sufficiently large. Bayes consistency is also evident in other nonparametric pattern\nclassifiers.\nSeveral methods for estimating RB from sample data have previously been proposed, e.g.,\n(Devijver, 1985), (Fukunaga, 1985), (Fukunaga and Hummels, 1987), (Garnett and Yau,\n1977), and (Loizou and Maybank, 1987). Typically, these methods involve constructing\nsequences of k-nearest neighbor classifiers, with increasing values of k and m. The misclassification rates are estimated using an independent test sample, from which upper and\nlower bounds to RB are obtained. Because these experiments are necessarily performed\nwith finite reference samples, these bounds are often imprecise. This is especially true for\nproblems in which Rm converges to Roo(k) at a slow rate. In order to remedy this deficiency,\nit is necessary to understand the manner in which the limit in Property 1 is achieved. In the\nnext section we describe how this information can be used to construct new estimators for\nthe Bayes risk of sufficiently smooth claSSification problems.\n\n3\n\nNEW ESTIMATORS OF THE BAYES RISK\n\nFor a subset of multiclass classification problems that can be described by probability\ndensities with uniformly bounded partial derivatives up through order N + 1 (with N 2: 2),\nthe finite-sample risk of a k-nearest-neighbor classifier that uses a weighted Lp metric can\nbe represented by the truncated asymptotic expansion\nN\n\nRm\n\n= Roo(k) + 2:::Cjm- j/n + 0\n\n(m- CN +1)/n) ,\n\n(4)\n\nj=2\n\n(Psaltis, Snapp, and Venkatesh, 1994), and (Snapp and Venkatesh, 1995). In the above,\nn equals the dimensionality of the feature vectors, and Roo(k), C2, ... ,CN, are the expansion coefficients that depend upon the probability distributions that define the pattern\nclassification problem.\nThis asymptotic expansion provides a parametric description of how the finite-sample risk\nRm converges to its infinite sample limit Roo(k). Using a large sample of classified data,\none can obtain statistical estimates of the finite-sample risk flm for different values of\nm. Specifically, let {md denote a sequence of M different sample sizes, and select fixed\nvalues for k and N. For each value of mi, construct an ensemble of k-nearest-neighbor\nclassifiers, i.e., for each classifier construct a random reference sample X mi by selecting\nmi patterns with replacement from the original large sample. Estimate the empirical risk\nof each classifier in the ensemble with an independently drawn set of "test" vectors. Let\nflmi denote the average empirical risk of the i-th ensemble. Then, using the resulting set\nof data points {(mi, RmJ}, find the values of the coefficients Roo(k), and C2 through CN,\nthat minimizes the sum of the squares:\n\n8\n\nM (\n\nflmi - Roo(k) -\n\nN)2\n~\nCjm;j/n\n\n(5)\n\nSeveral inequalities can then be used obtain approximations of RB from the estimated value\nof Roo(k). For example, if k = 1, then Cover and Hart\'s inequality in Property 1 implies\nthat\nRoo(l) < R < R (1).\n2\nB_\n00\nTo enable an estimate of RB with preciSion c, choose k > 2/c 2 , and estimate Roo(k) by the\nabove methOd. Then Devroye\'s inequality (3) implies\nRoo(k) - c ~ Roo(k)(1 - c) ~ RB ~ Roo(k).\n\n\x0cEstimating the Bayes Risk from Sample Data\n\n4\n\n235\n\nEXPERIMENTAL RESULTS\n\nThe above procedure for estimating RB was applied to two pattern recognition problems.\nFirst consider the synthetic, two-class problem with prior probabilities PI = P2 = 1/2, and\nnormally distributed, class-conditional densities\n\nf( x)=\nI\n\n1\n\n(27r)n/2\n\ne-H(Xl+(-1)t)2+I:~=2xn\n\n\'\n\nfor f = 1 and 2. Pseudorandom labeled feature vectors (x, f) were numerically generated in\naccordance with the above for dimensions n = 1 and n = 5. Twelve sample sizes between\n10 and 3000 were examined. For each dimension and sample size the risks Rm of many\nindependent k-nearest-neighborclassifiers with k = 1,7, and 63 were empirically estimated.\n(Because the asymptotic expansion (4) does not accurately describe the very small sample\nbehavior of the k-nearest-neighbor classifier, sample sizes smaller than 2k were not included\nin the fit.)\nEstimates of the coefficients in (5) for six different fits appear in the first equation of each cell\nin the third and fourth columns of Table 1. For reference, the second column contains the\nvalues of RooCk) that were obtained by numerically evaluating an exact integral expression\n(Cover and Hart, 1967). Estimates of the Bayes risk appear in the second equation of each\ncell in the third and fourth columns. Cover and Hart\'s inequality (2) was used for the\nexperiments that assumed k = 1, and Devroye\'s inequality (3) was used if k ~ 7. For thiS\nproblem, formula (1) evaluates to RB = (l/2)erfc(I/V2) = 0.15865.\n\nTable 1: Estimates of the model coefficients and Bayes error for a classification problem\nwith two normal classes.\nk\n\n1\n\n7\n\n63\n\nRoo(k)\n\n0.2248\n\n0.1746\n\n0.1606\n\nn=1\n\n(N=2)\n\n0.6536\nm2\nRB =0.172 ? 0.057\n\nR m =0.2287 +\n\n4.842\nm\nRB =0.152 ?0.023\n\nRm =0.1744 + -2-\n\n20.23\nm\nRB =0.157 ? 0.004\n\nRm =0.1606 + - 2 -\n\nn=5\n\n(N =6)\n\n0.0222\n0.1121 0.2001\n+\n5\n4\n5\n2\nm6/ 5\nm /\nm /\nRB =0.172 ? 0.057\n\nRm =0.2287 +\n\n0.2218\n1.005 3.782\n-+-m4/ 5 m 6/ 5\nm 2/ 5\nRB =0.148 ? 0.022\n\nR m =0.1700+\n\n0.1002 1.426 10.96\n- - 4-5+ -6 m /\nm /5\nm 2/ 5\nRB =0.156 ? 0.004\n\nRm =0.1595 +\n\nThe second pattern recognition problem uses natural data; thus the underlying probability\ndistributions are not known. A pool of 222 classified multispectral pixels were was extracted\nfrom a seven band satellite image. Each pixel was represented by five spectral components,\nx = (Xl, .. . ,X5), each in the range 0 ~ X" ~ 255. (Thus, n = 5.) The class label of\neach pixel was determined by one of the remaining spectral components, 0 ~ y ~ 255.\nTwo pattern classes were then defined: Wl = {y < B}, and W2 = {y ~ B}, where B was a\npredetermined threshOld. (This particular problem was chosen to test the feasibility of this\nmethod. In future work, we will examine more interesting pixel claSsification problems.)\n\n\x0cR. R. SNAPP, T. XU\n\n236\n\nTable 2: Coefficients that minimize the squared error fit for different N. Note that\nand Cs = 0 in (2) ifn ~ 4 (Psaltis, Snapp, and Venkatesh, 1994).\n\nN\n\nRoo(l)\n\n2\n\n0.0757133\n\n0.126214\n\n4\n\n0.0757846\n\n0.124007\n\n0.0132804\n\n6\n\n0.0766477\n\n0.0785847\n\n0.689242\n\nC3\n\n=0\n\n-2.68818\n\nWith k = 1, a large number of Bernoulli trials (e.g., 2~1000) were performed for each\nvalue of mi . Each trial began by constructing a reference sample of mi classified pixels\nchosen at random from the pool. The risk of each reference sample was then estimated by\nclassifying t pixels with the nearest-neighbor algorithm under a Euclidean metric. Here,\nthe t pixels, with 2000 ~ t ~ 20000, were chosen independently, with replacement, from\nthe pool. The risk 11m. was then estimated as the average risk of each reference sample\nof size mi . (The number of experiments performed for each value of mi, and the values\noft, were chosen to ensure that the variance of\nwas sufficiently small, less than 10- 4\nin this case.) This process was repeated for M = 33 different values of mi in the range\n100 ~ mi ~ 15000. Results of these experiments are displayed in Table 2 and Figure 1\nfor three different values of N. Note that the robustness of the fit begins to dissolve, for this\ndata, at N = 6, either the result of overfitting, or insuffiCient smoothness in the underlying\nprobability distributions. However, the estimate for Roo(l) appears to be stable. For this\nclaSSification problem, we thus obtain RB = 0.0568 ? 0.0190.\n\nHm.\n\n5\n\nCONCLUSION\n\nThe described method for estimating the Bayes risk is based on a recent asymptotic analysis\nof the finite-sample risk of the k-nearest-neighbor classifier (Snapp and Venkatesh, 1995).\nRepresenting the finite-sample risk as a truncated asymptotic series enables an efficient\nestimation of the infinite-sample risk Roo(k) from the classifier\'s finite-sample behavior.\nThe Bayes risk can then be estimated by the Bayes consistency of the k-nearest-neighbor\nalgorithm. Because such finite-sample analyses are difficult, and consequently rare, this\nnew method has the potential to evolve into a useful algorithm for estimating the Bayes risk.\nFurther improvements in efficiency may be obtained by incorporating principles of optimal\nexperimental deSign, cf., (Elfving, 1952) and (Federov, 1972).\nIt is important to emphasize, however, that the validity of (4) rests on several rather strong\nsmoothness assumptions, including a high-degree of differentiability of the class-conditional\nprobability densities. For problems that do not satisfy these conditions, other finite-sample\ndescriptions need to be constructed before this method can be applied. Nevertheless, there\nis much evidence that nature favors smoothness. Thus, these restrictive assumptions may\nstill be applicable to many important problems.\nAcknowledgments\n\nThe work reported here was supported in part by the National Science Foundation under\nGrant No. NSF OSR-9350540 and by Rome Laboratory, Air Force Material Command,\nUSAF, under grant number F30602-94-1-OOlO.\n\n\x0c237\n\nEstimating the Bayes Risk from Sample Data\n\n-1.8\n\n-rl\nI\n\n-2.0\n\nt:\n\n-0.::\n0\n\n-2.2\n\n\' ol)\n\n0\n\n-2.4\n\n100\n\n1000\nm\n\n10000\n\nFigure 1: The best fourth-order (N = 4) fit of Eqn. (5) to 33 empirical estimates of Hmo\nfor a pixel classification problem obtained from a multispectral Landsat image. Using\nRXI = 0.0758, the fourth-order fit, Rm =0.0758 + 0.124m- 2 / 5 + 0.0133m - 4/5, is plotted\non a log-log scale to reveal the significance of the j = 2 term.\n\nReferences\nT. M. Cover and P. E. Hart, "Nearest neighbor pattern classification," IEEE Trans. Inform.\nTheory,vol.IT-13,1967,pp.21-27.\n\nP. A. Devijver, "A multiclass, k - N N approach to Bayes risk estimation," Pattern Recognition Letters, vol. 3, 1985, pp. 1-6.\nL. Devroye, "On the asymptotic probability of error in nonparametric discrimination," Annals Of Statistics, vol. 9, 1981, pp. 1320-1327.\n\nR. O. Duda and P. E. Hart, Pattern Classification and Scene Analysis. New York, New York:\nJohn Wiley & Sons, 1973.\n\nG. Elfving, "Optimum allocation in linear regression theory," Ann. Math. Statist., vol. 23,\n1952,pp.255-262.\nV. V. Federov, Theory Of Optimal Experiments, New York, New York: Academic Press,\n1972.\nE. Fix and J. L. Hodges, "Discriminatory Analysis: Nonparametric Discrimination: Consistency Properties," from Project 21-49-004, Repon Number 4, UASF School of Aviation\nMedicine, Randolf Field, Texas, 1951, pp. 261-279.\n\n\x0c238\n\nR. R. SNAPP, T. XU\n\nK. Fukunaga, "The estimation of the Bayes error by the k-nearest neighbor approach," in L.\nN. Kanal and A. Rosenfeld (ed.), Progress in Pattern Recognition, vol. 2, Elesvier Science\nPublishers B.V. (North Holland), 1985, pp. 169-187.\nK. Fukunaga and D. Hummels, "Bayes error estimation using Parzen and k-NN procedures,"\nIEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 9, 1987, pp. 634-643.\n\nJ. M. Garnett, III and S. S. Yau, "Nonparametric estimation of the Bayes error of feature\nextractors using ordered nearest neighbor sets," IEEE Transactions on Computers, vol. 26,\n1977,pp.46-54.\nG. Loizou and S. J. Maybank, "The nearest neighbor and the Bayes error rate," IEEE\nTransactions on Pattern Analysis and Machine Intelligence, vol. 9, 1987, pp. 254-262.\nD. Psaltis, R. R. Snapp, and S. S. Venkatesh, "On the finite sample performance of the\nnearest neighbor classifier," IEEE Trans. Inform. Theory, vol. IT-40, 1994, pp. 820--837.\nR. R. Snapp and S. S. Venkatesh, "k Nearest Neighbors in Search of a Metric," 1995,\n(submitted).\n\n\x0c'
p83145
sg10
S'SPERT-II: A Vector Microprocessor\nSystem and its Application to Large\nProblems in Backpropagation Training\n\nJohn Wawrzynek, Krste Asanovic, & Brian Kingsbury\nUniversity of California at Berkeley\nDepartment of Electrical Engineering and Computer Sciences\nBerkeley, CA 94720-1776\n{johnw ,krste,bedk }@cs.berkeley.edu\nJames Beck, David Johnson, & Nelson Morgan\nInternational Computer Science Institute\n1947 Center Street, Suite 600\nBerkeley, CA 94704-1105\n{beck,davidj,morgan}@icsi.berkeley.edu\n\nAbstract\nWe report on our development of a high-performance system for\nneural network and other signal processing applications. We have\ndesigned and implemented a vector microprocessor and packaged it as an attached processor for a conventional workstation.\nWe present performance comparisons with commercial workstations on neural network backpropagation training. The SPERT-II\nsystem demonstrates significant speedups over extensively handoptimization code running on the workstations.\n\n1\n\nIntroduction\n\nWe are working on pattern recognition problems using neural networks with a large\nnumber of parameters. Because of the large computational requirements of our area\nof research, we set out to design an integrated circuit that would serve as a good\nbuilding block for our systems. Initially we considered designing extremely specialized chips, as this would maximize performance for a particular algorithm. However,\nthe algorithms we use undergo considerable change as our research progresses. Still,\nwe needed to provide some specialization if our design was to offer significant improvement over commercial workstation systems. Competing with workstations is\n\n\x0c620\n\nJ. WAWRZYNEK, K. ASANOVIC, B. KINGSBURY, J. BECK, D. JOHNSON, N. MORGAN\n\na challenge to anyone designing custom programmable processors, but as will be\nshown in this paper, one can still provide a performance advantage by focusing on\none general class of computation.\nOur solution was to design a vector microprocessor, TO, optimized for fixed-point\ncomputations, and to package this as an inexpensive workstation accelerator board.\nIn this manner, we gain a considerable performance/cost advantage for neural network and other signal processing algorithms, while leveraging the commercial workstation environment for software development and I/O services.\nIn this paper, we focus on the neural network applications ofthe SPERT-II system.\nWe are also investigating other applications in the areas of hum an-machine interface\nand multimedia processing, as we believe vector microprocessors show promise in\nproviding the flexible, cost-effective, high-performance computing required.\nSection 2 discusses the design of the hardware, followed in Section 3 by a discussion\nof the software environment we are developing and a discussion of related systems\nin Section 4. In Section 5 we discuss how we map a backpropagation training task\nto the system and in Section 6 we compare the resulting performance with two\ncommercial workstation systems.\n\n2\n\nSPERT- II System\n\nSPERT-II is a double slot SEus card for use in Sun compatible workstations and is\nshown in Figure 1. The board contains a TO vector microprocessor and its memory,\na Xilinx FPGA device for interfacing with the host, and various system support\ndevices.\nSPERT?11\nBoard\n\nTO Chip\n\nData\n\n8MBSRAM\n\nXilinx\n\nFPGA\nHost Wor1<station\n\nFigure 1: SPERT-II System Organization\n\n2.1\n\nThe TO vector microprocessor\n\nDevelopment of the TO vector microprocessor follows our earlier work on the original\nSPERT VLIW /SIMD neuro-microprocessor (Wawrzynek, 1993). The most significant change we have made to the architecture is to move to a vector instruction set\narchitecture (IS A) , based on the industry standard MIPS RISe scalar ISA (Kane,\n1992) extended with vector coprocessor instructions. The resulting ISA, which we\ncall Torrent, offers important advantages over our previous design. We gain access to\nexisting software tools for the MIPS architecture, including optimizing e compilers,\nassemblers, linkers, and debuggers. VLIW machines expose details of the hardware\nimplementation at the instruction set level, and so must change instruction sets\n\n\x0cSPERT-II: A Vector Microprocessor System\n\n621\n\n~hen scaling to higher degrees of on-chip parallelism. In contrast, vector ISAs provide a simple abstraction of regular data parallelism that enables different hardware\nimplementations to make different trade-offs between cost and performance while\nremaining software compatible. Compared with the VLIW /SIMD design, the vector\nISA reduces requirements on instruction cache space and fetch bandwidth. It also\nmakes it easier to write optimized library routines in assembly language, and these\nlibrary routines will still run well on future devices with greater on-chip parallelism.\nIn the design of the TO vector microprocessor, the main technique we employ to\nimprove cost-performance over a commercial general purpose processor is to integrate multiple fixed-point datapaths with a high-bandwidth memory system. Fast\ndigital arithmetic units, multipliers in particular, require chip area proportional to\nthe square of the number of operand bits. In modern microprocessors and digital\nsignal processors a single floating-point unit takes up a significant portion ofthe chip\narea. High-precision arithmetic units also requires high memory bandwidth to move\nlarge operands. However, for a wide class of problems, full-precision floating-point,\nor even high-precision fixed-point arithmetic, is not needed. Studies by ourselves\nand others have shown that for error back-propagation training of neural networks,\n16-bit weights and 8-bit activation values provide similar training performance to\nIEEE single-precision floating-point (Asanovic, 1991).\nHowever, fast fixed-point multiply-adds alone are not sufficient to increase performance on a wide range of problems. Other components of a complete application\nmay dominate total compute time if only multiply-add operations are accelerated.\nOur processor integrates a fast general-purpose RISC core, and includes general\npurpose operations in its vector instruction set to obtain a balanced design.\nThe TO processor is a complete single chip implementation of the Torrent architecture. It was fabricated in Hewlett-Packard\'s CMOS26B process using 1.0 pm\nscalable CMOS design rules and two layers of metal. The die measures 16.75mm x\n16.75mm, and contains 730,701 transistors. TO runs at an internal clock rate of\n40MHz.\nThe main components of TO are the MIPS-II compatible RISC CPU with an onchip instruction cache, a vector unit coprocessor, a 128-bit wide external memory\ninterface, and an 8-bit wide serial host interface (TSIP) and control unit. The\nexternal memory interface supports up to 4 GB of memory over a 128-bit wide data\nbus. The current SPERT-II board uses 16, 4 Mb SRAM parts to provide 8 MB of\nmam memory.\nAt the core of the TO processor is a MIPS-II compatible 32-bit integer RISC processor with a 1 KB instruction cache. The system coprocessor provides a 32-bit\ncounter/timer and registers for host synchronization and exception handling.\nThe vector unit contains a vector register file with 16 vector registers, each holding\n32 elements of 32 bits each, and three vector functional units, VPO, VP1, and\nVMP. VPO and VPl are vector arithmetic functional units. With the exception of\nmultiplies, that must execute in VPO, either pipeline can execute any arithmetic\noperation. The multipliers perform 16-bit x 16-bit multiplies producing 32-bit\nresults. All other arithmetic, logical and shift functions operate on 32 bits. VMP\nis the vector memory unit, and it handles all vector load/store operations, scalar\nload/store operations, and the vector insert/extract operations.\nAll three vector functional units are composed of 8 parallel pipelines, and so can\neach produce up to 8 results per cycle. The TO memory interface has a single\nmemory address port, therefore non-unit stride and indexed memory operations are\nlimited to a rate of one element per cycle.\n\n\x0c622\n\nJ. WAWRZYNEK, K. ASANOVIC, B. KINGSBURY, J. BECK, D. JOHNSON, N. MORGAN\n\nThe elements of a vector register are striped across all 8 pipelines. With the maximum vector length of 32 , a vector functional unit can accept a new instruction\nevery 4 cycles. TO can saturate all three vector functional units by issuing one\ninstruction per cycle to each, leaving a single issue slot every 4 cycles for the scalar\nunit. In this manner, TO can sustain up to 24 operations per cycle. Several important library routines, such as matrix-vector and matrix-matrix multiplies, have\nbeen written which achieve this level of performance. All vector pipeline hazards\nare fully interlocked in hardware, and so instruction scheduling is only required to\nimprove performance, not to ensure correctness.\n\n3\n\nSPERT-II Software Environment\n\nThe primary design goal for the SPERT-II software environment was that it should\nappear as similar as possible to a conventional workstation environment. This\nshould ease the task of porting existing workstation applications, as well as provide\na comfortable environment for developing new code.\nThe Torrent instruction set architecture is based on the MIPS-II instruction set,\nwith extra coprocessor instructions added to access the vector unit functionality.\nThis compatibility allows us to base our software environment on the GNU tools\nwhich already include support for MIPS based machines. We have ported the\ngee C/C++ compiler, modified the gdb symbolic debugger to debug TO programs\nremotely from the host, enhanced the gas assembler to understand the new vector\ninstructions and to schedule code to avoid interlocks, and we also employ the GNU\nlinker and other library management utilities.\nCurrently, the only access to the vector unit we provide is either through library\nroutines or directly via the scheduling assembler. We have developed an extensive\nset of optimized vector library routines including fixed-point matrix and vector\noperations, function approximation through linear interpolation, and IEEE single\nprecision floating-point emulation. The majority of the routines are written in\nTorrent assembler, although a parallel set of functions have been written in ANSI\nC to allow program development and execution on workstations. Finally, there is a\nstandard C library containing the usual utility, I/O and scalar math routines.\nAfter compilation and linking, a TO executable is run on the SPERT-II board by\ninvoking a "server" program on the host. The server loads a small operating system\n"kernel" into TO memory followed by the TO executable. While the TO application\nruns, the server services I/O requests on behalf of the TO process.\n\n4\n\nRelated Systems\n\nSeveral programmable digital neurocomputers have been constructed, most notably\nsystems based on the CNAPS chip from Adaptive Solutions (Hammerstrom, 1990)\nand the SYNAPSE-I, based on the MA-16 chip from Siemens (Ramacher, 1991).\nThe Adaptive Solutions CNAPS-I064 chip contains a SIMD array with 64 16-bit\nprocessing elements (PEs) per chip. Systems require an external microcode sequencer. The PEs have 16-bit datapaths with a single 32-bit accumulator, and are\nless flexible than the TO datapaths. This chip provides on-chip memory for 128K\n16-bit weights, distributed among the individual PEs. Off-chip memory bandwidth\nis limited by an 8-bit port. In contrast, TO integrates an on-chip CPU that acts as\ncontroller, and provides fast access to a external memory equally accessible by all\ndatapaths thereby increasing the range of applications that can be run efficiently.\n\n\x0cSPERT-n: A Vector Microprocessor System\n\n623\n\nLike SPERT-II, the SYNAPSE-l leverages commercial memory parts. It features\nan array of MA-16 chips connected to interleaved DRAM memory banks. The MA16 chips require extensive external circuitry, including 68040 CPUs with attached\narithmetic pipelines, to execute computations not supported by the MA-16 itself.\nThe SYNAPSE-l system is a complex and expensive multi-board design, containing several different control streams that must be carefully orchestrated to run an\napplication. However, for some applications the MA-16 could potentially provide\ngreater throughput than TO as the former\'s more specialized architecture permits\nmore multiply-add units on each chip.\n\n5\n\nMapping Backpropagation to TO\n\nOne artificial neural network (ANN) training task that we have done is taken from\na speaker-independent continuous speech recognition system. The ANN is a simple\nfeed-forward multi-layer percept ron (MLP) with three layers. Typical MLPs have\nbetween 100-400 input units. The input layer is fully connected to a hidden layer of\n100-4000 hidden units. The hidden layer is fully connected to an output layer that\ncontains one output per phoneme, typically 56-61. The hidden units incorporate\na standard sigmoid activation function. The output units compute a "soft-max"\nactivation function. All training is "on-line", with the weight matrices updated\nafter each pattern presentation.\nAll of the compute-intensive sections can be readily vectorized on TO.\nThree operations are performed on the weight matrices: forward propagation, error\nback-propagation, and weight update. These operations are available as three standard linear algebra routines in the TO library: vector-matrix multiply, matrix-vector\nmultiply, and scaled outer-product accumulation, respectively.\nTO can sustain one multiply-add per cycle in each of the 8 datapath slices, and\ncan support this with one 16-bit memory access per cycle to each datapath slice\nprovided that vector accesses have unit stride. The loops for the matrix operations\nare rearranged to perform only unit-stride memory accesses, and memory bandwidth\nrequirements are further reduced by tiling matrix accesses and reusing operands\nfrom the vector registers whenever possible.\nThere are a number of other operations required while handling input and output\nvectors and activation values. While these require only O(n) computation versus\nthe O(n 2 ) requirements of the matrix operations, they would present a significant\noverhead on smaller networks if not vectorized.\nThe sigmoid activation function is implemented using a library piecewise-linear\nfunction approximation routine. The function approximation routine makes use\nof the vector indexed load operations to perform the table lookups. Although TO\ncan only execute vector indexed operations at the rate of one element transfer\nper cycle, the table lookup routine can simultaneously perform all the arithmetic\noperations for index calculation and linear interpolation in the vector arithmetic\nunits, achieving a rate of one 16-bit sigmoid result every 2 cycles. Similarly, a\ntable based vector logadd routine is used to implement the soft-max function, also\nproducing one result every 2 cycles.\nTo simplify software porting, the MLP code uses standard IEEE single-precision\nfloating-point for input and output values. Vector library routines convert formats\nto the internal fixed-point representation. These conversion routines operate at the\nrate of up to 1 conversion every 2 cycles.\n\n\x0c624\n\n6\n\nJ. WAWRZYNEK, K. ASANOVIC, B. KINGSBURY, J. BECK, D. JOHNSON, N. MORGAN\n\nPerformance Evaluation\n\nWe chose two commercial RISC workstations against which to compare the performance of the SPERT-II system. The first is a SPARCstation-20/61 containing a\nsingle 60 MHz SuperSPARC+ processor with a peak performance of60 MFLOPS, 1\nMB of second level cache, and 128 MB of DRAM main memory. The SPARCstation20/61 is representative of a current mid-range workstation. The second is an IBM\nRS/6000-590, containing the RIOS-2 chipset running at 72 MHz with a peak performance of 266 MFLOPS, 256 KB of primary cache, and 768 MB of DRAM main\nmemory. The RS/6000 is representative of a current high-end workstation.\nThe workstation version of the code performs all input and output and all computation using IEEE single precision floating-point arithmetic. The matrix and vector\noperations within the back prop algorithm have been extensively hand optimized,\nusing manual loop unrolling together with register and cache blocking.\nThe SPERT-II numbers are obtained for a single TO processor running at 40 MHz\nwith 8 MB of SRAM main memory. The SPERT-II version of the application maintains the same interface, with input and output in IEEE single precision floatingpoint format, but performs all MLP computation using saturating fixed-point arithmetic with 16-bit weights, 16-bit activation values, and 32-bit intermediate results.\nThe SPERT-II timings below include the time for conversion between floating-point\nand fixed-point for input and output.\nFigure 2 shows the performance of the three systems for a set of three-layer networks on both backpropagation training and forward propagation. For ease of\npresentation we use networks with the same number of units per layer . Table 1\npresents performance results for two speech network architectures. The general\ntrend we observe in these evaluations is that for small networks the three hardware\nsystems exhibit similar performance, while for larger network sizes the SPERT-II\nsystem demonstrates a significant performance advantage. For large networks the\nSPERT-II system demonstrates roughly 20-30 times the performance of a SPARC20\nworkstation and 4-6 times the performance of the IBM RS/6000-590 workstation.\nAcknowledgements\nThanks to Jerry Feldman for his contribution to the design of the SPERT-II system,\nBertrand Irrisou for his work on the TO chip, John Hauser for Torrent libraries, and\nJohn Lazzaro for his advice on chip and system building. Primary support for this\nwork was from the ONR, URI Grant N00014-92-J-1617 and ARPA contract number\nN0001493-C0249. Additional support was provided by the NSF and ICS!.\n\n\x0cSPERT-II: A Vector Microprocessor System\n\n625\n\nForward Pass\n\nTraining\n\n300.-------------------------~\n250\n\n80\n\n<:I""\'i-\\\\\nS~<.. ......... . .. ... .. . ............. ?.. ?. .. . .. . .. ?. . . .. ... .. .. ..\n\nSt>Ef\\i-\\\\\n\n~80\nC/)\n\nfii200\nIl.\n\nIl.\n\no\n\n~150\n\nB\n:::!;\n\n"C\nCD\nCD\n\nalCD\n\n~40\n\nc%loo\n\na.\n\nC/)\n\n\'BM RS/6000\n50\n\n..... .. ........ . ........ .............. . . . .. . .. .... .\n\n??????????????????????? ??? ??? ????? ????? ??? ?\'BM?RSisooo ..... .\n\n20\n\n. ? ?.......?...........?. ? .... .... ... .... "\n\nSPARC20/61\n\noL=~==~~~==~==~\no\n200\n400\n600\n800\n1,000\nLayer Size\n\n--------------------\n\n::======~====:L===S=PA~R~C~2~~6~1~\noL\no\n\n200\n\n400\n\n600\n\n800\n\n1,000\n\nLayer Size\n\nFigure 2: Performance Evaluation Results (all layers the same size).\n\nTable 1: Performance Evaluation for Selected Net Sizes.\nnet size\nIBM\nnet type\n(in x hidden x out) SPERT-II SPARC20 RS/6000-590\nForward Pass (MCPS)\nsmall speech net\n153 x 200 x 56\n17.6\n43.0\n181\nlarge speech net\n342 x 4000 x 61\n276\n11.3\n45.1\nTraining (MCUPS)\nsmall speech net\n153 x 200 x 56\n16.7\n55.8\n7.00\n17.2\nlarge speech net\n342 x 4000 x 61\n78 .7\n4.18\nReferences\nKrste Asanovic and Nelson Morgan. Experimental Determination of Precision Requirements for Back-Propagation Training of Artificial Neural Networks. In Proc.\n2nd Inti. Conf. on Microelectronics for Neural Networks, Munich, Oct. 1991.\nD. Hammerstrom . A VLSI architecture for High-Performance, Low-Cost, On-Chip\nLearning. In Proc. Intl. Joint Cant on Neural Networks, pages 11-537-543, 1990.\nG . Kane, and Heinrich, J . MIPS RISC Architecture. Prentice Hall, 1992.\nU. Ramacher, J. Beichter, W. Raab, J. Anlauf, N. Bruls, M. Hachmann, and\nM. Wesseling. Design of a 1st Generation Neurocomputer. In VLSI Design of\nNeural Networks. Kluwer Academic, 1991.\nJ. Wawrzynek, K. Asanovic, and N. Morgan. The Design ofa Neuro-Microprocessor.\nIEEE Journal on Neural Networks, 4(3), 1993.\n\n\x0c'
p83146
sg40
S'On Neural Networks with Minimal\nWeights\n\nJ ehoshua Bruck\n\nVasken Bohossian\n\nCalifornia Institute of Technology\nMail Code 136-93\nPasadena, CA 91125\nE-mail: {vincent, bruck }?Iparadise. cal tech. edu\n\nAbstract\nLinear threshold elements are the basic building blocks of artificial\nneural networks. A linear threshold element computes a function\nthat is a sign of a weighted sum of the input variables. The weights\nare arbitrary integers; actually, they can be very big integers-exponential in the number of the input variables. However, in\npractice, it is difficult to implement big weights. In the present\nliterature a distinction is made between the two extreme cases:\nlinear threshold functions with polynomial-size weights as opposed\nto those with exponential-size weights. The main contribution of\nthis paper is to fill up the gap by further refining that separation.\nNamely, we prove that the class of linear threshold functions with\npolynomial-size weights can be divided into subclasses according\nto the degree of the polynomial. In fact, we prove a more general\nresult- that there exists a minimal weight linear threshold function\nfor any arbitrary number of inputs and any weight size. To prove\nthose results we have developed a novel technique for constructing\nlinear threshold functions with minimal weights.\n\n1\n\nIntroduction\n\nHuman brains are by far superior to computers for solving hard problems like combinatorial optimization and image and speech recognition, although their basic building blocks are several orders of magnitude slower. This observation has boosted\ninterest in the field of artificial neural networks [Hopfield 82]\' [Rumelhart 82]. The\nlatter are built by interconnecting multiple artificial neurons (or linear threshold\ngates), whose behavior is inspired by that of biological neurons . Artificial neural\nnetworks have found promising applications in pattern recognition, learning and\n\n\x0c247\n\nOn Neural Networks with Minimal Weights\n\nother data processing tasks. However most of the research has been oriented towards the practical aspect of neural networks, simulating or building networks for\nparticular tasks and then comparing their performance with that of more traditional\nmethods for those particular tasks. To compare neural networks to other computational models one needs to develop the theoretical settings in which to estimate\ntheir capabilities and limitations.\n\n1.1\n\nLinear Threshold Gate\n\nThe present paper focuses on the study of a single linear threshold gate (artificial\nneuron) with binary inputs and output as well as integer weights (synaptic coefficients). Such a gate is mathematically described by a linear threshold function.\n\nDefinition 1 (Linear Threshold FUnction)\nA linear threshold function of n variables is a Boolean function\n{ -1, 1} that can be written as\nf( x.... ) -- sgn (F( x....? -for any\n\n{\n-\n\n1 ,for F(x)\n1 ,0therW1se\n. ~0\n\nx E {-1, 1}n and a fixed tV E\n\nf\n\n{ -1, I} n\n\n~\n\nn\n\n, where F(x)\n\n= tV? x = L\n\nWiXi\n\ni=1\n\nzn.\n\nAlthough we could allow the weights Wi to be real numbers, it is known [Muroga 71),\n[Raghavan 88) that for a, binary input neuron, one needs O( n log n) bits per weight,\nwhere n is the number of inputs. So in the rest ofthe paper, we will assume without\nloss of generality that all weights are integers.\n\n1.2\n\nMotivation\n\nMany experimental results in the area of neural networks have indicated that the\nmagnitudes of the coefficients in the linear threshold elements grow very fast with\nthe size of the inputs and therefore limit the practical use of the network. One\nnatural question to ask is the following. How limited is the computational power of\nthe network if one limits oneself to threshold elements with only "small" growth in\nthe size of the coefficients? To answer that question we have to define a measure of\nthe magnitudes of the weights. Note that, given a function I, the weight vector tV\nis not unique (see Example 1 below).\n\nDefinition 2 (Weight Space)\nGiven a lineal\' threshold function f we define W as the set of all weights that satisfy\nDefinition 1, that is W = {UI E zn : Vx E {-1, 1}n,sgn(tV? x) = f(x)}.\nHere follows a measure of the size of the weights.\n\nDefinition 3 (Minimal Weight Size)\nWe define the size of a weight vector as the sum of the absolute values of the weights.\nThe minimal weight size of a linear threshold function is defined as :\nn\n\nS[j)\n\n= ~ia/L IWi I)\n,=1\n\nThe particular vector that achieves the minimum is called a minimal weight vector.\nNaturally, S[f) is a function of n.\n\n\x0cV. BOHOSSIAN, J. BRUCK\n\n248\n\nIt has been shown [Hastad 94], [Myhill 61], [Shawe-Taylor 92], (Siu 91] that there\nexists a linear threshold function that can be implemented by a single threshold\nelement with exponentially growing weights, S[j] \'" 2\'1, but cannot be implemented\nby a threshold element with smaller: polynomialy growing weights, S[j] \'" n d , d\nconstant. In light of that result the above question was dealt with by defining a\nclass within the set of linear threshold functions: the class of functions with "small"\n(Le. polynomialy growing) weights [Siu 91]. Most of the recent research focuses on\nthe power of circuits with small weights, relative to circuits with arbitrary weights\n[Goldmann 92], [Goldman 93]. Rather than dealing with circuits we are interested\nin studying a single threshold gate. The main contribution of the present paper is\nto further refine the division of small versus arbitrary weights. We separate the set\nof functions with small weights into classes indexed by d, the degree of polynomial\ngrowth and show that all of them are non-empty. In particular, we develop a\ntechnique for proving that a weight vector is minimal. We use that technique to\nconstruct a function of size S[j] = s for an arbitrary s.\n\n1.3\n\nApproach\n\nThe main difficulty in analyzing the size of the weights of a threshold element is due\nto the fact that a single linear threshold function can be implemented by different\nsets of weights as shown in the following example.\nExample 1 (A Threshold FUnction with Minimal Weights)\nConsider the following two sets of weights (weight vectors).\n\ntih = (124), FI(X) =\n\n+ 2X2 + 4X3\n2XI + 4X2 + 8X3\nXl\n\nW2 = (248), F2(X) =\nThey both implement the same threshold function\n\nf(X) = sgn(F2(x? = sgn(2FI (x? = sgn(FI (x?\nA closer look reveals that f(x) = sgn(x3), implying that none of the above weight\nvectors has minimal size. Indeed, the minimal one is W3 = (00 1) and S(J] = 1.\nIt is in general difficult to determine if a given set of weights is minimal [Amaldi 93],\n[Willis 63]. Our technique consists of limiting the study to only a particular subset\nof linear threshold functions, a subset for which it is possible to prove that a given\nweight vector is minimal. That subset is loosely defined by the requirement that\nthere exist input vectors for which f(x) = f( -x). The existence of such a vector,\ncalled a root of f, puts a constraint on the weight vector used to implement f. The\nlarger the set of roots - the larger the constraint on the set of weight vectors, which\nin turn helps determine the minimal one. A detailed description of the technique is\ngiven in Section 2.\n1.4\n\nOrganization\n\nHere follows a brief outline of the rest of the paper. Section 2 mathematically defines\nthe setting of the problem as well as derives some basic results on the properties\nof functions that admit roots. Those results are used as bUilding blocks for the\nproof of the main results in Section 3. It also introduces a construction method\nfor functions with minimal weights. Section 3 presents the main result: for any\nweight size, s, and any nunlber of inputs, n, there exists an n-input linear threshold\nfllllction that requires weights of size S[f] = s. Section 4 presents some applications\nof the result of Section 3 and indicates future research directions.\n\n\x0c249\n\nOn Neural Networks with Minimal Weights\n\n2\n\nConstruction of Minimal Threshold Functions\n\nThe present section defines the mathematical tools used to construct functions with\nminimal weights.\n\n2.1\n\nMathematical setting\n\nWe are interested in constructing functions for which the minimal weight is easily\ndetermined. Finding the minimal weight involves a search, we are therefore interested in finding functions with a constrained weight spaces. The following tools\nallows us to put constraints on W.\n\nDefinition 4 (Root Space of a Boolean Function)\nA vector v E {-I, 1} n such that 1(V) = 1(-V) is called a root of I. We define the\nroot space, R, as the set of all roots of I.\nDefinition 5 (Root Generator Matrix)\nFor a given weight vector\nE W and a root v E R, the root generator matrix,\nG = (gij), is a (n x k)-matrix, with entries in {-I, 0,1}, whose rows 9 are orthogonal\nto w and equal to vat all non-zero coordinates, namely,\n\nw\n\n1.\n\nGw = 0\n\n2. 9ij\n\n=\n\n?or\n\n9ij\n\n= Vj\n\nfor all i and j.\n\nExample 2 (Root Generator Matrix)\nSuppose that we are given a linear threshold function specified by a weight\nvector w = (1,1,2,4,1,1,2,4). By inspection we determine one root v =\n(1,1,1,1, -1, -1, -1, -1). Notice that WI + W2 - W7 =\nwhich can be written\nas g. w = 0, where 9 = (1,1,0,0,0,0, -1,0) is a row of G. Set r=\n2g. Since 9\nis equal to vat all non-zero coordinates, r E {-I, I} n. Also r? w = v? w+ g. w = 0.\nWe have generated a new root : r = (-1, -1, 1, 1, -1, -1, 1, -1).\n\n?\n\nLemma 6 (Orthogonality of G and W)\nFor a given weight vector w E Wand a root\nilG T =\n\nv-\n\nvE R\n0\n\nholds for any weight vector il E W.\n\nProof. For an arbitrary il E Wand an arbitrary row, gi, of G, let if = v - 2gi.\nBy definition of gi, if E {-I,1}n and if? w= 0. That implies I(if) = I(-if) : if\nis a root of I. For any weight vector il E W, sgn(il? if) = sgn( -il? if). Therefore\nil? (v - 2gi) = and finally, since v? il = we get il? gi = 0. 0\n\n?\n\n?\n\nLemma 7 (Minimality)\nFor a given weight vector w E W and a root v E R if rank( G) = n - 1 (Le. G\nhas n - 1 independent rows) and IWil = 1 for some i, then w is the minimal weight\nvector.\nProof. From Lemma 6 any weight vector il satisfies ilGT = O. rank( G) = n - 1\nimplies that dim(W) = 1, i.e. all possible weight vectors are integer multiples of\neach other. Since IWi I = 1, all vectors are of the form il = kw, for k ~ 1. Therefore\nhas the smallest size. 0\n\nw\n\nWe complete Example 2 with an application of Lemma 7.\n\n\x0cV. BOHOSSIAN, J. BRUCK\n\n250\n\nExample 3 (Minimality)\nGiven ill = (1,1,2,4,1,1,2,4) and\n\nG=\n\nv = (1,1,1,1, -1, -1, -1, -1) we can construct:\n\n0\n0\n1 0 0 0 -1\n0\n1 0 0\n0 -1\n0 -1\n0 1 0\n0\n0\n0\n0 0 1\n0\n0 -1\n0\n1 0 0 0\n0 -1\n0\n1 1 0 0\n0\n0\n1 1 1 0\n0\n\n0\n0\n0\n\nIt is easy to verify that rank( G) = n - 1\nminimal and 8[/] = 16.\n2.2\n\n=\n\n0\n0\n0\n\n-1\n0\n0\n\n-1\n\n7 and therefore, by Lemma 7, ill is\n\nConstruction of minimal weight vectors\n\nIn Example 3 we saw how, given a weight vector, one can show that it is minimal.\nIn this section we present an example of a linear threshold function with minimal\nweight size, with an arbitrary number of input variables.\nWe would like to construct a weight vector and show that it is minimal. Let\nthe number of inputs, n, be even. Let ill consist of two identical blocks :\n(Wl,W2, ... ,Wn /2,Wl,W2, ... ,Wn /2)\' Clearly, if = (1,1,; .. ,1,-1,-1, ... ,-1) is a root\nand G is the corresponding generator matrix.\n\nG=\n\n3\n\n1 0 0 0\n1 0 0\n0 1 0\n\n0\n0\n0\n\n0\n0\n0\n\n0\n0\n0\n\n-1\n\n0\n\n0\n0\n\n-1\n0\n\n0\n0\n-1\n\n0\n0\n\n0\n0\n\n0\n0\n\n1 0\n0 1\n\n0\n0\n\n0\n0\n\n0\n0\n\n0\n0\n\n0\n\n0\n\n0\n0\n\n0\n\n0\n\n0\n0\n\n0\n\n0\n0\n0\n\n0\n0\n\n0\n0\n\n-1\n\n0\n\n0\n\n0\n0\n0\n\n0\n-1\n\nThe Main Result\n\nThe following theorem states that given an integer s and a number of variables n\nthere exists a function of n variables and minimal weight size s.\nTheorem 8 (Main Result)\nFor any pair (s,n) that satisfies\n, for n even\n, for n odd\n2. seven\n\nthere exists a linear threshold function of n variables,\n8[J] = s.\n\nI, with minimal\n\nweight size\n\nProof. Given a pair (s, n), that satisfies the above conditions we first construct\na weight vector w that satisfies E~l IWil = s, then show that it is the minimal\nweight vector ofthe function I(x) = sgn(w?X). The proof is shown only for n even.\nCONSTRUCTION.\n\n1. Define (at, a2, ... , an /2)\n\n= (1,1, ... , 1).\n\n\x0c251\n\nOn Neural Networks with Minimal Weights\nn/2\n\n. 2\n\n2. If L,:::l a, < s/2 then increase by one the smallest a, such that a, < 2\'- .\n(In the case of a tie take the Wi with smallest index i).\n3. Repeat the previous step until L~; ai\n(1,1,2,4, ... , 2~ - 2).\n4. Set\n\n=\n\ns /2 or (aI, a2, ... , aN) =\n\nw= (al,a2, ... ,a n /2,al,a2, ... ,a n /2)\'\n\nBecause we increase the size by one unit at a time the algorithm will converge to the\ndesired result for any integer s that satisfies n ~ s ~ 2~. We have a construction\nfor any valid (s, n) pair. Let us show that wis minimal.\nGiven that w = (aI, a2, ... , a n /2, aI, a2, ... , aaj2) we find a root v =\n(1, 1, ... , 1, -1, -1, ... , -1) and n/2 rows of the generator matrix G corresponding to\nthe equations w, = wH ~. To form additional rows note that the first k ais are\npowers of two (where k depends on sand n). Those can be written as a, = L~:~ aj\nand generate k - 1 rows. And finally note that all other ai, i > k, are smaller than\n2k+l. Hence, they can be written as a binary expansion a, = L~:::l aijaj where\naij E {O, I}. There are\nk such weights. G has a total of n -1 independent rows.\nrank(G) = n -1 and WI = 1, therefore by Lemma 7, tV is minimal and S[J] = s. 0\nMINIMALITY.\n\n-r -\n\nExample 4 (A Function of 10 variables and size S[fJ = 26)\nWe start with a = (1,1,1,1,1). We iterate: (1,1,2,1,1), (1,1,2,2,1), (1,1,2,2,2),\n(1,1,2, 3,2), (1,1,2,3,3) , (1,1,2,4,3), (1,1,2,4,4), and finally (1,1 , 2,4,5). The\nconstruction algorithm converges to a = (1,1,2,4,5). We claim that tV = (a, a) =\n(1,1,2,4,5,1,1,2,4,5) is minimal. Indeed, v = (1,1,1,1,1, -1, -1, -1, -1, -1) and\n1 0 0 0\n1 0 0\n0 1 0\n0 0 1\n\nG=\n\n0\n0\n0\n0\n\n0\n0\n0\n0\n\n0 1\n0 0\n0 0\n1\n1 1 1 0 0\n1 0 0 1 0\n0\n1 0\n\n0\n0\n1 0\n\n-1\n\n0\n\n0\n0\n0\n0\n0\n0\n0\n0\n\n-1\n0\n0\n0\n\n-1\n0\n0\n0\n\n0\n0\n\n-1\n0\n0\n0\n\n-1\n0\n0\n\n0\n0\n0\n\n-1\n0\n0\n0\n\n-1\n0\n\n0\n0\n0\n0\n-1\n0\n0\n0\n-1\n\nis a matrix of rank 9.\n\nExample 5 (Functions with Polynomial Size)\nThis example shows an application of Theorem 8. We define fred) as the set of\nlinear threshold functions for which S[I} ~ n d ? The Theorem states that for any\neven n there exists a function 1 of n variables and minimum weight S[I] = n d ? The\n- - (d- I)\n\nimplication is that for all d, LT\n\n4\n\n- - (d)\n\nis a proper subset of LT\n\nConclusions\n\nWe have shown that for any reasonable pair of integers (n, s), where s is even, there\nexists a linear threshold function of n variables with minimal weight size S[J} = s.\nWe have developed a novel technique for constructing linear threshold functions\nwith minimal weights that is based on the existence of root vectors. An interesting\napplication of our method is the computation of a lower bound on the number\nof linear threshold functions [Smith 66}. In addition, our technique can help in\nstudying the trade-otIs between a number of important parameters associated with\n\n\x0c252\n\nV. BOHOSSIAN, 1. BRUCK\n\nlinear threshold (neural) circuits, including, the munber of elements, the number of\nlayers, the fan-in, fan-out and the size of the weights.\n\nAcknow ledgements\nThis work was supported in part by the NSF Young Investigator Award CCR9457811, by the Sloan Research Fellowship, by a grant from the IBM Almaden\nResearch Center, San Jose, California, by a grant from the AT&T Foundation and\nby the center for Neuromorphic Systems Engineering as a part of the National\nScience Foundation Engineering Research Center Program; and by the California\nTrade and Commerce Agency, Office of Strategic Technology.\n\nReferences\n[Amaldi 93] E. Amaldi and V. Kann. The complexity andapproximabilityoffinding\nmaximum feasible subsystems of linear relations. Ecole Polytechnique Federale\nDe Lausanne Technical Report, ORWP 93/11, August 1993.\n[Goldmann 92] M. Goldmann, J. Hastad, and A. Razborov. Majority gates vs. general weighted threshold gates. Computational Complexity, (2):277-300, 1992.\n[Goldman 93] M. Goldmann and M. Karpinski. Simulating threshold circuits by\nmajority circuits. In Proc. 25th ACM STOC, pages pp. 551- 560, 1993.\n[Hastad 94] .1. Hastad. On the size of weights for threshold gates. SIAM. J. Disc.\nMath., 7:484-492, 1994.\n[Hopfield 82) .1. Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proc. of the USA National Academy of Sciences,\n79:2554- 2558, 1982.\n[Muroga 71) M. Muroga. Threshold Logic and its Applications. Wiley-Interscience,\n1971.\n[Myhill 61) J. Myhill and W. H. Kautz. On the size of weights required for linearinput switching functions. IRE Trans. Electronic Computers, (EClO):pp. 288290, 1961.\n[Raghavan 88] P. Raghavan. Learning in threshold networks: a computational\nmodel and applications. Technical Report RC 13859, IBM Research, July\n1988.\n[Rumelhart 82] D. Rumelhart and J. McClelland. Parallel distributed processing:\nExplorations in the microstructure of cognition. MIT Press, 1982.\n[Shawe-Taylor 92] J. S. Shawe-Taylor, M. H. G. Anthony, and W. Kern. Classes\nof feedforward neural networks and their circuit complexity. Neural Networks,\nVol. 5:pp. 971- 977, 1992.\n[Siu 91] K. Siu and J. Bruck. On the power of threshold circuits with small weights.\nSIAM J. Disc. Math., Vol. 4(No. 3):pp. 423-435, August 1991.\n[Smith 66] D. R. Smith. Bounds on the number of threshold functions. IEEE\nTransactions on Electronic Computers, June 1966.\n[Willis 63] D. G. Willis. Minimum weights for threshold switches. In Switching\nTheory in Space Techniques. Stanford University Press, Stanford, Calif., 1963.\n\n\x0c'
p83147
sS'1085'
p83148
S'?\n\n?\n\n\x14\x13\n\n\x01 \x02\x04\x03\x06\x05\x08\x07\n\t\x0c\x0b\x0e\n\x10\x0f\x12\x11\n(\n?!?\n\n\x01 )\x04* \x05\n+\n\n,\n?\n\n\x01 )-\x13\n\n\x15\x17\x16\x19\x18 \x0b\n\x0b.\x070/\n\n\x07\x1f\x1e\x08\x05\n\n\x0b\n\n'
p83149
sg283
S'High-Speed Airborne Particle Monitoring\nUsing Artificial Neural Networks\n\nAlistair Ferguson\nERDC, Univ. of Hertfordshire\nA.Ferguson@herts.ac.uk\n\nTheo Sabisch\nDept. Electrical and Electronic Eng.\nUniv. of Hertfordshire\n\nPaul Kaye\nERDC, Univ. of Hertfordshire\n\nLaurence C. Dixon\nNOC, Univ. of Hertfordshire\n\nHamid Bolouri\nERDC, Univ. of Hertfordshire, Herts, ALtO 9AB, UK\n\nAbstract\nCurrent environmental monitoring systems assume particles to be\nspherical, and do not attempt to classify them. A laser-based system developed at the University of Hertfordshire aims at classifying airborne particles through the generation of two-dimensional\nscattering profiles. The pedormances of template matching, and\ntwo types of neural network (HyperNet and semi-linear units) are\ncompared for image classification. The neural network approach is\nshown to be capable of comparable recognition pedormance, while\noffering a number of advantages over template matching.\n\n1\n\nIntroduction\n\nReliable identification of low concentrations of airborne particles requires high speed\nmonitoring of large volumes of air, and incurs heavy computational overheads. An\ninstrument to detect particle shape and size from spatial light scattering profiles has\n\n\x0cHigh-speed Airborne Particle Monitoring Using Artificial Neural Networks\n\n981\n\npreviously been described [6]. The system constrains individual particles to traverse\na laser beam. Thus, spatial distributions of the light scattered by individual particles\nmay be recorded as two dimensional grey-scale images.\nDue to their highly distributed nature, Artificial Neural Networks (ANNs) offer the\npossibility of high-speed non-linear pattern classification. Their use in particulate\nclassification has already been investigated. The work by Kohlus [7] used contour\ndata extracted from microscopic images of particles, and so was not real-time. While\nusing laser scattering data to allow real-time analysis, Bevan [2] used only three\nphotomultipliers, from which very little shape information can be collected.\nThis paper demonstrates the plausibility of particle classification based on shape\nrecognition using an ANN. While capable of similar recognition rates, the neural\nnetworks are shown to offer a number of advantages over template matching.\n\n2\n\nThe HyperN et Architecture\n\nHyperNet is the term used to denote the hardware model of a RAM-based sigma-pi\nneural architecture developed by Gurney [5]. The architecture is similar in nature\nto the pRAM of Gorse and Taylor (references in [4]). The amenability of these\nnodes to hardware realisation has been extensively investigated, leading to custom\nVLSI implementations of both nodes [3, 4]. Each HyperNet node is termed a multicube unit (MeU), and consists of a number of subunits, each with an arbitrary\nnumber of inputs. j references the nodes, with i = 1, ... ,Ii indexing the subunits.\nIJ denotes the site addresses, and is the set of bit strings 1J1, ... ,lJn wl1ere n denotes\nthe number of inputs to the subunit. Zc refers to the cth real-valued input, with\nZc E [0,1] and Zc == (1 - zc). For each of the 2n site store locations, two sets are\nif IJc = 0; c E\nif IJc = 1. The access probability p(lJii) for\ndefined: c E\nlocation IJ in subunit i of hidden layer node j is therefore\n\nM:!o\n\nM:!t\n\n(1)\n\nThe activation (ai ) is formed by accumulating the proportional site values (SIS\';)\nfrom every subunit. The activation is then passed through a sigmoidal transfer\nfunction to yield the node output (yi).\n(2)\n.\n.\ny\' = u(a1 ) =\n\n1\n\n.\n1 + e a1 / p\n\n(3)\n\nwhere p is a positive parameter determining the steepness of the sigmoidal curve.\nBy combining equations (1) and (2), it becomes apparent that the node is a higherorder or sigma-pi node [9]. A wide variety of learning algorithms have been tailored\nfor these nodes, notably reward-penalty and back-propagation [5].\n\n\x0c982\n\n3\n\nA. FERGUSON, T. SABISCH, P. KAYE. L. C. DIXON. H. BOWURJ\n\nDescription of the Particle Monitoring System\n\nThe instrument draws air through the laser scattering chamber at approximately\n1.5 min- 1 , and is constrained to a column of approximately 0.8mm diameter at the\nintersection with the laser beam. Light scattered into angles between 300 and 141 0\nto the beam direction is reflected through the optics and onto the photocathode of\nan intensified CCD (charge-coupled device), thus giving rise to the scattering profile.\nThe imaging device used has a pixel resolution of 385 x 288, which is quantised into\n2562 8-bit pixels by the frame grabbing processor card of the host computer.\nData was collected on eight particle types, namely: long and short caffeine fibres;\n31lm and 121lm micro-machined silicon dioxide fibres; copper flakes (2- 5Ilm in length\nand O.lllm thick); 31lm and 4.31lm polystyrene spheres; and salt crystals. An exemplar profile for each class is given in figure 1. Almost all the image types are highly\nvariable. In particular, the scattering profile obtained for a fibrous particle is affected by its orientation as it passes through the laser beam. The scattering profiles\nare intrinsically centred, with the scaling giving important information regarding\nthe size of the particle. The experiments reported here use 100 example scattering\nprofiles for each of the eight particle classes. For each class, 50 randomly selected\nimages were used to construct the templates or train the neural network (training\nset), and the remainder used to test the performance of the pattern classifiers.\n\n4\n\nExperimental Results\n\nThe performance of template matching is compared to both HyperNet and networks\nof semi-linear units. In all experiments, high-speed classification is emphasised by\n\n?\n~.\n\n~\n\n" l,: ~t..\n,\n\n?\'.,\n\n..\n\n.\n\n. ,\n\n~..,\n\n.\nt \',\n\n,\n\n.\n\n\'.\n\n\'l.J;\n\nFigure 1: Exemplar Image Profile For Each Of The Eight Benchmark Classes\n\n\x0cHigh-speed Airborne Particle Monitoring Using Artificial Neural Networks\n\n983\n\navoiding image preprocessing operations such as transformation to the frequency\ndomain, histogram equalisation, and other filtering operations. Furthermore, all\nexperiments use the scatter profile image as input, and include no other information.\nThe current monitoring system produces a 2562 8-bit pixel image. The sensitivity of\nthe camera is such that a single pixel can represent the registration of a single photon\nof light. Two possible methods of reducing computation, implementable through\nthe use of a cheaper, less sensitive camera were investigated. The first grouped\nneighbouring pixels to form a single average intensity value. The neighbourhood\nsize was restricted to powers of two, producing images ranging in size from 2562 to\n42 pixels. The second banded grey levels into groups, again in powers of two. Each\npixel could therefore range from eight bits down to one.\n\n4.1\n\nTemplate Matching Results\n\nThe construction of reference templates is crucial to successful classification. Two\napproaches to template construction were investigated\n\n<D Single reference image for each class. Various techniques were applied ranging from individual images, to mode, median, and mean averaged templates.\nMean averaged templates were found to lead to the highest classification\nrates. In this approach, each pixel location in the template takes on the\naveraged value of that location across the 50 training images.\n? Multiple templates per class. A K-means clustering algorithm [1] was used\nto identify clusters of highly correlated images within each class. The initial\ncluster centres were hand selected. The maximum number of clusters within\neach class was limited to six. For each cluster, the reference template was\nconstructed using the mean averaging approach above.\nTables 1 and 2 summarise the recognition rates achieved using single, and multiple\nmean averaged templates for each particle class. In both cases, the best average\nrecognition rate using this approach was gained with 1282 3-bit pixel images. With\na single template this lead to a recognition rate of 78.2%, increasing to 85.2% for\nmultiple templates. However, the results for both 162 and 82 pixel images are\nreasonable approximations of the best performance, and represent an acceptable\ntrade-off between computational cost and performance. With few exceptions, multiple templates per class led to higher recognition rates than for the corresponding\nsingle template results. This is attributable to the variability of the particles within\na class. As expected, the effect of grey level quantisation is inversely proportional\nto that of local averaging.\nIn order to evaluate the efficiency of the template construction methods, every image\n\nin the training set was used as a reference template. 2562 8-bit, 1282 3-bit, and 642\n2-bit pixel images were used for these experiments. However, the recognition rate\ndid not exceed 85%, demonstrating the success of the template generation schemes\npreviously employed.\n\n\x0c984\n\nA. FERGUSON, T. SABISCH, P. KAYE, L. C. DIXON, H. BOLOURI\n\nTable 1: Single Template Per Class % Recognition Rates\nimage size\ngrey levels\n42\n2562 1282\n64 2\n32 2\n162\n82\n73.5 75.0 74.7 74.7 74.7 75.0 67.2\n256\n128\n73.5 75.0 74.7 74.7 74.5 75.0 68.5\n64\n73.0 75.0 74.5 74.5 74.2 74.7 66.2\n32\n73.0 74.7 75.2 75.5 74.7 74.2 66.5\n16\n74.0 76.0 76.7 76.0 75.0 15.5 56.0\n15.5 18.2 11.5 11.5 16.0 73.7 38.7\n8\n68.4\n69.7 71.0 70.7 69.7 58.5 18.7\n4\n2\n69.7 68.7 65.5 66.2 46.2 23.0 16.6\n\nTable 2: Multiple Templates Per Class % Recognition Rates\nimage size\ngrey levels\n256 2 1282\n64 2\n32 2\n162\n42\n82\n78.0 80.0 80.2 80.5 79.0 76.7 10.2\n256\n128\n78.5 80.2 80.5 80.5 79.0 77.0 69.7\n64\n78.7 80.2 80.2 80.5 79.2 76.0 69.2\n78.2 81.2 81.7 80.0 78.7 76.7 67.7\n32\n80.2 83.5 83.0 81.2 79.5 78.5 56.0\n16\n8\n82.2 85.2 84.5 84.1 81.0 80.0 43.5\n4\n72.7 74.5 72.2 72.2 69.5 61.2 39.2\n69.7 70.2 70.7 62.7 51.7 51.7 0.03\n2\n\n4.2\n\nNeural Network Results\n\nA fully connected three layer feed-forward network was used in all experiments. The\nnumber of hidden layer neurons was equal to the square root of the number of pixels.\nThe target patterns were chosen to minimise the number of output layer nodes, while\nensuring an equitable distribution of zeros and ones. Six output layer neurons were\nused to give a minimum Hamming distance of two between target patterns. The\nclassification of a pattern was judged to be the particle class whose target pattern\nwas closest (lowest difference error). The HyperNet architecture was trained using\nsteepest descent, though the line search was hardware based and inexact. The semilinear network was trained using a variety of back-propagation type algorithms, with\nthe best results obtained reported. Both networks were randomly initialised. Due\nto the enormous training overhead, only 162 and 82 pixel images were tried. The\nrecognition rates achieved are given in table 3.\nBoth neural networks are significantly better than the single, and some of the multiple template matching results. With optimisation of the network structures, it is\nlikely that the ANNs could exceed the performance of multiple templates.\n\n\x0cHigh-speed Airborne Particle Monitoring Using Artificial Neural Networks\n\n985\n\nTable 3: Neural Network % Recognition Rates\nQuantisation Levels\nClassifier\n162 4 bit 1162 3-bit 82 4-bit 82 3-bit\nHyperNet\n82.3\n83.0\n76.8\n83.8\nSemi-linear\n84.5\n76.0\n86.3\n77.8\n\nI\n\nle+09\n\n,\n,,\n\n,-.. le+08\n\'"c::\n\'-"\n\n,,\n\n,\n\n~ Ie+{)?\n\no~\n\nll!\n0;;\n\n\'"\n8\n\n~\n\nle+06\n\nle+05\nle+04\n\n82\n\n162\n\n322\n\n642\n\n1282\n\n2562\n\nNumber of pixels\n\nFigure 2: Hardware classification speeds for a single pattern against image size\n\n5\n\nSpeed Considerations\n\nSingle processor, pipelined hardware implementations of the three classification\ntechniques have been considered. A fast (45ns) multiply-accumulate chip (Logic\nDevices Ltd, LMA201O) was utilised for semi-linear units. Both template matching\nand HyperNet were implemented using the Logic Devices LGC381 ALU (26ns per\naccumulate). The cost of these devices is approximately the same (?10-20). The\nHyperNet implementation uses a bit-stream approach to eliminate the probability\nmultiplications [8], with a stream length of 256 bits. Figure 2 plots single pattern\nprocessing time for each classifier against image size.\nFor small image resolutions, the semi-linear network offers the best performance, being almost three times faster than template matching. However, template matching\nand HyperNet yield faster performance at higher image resolutions. At the optimum (indicated by template matching results (?4.1); 1282 pixels), HyperNet is\nalmost seven times faster than the comparable implementation of semi-linear units.\nWhile the hardware performance of template matching is similar to HyperNet, it\nsuffers from a number of disadvantages to which the neural approaches are immune\nCD Recognition rate is dependent on the choice of reference images.\n\n? Multiple reference images must be used to achieve good recognition rates\n\n\x0c986\n\nA. FERGUSON, T. SABISCH, P. KAYE, L. C. DIXON, H. BOLOURI\n\nwhich drastically increases the amount of computation required.\n\n6\n\n@\n\nNew reference images must be found whenever a new class is introduced.\n\n@\n\nDifficult to make behaviour adaptive, ie. respond to changing conditions.\n\nConclusions\n\nThe feasibility of constructing an airborne particle monitoring system capable of\nreliable particle identification at high speeds has been demonstrated. Template\nmatching requires multiple reference images and is cumbersome to develop. The\nneural networks offer easier training procedures and equivalent recognition rates. In\naddition, HyperNet has the advantage of high speed operation at large image sizes.\nAcknowledgements\nThe authors would like to thank Dr. Eric Dykes and Dr. Edwin Hirst at the University of Hertfordshire, Dr. Kevin Gurney at BruneI University, and the EPSRC\nand the Royal Society for financial support.\n\nReferences\n[1] Stephen Banks. Signal Processing, Image Processing, and Pattern Recognition.\nPrentice Hall, 1990.\n[2] A V Bevan et al. The application of neural networks to particle shape classification. Journal of Aerosol Science, 23(Suppl. 1):329-332, 1992.\n[3] Hamid Bolouri et al. Design, manufacture, and evaluation of a scalable highperformance neural system. Electronics Letters, 30(5):426-427, 3 March 1994.\n[4] T G Clarkson et al. The pRAM: An adaptive VLSI chip. IEEE \'Ihmsactions\non Neural Networks, 4(3):408-412, May 1993.\n[5] Kevin N Gurney. Learning in networks of structured hypercubes. PhD thesis,\nDepartment of Electrical Engineering, UK, 1995.\n[6] Paul H Kaye et al. Airborne particle shape and size classification from spatial\nlight scattering profiles. Journal of Aerosol Science, 23(6):597--611, 1992.\n[7] R Kohlus et al. Particle shape analysis as an example of knowledge extraction\nby neural nets. Part. Part. Syst. Charact., 10:275-278, 1993.\n[8] Paul Morgan et al. Hardware implementation of a real-valued sigma-pi network.\nIn Artificial Neural Networks 5, volume 2, pages 351-356, North-Holland, 1995.\n[9] David E Rumelhart et al. Parallel Distributed Processing: Explorations in the\nMacrostructure of Cognition, volume 1. MIT Press, 1986.\n\n\x0cPART IX\nCONTROL\n\n\x0c\x0c'
p83150
sg70
S'Unsupervised Pixel-prediction\n\nWilliam R. Softky\nMath Resp.arch Branch\nNIDDK, NIH\n9190 Wisconsin Ave #350\nBethesda, MD 20814\nbill@homer.niddk.nih.gov\n\nAbstract\nWhen a sensory system constructs a model of the environment\nfrom its input, it might need to verify the model\'s accuracy. One\nmethod of verification is multivariate time-series prediction: a good\nmodel could predict the near-future activity of its inputs, much\nas a good scientific theory predicts future data. Such a predicting model would require copious top-down connections to compare\nthe predictions with the input. That feedback could improve the\nmodel\'s performance in two ways : by biasing internal activity toward expected patterns, and by generating specific error signals if\nthe predictions fail. A proof-of-concept model-an event-driven,\ncomputationally efficient layered network, incorporating "cortical"\nfeatures like all-excitatory synapses and local inhibition- was constructed to make near-future predictions of a simple, moving stimulus. After unsupervised learning, the network contained units not\nonly tuned to obvious features of the stimulus like contour orientation and motion, but also to contour discontinuity ("end-stopping")\nand illusory contours.\n\n1\n\nIntroduction\n\nSomehow, brains make very accurate models of the outside world from their raw\nsensory input. How might brains check and improve those models? What signal is\nthere to verify a model of the world?\nThe scientific method faces a similar problem: how to verify theories. In science,\ntheories are verified by predicting future data, using the implicit assumption that\n\n\x0cW.R.SOFfKY\n\n810\n\ngood predictions can only result from good models. By analogy, it is possible that\nbrains predict their afferent input (e.g. at the thalamus), and that making such\npredictions and using them as feedback is a unifying design principle of cortex.\nThe proof-of-concept model presented here uses unsupervised Hebbian learning to\npredict, pixel-wise, the location of a moving pattern slightly in the future.\nWhy try prediction?\n? Predicting future data usually requires a good generative model. For instance: to\npredict the brightness of individual TV pixels even a fraction of a second in advance,\none would need models of contours, objects, motion, occlusion, shadow, etc.\n? A successful prediction can help filter out input noise, like a Kalman filter.\n? A failed prediction provides a specific, high-dimensional error signal.\n? Prediction is not only possible in cortex-which has massive feedback\nconnections-but necessary as well, because those feedback fibers, their target dendrites, and synaptic integration impose inevitable delays. So for a feedback signal\nto arrive at the cell body "on time," it would need to have been generated tens of\nmilliseconds earlier, as a prediction of imminent activity.\n\n? In this model, "prediction" means producing spikes in advance which will correlate\nwith subsequent input spikes. Specifically, the network\'s goal is to produce at each\ngrid point a train of spikes at times Pj which predicts the input train Ik, in the\nsense of maximizing their normalized cross-correlation. The objective function L\n("likeness") can be expressed in terms of a smoothing "bump" function B(t:J;, ty)\n(of spikes at times t:J; and ty) and a correlation function C(trainl, train2, ~t):\nexp ( -It:J;T- t y l )\nC(P,I,\n\n~T)\n\nL: L: B(P + ~t, Ik)\nj\n\nj\n\nL(P,I,~T)\n\nk\n\nC(P, I,\n\n~T)\n\nJC(P, P, O)C(I, 1,0)\n\n? In order to avoid a trivial but useless prediction ("the weather tomorrow will be\njust like today;\'), one must ensure that a unit cannot usually predict its own firing\n(for example, pick ~t ~ T greater than the autocorrelation time of a spike train).\n\n2\n\nModel\n\nThe input to the network is a 16 x 16 array of spike trains, with toroidal array\nboundary conditions. The spikes are driven by a "stimulus" bar of excitation one\nunit wide and seven units long, which moves smoothly perpendicular to its orientation behind the array (in a broad circle, so that all orientations and directions\nare represented; Fig. 1A). The stimulus point transiently generates spikes at each\ngrid point there according to a Poisson process: the whole array of spikes can be\nvisualized as a twinkling, moving contour.\n\n\x0cUnsupervised Pixel-prediction\n\nA\n\n811\n\nB\n\ntrigger &\nforward\nhelper\n\nt ----\n\nsyn~\n\n_\n\n_\n\n/.. _\n_\n_\n_\n_\ninputs\n\n_\n\n_\n\ndelay\n\n_\n\n_\n\ntuned, precise, predictive\nfeedback\n\nFigure 1: A network predicts dynamic patterns. A A moving pattern on\na grid of spiking pixels describes a slow circle, and drives activity in a network\nabove. B The three-layer network learns to predict that activity just before it\noccurs. Forward connections, evolving by Hebbian rules, produce top-level units\nwith coarse receptive fields and fine stimulus-tuning (e.g. contour orientation and\nmotion). Each spike from a top unit is "bound" (by coincidence detection) with\nthe particular spike which triggered it, to produce feedback which is both stimulustuned and spatially specific. A Hebb rule determines how the delayed, predictive\nfeedback will drive middle-layer units and be compared to input-layer units. Because\nall connections are excitatory, winner-take-all inhibition within local groups of units\nprevents runaway excitation.\n2.1\n\nNetwork Structure\n\nThe network has three layers. The bottom layer contains the spiking pixels, and\nthe "surprise" units described below. The middle layer, having the same spatial\nresolution as the input, has four coarsely-tuned units per input pixel. And the\ntop layer contains the most finely-tuned units, spaced at half the spatial resolution\n(at every fourth gridpoint, i.e. with coarser spatial resolution and larger receptive\nfields). The signal flow is bi-directional [10, 7], with both forward and feedback\nsynaptic connections. All connections between units are excitatory, and excitation\nis kept in check by local winner-take-all inhibition (WTA). For example, a given\ninput spike can only trigger one spike out of the 16 units directly above it in the\ntop layer (Fig. IB).\nUnsupervised learning occurs through two local Hebb-like rules. Forward connections evolve to make nearby (competing) units strongly anticorrelated-for instance,\nunits typically become tuned to different contour orientations and directions of\nmotion-while feedback connections evolve to maximally correlate delayed feedback\nsignals with their targets.\n2.2\n\nBinary multiplication in single units\n\nWhile some neural models implement multiplication as a nonlinear function of the\nsum of the inputs, the spiking model used here implements multiplication as a\nbinary operation on two distinct classes of synapses.\n\n\x0cW.R.SOFrKY\n\n812\n\nA\n\n~\'I~ ~ ~\n\nhelper\n"helper"\n\ninh\n\ntrigger\n\nI I I\n\ncomc.\n\ndetector delay\n\nB\nprediction\nof X\n\nI\n\n.m~\nV\n\n-\n\n-\n\n---\n\n*--------\n\nI\n\nout\n\nFigure 2: Multiplicative synapses and surprise detection. A A spiking unit\nmultiplies two types of synaptic inputs: the "helper" type increments an internal\nbias without triggering a spike, and the "trigger" type can trigger a spike (*),\nwithout incrementing, but only if the bias is above a threshold. Spike propagation\nmay be discretely delayed, and coincidences of two units fired by the same input\nspike can be detected. B Once the network has generated a (delayed) prediction of\na given pixel\'s activity, the match of prediction and reality can be tested by specialpurpose units: one type which detects unpredicted input, the other which detects\nunfulfilled predictions. The firing of either type can drive the network\'s learning\nrules, so units above can become tuned to consistent patters of failed predictions,\nas occur at discontinuities and illusory contours.\n\nA helper synapse, when activated by a presynaptic spike, will increment or decrement the postsynaptic voltage without ever initiating a spike. A trigger synapse, on\nthe other hand, can initiate a spike (if the voltage is above the threshold determined\nby its WTA neighbors), but cannot adjust the voltage (Fig. 2A; the helper type is\nloosely based on the weak, slow NMDA synapses on cortical apical dendrites, while\ntriggers are based on strong, brief AMPA synapses on basal dendrites.) Thus, a\nunit can only fire when both synaptic types are active, so the output firing rate\napproximates the product of the rates of helpers and triggers. Each unit has two\ncharacteristic timescales: a slower voltage decay time, and the essentially instantaneous time necessary to trigger and propagate a spike.\nThis scheme has two advantages. One is that a single cell can implement a relatively\n"pure" multiplication of distinct inputs, as required for computations like motiondetection. The other advantage is that feedback signals, restricted to only helper\nsynapses, cannot by themselves drive a cell, so closed positive-feedback loops cannot\n"latch" the network into a fixed state, independent of the input. Therefore, all\ntrigger synapses in this network are forward, while all delayed, lateral, and feedback\nconnections are of the helper type .\n2.3\n\nFeedback\n\nThere are two issues in feedback: How to construct tuned, specific feedback, and\nwhat to do with the feedback where it arrives.\n\n\x0cUnsupervised Pixel-prediction\n\n813\n\nAn accurate prediction requires information about the input: both about its exact\npresent state, and about its history over nearby space and recent time. In this model,\nthose signals are distinct: spatial and temporal specificity is given by each input\nspike, and the spatia-temporal history is given by the stimulus-tuned responses of\nthe slow, coarse-grained units in the top layer. Spatially-precise feedback requires\nrecombining those signals. (Feedback from V1 cortical Layer VI to thalamus has\nrecently been shown to fit these criteria, being both spatially refined and directionselective; [3] Grieve & Sillito, 1995).\nIn this network, each feedback signal results from the AND of spikes from a inputlayer spike (spatially specific) and the resulting top-layer spike it produces (stimulustuned). This "binding" across levels of specificity requires single-spike temporal\nprecision, and may even be one of the perceptual uses for spike timing in cortex\n\n[1, 9].\n2.4\n\nSurprise detection\n\nOnce predictive feedback is learned, it can be used in two ways: biasing units toward\nexpected activity, and comparing predictions against actual input. Feedback to the\nmiddle layer is used as a bias signal through helper synapses, by adding the feedback\nto the bias signal. But feedback to the bottom , input-layer is compared with actual\ninput by means of special "surprise" units which subtract prediction from input\n(and vice versa).\nBecause both prediction and input are noisy signals, their difference is even noisier,\nand must be both temporally smoothed and thresholded to generate a mismatchspike. In this model , these prediction/input differences are accomplished pixel-bypixel using ad-hoc units designed for the purpose (Fig. 2B). There is no indication\nthat cortex operates so simplistically, but there are indications that cortical cells\nare in general sensitive to mismatches between expectation and reality, such as\ndiscontinuities in space (edges) , in time (on- and off-responses), and in context\n(saliency) .\nThe resulting error vector can drive upper-layer units just as the input does, so that\nthe network can learn patterns of failed predictions, which typically correspond to\ndiscontinuities in the stimulus. Learning consistent patterns of bad predictions is\na completely generic recipe for discovering such discontinuitites, which often correspond closely to visually important features like contour ends, corners, illusory\ncontours, and occlusion .\n\n3\n\nResults and Discussion\n\nAfter prolonged exposure to the stimulus, the network produces a blurred cloud of\nspikes which anticipates the actual input spikes, but which also consistently predicts\ninput beyond the bar\'s ends (leading to small clouds of surprise-unit activity tracking the ends). The top-level units, driven both by input signals and by feedback ,\nbecome tuned either to different motions of the bar itself (due to Hebbian learning\nof the input), or to different motions of its ends (due to Hebbian learning of the\nsurprise-units); see Fig. 3. Cells tuned to contour ends ( "end-stopped") have been\nfound in visual cortex [11], although the principles of their genesis are not known .\nUsing the same parameters but a different stimlus, the network can also evolve units\n\n\x0cW.R.SOFfKY\n\n814\n38\n\n36\n\n34\n32\n30\nCD\n\n........- ...\nXX>IMIOQO(\n\n28\n26\n\na. 24\n~ 22\nC\\I 20\nQ;~ 1186\n...J 14\n12\n10\n\n8\n6\n4\n2\n14\nCD 12\n~ 10\n8\n\nQ; 6\n4\n\n!\n\n2\n\n8+t!tHIIIIII__IIIH!Io\'\n+ <H-$iIII-\n\nx x x _\n\n*- -\n\n\\=.\n\n\\= +\n\'\\=\n\nx\n\n--.III.BijMl!mj.\n+\n\n?_\n\n???\n\nX\n\n*\n>MC1M_cc_IIf-1\n\nt\n\n~\n\no\nFigure 3: Single units are highly stimulus-specific. Spikes from all units at\none location are shown (with time) as a stimlus bar (insets) passes them with six\ndifferent relative positions and motions . Out of the many units available, only one\nor two are active in each layer for a given stimulus configuration. The inactive\nunits are tuned to stimulus orientations not shown here. Some units are driven by\n"surprise" units (Figure 2 and text), and respond only to the bar\'s ends (. and x),\nbut not to its center (+). Such responses lag behind those of ordinary units, because\nthey must temporally integrate to determine whether a significant mismatch exists\nbetween the noisy prediction and the noisy input. Spikes from five passes have been\nsummed to show the units\' reliability.\n\nwhich detect the illusory contours present in certain moving gratings.\nSeveral researchers propose that cortex (or similar networks) might use feedback\npathways to recreate or regenerate their (static) input [7,4, 10]. The approach here\nrequires instead that the network forecast future (dynamic) input [8] . In a general\nsense, predicting the future is a better test of a model than predicting the present,\nin the same sense that scientific theories which predict future experimental data are\nmore persuasive than theories which predict existing data. Prediction of the raw\ninput has advantages over prediction of some higher-level signal [5, 6, 2]: the raw\ninput is the only unprocessed "reality" available to the network, and comparing the\nprediction with that raw input yields the highest-dimensional error vector possible.\nSpiking networks are likewise useful. As in cortex, spikes both truncate small inputs\nand contaminate them with quantization-noise, crucial practical problems which\nreal-valued networks avoid. Spike-driven units can implement purely correlative\ncomputations like motion-detection, and can avoid parasitic positive-feedback loops.\nSpike timing can identify which of many possible inputs fired a given unit, thereby\nmaking possible a more specific feedback signal. The most practical benefit is that\ninteractions among rare events (like spikes) are much faster to compute than real-\n\n\x0cUnsupervised Pixel-prediction\n\n815\n\nvalued ones; this particular network of 8000 units and 200,000 synapses runs faster\nthan the workstation can display it.\nThis model is an ad-hoc network to illustrate some of the issues a brain might face\nin trying to predict its retinal inputs; it is not a model of cortex. Unfortunately, the\nhypothesis that cortex predicts its own inputs does not suggest any specific circuit\nor model to test. But two experimental tests may be sufficiently model-independent.\nOne is that cortical "non-classical" receptive fields should have a temporal structure\nwhich reflects the temporal sequences of natural stimuli, so a given cell\'s activity will\nbe either enhanced or suppressed when its input matches contextual expectations.\nAnother is that feedback to a single cell in thalamus, or to an individual cortical\napical dendrite, should arrive on average earlier than afferent input to the same\ncell.\n\nReferences\n[1] A. Engel , P. Koenig, A. Kreiter, T. Schillen, and W. Singer. Temporal coding\nin the visual cortex: New vistas on integration in the nervous system. TINS,\n15:218-226, 1992.\n[2] K. Fielding and D. Ruck. Recognition of moving light displays using hidden\nmarkov models. Pattern Recognition, 28:1415-1421,1995.\n[3] K. 1. Grieve and A. M. Sillito. Differential properties of cells in the feline\nprimary visual cortex providing the cortifugal feedback to the lateral geniculate\nnucleus and visual claustrum. J. Neurosci., 15:4868-4874,1995.\n[4] G. Hinton, P. Dayan, B. Frey, and R. Neal. The wake-sleep algorithm for\nunsupervised neural networks. Science, 268:1158-1161,1995.\n[5] P. R. Montague and T. Sejnowski. The predictive brain: Temporal coincidence\nand? temporal order in synaptic learning mechanisms. Learning and Memory,\n1:1-33, 1994.\n[6] P. Read Montague, Peter Dayan, Christophe Person, and T. Sejnowski. Bee\nforaging in uncertain environments using predictive hebbian learning. Nature,\n377:725-728, 1995.\n[7] D. Mumford . Neuronal architectures for pattern-theoretic problems. In C. Koch\nand J. Davis, editors, Large-scale theories of the cortex, pages 125-152. MIT\nPress, 1994.\n[8] W. Softky. Could time-series prediction assist visual processing? Soc. Neurosci.\nAbstracts, 21:1499, 1995.\n[9] W. Softky. Simple codes vs. efficient codes. Current Opinion in Neurbiology,\n5:239-247, 1995.\n\n[10] S. Ullman. Sequence-seeking and counterstreams: a model for bidirectional information flow in cortex. In C. Koch and J . Davis, editors, Large-scale theories\nof the cortex, pages 257-270. MIT Press, 1994.\n[11] S. Zucker, A. Dobbins, and L. Iverson. Two stages of curve detection suggest\ntwo styles of visual computation. Neural Computation, 1:68-81, 1989.\n\n\x0c'
p83151
sg26
S'Prediction of Beta Sheets in Proteins\nAnders Krogh\nThe Sanger Centre\nHinxton, Carobs CBIO IRQ, UK.\nEmail: krogh@sanger.ac. uk\n\nS~ren Kamaric Riis\nElectronics Institute, Building 349\nTechnical University of Denmark\n2800 Lyngby, Denmark\nEmail: riis@ei.dtu.dk\n\nAbstract\nMost current methods for prediction of protein secondary structure\nuse a small window of the protein sequence to predict the structure\nof the central amino acid. We describe a new method for prediction\nof the non-local structure called ,8-sheet, which consists of two or\nmore ,8-strands that are connected by hydrogen bonds. Since,8strands are often widely separated in the protein chain , a network\nwith two windows is introduced. After training on a set of proteins\nthe network predicts the sheets well, but there are many false positives. By using a global energy function the ,8-sheet prediction is\ncombined with a local prediction of the three secondary structures\na-helix, ,8-strand and coil. The energy function is minimized using\nsimulated annealing to give a final prediction.\n\n1\n\nINTRODUCTION\n\nProteins are long sequences of amino acids. There are 20 different amino acids with\nvarying chemical properties, e. g. , some are hydrophobic (dislikes water) and some\nare hydrophilic [1]. It is convenient to represent each amino acid by a letter and\nthe sequence of amino acids in a protein (the primary structure) can be written as\na string with a typical length of 100 to 500 letters. A protein chain folds back on\nitself, and the resulting 3D structure (the tertiary structure) is highly correlated to\nthe function of the protein. The prediction of the 3D structure from the primary\nstructure is one of the long-standing unsolved problems in molecular biology. As\nan important step on the way a lot of work has been devoted to predicting the\nlocal conformation of the protein chain, which is called the secondary structure.\nNeural network methods are currently the most successful for predicting secondary\nstructure. The approach was pioneered by Qian and Sejnowski [2] and Bohr et al.\n[3], but later extended in various ways, see e.g. [4] for an overview. In most of this\nwork, only the two regular secondary structure elements a-helix and ,8-strand are\nbeing distinguished, and everything else is labeled coil. Thus, the methods based\n\n\x0c918\n\nA. KROGH, S. K. RIIS\n\nH-\\\n\nt\n\no=c\n\nH-\\\n,,=c!"\n\n{-~\n\n{-H\n\n/=0\nH- \\ \' H-\\\nf\n\no=c\n\nfa\n\n/o=c\n\n~-o ~-:\n\nFigure 1: Left: Anti-parallel,B-sheet. The vertical lines correspond to the backbone\nof the protein. An amino acid consists of N-Ca-C and a side chain on the C a that\nis not shown (the 20 amino acids are distinguished by different side chains). In the\nanti-parallel sheet the directions of the strands alternate, which is here indicated\nquite explicitly by showing the middle strand up-side down. The H-bonds between\nthe strands are shown by 11111111. A sheet has two or more strands, here the antiparallel sheet is shown with three strands. Right: Parallel ,B-sheet consisting of two\nstrands .\non a local window of amino acids give a three-state prediction of the secondary\nstructure of the central amino acid in the window.\nCurrent predictions of secondary structure based on single sequences as input have\naccuracies of about 65-66%. It is widely believed that this accuracy is close to\nthe limit of what can be done from a local window (using only single sequences as\ninput) [5], because interactions between amino acids far apart in the protein chain\nare important to the structure. A good example of such non-local interactions\nare the ,B-sheets consisting of two or more ,B-strands interconnected by H-bonds,\nsee fig. 1. Often the ,B-strands in a sheet are widely separated in the sequence,\nimplying that only part of the available sequence information about a ,B-sheet can\nbe contained in a window of, say, 13 amino acids. This is one of the reasons why the\naccuracy of ,B-strand predictions are generally lower than the accuracy of a-helix\npredictions. The aim of this work is to improve prediction of secondary structures\nby combining local predictions of a-helix, ,B-strand and coil with a non-local method\npredicting ,B-sheets.\nOther work along the same directions include [6] in which ,B-sheet predictions are\ndone by linear methods and [7] where a so-called density network is applied to the\nproblem.\n\n2\n\nA NEURAL NETWORK WITH TWO WINDOWS\n\nWe aim at capturing correlations in the ,B-sheets by using a neural network with\ntwo windows, see fig. 2. While window 1 is centered around amino acid number i\n(ai), window 2 slides along the rest of the chain. When the amino acids centered in\neach of the two windows sit opposite each other in a ,B-sheet the target output is 1,\nand otherwise O. After the whole protein has been traversed by window 2, window 1\nis moved to the next position (i + 1) and the procedure is repeated. If the protein is\nL amino acids long this procedure yields an output value for each of the L(L -1)/2\n\n\x0cPrediction of Beta Sheets in Proteins\n\n919\n\nFigure 2: Neural network for predicting ,B-sheets. The network\nemploys weight sharing to improve the encoding of the amino\nacids and to reduce the number\nof adjustable parameters.\n\npairs of amino acids. We display the output in a L x L gray-scale image as shown in\nfig. 3. We assume symmetry of sheets, i.e., if the two windows are interchanged, the\noutput does not change. This symmetry is ensured (approximately) during training\nby presenting all inputs in both directions.\nEach window of the network sees K amino acids. An amino acid is represented by a\nvector of20 binary numbers all being zero, except one, which is 1. That is, the amino\nacid A is represented by the vector 1,0,0, ... ,0 and so on. This coding ensures that\nthe input representations are un correlated , but it is a very inefficient coding, since\n20 amino acids could in principle be represented by only 5 bit. Therefore, we use\nweight sharing [8] to learn a better encoding [4]. The 20 input units corresponding\nto one window position are fully connected to three hidden units. The 3 x (20 + 1)\nweights to these units are shared by all window positions, i.e., the activation of the\n3 hidden units is a new learned encoding of the amino acids, so instead of being\nrepresented by 20 binary values they are represented by 3 real values. Of course the\nnumber of units for this encoding can be varied, but initial experiments showed that\n3 was optimal [4]. The two windows of the network are made the same way with\nthe same number of inputs etc .. The first layer of hidden units in the two windows\nare fully connected to a hidden layer which is fully connected to the output unit, see\nfig. 2. Furthermore, two structurally identical networks are used: one for parallel\nand one for anti-parallel ,B-sheets.\nThe basis for the training set in this study is the set of 126 non-homologous protein\nchains used in [9], but chains forming ,B-sheets with other chains are excluded. This\nleaves us with 85 proteins in our data set. For a protein of length L only a very small\nfraction of the L(L - 1)/2 pairs are positive examples of ,B-sheet pairs. Therefore\nit is very important to balance the positive and negative examples to avoid the\nsituation where the network always predicts no ,B-sheet. Furthermore, there are\nseveral types of negative examples with quite different occurrences: 1) two amino\nacids of which none belong to a ,B-sheet; 2) one in a ,B-sheet and one which is not in\na ,B-sheet; 3) two sitting in ,B-sheets, but not opposite to each other. The balancing\nwas done in the following way. For each positive example selected at random a\nnegative example from each of the three categories were selected at random.\nIf the network does not have a second layer of hidden units, it turns out that the\nresult is no better than a network with only one input window, i.e., the network\ncannot capture correlations between the two windows. Initial experiments indicated\nthat about 10 units in the second hidden layer and two identical input windows of\nsize K = 9 gave the best results. In fig. 3(left) the prediction of anti-parallel sheets\nis shown for the protein identified as 1acx in the Brookhaven Protein Data Bank\n\n\x0cA. KROGH, S. K. RIIS\n\n920\n\n120\n\n100\n\n:g..\n\'"o\n\n80\n\n.!:\n\n~ 60\n\n/\n40\n\n".\n20\n\nFigure 3: Left: The prediction of anti-parallel ,8-sheets in the protein laex. In the\nupper triangle the correct structure is shown by a black square for each ,8-sheet\npair. The lower triangle shows the prediction by the two-window network. For\nany pair of amino acids the network output is a number between zero (white) and\none (black), and it is displayed by a linear gray-scale. The diagonal shows the\nprediction of a-helices. Right: The same display for parallel ,8-sheets in the protein\n4fxn. Notice that the correct structure are lines parallel to the diagonal, whereas\nthey are perpendicular for anti-parallel sheets. For both cases the network was\ntrained on a training set that did not contain the protein for which the result is\nshown.\n\n[10]. First of all, one notices the checker board structure of the prediction of ,8sheets. This is related to the structure of ,8-sheets. Many sheets are hydrophobic\non one side and hydrophilic on the other. The side chains of the amino acids in\na strand alternates between the two sides of the sheet, and this gives rise to the\nperiodicity responsible for the pattern.\nAnother network was trained on parallel ,8-sheets. These are rare compared to\nthe anti-parallel ones, so the amount of training data is limited. In fig. 3(right)\nthe result is shown for protein 4fxn. This prediction seems better than the one\nobtained for anti-parallel sheets, although false positive predictions still occurs at\nsome positions with strands that do not pair. Strands that bind in parallel ,8-sheets\nare generally more widely separated in the sequence than strands in anti-parallel\nsheets. Therefore, one can imagine that the strands in parallel sheets have to be\nmore correlated to find each other in the folding process, which would explain the\nbetter prediction accuracy.\nThe results shown in fig. 3 are fairly representative. The network misses some of the\nsheets, but false positives present a more severe problem. By calculating correlation\ncoefficients we can show that the network doe!> capture some correlations, but they\nseem to be weak. Based on these results, we hypothesize that the formation of ,8sheets is only weakly dependent on correlations between corresponding ,8-strands.\nThis is quite surprising. However weak these correlations are, we believe they can\nstill improve the accuracy of the three state secondary structure prediction. In\norder to combine local methods with the non-local ,8-sheet prediction, we introduce\na global energy function as described below.\n\n\x0c921\n\nPrediction of Beta Sheets in Proteins\n\n3\n\nA GLOBAL ENERGY FUNCTION\n\nWe use a newly developed local neural network method based on one input window\n[4] to give an initial prediction of the three possible structures. The output from\nthis network is constrained by soft max [11], and can thus be interpreted as the\nprobabilities for each of the three structures. That is, for amino acid ai, it yields\nthree numbers Pi,n, n = 1,2 or 3 indicating the probability of a-helix (Pi,l) , (3sheet (pi,2), or coil (pi,3). Define Si,n\n1 if amino acid i is assigned structure n\nand Si,n = 0 otherwise. Also define hi,n = 10gPi,n. We now construct the \'energy\nfunction\'\n(1)\n\n=\ni\n\nn\n\nwhere weights Un are introduced for later usage. Assuming the probabilities Pi,n are\nindependent for any two amino acids in a sequence, this is the negative log likelihood\nof the assigned secondary structure represented by s, provided that Un = 1. As it\nstands, alone, it is a fairly trivial energy function, because the minimum is the\nassignment which corresponds to the prediction with the maximum Pi,n at each\nposition i-the assignment of secondary structure that one would probably use\nanyway.\nFor amino acids ai and aj the logarithm of the output of the (3-sheet network\ndescribed previously is called qfj for parallel (3-sheets and qfj for anti-parallel sheets.\nWe interpret these numbers as the gain in energy if a (3-sheet pair is formed. (As\nmore terms are added to the energy, the interpretation as a log-likelihood function\nis gradually fading.) If the two amino acids form a pair in a parallel (3-sheet, we\nset the variable T~ equal to 1, and otherwise to 0, and similarly with Tii for antiparallel sheets. Thus the Tii and T~ are sparse binary matrices. Now the total\nenergy of the (3-sheets can be expressed as\n\nHf3(s, T a, TP) = - ~[CaqfjTij\n\n+ CpqfjT~],\n\n(2)\n\n\'J\n\nwhere Ca and Cp determine the weights of the two terms in the function. Since\nan amino acid can only be in one structure, the dynamic T and S variables are\nconstrained: Only Tii or T~ can be 1 for the same (i, j), and if any of them is 1 the\namino acids involved must be in a (3-sheet, so Si,2 = Sj,2 = 1. Also, Si ,2 can only be\n1 if there exists a j with either Iii or T~ equal to 1. Because of these constraints\nwe have indicated an S dependence of H f3.\nThe last term in our energy function introduces correlations between neighboring\namino acids. The above assumption that the secondary structure of the amino acids\nare independent is of course a bad assumption, and we try to repair it with a term\n\nHn(s) =\n\nL: L: Jnm Si,n Si+l,m,\ni\n\nnm\n\n(3)\n\nthat introduces nearest neighbor interactions in the chain. A negative J11, for\ninstance, means that a following a is favored, and e.g., a positive h2 discourages\na (3 following an a.\nNow the total energy is\n\n(4)\nSince (3-sheets are introduced in two ways, through h i ,2 and qij, we need the weights\nUn in (1) to be different from 1.\nThe total energy function (4) has some resemblance with a so-called Potts glass\nin an external field [12]. The crucial difference is that the couplings between the\n\n\x0cA. KROGH, S. K. RIIS\n\n922\n\n\'spins\' Si are dependent on the dynamic variables T. Another analogy of the energy\nfunction is to image analysis, where couplings like the T\'s are sometimes used as\nedge elements.\n\n3.1\n\nPARAMETER ESTIMATION\n\nThe energy function contains a number of parameters, Un, Ca , C p and J nm . These\nparameters were estimated by a method inspired by Boltzmann learning [13]. In\nthe Boltzmann machine the estimation of the weights can be formulated as a minimization of the difference between the free energy of the \'clamped\' system and\nthat of the \'free-running\' system [14]. If we think of our energy function as a free\nenergy (at zero temperature), it corresponds to minimizing the difference between\nthe energy of the correct protein structure and the minimum energy,\n\nwhere p is the total number of proteins in the training set. Here the correct structure\nof protein J-l is called S(J-l) , Ta(J-l), TP(p), whereas s(J-l), Ta(J-l) , TP(J-l) represents the\nstructure that minimizes the energy Htotal. By definition the second term of C is\nless than the first, so C is bounded from below by zero.\nThe cost function C is minimized by gradient descent in the parameters. This is\nin principle straightforward, because all the parameters appear linearly in Htotal.\nHowever, a problem with this approach is that C is minimal when all the parameters\nare set to zero, because then the energy is zero. It is cured by constraining some of\nthe parameters in Htotal. We chose the constraint l:n Un = 1. This may not be the\nperfect solution from a theoretical point of view, but it works well. Another problem\nwith this approach is that one has to find the minimum of the energy Htotal in the\ndynamic variables in each iteration of the gradient descent procedure. To globally\nminimize the function by simulated annealing each time would be very costly in\nterms of computer time. Instead of using the (global) minimum of the energy for\neach protein, we use the energy obtained by minimizing the energy from the correct\nstructure. This minimization is done by a greedy algorithm in the following way.\nIn each iteration the change in s, Ta, TP which results in the largest decrease in\nHtotal is carried out. This is repeated until any change will increase Htotal. This\nalgorithm works towards a local stability of the protein structures in the training\nset. We believe it is not only an efficient way of doing it, but also a very sensible\nway. In fact, the method may well be applicable in other models, such as Boltzmann\nmachines.\n\n3.2\n\nSTRUCTURE PREDICTION BY SIMULATED ANNEALING\n\nAfter estimation of the parameters on which the energy function Htotal depends, we\ncan proceed to predict the structure of new proteins. This was done using simulated\nannealing and the EBSA package [15]. The total procedure for prediction is,\n1. A neural net predicts a-helix, ,8-strand or coil. The logarithm of these\npredictions give all the hi,n for that protein.\n2. The two-window neural networks predict the ,8-sheets. The result is the qfj\nfrom one network and the qfj from the other.\n3. A random configuration of S, Ta, TP variables is generated from which the\nsimulated annealing minimization of Htotal was started. During annealing,\nall constraints on s, Ta, TP variables are strictly enforced.\n\n\x0c923\n\nPrediction of Beta Sheets in Proteins\n\n4. The final minimum configuration s is the prediction of the secondary structure. The ,B-sheets are predicted by a and\n\nt\n\ntv.\n\nUsing the above scheme, an average secondary structure accuracy of 66.5% is obtained by seven-fold cross validation. This should be compared to 66.3% obtained\nby the local neural network based method [4] on the same data set. Although these\npreliminary results do not represent a significant improvement, we consider them\nvery encouraging for future work. Because the method not only predicts the secondary structure, but also which strands actually binds to form ,B-sheets, even a\nmodest result may be an important step on the way to full 3D predictions.\n\n4\n\nCONCLUSION\n\nIn this paper we introduced several novel ideas which may be applicable in other\ncontexts than prediction of protein structure. Firstly, we described a neural network\nwith two input windows that was used for predicting the non-local structure called\n,B-sheets. Secondly, we combined local predictions of a-helix, ,B-strand and coil\nwith the ,B-sheet prediction by minimization of a global energy function. Thirdly,\nwe showed how the adjustable parameters in the energy function could be estimated\nby a method similar to Boltzmann learning.\nWe found that correlations between ,B-strands in ,B-sheets are surprisingly weak.\nUsing the energy function to combine predictions improves performance a little.\nAlthough we have not solved the protein folding problem, we consider the results\nvery encouraging for future work. This will include attempts to improve the performance of the two-window network as well as experimenting with the energy function,\nand maybe add more terms to incorporate new constraints.\nAcknowledgments: We would like to thank Tim Hubbard, Richard Durbin and\nBenny Lautrup for interesting comments on this work and Peter Salamon and\nRichard Frost for assisting with simulated annealing. This work was supported\nby a grant from the Novo Nordisk Foundation.\n\nReferences\n[1] C. Branden and J. Tooze, Introduction to Protein Structure (Garland Publishing,\n[2]\n[3]\n[4]\n[5]\n\n[6]\n[7]\n[8]\n[9]\n[10]\n\n[11]\n[12]\n[13]\n[14]\n[15]\n\nInc., New York, 1991).\nN. Qian and T. Sejnowski, Journal of Molecular Biology 202, 865 (1988).\nH. Bohr et al., FEBS Letters 241, 223 (1988).\nS. Riis and A. Krogh, Nordita Preprint 95/34 S, submitted to J. Compo BioI.\nB. Rost, C. Sander, and R. Schneider, J Mol. BioI. 235, 13 (1994).\nT. Hubbard, in Proc. of the 27th HICSS, edited by R. Lathrop (IEEE Computer Soc.\nPress, 1994), pp. 336-354.\nD. J. C. MacKay, in Maximum Entropy and Bayesian Methods, Cambridge 1994,\nedited by J. Skilling and S. Sibisi (Kluwer, Dordrecht, 1995).\nY. Le Cun et al., Neural Computation 1, 541 (1989).\nB. Rost and C. Sander, Proteins 19, 55 (1994).\nF. Bernstein et al., J Mol. BioI. 112,535 (1977).\nJ. Bridle, in Neural Information Processing Systems 2, edited by D. Touretzky (Morgan Kaufmann, San Mateo, CA, 1990), pp. 211-217.\nK. Fisher and J. Hertz, Spin glasses (Cambridge University Press, 1991).\nD. Ackley, G. Hinton, and T. Sejnowski, Cognitive Science 9, 147 (1985).\nJ. Hertz, A. Krogh, and R. Palmer, Introduction to the Theory of Neural Computation\n(Addison-Wesley, Redwood City, 1991).\nR. Frost, SDSC EBSA, C Library Documentation, version 2.1. SDSC Techreport.\n\n\x0c'
p83152
sg277
S'Using the Future to "Sort Out" the\nPresent: Rankprop and Multitask\nLearning for Medical Risk Evaluation\n\nRich Caruana, Shumeet Baluja, and Tom Mitchell\nSchool of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213\n(caruana, baluja, mitchell)@cs.cmu.edu\n\nAbstract\nA patient visits the doctor; the doctor reviews the patient\'s history,\nasks questions, makes basic measurements (blood pressure, .. .), and\nprescribes tests or treatment . The prescribed course of action is\nbased on an assessment of patient risk-patients at higher risk are\ngiven more and faster attention. It is also sequential- it is too\nexpensive to immediately order all tests which might later be of\nvalue . This paper presents two methods that together improve\nthe accuracy of backprop nets on a pneumonia risk assessment\nproblem by 10-50%. Rankprop improves on backpropagation with\nsum of squares error in ranking patients by risk. Multitask learning\ntakes advantage of future lab tests available in the training set, but\nnot available in practice when predictions must be made. Both\nmethods are broadly applicable.\n\n1\n\nBackground\n\nThere are 3,000,000 cases of pneumonia each year in the U.S., 900,000 of which\nare admitted to the hospital for treatment and testing. Most pneumonia patients\nrecover given appropriate treatment, and many can be treated effectively without\nhospitalization. Nonetheless, pneumonia is serious: 100,000 of those hospitalized\nfor pneumonia die from it, and many more are at elevated risk if not hospitalized.\n\n1.1\n\nThe Problem\n\nA primary goal of medical decision making is to accurately, swiftly, and economically identify patients at high risk from diseases like pneumonia so they may be\nhospitalized to receive aggressive testing and treatment; patients at low risk may be\nmore comfortably, safely, and economically treated at home. Note that the diagno-\n\n\x0cR. CARUANA, S. BALUJA, T. MITCHELL\n\n960\n\nsis of pneumonia has already been made; the goal is not to determine the illness, but\nhow much risk the illness poses to the patient. Some of the most useful tests for doing this require hospitalization and will be available only if preliminary assessment\nindicates it is warranted. Low risk patients can safely be treated as outpatients and\ncan often be identified using measurements made prior to admission .\nThe problem considered in this paper is to learn to rank pneumonia patients according to their probability of mortality. We present two learning methods that\ncombined outperform standard backpropagation by 10-50% in identifying groups\nof patients with least mortality risk . These methods are applicable to domains\nwhere the goal is to rank instances according to a probability function and where\nuseful attributes do not become available until after the prediction must be made.\nIn addition to medical decision making, this class includes problems as diverse as\ninvestment analysis in financial markets and autonomous vehicle navigation .\n1.2\n\nThe Pneumonia Database\n\nThe Medis Pneumonia Database [6] contains 14,199 pneumonia cases collected from\n78 hospitals in 1989. Each patient in the database was diagnosed with pneumonia\nand hospitalized. 65 measurements are available for most patients. These include\n30 basic measurements typically acquired prior to hospitalization such as age, sex,\nand pulse, and 35 lab results such as blood counts or gases not available until after\nhospitalization. The database indicates how long each patient was hospitalized and\nwhether the patient lived or died. 1,542 (10.9%) of the patients died.\n1.3\n\nThe Performance Criterion\n\nThe Medis database indicates which patients lived or died. The most useful decision\naid for this problem would predict which patients will live or die. But this is too\ndifficult. In practice, the best that can be achieved is to estimate a probability\nof death (POD) from the observed symptoms. In fact, it is sufficient to learn to\nrank patients by POD so lower risk patients can be discriminated from higher risk\npatients. The patients at least risk may then be considered for outpatient care.\nThe performance criterion used by others working with the Medis database [4] is the\naccuracy with which one can select a prespecified fraction of the patient population\nthat do not die. For example, given a population of 10,000 patients, find the 20%\nof this population at least risk. To do this we learn a risk model and a threshold\nfor this model that allows 20% of the population (2000 patients) to fall below it. If\n30 of the 2000 patients below this threshold died, the error rate is 30/2000 = 0.015.\nWe say that the error rate for FOP 0.20 is 0.015 for this model ("FOP" stands for\nfraction of population). In this paper we consider FOPs 0.1, 0.2, 0.3, 0.4, and 0.5 .\nOur goal is to learn models and model thresholds, such that the error rate at each\nFOP is minimized. Models with acceptably low error rates might then be employed\nto help determine which patients do not require hospitalization.\n\n2\n\nMethodology\n\nThe Medis database is unusually large, with over 14K training patterns. Because we\nare interested in developing methods that will be effective in other domains where\ndatabases of this size are not available, we perform our experiments using small\ntraining sets randomly drawn from the 14K patterns and use the remaining patterns\nas test sets. For each method we run ten trials. For each trial we randomly sample\n2K patterns from the 14K pool for training. The 2K training sample is further split\ninto a 1K backprop set used to train the net and a 1K halting set used to determine\n\n\x0cRankprop and Multitask Learning for Medical Risk Evaluation\n\n961\n\nwhen to halt training.! Once the network is trained, we run the 1K halt set through\nthe model again to find the threshold that passes 10%,20%,30%,40%, and 50% of\nthe halt set. The performance ofthe model is evaluated on the 12K unused patterns\nby determining how many of the cases that fall below threshold in this test set die.\nThis is the error rate for that model at that FOP.\n\n3\n\nThe Traditional Approach: SSE on 0/1 Targets\n\nSections 3-5 present three neural net approaches to pneumonia risk prediction. This\nsection presents the standard approach: using backpropagation on sum of squares\nerrors (SSE) with 0=lives/1=dies to predict mortality. This works well if early\nstopping is used to prevent overfitting. Section 4 presents rank prop (SSE on ranks\ninstead of 0/1 targets). Rankprop, which learns to rank patients by risk instead\nof directly predicting mortality, works better. Section 5 uses multitask learning\n(MTL) to benefit from tests in the database that in practice will not be available\nuntil after deciding to admit the patient. Rankprop with MTL works even better.\nThe straightforward approach to this problem is to use backprop to train a net to\nlearn to predict which patients live or die, and then use the real-valued predictions of\nthis net to sort patients by risk. This net has 30 inputs, 1 for each of the observed\npatient measurements, a hidden layer with 8 units 2 , and a single output trained\nwith O=lived, 1=died. 3 Given an infinite training set, a net trained this way should\nlearn to predict the probability of death for each patient, not which patients live or\ndie. In the real world, however, where we rarely have an infinite number of training\ncases, a net will overtrain and begin to learn a very nonlinear function that outputs\nvalues near 0/1 for cases in the training set, but which does not generalize well. In\nthis domain it is critical to use early stopping to halt training before this happens .\nTable 1 shows the error rates of nets trained with SSE on 0/1 targets for the five\nFOPs. Each entry is the mean of ten trials. The first entry in the table indicates\nthat on average, in the 10% of the test population predicted by the nets to be at\nleast risk, 1.4% died. We do not know the best achievable error rates for this data.\nTable 1: Error Rates of SSE on 0/1 Targets\nFOP\nError Rate\n\n4\n\nUsing Rankprop to Rank Cases by Risk\n\nBecause the goal is to find the fraction of the population least likely to die, it is\nsufficient just to learn to rank patients by risk. Rankprop learns to rank patients\nwithout learning to predict mortality. "Rankprop" is short for "backpropagation\nusing sum of squares errors on estimated ranks". The basic idea is to sort the\ntraining set using the target values, scale the ranks from this sort (we scale uniformly\nto [0.25,0.75] with sigmoid output units), and use the scaled ranks as target values\nfor standard backprop with SSE instead of the 0/1 values in the database.\nlperformance at different FOPs sometimes peaks at different epochs. We halt training\nseparately for each FOP in all the experiments to insure this does not confound results.\n2To make comparisons between methods fair, we first found hidden layer sizes and\nlearning parameters that performed well for each method.\n3Different representations such as 0.15/0.85 and different error metrics such as cross\nentropy did not perform better than SSE on 0/1 targets.\n\n\x0cR. CARUANA, S. BALUJA, T. MITCHELL\n\n962\n\nIdeally, we\'d rank the training set by the true probabilities of death. Unfortunately,\nall we know is which patients lived or died. In the Medis database, 89% of the target\nvalues are O\'s and 11% are l\'s. There are many possible sorts consistent with these\nvalues. Which sort should backprop try to fit? It is the large number of possible\nsorts of the training set that makes backpropagating ranks challenging. Rankprop\nsolves this problem by using the net model as it is being learned to order the training\nset when target values are tied. In this database, where there are many ties because\nthere are only two target values, finding a proper ranking of the training set is a\nserious problem. Rankprop learns to adjust the target ranks of the training set at\nthe same time it is learning to predict ranks from that training set.\nHow does rankprop do this? Rankprop alternates between rank passes and backprop\npasses. On the rank pass it records the output of the net for each training pattern.\nIt then sorts the training patterns using the target values (0 or 1 in the Medis\ndatabase), but using the network\'s predictions for each pattern as a secondary\nsort key to break ties. The basic idea is to find the legal rank of the target values (0\nor 1) maximally consistent with the ranks the current model predicts. This closest\nmatch ranking of the target values is then used to define the target ranks used on\nthe next backprop pass through the training set. Rankprop\'s pseudo code is:\n\nforeach epoch do {\nforeach pattern do {\nnetwork_output[pattern] = forward_pass(pattern)}\ntarget_rank = sort_and_scale_patterns(target_value, network_output)\nforeach pattern do {\nbackprop(target_rank[pattern] - network_output[pattern])}}\nwhere "sorkand..scale_patterns" sorts and ranks the training patterns using the sort\nkeys specified in its arguments , the second being used to break ties in the first.\nTable 2 shows the mean rankprop performance using nets with 8 hidden units.\nThe bottom row shows improvements over SSE on 0/1 targets. All differences are\nstatistically significant. See Section 7.1 for discussion of why rank prop works better.\nTable 2: Error Rates of Rankprop and Improvement Over Standard Backprop\nFOP\nError Rate\n% Change\n\n5\n\nLearning From the Future with Multitask Learning\n\nThe Medis database contains results from 36 lab tests that will be available only\nafter patients are hospitalized. Unfortunately, these results will not be available\nwhen the model is used because the patients will not yet have been admitted . Multitask learning (MTL) improves generalization by having a learner simultaneously\nlearn sets of related tasks with a shared representation; what is learned for each\ntask might benefit other tasks. In this application , we use MTL to benefit from the\nfuture lab results. The extra lab values are used as extra backprop outputs as shown\nin Figure 1. The extra outputs bias the shared hidden layer towards representations\nthat better capture important features of the domain. See [2][3][9] for details about\nMTL and [1] for other ways of using extra outputs to bias learning.\nThe MTL net has 64 hidden units . Table 3 shows the mean performance of ten runs\nof MTL with rankprop. The bottom row shows the improvement over rankprop\n\n\x0cRankprop and Multitask Learning for Medical Risk Evaluation\n\nMortality\n\nRank\n\nRANKPROP\nOUTPUT\n\n1\n\n963\n\n~--~\n\nH e matocnt\n\nWhile Blood\nCell ("oun l\n\n1\n\nPn t.a.<i1ilUm\n\n1\n\n- - FUT\\JRE LABS\n\n1\n\n~\n\nOUTPUT LAYER\n\n~~o~\n\nSHAREDHIDOEN LAYER\n\nINPUT LAYER\n\nINPUTS\n\nFigure 1: Using Future Lab Results as Extra Outputs To Bias Learning\nalone. Although MTL lowers error at each FOP, only the differences at FOP = 0.3,\n0.4, and 0.5 are statistically significant with ten trials. Feature nets [7], a competing\napproach that trains nets to predict the missing future labs and uses the predictions\nas extra net inputs does T}ot yield benefits comparable to MTL on this problem.\nTable 3: Error Rates of Rankprop+MTL and Improvement Over Rankprop Alone\nFOP\nError Rate\n% Change\n\n6\n\nComparison of Results\n\nTable 4 compares the performance of backprop using SSE on 0/1 targets with the\ncombination of rankprop and multitask learning. On average, Rankprop+MTL reduces error more than 25%. This improvement is not easy to achieve-experiments\nwith other learning methods such as Bayes Nets, Hierarchical Mixtures of Experts,\nand K-Nearest Neighbor (run not by us , but by experts in their use) indicate SSE\non 0/1 targets is an excellent performer on this domain[4].\nTable 4: Comparison Between SSE on 0/1 Targets and Rankprop+MTL\nFOP\nSSE on 0/1\nRankprop+ MTL\n% Change\n\n7\n7.1\n\n0.1\n.0140\n.0074\n-47.1%\n\nI\n\n0.2\n.0190\n.0127\n-33.2%\n\nI\n\n0.3\n.0252\n.0197\n-21.8%\n\nI\n\n0.4\n.0340\n.0269\n-20.9%\n\nI\n\n0.5\n.0421\n.0364\n-13.5%\n\nDiscussion\nWhy Does Rankprop Work?\n\nWe are given data from a target function f (x). Suppose the goal is not to learn a\nmodel of f(x), but to learn to sort patterns by f(x). Must we learn a model of f(x)\nand use its predictions for sorting? No . It suffices to learn a function g( x) such that\nfor all Xl , X2, [g(xd::; g(X2)]- [J(xd::; f(X2)]. There can be many such functions\ng(x) for a given f(x), and some of these may be easier to learn than f(x).\n\n\x0cR. CARUANA, S. BALUJA, T. MITCHELL\n\n964\n\nConsider the probability function in Figure 2.1 that assigns to each x the probability\np = f(x) that the outcome is 1; with probability 1 - p the outcome is O. Figure\n2.2 shows a training set sampled from this distribution. Where the probability is\nlow , there are many O\'s. Where the probability is high , there are many l \'s. Where\nthe probability is near 0.5, there are O\'s and 1 \'so This region causes problems for\nbackprop using SSE on 0/1 targets: similar inputs are mapped to dissimilar targets .\n\n.\n\ni2\n\n.8\n\nI"\n111111 111111\nI\n\n08\n\n000 0 011001010110001101111\'\n\nf ??\nt\n\ni ::\n02\n\n0...\n\n0.6\n\n08\n\nllllllllllll\n02\n\n04\n\n06\n\n08\n\n02\n\n04\n\n06\n\n08\n\nFigure 2: SSE on 0/1 Targets and on Ranks for a Simple Probability Function\nBackprop learns a very nonlinear function if trained on Figure 2.2. This is unfortunate: Figure 2.1 is smooth and maps similar inputs to similar outputs. If the\ngoal is to learn to rank the data, we can learn a simpler , less nonlinear function\ninstead. There exists a ranking of the training data such that if the ranks are used\nas backprop target values, the resulting function is less nonlinear than the original\ntarget function. Figure 2.3 shows these target rank values. Similar input patterns\nhave more similar rank target values than the original target values .\nRankprop tries to learn simple functions that directly support ranking. One difficulty with this is that rankprop must learn a ranking of the training data while also\ntraining the model to predict ranks . We do not yet know under what conditions this\nparallel search will converge. We conjecture that when rankprop does converge, it\nwill often be to simpler models than it would have learned from the original target\nvalues (0/1 in Medis), and that these simpler models will often generalize better.\n\n7.2\n\nOther Applications of Rankprop and Learning From the Future\n\nRankprop is applicable wherever a relative assessment is more useful or more learnable than an absolute one. One application is domains where quantitative measurements are not available, but relative ones are[8]. For example, a game player\nmight not be able to evaluate moves quantitatively , but might excel at relative\nmove evaluation[10]. Another application is where the goal is to learn to order data\ndrawn from a probability distribution, as in medical risk prediction . But it can also\nbe applied wherever the goal is to order data. For example, in information filtering\nit is usually important to present more useful information to the user first, not to\npredict how important each is[5].\nMTL is a general method for using related tasks. Here the extra MTL tasks are\nfuture measurements. Future measurements are available in many offline learning\nproblems where there is opportunity to collect the measurements for the training\nset. For example, a robot or autonomous vehicle can more accurately measure the\nsize, location, and identity of objects when it passes near them-road stripes can be\ndetected reliably as a vehicle passes alongside them, but detecting them far ahead of\na vehicle is hard. Since driving brings future road into the car\'s present, stripes can\nbe measured accurately when passed and used as extra features in the training set .\nThey can\'t be used as inputs for learning to drive because they will not be available\nuntil too late when driving. As MTL outputs , though, they provide information\n\n\x0cRankprop and Multitask Learning for Medical Risk Evaluation\n\n965\n\nthat improves learning without requiring they be available at run time[2] .\n\n8\n\nSummary\n\nThis paper presents two methods that can improve generalization on a broad class\nof problems. This class includes identifying low risk pneumonia patients. The\nfirst method, rankprop , tries to learn simple models that support ranking future\ncases while simultaneously learning to rank the training set. The second, multitask\nlearning, uses lab tests available only during training, as additional target values to\nbias learning towards a more predictive hidden layer. Experiments using a database\nof pneumonia patients indicate that together these methods outperform standard\nbackpropagation by 10-50%. Rankprop and MTL are applicable to a large class of\nproblems in which the goal is to learn a relative ranking over the instance space,\nand where the training data includes features that will not be available at run\ntime. Such problems include identifying higher-risk medical patients as early as\npossible, identifying lower-risk financial investments, and visual analysis of scenes\nthat become easier to analyze as they are approached in the future.\nAcknowledgements\nWe thank Greg Cooper, Michael Fine, and other members of the Pitt/CMU Cost-Effective\nHealth Care group for help with the Medis Database. This work was supported by ARPA\ngrant F33615-93-1-1330, NSF grant BES-9315428, Agency for Health Care Policy and\nResearch grant HS06468, and an NSF Graduate Student Fellowship (Baluja) .\n\nReferences\n[1] Y .S. Abu-Mostafa, "Learning From Hints in Neural Networks," Journal of Complexity\n6:2, pp. 192-198, 1989.\n[2] R. Caruana, "Learning Many Related Tasks at the Same Time With Backpropagation," Advances in Neural Information Processing Systems 7, pp. 656-664, 1995.\n[3] R. Caruana, "Multitask Learning: A Knowledge-Based Source of Inductive Bias,"\nProceedings of the 10th International Conference on Machine Learning, pp. 41-48,\n1993.\n[4] G. Cooper, et al., "An Evaluation of Machine Learning Methods for Predicting Pneumonia Mortality," submitted to AI in Medicine, 1995.\n[5] K. Lang, "NewsWeeder: Learning to Filter News," Proceedings of the 12th International Conference on Machine Learning, pp. 331-339, 1995.\n[6] M. Fine, D. Singer, B. Hanusa, J . Lave, and W. Kapoor, "Validation of a Pneumonia\nPrognostic Index Using the MedisGroups Comparative Hospital Database," American\nJournal of Medicine, 94 1993.\n[7] I. Davis and A . Stentz, "Sensor Fusion For Autonomous Outdoor Navigation Using\nNeural Networks," Proceedings of IEEE \'s Intelligent Robots and Systems Conference,\n1995.\n[8] G.T. Hsu, and R. Simmons, "Learning Footfall Evaluation for a Walking Robot, "\nProceedings of the 8th International Conference on Machine Learning, pp. 303-307,\n1991.\n[9] S.C. Suddarth and A.D.C. Holden, "Symbolic-neural Systems and the Use of Hints for\nDeveloping Complex Systems," International Journal of Man-Machine Studies 35:3,\npp. 291-311, 1991.\n[10] P. Utgoff and S. Saxena, "Learning a Preference Predicate," Proceedings of the 4th\nInternational Conference on Machine Learning, pp. 115-121, 1987.\n\n\x0c'
p83153
sg163
S'Symplectic Nonlinear Component\nAnalysis\n\nLucas C. Parra\nSiemens Corporate Research\n755 College Road East, Princeton, NJ 08540\nlucas@scr.siemens.com\n\nAbstract\nStatistically independent features can be extracted by finding a factorial representation of a signal distribution. Principal Component\nAnalysis (PCA) accomplishes this for linear correlated and Gaussian distributed signals. Independent Component Analysis (ICA),\nformalized by Comon (1994), extracts features in the case of linear statistical dependent but not necessarily Gaussian distributed\nsignals. Nonlinear Component Analysis finally should find a factorial representation for nonlinear statistical dependent distributed\nsignals. This paper proposes for this task a novel feed-forward,\ninformation conserving, nonlinear map - the explicit symplectic\ntransformations. It also solves the problem of non-Gaussian output\ndistributions by considering single coordinate higher order statistics.\n\n1\n\nIntroduction\n\nIn previous papers Deco and Brauer (1994) and Parra, Deco, and Miesbach (1995)\nsuggest volume conserving transformations and factorization as the key elements\nfor a nonlinear version of Independent Component Analysis . As a general class\nof volume conserving transformations Parra et al. (1995) propose the symplectic\ntransformation . It was defined by an implicit nonlinear equation, which leads to a\ncomplex relaxation procedure for the function recall. In this paper an explicit form\nof the symplectic map is proposed, overcoming thus the computational problems.\n\n\x0c438\n\nL. C.PARRA\n\nIn order to correctly measure the factorization criterion for non-Gaussian output\ndistributions, higher order statistics has to be considered. Comon (1994) includes\nin the linear case higher order cumulants of the output distribution. Deco and\nBrauer (1994) consider multi-variate, higher order moments and use them in the\ncase of nonlinear volume conserving transformations. But the calculation of multicoordinate higher moments is computational expensive.\nThe factorization criterion for statistical independence can be expressed in terms of\nminimal mutual information. Considering only volume conserving transformations\nallows to concentrate on single coordinate statistics, which leads to an important\nreduction of computational complexity. So far, this approach (Deco & Schurman,\n1994; Parra et aI., 1995) has been restricted to second order statistic. The present\npaper discusses the use of higher order cumulants for the estimation of the single\ncoordinate output distributions. The single coordinate entropies measured by the\nproposed technique match the entropies of the sampled data more accurately. This\nleads in turns to better factorization results.\n\n2\n\nStatistical Independence\n\nMore general than decorrelation used in PCA the goal is to extract statistical\nindependent features from a signal distribution p(x). We look for a deterministic transformation on ~n: y = f(x) which generates a factorial representation\np(y) = It p(Yd, or at least a representation where the individual coordinates P(Yi)\nof the output variable yare "as factorial as possible". This can be accomplished\nby minimizing the mutual information M I[P(y)].\nn\n\no::; M I[P(y)] = L\n\nH[P(Yi)] - H[P(y)],\n\n(1)\n\ni=l\n\nsince M I[P(y)] = 0 holds if p(y) is factorial. The mutual information can be used\nas a measure of "independence". The entropies H in the definition (1) are defined\nas usual by H[P(y)] = - J~oop(y)lnp(y)dy.\nAs in linear PCA we select volume conserving transformations, but now without\nrestricting ourselves to linearity. In the noise-free case of reversible transformations\nvolume conservation implies conservation of entropy from the input x to the output\ny, i.e. H[P(y)] = H[P(x)] = canst (see Papoulis, 1991). The minimization of mutual\ninformation (1) reduces then to the minimization of the single coordinate output\nentropies H[P(Yi)]. This substantially simplifies the complexity of the problem,\nsince no multi-coordinate statistics is required.\n2.1\n\nMeasuring the Entropy with Cumulants\n\nWith an upper bound minimization criterion the task of measuring entropies can\nbe avoided (Parra et aI., 1995):\n\n(2)\n\n\x0c439\n\nSymplectic Nonlinear Component Analysis\n\nEdgeworth appIOlClmatlOr\'l to second and fanh order\nO.B,----~--~-~--~-___,\n\n0.7\n~.\n\n0.6\n\ndQ(y1)/dY1\n\n-----~> )i\n\n:\n\n~0 . 5\n\n~\n\n04\n\n~\n\n03\n\n>-\n\n~\nQ..\n\n1\n\n0.2\n\n0.,\n\no .O.~~--=---,!------=---~----:\n\nFigure 1: LEFT: Doted line: exponential distribution with additive Gaussian noise\nsampled with 1000 data points. (noise-variance/decay-constant = 0.2). Dashed\nline: Gaussian approximation equivalent to the Edgeworth approximation to second\norder. Solid line: Edgeworth approximation including terms up to fourth order.\nRIGHT: Structure of the volume conserving explicit symplectic map.\n\nThe minimization of the individual output coordinate entropies H(P(Yi)] simplifies\nto the minimization of output variances (Ti. For the validity of that approach it is\ncrucial that the map y = f(x) transforms the arbitrary input distribution p(x) into\na Gaussian output distribution. But volume conserving and continuous maps can\nnot transform arbitrary distributions into Gaussians. To overcome this problem one\nincludes statistics - higher than second order - to the optimization criterion.\nComon (1994) suggests to use the Edgeworth expansion of a probability distribution. This leads to an analytic expression of the entropy in terms of measurable\nhigher order cumulants. Edgeworth expands the multiplicative correction to the\nbest Gaussian approximation of the distribution in the orthonormal basis of Hermite polynomials hcr(y). The expansion coefficients are basically given by the cumulants Ccr of distribution p~y). The Edgeworth expansions reads for a zero-mean\ndistribution with variance (T , (see Kendall & Stuart, 1969)\n2\n\np(y)\n\n-l-e-~\n-j2;(J\n\nf(y)\n(3)\n\nNote, that by truncating this expansion at a certain order, we obtain an approximation Papp(Y), which is not strictly positive. Figure 1, left shows a sampled\nexponential distribution with additive Gaussian noise.\nBy cutting expansion (3) at fourth order, and further expanding the logarithm in\ndefinition of entropy up to sixth order , Comon (1994) approximates the entropy by,\n\n\x0cL.C.PARRA\n\n440\n\n1\n1 c?\n1 c~\n7 c~\nH(P(Y)app] ~ 2"ln(271\'e) + In 0\' - 120\'6 - 480\'8 - 480\'12\n\n1 c~\n\nC4\n\n+ 8" 0\'60\'4\n\n(4)\n\nWe suggest to use this expression to minimize the single coordinate entropies in the\ndefinition of the mutual information (1).\n\n2.2\n\nMeasuring the Entropy by Estimating an Approximation\n\nNote that (4) could only be obtained by truncating the expansion (3). It is therefore limited to fourth order statistic, which might be not enough for a satisfactory\napproximation. Besides, the additional approximation of the logarithm is accurate\nonly for small corrections to the best Gaussian approximation, i.e. for fey) ~ 1.\nFor distributions with non-Gaussian tails the correction terms might be rather large\nand even negative as noted above. We therefore suggest alternatively, to measure\nthe entropy by estimating the logarithm of the approximated distribution In Papp (y)\nwith the given data points Yv and using Edgeworth approximation (3) for Papp (y),\n\n1 N\n\nH(P(y)] ~ - N\n\nL lnpapp (Yv) = canst + In 0\' -\n\nv=1\n\n1 N\n\nN LIn f(yv)\n\n(5)\n\nv=1\n\nFurthermore, we suggest to correct the truncated expansion Papp by setting\nfapp (y) -+ 0 for all fapp (y) < O. For the entropy measurement (5) there is in\nprinciple no limitation to any specific order.\nIn table 1 the different measures of entropy are compared. The values in the row\nlabeled \'partition\' are measured by counting the numbers n(i) of data points falling\nin equidistant intervals i of width D.y and summing -pC i)D.y lnp(i) over all intervals,\nwith p(i)D.y = n(i)IN. This gives good results compared to the theoretical values\nonly because of the relatively large sampling size. These values are presented here\nin order to have an reliable estimate for the case of the exponential distribution,\nwhere cumulant methods tend to fail.\nThe results for the exponential distribution show the difficulty of the measurement\nproposed by Comon, whereas the estimation measurement given by equation (5) is\nstable even when considering (for this case) unreliable 5th and 6th order cumulants.\nThe results for the symmetric-triangular and uniform distribution demonstrate the\ninsensibility of the Gaussian upper bound for the example of figure 2. A uniform\nsquared distribution is rotated by an angle a. On the abscissa and ordinate a\ntriangular or uniform distribution are observed for the different angles a = II/4\nor a = 0 respectively. The approximation of the single coordinate entropies with\na Gaussian measure is in both cases the same. Whereas measurements including\nhigher order statistics correctly detect minimal entropy (by fixed total information)\nfor the uniform distribution at a = O.\n\n3\n\nExplicit Symplectic Transformation\n\nDifferent ways of realizing a volume conserving transformation that guarantees\nH(P(x)] = H(P(x)] have been proposed (Deco & Schurman, 1994; Parra et aI.,\n\n\x0cSymplectic Nonlinear Component Analysis\n11easured entropy of\nsampled distributions\npartition\nGaussian upper bound (2)\nComan, eq. (4)\nEstimate (5) - 4th order\nEstimate (5) - 6th order\ntheoretical value\n\n441\n\nGauss\n\nuniform\n\n1.35 ? .02\n1.415 ? .02\n1.414 ? .02\n1.414 ? .02\n1.414 ? .02\n1.419\n\n.024 ? .006\n.18 ? .016\n.14 ? .015\n.13 ? .015\n.092 ? .001\n.0\n\ntriangular\nsymmetric\n.14 ? .02\n.18 ? .02\n.17 ? .02\n.17?.02\n.16 ? .02\n.153\n\nexponential\nnoise\n1.31 ? .03\n1.53 ? .04\n3.0 ? 2.5\n1.39 ? .05\n1.3 ? .5\n\n+ Gauss\n\nTable 1: Entropy values for different distributions sampled with N = 1000 data\npoints and the different estimation methods explained in the text . The standard\ndeviations are obtained by multiple repetition of the experiment.\n1995). A general class of volume conserving transformations are the symplectic\nmaps (Abraham & Marsden, 1978). An interesting and for our purpose important\nfact is that any symplectic transformation can be expressed in terms of a scalar\nfunction. And in turn any scalar function defines a symplectic map. In (Parra\net al., 1995) a non-reflecting symplectic transformation has been presented. But\nits implicit definition results in the need of solving a nonlinear equation for each\ndata point. This leads to time consuming computations which limit in practice the\napplications to low dimensional problems (n~ 10). In this work reflecting symplectic transformations with an explicit definition are used to define a "feed-forward"\nvolume conserving maps . The input and output space is divided in two partitions\nx = (Xl, X2) and Y = (Yl, Y2), with Xl, X2, Yl , Y2 E ?R n / 2 .\n\n(6)\nThe structure of this symplectic map is represented in figure 1, right. Two scalar\nfunctions P : ?R n / 2 1-+ ?R and Q : ?R n / 2 1-+ ?R can be chosen arbitrarily. Note that\nfor quadratic functions equation (6) represents a linear transformation. In order\nto have a general transformation we introduce for each of these scalar functions a\n3-layer perceptron with nonlinear hidden units and a single linear output unit:\n\n(7)\nThe scalar functions P and Q are parameterized by the network parameters\nWl, W2 E Rm and Wl, W 2 E Rm x Rn/2. The hidden-unit, nonlinear activation\nfunction 9 applies to each component of the vectors WlYl and W2X2 respectively.\nBecause of the structure of equation (6) the output coordinates Yl depend only additively on the input coordinates Xl. To obtain a more general nonlinear dependence\na second symplectic layer has to be added.\nTo obtain factorial distributions the parameters of the map have to be trained.\nThe approximations of the single coordinate entropies (4) or (5) are inserted in the\nmutual information optimization criterion (1). These approximations are expressed\nthrough moments in terms of the measured output data points. Therefore , the\n\n\x0cL.C.PARRA\n\n442\nO,B,.---~-~-~-~-~-~-~---,\n\n0,6\n\n,,\n\n0,4\n0,2\n\n-0.2\n-0.4\n\n-0.6\n\n. :.\':\n.....\n\n... ,,\n\n:\' "\n. :.,\'\n\n,\n\n-~~,B---0~,6-~-0~.4---0~,2-~--0~.2--0~.4--0~.6-~0,B\n\nFigure 2: Sampled 2-dimensional squared uniform distribution rotated by 7l" /4. Solid\nlines represent the directions found by any of the higher order techniques explained\nin the text. Dashed lines represent directions calculated by linear PCA. (This result\nis arbitrary and varies with noise) .\ngradient of these expressions with respect to parameters ofthe map can be computed\nin principle. For that matter different kinds of averages need to be computed.\nEven though, the computational complexity is not substantially increased compared\nwith the efficient minimum variances criterion (2), the complexity of the algorithm\nincreases considerably. Therefore, we applied an optimization algorithm that does\nnot require any gradient information. The simple stochastic and parallel update\nalgorithm ALOPEX (Unnikrishnan & Venugopal, 1994) was used.\n\n4\n\nExperiments\n\nAs explained above, finding the correct statistical independent directions of a rotated two dimensional uniform distribution causes problems for techniques which\ninclude only second order statistic. The statistical independent coordinates are simply the axes parallel to the edges of the distribution (see figure 2). A rotation i. e.\na linear transformation suffices for this task. The covariance matrix of the data is\ndiagonal for any rotation of the squared distribution and, hence, does not provide\nany information about the correct orientation of the square. It is well known, that\nPCA fails to find in the case of non-Gaussian distributions the statistical independent coordinates. Similarly the Gaussian upper bound technique (2)is not capable\nto minimize the mutual information in this case. Instead, with anyone of the higher\norder criteria explained in the previous section one finds the appropriate coordinates\nfor any linearly transformed multi-dimensional uniform distribution. This has been\nobserved empirically for a series of setups. The symplectic map was restricted in\nthis experiments to linea1;ity by using square scalar functions.\nThe second example shows that the proposed technique in fact finds nonlinear\nrelations between the input coordinates. An one-dimensional signal distributed\naccording to the distribution of figure 1 was nonlinearly transformed into a two-\n\n\x0cSymplectic Nonlinear Component Analysis\n\n. \'.: <~.,\n\n.\' .\n\n443\n\n. : .. ;\n\'\n\nFigure 3: Symplectic map trained with 4th and 2nd order statistics corresponding\nto the equations (5) and (2) respectively. Left: input distribution. The line at\nthe center of the distribution gives the nonlinear transformed noiseless signal distributed according to the distribution shown in figure 1. Center and Right : Output\ndistribution of the symplectic map corresponding to the 4th order (right) and 2nd\norder (center) criterion.\ndimensional signal and corrupted with additive noise, leading to the distribution\nshown in figure 3, left. The task of finding statistical independent coordinates has\nbeen tackled by an explicit symplectic transformation with. n = 2 and m = 6.\nOn figure 3 the different results for the optimization according to the Gaussian\nupper bound criterion (2) and the approximated entropy criterion (5) are shown.\nObviously considering higher order statistics in fact improves the result by finding\nthe better representation of the nonlinear dependency.\n\nReference\nAbraham, R., & Marsden, J . (1978). Foundations of Mechanics The BenjaminCummings Publishing Company, Inc., London.\nComon, P. (1994). Independent component analysis, A new concept Signal Processing, 36, 287- 314.\nDeco, G., & Brauer, W. (1994). Higher Order Statistical Decorrelation by Volume\nConcerving Nonlinear Maps. Neural Networks, ? submitted.\nDeco, G., & Schurman, B. (1994). Learning Time Series Evolution by Unsupervised\nExtraction of Correlations. Physical Review E, ? submitted.\nKendall, M. G., & Stuart, A. (1969). The Advanced Theory of Statistics (3 edition).,\nVol. 1. Charles Griffin and Company Limited, London.\nPapoulis, A. (1991). Probability, Random Variables, and Stochastic Processes. Third\nEdition, McGraw-Hill, New York.\nParra, L., Deco, G., & Miesbach, S. (1995).\nRedundancy reduction with\ninformation-preserving nonlinear maps. Network, 6(1), 61-72.\nUnnikrishnan, K., P., & Venugopal, K., P. (1994). Alopex: A Correlation-Based\nLearning Algorithm for Feedforward and Recurrent Neural Networks. Neural\nComputation, 6(3), 469- 490.\n\n\x0c'
p83154
sg89
S'Generalization in Reinforcement Learning:\nSafely Approximating the Value Function\n\nJustin A. Boyan and Andrew W. Moore\nComputer Science Department\nCarnegie Mellon University\nPittsburgh, PA 15213\njab@cs.cmu.edu, awm@cs.cmu .edu\n\nAbstract\nA straightforward approach to the curse of dimensionality in reinforcement learning and dynamic programming is to replace the\nlookup table with a generalizing function approximator such as a neural net. Although this has been successful in the domain of backgammon, there is no guarantee of convergence. In this paper, we show\nthat the combination of dynamic programming and function approximation is not robust, and in even very benign cases, may produce\nan entirely wrong policy. We then introduce Grow-Support, a new\nalgorithm which is safe from divergence yet can still reap the benefits\nof successful generalization .\n\n1\n\nINTRODUCTION\n\nReinforcement learning-the problem of getting an agent to learn to act from sparse,\ndelayed rewards-has been advanced by techniques based on dynamic programming\n(DP). These algorithms compute a value function which gives, for each state, the minimum possible long-term cost commencing in that state. For the high-dimensional\nand continuous state spaces characteristic of real-world control tasks, a discrete representation of the value function is intractable; some form of generalization is required.\nA natural way to incorporate generalization into DP is to use a function approximator,\nrather than a lookup table, to represent the value function. This approach, which\ndates back to uses of Legendre polynomials in DP [Bellman et al., 19631, has recently\nworked well on several dynamic control problems [Mahadevan and Connell, 1990, Lin,\n1993] and succeeded spectacularly on the game of backgammon [Tesauro, 1992, Boyan,\n1992]. On the other hand, many sensible implementations have been less successful\n[Bradtke, 1993, Schraudolph et al., 1994]. Indeed, given the well-established success\n\n\x0c370\n\nJustin Boyan, Andrew W. Moore\n\non backgammon, the absence of similarly impressive results appearing for other games\nis perhaps an indication that using function approximation in reinforcement learning\ndoes not always work well.\nIn this paper, we demonstrate that the straightforward substitution of function approximators for lookup tables in DP is not robust and, even in very benign cases, may\ndiverge, resulting in an entirely wrong control policy. We then present Grow-Support,\na new algorithm designed to converge robustly. Grow-Support grows a collection of\nstates over which function approximation is stable. One-step backups based on Bellman error are not used; instead, values are assigned by performing "rollouts" -explicit\nsimulations with a greedy policy. We discuss potential computational advantages of\nthis method and demonstrate its success on some example problems for which the\nconventional DP algorithm fails.\n\n2\n\nDISCRETE AND SMOOTH VALUE ITERATION\n\nMany popular reinforcement learning algorithms, including Q-Iearning and TD(O),\nare based on the dynamic programmin~ algorithm known as value iteration [Watkins,\n1989, Sutton, 1988, Barto et al., 1989J, which for clarity we will call discrete value\niteration. Discrete value iteration takes as input a complete model of the world as a\nMarkov Decision Task, and computes the optimal value function J*:\n\nJ* (x)\n\n= the minimum possible sum of future costs starting from x\n\nTo assure that J* is well-defined, we assume here that costs are nonnegative and that\nsome absorbing goal state-with all future costs O-is reachable from every state. For\nsimplicity we also assume that state transitions are deterministic. Note that J* and\nthe world model together specify a "greedy" policy which is optimal for the domain:\noptimal action from state x\n\n= argmin(CosT(x,\na) + J*(NEXT-STATE(X, a)))\naEA\n\nWe now consider extending discrete value iteration to the continuous case: we replace\nthe lookup table over all states with a function approximator trained over a sample of\nstates. The smooth value iteration algorithm is given in the appendix. Convergence\nis no longer guaranteed; we instead recognize four possible classes of behavior:\ngood convergence The function approximator accurately represents the intermediate value functions at each iteration (that is, after m iterations, the value\nfunction correctly represents the cost of the cheapest m-step path), and successfully converges to the optimal J* value function.\nlucky convergence The function approximator does not accurately represent the\nintermediate value functions at each iteration; nevertheless, the algorithm\nmanages to converge to a value function whose greedy policy is optimal.\nbad convergence The algorithm converges, i.e. the target J-values for the N training points stop changing, but the resulting value function and policy are\npoor.\ndivergence Worst of all: small fitter errors may become magnified from one iteration\nto the next, resulting in a value function which never stops changing.\nThe hope is that the intermediate value functions will be smooth and we will achieve\n"good convergence." Unfortunately, our experiments have generated all four of these\nbehaviors-and the divergent behavior occurs frequently, even for quite simple problems.\n\n\x0cGeneralization in Reinforcement Learning: Safely Approximating the Value Function\n\n2.1\n\n37 J\n\nDIVERGENCE IN SMOOTH VALUE ITERATION\n\nWe have run simulations in a variety of domains-including a continuous gridworld,\na car-on-the-hill problem with nonlinear dynamics, and tic-tac-toe versus a stochastic opponent-and using a variety of function approximators, including polynomial\nregression, backpropagation, and local weighted regression. In our experiments, none\nof these function approximators was immune from divergence.\nThe first set ofresults is from the 2-D continuous gridworld, described in Figure 1.\nBy quantizing the state space into a 100 x 100 grid, we can compute J* with discrete\nvalue iteration, as shown in Figure 2. The optimal value function is exactly linear:\nJ*(x, y) = 20 - lOx - lOy.\nSince J* is linear, one would hope smooth value iteration could converge to it with a\nfunction approximator as simple as linear or quadratic regression. However, the intermediate value functions of Figure 2 are not smooth and cannot be fit accurately by\na low-order polynomial. Using linear regression on a sample of 256 randomly-chosen\nstates, smooth value iteration took over 500 iterations before "luckily" converging to\noptimal. Quadratic regression, though it always produces a smaller fit error than linear regression, did not converge (Figure 3). The quadratic function, in trying to both\nbe flat in the middle of state space and bend down toward 0 at the goal corner, must\ncompensate by underestimating the values at the corner opposite the goal. These\nunderestimates then enlarge on each iteration, as the one-step DP lookaheads erroneously indicate that points can lower their expected cost-to-go by stepping farther\naway from the goal. The resulting policy is anti-optimal.\nfontinuous Gridworld\n\nJ*(x,y)\n\n0.8\n0.6\n\n>.\n0.4\n0.2\n0L-0~.~2-0~.~4-0~.~6~0~.~8~1\n\nx\n\nFigure 1: In the continuous gridworld domain, the state is a point (x, y) E [0,1]2. There are\nfour actions corresponding to short steps (length 0.05, cost 0.5) in each compass direction,\nand the goal region is the upper right-hand corner. l*(x, y) is linear.\nIteration 12\n\nIteration 25\n\n.8\n\n1\n\nFigure 2: Computation of 1* by discrete value iteration\n\nIteration 40\n\n\x0cJustin Boyan, Andrew W. Moore\n\n372\n\nIteration 17\n\nIteration 43\n\nIteration 127\n\n1\n.8\n\n.8\n\n.8\n\n1\n\nFigure 3: Divergence of smooth value iteration with quadratic regression (note z-axis).\nJ*(x , y)\n\nIteration 144\n\no.\no.\n>.\n\no.\n\n.8\n\no.\n0.20 . 40.60 . 8\nx\n\n1\n1\n\nFigure 4: The 2-D continuous gridworld with puddles, its optimal value function, and a\ndiverging approximation of the value function by Local Weighted Regression (note z-axis).\ncar- o n-the-Hill\n\nJ* (pa s, vel)\n\n0.5\n\npas\n\nFigure 5: The car-on-the-hill domain. When the velocity is below a threshold, the car must\nreverse up the left hill to gain enough speed to reach the goal, so r is discontinuous.\nIteration 11\n\nIterati on 101\n\nIteration 201\n\nFigure 6: Divergeri\'ce oYsmooth value iteration wit~\'\nfor car-on-th~~hill~ The\nneural net, a 2-layer MLP with 80 hidden units, was trained for 2000 epochs per iteration.\n\nIt may seem as though the divergence of smooth value iteration shown above can be\nattributed to the global nature of polynomial regression. In fact, when the domain\nis made slightly less trivial, the same types of instabilities appear with even a highly\n\n\x0cGeneralization in Reinforcement Learning: Safely Approximating the Value Function\n\n373\n\nTable 1: Summary of convergence results: Smooth value iteration\nDomain\n2-D grid world\n2-D puddle world\nCar-on-the-hill\n\nLinear\nlucky\n\nQuadratic\ndiverge\n\n-\n\n-\n\n-\n\n-\n\nLWR\ngood\ndiverge\ngood\n\nBackprop\nlucky\ndiverge\ndiverge\n\nlocal memory-based function approximator such as local weighted regression (LWR)\n[Cleveland and Delvin, 1988]. Figure 4 shows the continuous gridworld augmented\nto include two oval "puddles" through which it is costly to step. Although LWR can\nfit the corresponding J* function nearly perfectly, smooth value iteration with LWR\nnonetheless reliably diverges. On another two-dimensional domain, the car-on-the-hill\n(Figure 5), smooth value iteration with LWR did converge, but a neural net trained\nby backpropagation did not (see Figure 6) . Table 1 summarizes our results .\nIn light of such experiments, we conclude that the straightforward combination of\nDP and function approximation is not robust. A general-purpose learning method\nwill require either using a function approximator constrained to be robust during DP\n[Yee, 1992], or an algorithm which explicitly prevents divergence even in the face of\nimperfect function approximation, such as the Grow-Support algorithm we present\nin Section 3.\n2.2 RELATED WORK\nTheoretically, it is not surprising that inserting a smoothing process into a recursive\nDP procedure can lead to trouble. In [Thrun and Schwartz, 1993] one case is analyzed\nwith the assumption that errors due to function approximation bias are independently\ndistributed. Another area of theoretical analysis concerns inadequately approximated\nJ* functions. In [Singh and Yee, 1994] and [Williams, 1993] bounds are derived for the\nmaximum reduction in optimality that can be produced by a given error in function\napproximation. If a basis function approximator is used, then the reduction can be\nlarge [Sabes, 1993]. These results assume generalization from a dataset containing\ntrue optimal values; the true reinforcement learning scenario is even harder because\neach iteration of DP requires its own function approximation.\n\n3\n\nTHE GROW-SUPPORT ALGORITHM\n\nThe Grow-Support algorithm is designed to construct the optimal value function with\na generalizing function approximator while being robust and stable. It recognizes that\nfunction approximators cannot always be relied upon to fit the intermediate value\nfunctions produced by DP. Instead, it assumes only that the function approximator\ncan represent the final J* function accurately. The specific principles of Grow-Support\nare these:\n1. We maintain a "support" set of states whose final J* values have been computed, starting with goal states, and growing this set out from the goal. The\nfitter is trained only on these values, which we assume it is capable of fitting.\n2. Instead of propagating values by one-step DP backups, we use simulations\nwith the current greedy policy, called "rollouts". They explicitly verify the\nachievability of a state\'s cost-to-go estimate before adding that state to the\n\n\x0c374\n\nJustin Boyan, Andrew W. Moore\n\nsupport. In a rollout, the J values are derived from costs of actual paths to the\ngoal, not from the values of the previous iteration\'s function approximation.\nThis prevents divergence .\n3. We take maximum advantage of generalization. Each iteration, we add to\nthe support set any sample state which can, by executing a single action,\nreach a state that passes the rollout test. In a discrete environment, this\nwould cause the support set to expand in one-step concentric "shells" back\nfrom the goal. But in our continuous case, the function approximator may\nbe able to extrapolate correctly well beyond the support region-and when\nthis happens, we can add many points to the support set at once. This leads\nto the very desirable behavior that the support set grows in big jumps in\nregions where the value function is smooth.\nIteration 1,\n\nI Support I =4\n\nIteration 2,\n\n1Support 1=12\n\nIteration 3,\n\nISupportl=256\n\nFigure 7: Grow-Support with quadratic regression on the gridworld. (Compare Figure 3.)\nIteration 1,\n\nI Support I =3\n\nIteration 2,\n\nISupportl=213\n\nIteration 5,\n\nISupportl=253\n\nFigure 8: Grow-Support with LWR on the two-puddle gridworld. (Compare Figure 4.)\nIteration 3,\n\nI Support I =79\n\nIteration 8,\n\nISupportl=134\n\nIteration 14,\n\nISupportl=206\n\n3\n\nO.\n\n2\n\nO.\n\n-2\n\no.\n\nFigure 9: Grow-Support with backprop on car-on-the-hill. (Compare Figure 6.)\n\nThe algorithm, again restricted to the deterministic case for simplicity, is outlined in\nthe appendix. In Figures 7-9, we illustrate its convergence on the same combinations\nof domain and function approximator which caused smooth value iteration to diverge.\nIn Figure 8, all but three points are added to the support within only five iterations,\n\n\x0cGeneralization in Reinforcement Learning: Safely Approximating the Value Function\n\n375\n\nand the resulting greedy policy is optimal. In Figure 9, after 14 iterations, the algorithm terminates. Although 50 states near the discontinuity were not added to the\nsupport set, the resulting policy is optimal within the support set. Grow-support\nconverged to a near-optimal policy for all the problems and fitters in Table 1.\nThe Grow-Support algorithm is more robust than value iteration. Empirically, it was\nalso seen to be no more computationally expensive (and often much cheaper) despite\nthe overhead of performing rollouts. Reasons for this are (1) the rollout test is not\nexpensive; (2) once a state has been added to the support, its value is fixed and it\nneeds no more computation; and most importantly, (3) the aggressive exploitation\nof generalization enables the algorithm to converge in very few iterations. However,\nwith a nondeterministic problem, where multiple rollouts are required to assess the\naccuracy of a prediction, Grow-Support would become more expensive.\nIt is easy to prove that Grow-Support will always terminate after a finite number\nof iterations. If the function approximator is inadequate for representing the J*\nfunction, Grow-Support may terminate before adding all sample states to the support\nset. When this happens, we then know exactly which of the sample states are having\ntrouble and which have been learned. This suggests potential schemes for adaptively\nadding sample states to the support in problematic regions. Investigation of these\nideas is in progress.\n\nIn conclusion, we have demonstrated that dynamic programming methods may diverge when their tables are replaced by generalizing function approximators. Our\nGrow-Support algorithm uses rollouts, rather than one-step backups, to assign training values and to keep inaccurate states out of the training set. We believe these\nprinciples will contribute substantially to producing practical, robust, reinforcement\nlearning.\nAcknowledgements\nWe thank Scott Fahlman, Geoff Gordon, Mary Lee, Michael Littman and Marc Ringuette for\ntheir suggestions, and the NDSEG fellowship and NSF Grant IRI-9214873 for their support.\n\nAPPENDIX: ALGORITHMS\nSmooth Value Iteration(X, G, A, NEXT-STATE, COST, FITJ):\nGiven: _ a finite collection of states X = {Xl, X2, .. . XN} sampled from the\ncontinuous state space X C fR n , and goal region G C X\n_ a finite set of allowable actions A\n_ a deterministic transition function NEXT-STATE: X x A -+ X\n_ the I-step cost function COST: X x A -+ fR\n_ a smoothing function approximator FIT J\niter := 0\n]<0) [i] := 0 Vi = 1 ... N\n{X I t-+ J?(iter) [1] }\nrepeat\n!rain ~ITJ(iter) to approximate the training set:\n:\nIter .:= Iter + 1;\nXN t-+ /iter)[N]\nfor ~ := 1 ... N do\n.(iter) [.] ._ { 0\n.\nJ\n1.minaEA (COST(Xi,a) + FITJ(lter-I)(NEXT-STATE(xi,a)))\nuntil j array stops changing\n\nif Xi E G\notherwise\n\n\x0c376\n\nJustin Boyan, Andrew W. Moore\n\nsubroutine RoIloutCost(x, J):\nStarting from state x , follow the greedy policy defined by value function J until\neither reaching the goal, or exceeding a total path cost of J(x) + ?. Then return:\n--t the actual total cost of the path, if goal is reached from x with cost ~ J(x) + e\n--t 00, if goal is not reached in cost J(x) + ?.\nGrow-Support(X,G,A, NEXT-STATE, COST, FITJ):\nGiven: ? exactly the same inputs as Smooth Value Iteration.\nSUPPORT := {(Xi t-+ 0) I Xi E G}\nrepeat\nTrain FIT J to approximate the training set SUPPORT\nfor each Xi ~ SUPPORT do\nc := minaEA [COsT(xi,a) + RolloutCost(NEXT-STATE(Xi, a), FITJ)]\nif c < 00 then\nadd (Xi t-+ c) to the training set SUPPORT\nuntil SUPPORT stops growing or includes all sample points.\n\nReferences\n[Barto et al., 1989] A . Barto, R. Sutton , and C . Watkins . Learning and sequential decision making. Technical Report COINS 89- 95, Univ. of Massachusetts, 1989 .\n[Bellman et al., 1963] R . Bellman, R . Kalaba, and B . Kotkin. Polynomial approximation-a new computational technique in dynamic programming: Allocation processes . Mathematics of Computation, 17,\n1963.\n[Boyan , 1992] J. A . Boyan. Modular neural networks for learning context-dependent game strategies .\nMaster\'s thesis, Cambridge University, 1992.\n[Bradtke, 1993] S. J. Bradtke. Reinforcement learning applied to linear quadratic regulation. In S. J .\nHanson, J . Cowan, and C . L . Giles, editors, NIPS-5. Morgan Kaufmann , 1993.\n[Cleveland and Delvin, 1988] W . S. Cleveland and S. J. Delvin. Locally weighted regression : An approach\nto regression analysis by local fitting. JASA , 83(403):596-610, September 1988.\n[Lin, 1993] L.-J . Lin . Reinforcement Learning for Robots Using Neural Networks. PhD thesis, Carnegie\nMellon University, 1993.\n[Mahadevan and Connell , 1990] S. Mahadevan and J. Connell . Automatic programming of behavior-based\nrobots using reinforcement learning. Technical report, IBM T. J . Watson Research Center, NY 10598,\n1990 .\n[Sabes, 1993] P. Sabes . Approximating Q-values with basis function represent ations. In Proceedings of\nthe Fourth C onnectionist Models Summer School, 1993.\n[Schraudolph et al., 1994] N . Schraudolph, P . Dayan, and T. Sejnowski . Using TD(>.) to learn an evaluation function for the game of Go . In J. D. Cowan, G . Tesauro , and J . Alspector, editors, NIPS-6.\nMorgan Kaufmann, 1994.\n[Singh and Yee, 1994] S. P. Singh and R. Yee. An upper bound on the loss from approximate optimal-value\nfunctions . Machine Learning, 1994. Technical Note (to appear) .\n[Sutton, 1988] R . Sutton . Learning to predict by the methods of temporal differences. Machine Learning,\n3,1988.\n[Tesauro, 1992] G. Tesauro. Practical issues in temporal difference learning. Machine Learning, 8(3/4),\nMay 1992.\n[Thrun and Schwartz, 1993] S. Thrun and A . Schwartz. Issues in using function approximation for reinforcement learning. In Proceedings of the Fourth Connectionist Models Summer School, 1993.\n[Watkins, 1989] C . Watkins. Learning from Delayed Rewards. PhD thesis, Cambridge University, 1989 .\n[Williams, 1993] R. Williams . Tight performance bounds on greedy policies based on imperfect value\nfunctions . Technical Report NU-CCS-93-13, Northeastern University, 1993.\n[Yee, 1992] R . Yee.\nsachusetts, 1992.\n\nAbstraction in control learning.\n\nTechnical Report COINS 92-16 , Univ. of Mas-\n\n\x0c'
p83155
sg91
S"A Mixture Model System for Medical and\nMachine Diagnosis\n\nTerrence J. Sejnowski\n\nMagnus Stensmo\n\nComputational Neurobiology Laboratory\nThe Salk Institute for Biological Studies\n10010 North Torrey Pines Road\nLa Jolla, CA 92037, U.S.A.\n{magnus,terry}~salk.edu\n\nAbstract\nDiagnosis of human disease or machine fault is a missing data problem\nsince many variables are initially unknown. Additional information needs\nto be obtained. The j oint probability distribution of the data can be used to\nsolve this problem. We model this with mixture models whose parameters\nare estimated by the EM algorithm. This gives the benefit that missing\ndata in the database itself can also be handled correctly. The request for\nnew information to refine the diagnosis is performed using the maximum\nutility principle. Since the system is based on learning it is domain\nindependent and less labor intensive than expert systems or probabilistic\nnetworks. An example using a heart disease database is presented.\n\n1 INTRODUCTION\nDiagnosis is the process of identifying diseases in patients or disorders in machines by\nconsidering history, symptoms and other signs through examination. Diagnosis is a common\nand important problem that has proven hard to automate and formalize. A procedural\ndescription is often hard to attain since experts do not know exactly how they solve a\nproblem.\nIn this paper we use the information about a specific problem that exists in a database\n\n\x0c1078\n\nMagnus Stensmo. Terrence J. Sejnowski\n\nof cases. The disorders or diseases are determined by variables from observations and\nthe goal is to find the probability distribution over the disorders, conditioned on what has\nbeen observed. The diagnosis is strong when one or a few of the possible outcomes are\ndifferentiated from the others. More information is needed if it is inconclusive. Initially\nthere are only a few clues and the rest of the variables are unknown. Additional information\nis obtained by asking questions and doing tests. Since tests may be dangerous, time\nconsuming and expensive, it is generally not possible or desirable to find the answer to\nevery question. Unnecessary tests should be avoided .\n. There have been many attempts to automate diagnosis. Early work [Ledley & Lusted, 1959]\nrealized that the problem is not always tractable due to the large number of influences that\ncan exist between symptoms and diseases. Expert systems, e.g. the INTERNIST system\nfor internal medicine [Miller et al., 1982], have rule-bases which are very hard and time\nconsuming to build. Inconsistencies may arise when new rules are added to an existing\ndatabase. There is also a strong domain dependence so knowledge bases can rarely be\nreused for new applications.\nBayesian or probabilistic networks [Pearl, 1988] are a way to model a joint probability\ndistribution by factoring using the chain rule in probability theory. Although the models\nare very powerful when built, there are presently no general learning methods for their\nconstruction. A considerable effort is needed. In the Pathfinder system for lymph node\npathology [Heckerman et al., 1992] about 14,000 conditional probabilities had to be assessed\nby an expert pathologist. It is inevitable that errors will occur when such large numbers of\nmanual assessments are involved.\nApproaches to diagnosis that are based on domain-independent machine learning alleviate\nsome of the problems with knowledge engineering. For decision trees [Quinlan, 1986], a\npiece of information can only be used if the appropriate question comes up when traversing\nthe tree. This means that irrelevant questions can not be avoided. Feedforward multilayer\nperceptrons for diagnosis [Baxt, 1990] can classify very well, but they need full information\nabout a case. None of these these methods have adequate ways to handle missing data during\nlearning or classification.\nThe exponentially growing number of probabilities involved can make exact diagnosis\nintractable. Simple approximations such as independence between all variables and conditional independence given the disease (naive Bayes) introduce errors since there usually are\ndependencies between the symptoms. Even though systems based on these assumptions\nwork surprisingly well, correct diagnosis is not guaranteed. This paper will avoid these\nassumptions by using mixture models.\n\n2 MIXTURE MODELS\nDiagnosis can be formulated as a probability estimation problem with missing inputs. The\nprobabilities of the disorders are conditioned on what has currently been observed. If we\nmodel the joint probability distribution it is easy to marginalize to get any conditional\nprobability. This is necessary in order to be able to handle missing data in a principled\nway [Ahmad & Tresp, 1993]. Using mixture models [McLachlan & Basford, 1988], a\nsimple closed form solution to optimal regression with missing data can be formulated. The\nEM algorithm, a method from parametric statistics for parameter estimation, is especially\ninteresting in this context since it can also be formulated to handle missing data in the\n\n\x0cA Mixture Model System for Medical and Machine Diagnosis\n\n1079\n\ntraining examples [Dempster et al., 1977; Ghahramani & Jordan, 1994].\n\n2.1 THE EM ALGORITHM\nThe data underlying the model is assumed to be a set of N D-dimensional vectors X =\n{ Z I, . . . , Z N }. Each data point is assumed to have been generated independently from a\nmixture density with M components\nM\n\np(z)\n\nM\n\n= LP(z,Wj;Oj) = LP(Wj)p(zlwj;Oj),\n\n(1)\nj=l\nj=l\nwhere each mixture component is denoted by Wj. p(Wj), the a priori probability for\nmixturewj, and 8 = (0 1 , ... , OM) are the model parameters.\nTo estimate the parameters for the different mixtures so that it is likely that the linear combination of them generated the set of data points, we use maximum likelihood estimation. A\ngood method is the iterative Expectation-Maximization, or EM, algorithm [Dempster et al.,\n1977].\nTwo steps are repeated. First a likelihood is formulated and its expectation is computed in\nthe E-step. For the type of models that we will use, this step will calculate the probability\nthat a certain mixture component generated the data point in question. The second step\nis the M-step where the parameters that maximize the expectation are found. This can be\nfound analytically for models that can be written in an exponential form, e.g. Gaussian\nfunctions. Equations can be derived for both batch and on-line learning. Update equations\nfor Gaussian distributions with and without missing data will be given here, other distributions are possible, e.g. binomial or multinomial [Stensmo & Sejnowski, 1994]. Details and\nderivations can be found in [Dempster et al., 1977; Nowlan, 1991; Ghahramani & Jordan,\n1994; Stensmo & Sejnowski, 1994].\nFrom (1) we form the log likelihood of the data\nN\n\nN\n\nM\n\nL(8IX) = L logp(zi; OJ) = L log LP(Wj )P(Zi IWj; OJ).\nj=l\nThere is unfortunately no analytic solution to the logarithm of the sum in the right hand side\nof the equation. However, if we were to know which of the mixtures generated which data\npoint we could compute it. The EM algorithm solves this by introducing a set of binary\nindicator variables Z = {Zij}. Zij = 1 if and only if the data point Zi was generated by\nmixture component j. The log likelihood can then be manipulated to a form that does not\ncontain the log of a sum.\nThe expectation of %i using the current parameter values 8 k is used since %i is not known\ndirectly. This is the E-step of the EM algorithm. The expected value is then maximized in\ntheM-step. The two steps are iterated until convergence. The likelihood will never decrease\nafter an iteration [Dempster et al., 1977]. Convergence is fast compared to gradient descent.\nOne of the main motivations for the EM-algorithm was to be able to handle missing values\nfor variables in a data set in a principled way. In the complete data case we introduced\nmissing indicator variables that helped us solve the problem. With missing data we add\nthe missing components to the Z already missing [Dempster et aI., 1977; Ghahramani &\nJordan, 1994].\n\n\x0c1080\n\nMagnus Stensmo, Terrence J. Sejnowski\n\n2.2 GAUSSIAN MIXTURES\nWe specialize here the EM algorithm to the case where the mixture components are radial\nGaussian distributions. For mixture component j with mean I-'j and covariance matrix 1:j\nthis is\n\nThe form of the covariance matrix is often constrained to be diagonal or to have the\nsame values on the diagonal, 1:j = o} I. This corresponds to axis-parallel oval-shaped\nand radially symmetric Gaussians, respectively. Radial and diagonal basis functions can\nfunction well in applications [Nowlan, 1991], since several Gaussians together can form\ncomplex shapes in the space. With fewer parameters over-fitting is minimized. In the radial\ncase, with variance o}\n\nIn the E-step the expected value of the likelihood is computed. For the Gaussian case this\nbecomes the probability that Gaussian j generated the data point\n\nPj(Z)\n\n=\n\n!(Wj)Gj(z)\n.\nl:k=l P(Wk)Gk(Z)\n\nThe M-step finds the parameters that maximize the likelihood from the E-step. For complete\ndata the new estimates are\n\n(2)\nN\n\nwhere Sj = I:Pj(Zi).\ni=l\n\nWhen input variables are missing the Gj(z) is only evaluated over the set of observed\ndimensions O. Missing (unobserved) dimensions are denoted by U. The update equation\n= itY and use (2). The variance\nfor p(Wj) is unchanged. To estimate itj we set\nbecomes\n\nzf\n\nA least squares regression was used to fill in missing data values during classification. For\nmissing variables and Gaussian mixtures this becomes the same approach used by [Ahmad &\nTresp, 1993]. The result of the regression when the outcome variables are missing is a\nprobability distribution over the disorders. This can be reduced to a classification for\ncomparison with other systems by picking the outcome with the maximum of the estimated\nprobabilities.\n\n\x0cA Mixture Model System for Medical and Machine Diagnosis\n\n3\n\n1081\n\nREQUESTING MORE INFORMATION\n\nDuring the diagnosis process, the outcome probabilities are refined at each step based on\nnewly acquired knowledge. It it important to select the questions that lead to the minimal\nnumber of necessary tests. There is generally a cost associated with each test and the goal\nis to minimize the total cost. Early work on automated diagnosis [Ledley & Lusted, 1959]\nacknowledged the problem of asking as few questions as possible and suggested the use of\ndecision analysis for the solution. An important idea from the field of decision theory is the\nmaximum expected utility principle [von Neuman & Morgenstern, 1947]: A decision maker\nshould always choose the alternative that maximizes some expected utility of the decision .\nFor diagnosis it is the cost of misclassification . Each pair of outcomes has a utility u(x, y)\nwhen the correct diagnosis is x but y has been incorrectly determined. The expectation can\nbe computed when we know the probabilities of the outcomes.\nThe utility values have to be assessed manually in what can be a lengthy and complicated\nprocess. For this reason a simplification of this function has been suggested by [Heckerman et al., 1992]: The utility u(x, y) is 1 when both x and y are benign or both are malign,\nand 0 otherwise. This simplification has been found to work well in practice. Another\ncomplication with maximum expected utility principle can also make it intractable. In the\nideal case we would evaluate every possible sequence of future choices to see which is\nthe best. Since the size of the search tree of possibilities grows exponentially this is often\nnot possible. A simplification is to 100k ahead only one or a few steps at a time. This\nnearsighted or myopic approach has been tested in practice with good results [Gorry &\nBarnett, 1967; Heckerman et al., 1992] .\n\n4\n\nTHE DIAGNOSIS SYSTEM\n\nThe system we have developed has two phases. First there is a learning phase where a\nprobabilistic model is built. This model is then used for inference in the diagnosis phase.\nIn the learning phase, the joint probability distribution of the data is modeled using mixture\nmodels. Parameters are determined from a database of cases by the EM algorithm. The\nk-means algorithm is used for initialization. Input and output variables for each case are\ncombined into one vector per case to form the set of training patterns. The outcomes and\nother nominal variables are coded as J of N . Continuous variables are interval coded.\nIn the diagnosis phase, myopic one-step look-ahead was used and utilities were simplified\nas above. The following steps were performed:\n1. Initial observations were entered.\n2. Conditional expectation regression was used to fill in unknown variables.\n3. The maximum expected utility principle was used to recommend the next observation to make. Stop if nothing would be gained by further observations.\n4. The user was asked to determine the correct value for the recommended observation. Any other observations could be made, instead of or in addition to this.\n5. Continue with step 2.\n\n\x0cMagnus Stensmo, Terrence J. Sejnowski\n\n1082\n\nTable 1: The Cleveland Heart Disease database.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\nObservation\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\n\n11\n\nslope\n\n12\n13\n\nca\nthaI\nDisorder\nnum\n\n14\n\nDescription\nAge in years\nSex of subject\nChest pain\nResting blood pressure\nSerum cholesterol\nFasting blood sugar\nResting electrocardiogr.\nMax heart rate achieved\nExercise induced angina\nST depr. induced by\nexercise relative to rest\nSlope of peak exercise\nSTsegment\n# major vess. col. ftourosc.\nDefect type\nDescription\nHeart disease\n\nValues\ncontinuous\nmale/female\nfour types\ncontinuous\ncontinuous\nIt or gt 120 mg/dl\nfive values\ncontinuous\nyes/no\ncontinuous\nup/flat/down\n0-3\nnormal/fixed/reversible\nValues\nNot present/4 types\n\n5 EXAMPLE\nThe Cleveland heart disease data set from UC, Irvine has been used to test the system. It\ncontains 303 examples of four types of heart disease and its absence. There are thirteen\ncontinuous- or nominally-valued variables (Table 1). The continuous variables were interval\ncoded with one unit per standard deviation away from the mean value. This was chosen since\nthey were approximately normally distributed. Nominal variables were coded with one unit\nper value. In total the 14 variables were coded with 55 units. The EM steps were repeated\nuntil convergence (60-150 iterations). A varying number of mixture components (20-120)\nwere tried.\nPreviously reported results have used only presence or absence of the heart disease. The\nbest of these has been a classification rate of 78.9% using a system that incrementally\nbuilt prototypes [Gennari et al., 1989]. We have obtained 78.6% correct classification\nwith 60 radial Gaussian mixtures as described above. Performance increased with the\nnumber of mixture components. It was not sensitive to a varying number of mixture\ncomponents during training unless there were too few of them. Previous investigators have\npointed out that there is not enough information in the thirteen variables in this data set to\nreach 100% [Gennari et al., 1989].\nAn annotated transcript of a diagnosis session is shown in Figure 1.\n\n6 CONCLUSIONS AND FURTHER WORK\nSeveral properties of this model remain to be investigated. It should be tested on several\nmore databases. Unfortunately databases are typically proprietary and difficult to obtain.\nFuture prospects for medical databases should be good since some hospitals are now using\ncomputerized record systems instead of traditional paper-based. It should be fairly easy to\n\n\x0cA Mixture Model System for Medical and Machine Diagnosis\n\n1083\n\nThe leftmost number of the five numbers in a line is the estimated probability for no heart\ndisease, followed by the probabilities for the four types of heart disease. The entropy, defined\nas Pi log Pi' of the diagnoses are given at the same time as a measure of how decisive\nthe current conclusion is. A completely detennined diagnosis has entropy O. Initially all of\nthe variables are unknown and starting diagnoses are the unconditional prior probabilities.\n\nl:.\n\nDisorders (entropy = 1.85):\n0.541254 0.181518 0.118812\nWhat is cp ? 3\n\n0.115512\n\n0.042904\n\nThe first question is chest pain, and the answer changes the estimated probabilities. This\nvariable is continuous. The answer is to be interpreted how far from the mean the observation\nis in standard deviations. As the decision becomes more conclusive, the entropy decreases.\n\nDisorders (entropy = 0.69):\n0.888209 0.060963 0.017322\nWhat is age ? 0\n\n0.021657\n\n0.011848\n\nDisorders (entropy = 0.57):\n0.91307619 0.00081289 0.02495360\nWhat is oldpeak ? -2\n\n0.03832095\n\n0.02283637\n\nDisorders (entropy = 0.38):\n0.94438718 0.00089016 0.02539957\nWhat is chol? -1\n\n0.02691099\n\n0.00241210\n\n0.00507073\n\n0 . 00294036\n\nDisorders (entropy = 0.11):\n0.98848758 0.00028553 0.00321580\n\nWe have now detennined that the probability of no heart disease in this case is 98.8%. The\nremaining 0.2% is spread out over the other possibilities.\n\nFigure 1: Diagnosis example.\n\ngenerate data for machine diagnosis.\nAn alternative way to choose a new question is to evaluate the variance change in the output\nvariables when a variable is changed from missing to observed. The idea is that a variable\nknown with certainty has zero variance. The variable with the largest resulting conditional\nvariance could be selected as the query, similar to [Cohn et aI., 1995].\nOne important aspect of automated diagnosis is the accompanying explanation for the\nconclusion, a factor that is important for user acceptance. Since the basis functions have\nlocal support and since we have estimates for the probability of each basis function having\ngenerated the observed data, explanations for the conclusions could be generated.\nInstead of using the simplified utilities with values 0 and 1 for the expected utility calculations they could be learned by reinforcement learning. A trained expert would evaluate the\nquality of the diagnosis performed by the system, followed by adjustment of the utilities.\nThe 0 and 1 values can be used as starting values.\n\n\x0c1084\n\nMagnus Stensmo. Terrence J. Sejnowski\n\nAcknowledgements\nThe heart disease database is from the University of California, Irvine Repository of\nMachine Learning Databases and originates from R. Detrano, Cleveland Clinic Foundation.\nPeter Dayan provided helpful comments on an earlier version of this paper.\n\nReferences\nAhmad, S. & Tresp, V. (1993). Some solutions to the missing feature problem in vision.\nIn Advances in Neural Information Processing Systems, vol. 5, pp 393-400. Morgan\nKaufmann, San Mateo, CA.\nBaxt, W. (1990). Use of an artificial neural network for data analysis in clinical decisionmaking: The diagnosis of acute coronary occlusion. Neural Computation, 2(4),480489.\nCohn, D. A., Ghahramani, Z. & Jordan, M.1. (1995). Active learning with statistical models.\nIn Advances in Neural Information Processing Systems, vol. 7. Morgan Kaufmann, San\nMateo, CA.\nDempster, A., Laird, N. & Rubin, D. (1977). Maximum likelihood from incomplete data\nvia the EM algorithm. Journal of the Royal Statistical Society, Series, B., 39, 1-38.\nGennari, 1, Langley, P. & Fisher, D. (1989). Models of incremental concept formation.\nArtificial Intelligence, 40, 11-62.\nGhahramani, Z. & Jordan, M. (1994). Supervised learning from incomplete data via an EM\napproach. In Advances in Neural Information Processing Systems, vol. 6, pp 120-127.\nMorgan Kaufmann, San Mateo, CA.\nGorry, G. A. & Barnett, G. O. (1967). Experience with a model of sequential diagnosis.\nComputers and Biomedical Research, 1, 490-507.\nHeckerman, D., Horvitz, E. & Nathwani, B. (1992). Toward normative expert systems:\nPart I. The Pathfinder project. Methods of Information in Medicine, 31, 90-105.\nLedley, R. S. & Lusted, L. B. (1959). Reasoning foundations of medical diagnosis. Science,\n130(3366),9-21.\nMcLachlan, G. J. & Basford, K. E. (1988). Mixture Models: Inference and Applications to\nClustering. Marcel Dekker, Inc., New York, NY.\nMiller, R. A., Pople, H. E. & Myers, 1 D. (1982). Internist-I: An experimental computerbased diagnostic consultant for general internal medicine. New England Journal of\nMedicine, 307, 468-476.\nNowlan, S. J. (1991). Soft Competitive Adaptation: Neural Network Learning Algorithms\nbased on Fitting Statistical Mixtures. PhD thesis, School of Computer Science, Carnegie\nMellon University, Pittsburgh, PA.\nPearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible\nInference. Morgan Kaufmann, San Mateo, CA.\nQuinlan, 1 R. (1986). Induction of decision trees. Machine Learning, 1,81-106.\nStensmo, M . & Sejnowski, T. J. (1994). A mixture model diagnosis system. Tech. Rep.\nINC-9401, Institute for Neural Computation, University of California, San Diego.\nvon Neuman, J. & Morgenstern, O. (1947). Theory of Games and Economic Behavior.\nPrinceton University Press, Princeton, NJ.\n\n\x0c"
p83156
sg12
S'Associative Decorrelation Dynamics:\nA Theory of Self-Organization and\nOptimization in Feedback Networks\n\nDawei W. Dong*\nLawrence Berkeley Laboratory\nUniversity of California\nBerkeley, CA 94720\n\nAbstract\nThis paper outlines a dynamic theory of development and adaptation in neural networks with feedback connections. Given input ensemble, the connections change in strength according to an\nassociative learning rule and approach a stable state where the\nneuronal outputs are decorrelated . We apply this theory to primary visual cortex and examine the implications of the dynamical\ndecorrelation of the activities of orientation selective cells by the\nintracortical connections. The theory gives a unified and quantitative explanation of the psychophysical experiments on orientation\ncontrast and orientation adaptation. Using only one parameter , we\nachieve good agreements between the theoretical predictions and\nthe experimental data.\n\n1\n\nIntroduction\n\nThe mammalian visual system is very effective in detecting the orientations of lines\nand most neurons in primary visual cortex selectively respond to oriented lines and\nform orientation columns [1) . Why is the visual system organized as such? We\n*Present address: Rockefeller University, B272, 1230 York Avenue, NY, NY 10021-6399.\n\n\x0c926\n\nDawei W Dong\n\nbelieve that the visual system is self-organized, in both long term development and\nshort term adaptation, to ensure the optimal information processing.\nLinsker applied Hebbian learning to model the development of orientation selectivity and later proposed a principle of maximum information preservation in early\nvisual pathways [2]. The focus of his work has been on the feedforward connections\nand in his model the feedback connections are isotropic and unchanged during the\ndevelopment of orientation columns; but the actual circuitry of visual cortex involves extensive, columnar specified feedback connections which exist even before\nfunctional columns appear in cat striate cortex [3].\nOur earlier research emphasized the important role of the feedback connections in\nthe development of the columnar structure in visual cortex. We developed a theoretical framework to help understand the dynamics of Hebbian learning in feedback networks and showed how the columnar structure originates from symmetry\nbreaking in the development of the feedback connections (intracortical, or lateral\nconnections within visual cortex) [4].\nFigure 1 illustrates our theoretical predictions. The intracortical connections break\nsymmetry and develop strip-like patterns with a characteristic wave length which\nis comparable to the developed intracortical inhibitory range and the LGN-cortex\nafferent range (left). The feedforward (LGN-cortex) connections develop under the\ninfluence of the symmetry breaking development of the intracortical connections.\nThe developed feedforward connections for each cell form a receptive field which\nis orientation selective and nearby cells have similar orientation preference (right) .\nTheir orientations change in about the same period as the strip-like pattern of the\nintracortical connections.\n\nFigure 1: The results of the development of visual cortex with feedback connections. The\nsimulated cortex consists of 48 X 48 neurons, each of which connects to 5 X 5 other cortical\nneurons (left) and receives inputs from 7 X 7 LGN neurons (right). In this figure, white\ninclicates positive connections and black inclicates negative connections. One can see that\nthe change of receptive field\'s orientation (right) is highly correlated with the strip-like\npattern of intracortical connections (left).\n\nMany aspects of our theoretical predictions agree qualitatively with neurobiological observations in primary visual cortex. Another way to test the idea of optimal\n\n\x0cAssociative Correlation Dynamics\n\n927\n\ninformation processing or any self-organization theory is through quantitative psychophysical studies. The idea is to look for changes in perception following changes\nin input environments. The psychophysical experiments on orientation illusions\noffer some opportunities to test our theory on orientation selectivity.\nOrientation illusions are the effects that the perceived orientations of lines are affected by the neighboring (in time or space) oriented stimuli, which have been\nobserved in many psychophysical experiments and were attributed to the inhibitory\ninteractions between channels tuned to different orientations [5]. But there is no unified and quantitative explanation. Neurophysiological evidences support our earlier\ncomputational model in which intracortical inhibition plays the role of gain-control\nin orientation selectivity [6]. But in order for the gain-control mechanism to be\neffective to signals of different statistics, the system has to develop and adapt in\ndifferent environments.\nIn this paper we examine the implication of the hypothesis that the intracortical\nconnections dynamically decorrelate the activities of orientation selective cells, i.e.,\nthe intracortical connections are actively adapted to the visual environment, such\nthat the output activities of orientation selective cells are decorrelated. The dynamics which ensures such decorrelation through associative learning is outlined in the\nnext section as the theoretical framework for the development and the adaptation\nof intracortical connections. We only emphasize the feedback connections in the\nfollowing sections and assume that the feedforward connections developed orientation selectivities based on our earlier works. The quantitative comparisons of the\ntheory and the experiments are presented in section 3.\n\n2\n\nAssociative Decorrelation Dynamics\n\nThere are two different kinds of variables in neural networks. One class of variables\nrepresents the activity of the nerve cells, or neurons. The other class of variables\ndescribes the synapses, or connections, between the nerve cells. A complete model\nof an adaptive neural system requires two sets of dynamical equations, one for each\nclass of variables, to specify the evolution and behavior of the neural system.\nThe set of equations describing the change of the state of activity of the neurons is\ndVi\n\nadt\n-I\n\n= -ViI\n\n+ ~T.\nL..J .. v.. + 1I}}\n\nI\n\n(1)\n\nj\n\nin which a is a time constant, Tij is the strength of the synaptic connection from\nneuron j to neuron i, and Ii is the additional feedforward input to the neuron besides\nthose described by the feedback connection matrix nj . A second set of equations\ndescribes the way the synapses change with time due to neuronal activity. The\nlearning rule proposed here is\nB dnj = (V,. - V.\')!,\ndt\n\nin which B is a time constant and\nin the following.\n\nVi\'\n\nI\n\nI}\n\n(2)\n\nis the feedback learning signal as described\n\nThe feedback learning signal Vi\' is generated by a Hopfield type associative memory\nnetwork: Vi\' = Lj T/j Vi , in which T/j is the strength of the associative connection\n\n\x0c928\n\nDawei W Dong\n\nfrom neuron j to neuron i, which is the recent correlation between the neuronal\nactivities Vi and Vj determined by Hebbian learning with a decay term [4]\n\nB\n\n,dTfj\n\n,\n\ndt = -Iij + ViVj\n\n(3)\n\nin which B\' is a time constant. The Vi\' and T[j are only involved in learning and\ndo not directly affect the network outputs.\nIt is straight forward to show that when the time constants B\ndynamics reduces to\n\ndT\n\nB dt\n\n= (1- < VVT ?\n\n> > B\' > > a, the\n\n< VIT >\n\n(4)\n\nwhere bold-faced quantities are matrices and vectors and <> denotes ensemble\naverage. It is not difficult to show that this equation has a Lyapunov or "energy"\nfunction\nL = Tr(1- < VV T ?(1- < VVT\n(5)\nwhich is lower bounded and satisfies\n\n>f\n\ndL\n\n<0\n\ndt -\n\nand\n\ndL =0\n\ndt\n\n-+-\n\ndTij =\ndt\n\n0 I\'lor at,)\n11"\n\n(6)\n\nThus the dynamics is stable. When it is stable, the output activities are decorrelated ,\n<VVT >= 1\n(7)\nThe above equation shows that this dynamics always leads to a stable state where\nthe neuronal activities are decorrelated and their correlation matrix is orthonormal.\nYet the connections change in an associative fashion - equation (2) and (3) are\nalmost Hebbian . That is why we call it associative decorrelation dynamics. From information processing point of view, a network, self-organized to satisfy equation (7),\nis optimized for Gaussian input ensembles and white output noises [7].\n\nLinear First Order Analysis\nIn applying our theory of associative decorrelation dynamics to visual cortex to\ncompare with the psychophysical experiments on orientation illusions, the linear\nfirst-order approximation is used, which is\n\nT = TO + 6T,\nV = Va +6V,\n\nTO = 0, 6T ex - < I IT >\nVa = I, 6V = TI\n\n(8)\n\nwhere it is assumed that the input correlations are small. It is interesting to notice\nthat the linear first-order approximation leads to anti-Hebbian feedback connections: Iij ex - < /i/j > which is guarantteed to be stable around T = 0 [8].\n\n3\n\nQuantitative Predictions of Orientation Illusions\n\nThe basic phenomena of orientation illusions are demonstrated in figure 2 (left).\nOn the top, is the effect of orientation contrast (also called tilt illusion): within the\ntwo surrounding circles there are tilted lines; the orientation of a center rectangle\n\n\x0cAssociative Correlation Dynamics\n\n929\n\nappears rotated to the opposite side of its surrounding tilt. Both the two rectangles and the one without surround (at the left-center of this figure) are, in fact,\nexactly same. On the bottom, is the effect of orientation adaptation (also called\ntilt aftereffect): if one fixates at the small circle in one of the two big circles with\ntilted lines for 20 seconds or so and then look at the rectangle without surround,\nthe orientation of the lines of the rectangle appears tilted to the opposite side.\nThese two effects of orientation illusions are both in the direction of repulsion: the\napparent orientation of a line is changed to increase its difference from the inducing\nline. Careful experimental measurements also revealed that the angle with the\ninducing line is\n100 for maximum orientation adaptation effect [9] but 20 0 for\norientation contrast [10].\n<"V\n\n<"V\n\n1\n\nOl..---~-~-""\';:"\'\'\'\'\'\'\'\'\'---\'\n\n-90\n\n-45\n\no\n\nStimulus orientation\n\n45\n(J\n\n90\n\n(degree)\n\nFigure 2: The effects of orientation contrast (upper-left) and orientation adaptation (lowerleft) are attributed to feedback connections between cells tuned to different orientations\n(upper-right, network; lower-right, tuning curve).\n\nOrientation illusions are attributed to the feedback connections between orientation selective cells. This is illustrated in figure 2 (right). On the top is the network\nof orientation selective cells with feedback connections. Only four cells are shown.\nFrom the left, they receive orientation selective feedforward inputs optimal at -45 0 ,\n00 ,45 0 , and 90 0 , respectively. The dotted lines represent the feedback connections\n(only the connections from the second cell are drawn). On the bottom is the orientation tuning curve of the feedforward input for the second cell, optimally tuned to\nstimulus of 00 (vertical), which is assumed to be Gaussian of width (T = 20 0 ? Because of the feedback connections, the output of the second cell will have different\ntuning curves from its feedforward input, depending on the activities of other cells.\nFor primary visual cortex, we suppose that there are orientation selective neurons\ntuned to all orientations. It is more convenient to use the continuous variable e\ninstead of the index i to represent neuron which is optimally tuned to the orientation\nof angle e. The neuronal activity is represented by V(e) and the feedforward input\nto each neuron is represented by I(e). The feedforward input itself is orientation\n\n\x0c930\n\nDawei W. Dong\n\nselective: given a visual stimulus of orientation\n\nJ(e) =\n\neo, the input is\n\ne-(9-9 o )2/ q 2\n\n(9)\nThis kind of the orientation tuning has been measured by experiments (for references, see [6]). Various experiments give a reasonable tuning width around 20?\n?(7" = 20? is used for all the predictions).\nPredicted Orientation Adaptation\nFor the orientation adaptation to stimulus of angle eo, substituting equation (9)\ninto equation (8), it is not difficult to derive that the network response to stimulus\nof angle 0 (vertical) is changed to\n\nV(e)\n\n= e_ 92 / q2 _\n\nae-(9-9 o )2/ q 2 e-9~/2q2\n\n(10)\n\nin which (7" is the feedforward tuning width chosen to be 20? and a is the parameter\nof the strength of decorrelation feedback.\nThe theoretical curve of perceived orientation ?(eo) is derived by assuming the\nmaximum likelihood of the the neural population, i.e., the perceived angle ? is the\nangle at which Vee) is maximized. It is shown in figure 3 (right). The solid line is\nthe theoretical curve and the experimental data come from [9] (they did not give\nthe errors, the error bars are of our estimation,...., 0.2?). The parameter obtained\nthrough X2 fit is the strength of decorrelation feedback: a = 0.42.\n2.0 ;--\'"T""---,--...,.....-----.------,\n\n-\n\n~ 1.5\n\n-\n\n~ 3.0\n\n~\n\n~\nII)\n\nII)\n\nCD 1.0\n\n}\n\n."\n~\n\n."\n\n~\n\n0.5\n\n~\n\n2.0\n\nII)\n> 1.0\n\'il\n\n\'il\nQ.,\n\n4.0\n\n0.0 f - - - - - - - - - - - - - J\n\no\n\n10\n20\n30\n40\nSurround angle 80 (degree)\n\n50\n\ni:!\nII)\n\nQ.,\n\n0.0\n0\n\n10\n20\n30\n40\n50\nAdaptation angle 80 (degree)\n\nFigure 3: Quantitative comparison of the theoretical predictions with the experimental\ndata of orientation contrast (left) and orientation adaptation (right).\n\nIt is very interesting that we can derive a relationship which is independent of the\nparameter of the strength of decorrelation feedback a,\n\n(eo - ?m)(3e o - 2?m) = (7"2\n(11)\nin which eo is the adaptation angle at which the tilt aftereffect is most significant\nand ?m is the perceived angle.\nPredicted Orientation Contrast\nFor orientation contrast, there is no specific adaptation angle, i.e., the network has\ndeveloped in an environment of all possible angles. In this case, when the surround\nis of angle eo, the network response to a stimulus of angle e1 is\nVee) = e-(9-9 1)2/ q 2 _ ae-(9-9 o )2/ 3q 2\n(12)\n\n\x0cAssociative Correlation Dynamics\n\n931\n\nin which fr and a has the same meaning as for orientation adaptation. Again assuming the maximum likelihood, ?(eo), the stimulus angle e1 at which it is perceived\nas angle 0, is derived and shown in figure 3 (left). The solid line is the theoretical\ncurve and the experimental data come from [10] and their estimated error is "" 0.20.\nThe parameter obtained through X 2 fit is the strength of decorrelation feedback:\na = 0.32.\nWe can derive the peak position eo, i.e., the surrounding angle\norientation contrast is most significant,\n\n~e~ =\n3\n\nfr2\n\neo\n\nat which the\n\n(13)\n\nFor fr = 20 0 , one immediately gets eo = 24 0 ? This is in good agreement with\nexperiments, most people experience the maximum effect of orientation contrast\naround this angle.\nOur theory predicts that the peak position of the surround angle for orientation\ncontrast should be constant since the orientation tuning width fr is roughly the\nsame for different human observers and is not going to change much for different\nexperimental setups. But the peak value of the perceived angle is not constant since\nthe decorrelation feedback parameter a is not necessarily same, indeed, it could be\nquite different for different human observers and different experimental setups.\n\n4\n\nDiscussion\n\nFirst, we want to emphasis that in all the comparisons, the same tuning width fr is\nused and the strength of decorrelation feedback a is the only fit parameter. It does\nnot take much imagination to see that the quantitative agreements between the\ntheory and the experiments are good. Further more, we derived the relationships\nfor the maximum effects, which are independent of the parameter a and have been\npartially confirmed by the experiments.\nRecent neurophysiological experiments revealed that the surrounding lines did influence the orientation selectivity of cells in primary visual cortex of the cat [11].\nThose single cell experiments land further support to our theory. But one should\nbe cautioned that the cells in our theory should be considered as the average over\na large population of cells in cortex.\nThe theory not only explains the first order effects which are dominant in angle\nrange of 00 to 50 0 , as shown here, but also accounts for the second order effects\nwhich can be seen in 500 to 90 0 range, where the sign of the effects is reversed.\nThe theory also makes some predictions for which not much experiment has been\ndone yet, for example, the prediction about how orientation contrast depends on\nthe distance of surrounding stimuli from the test stimulus [7].\nFinally, this is not merely a theory for the development and the adaptation of\norientation selective cells, it can account for effect such as human vision adaptation\nto colors as well [7]. We can derive the same equation as Atick etal [12] which agrees\nwith the experiment on the appearance of color hue after adaptation. We believe\nthat future psychophysical experiments could give us more quantitative results to\nfurther test our theory and help our understanding of neural systems in general.\n\n\x0c932\n\nDawei W. Dong\n\nAcknowledgements\nThis work was supported in part by the Director, Office of Energy Research, Division of Nuclear Physics of the Office of High Energy and Nuclear Physics of the\nU.S. Department of Energy under Contract No. DE-AC03-76SF00098.\n\nReferences\n[1] Hubel DH, Wiesel TN, 1962 Receptive fields, binocular interactions, and functional\narchitecture in the cat\'s visual cortex J Physiol (London) 160, 106- 54. - 1963\nShape and arrangement of columns in cat\'s striate cortex J Physiol (London) 165,\n559-68.\n[2] Linsker R, 1986 From basic network principles to neural architecture ... Proc Natl\nAcad Sci USA 83, 7508 8390 8779. - , 1989 An application of the principle of maximum information preservation to linear systems Advances in Neural Information\nProcessing Systems 1, Touretzky DS, ed, Morgan Kaufman, San Mateo, CA 186-94.\n[3] Gilbert C, Wiesel T, 1989 Columnar Specificity of intrinsic horizontal and corticocortical connections in cat visual cortex J Neurosci 9(7), 2432-42. Luhmann HJ,\nMartinez L, Singer W, 1986 Development of horizontal intrinsic connections in cat\nstriate cortex Exp Brain Res 63, 443-8.\n[4] Dong DW, 1991 Dynamic properties of neural network with adapting synapses Proc\nInternational Joint Conference on Neural Networks, Seattle, 2, 255- 260. - , 1991\nDynamic Properties of Neural Networks Ph D thesis, University Microfilms International, Ann Arbor, ML Dong DW, Hopfield JJ, 1992 Dynamic properties of neural\nnetworks with adapting synapses Network: Computation in Neural Systems, 3(3),\n267- 83.\n[5] Gibson J J, Radner M, 1937 Adaptation, after-effect and contrast in the perception\nof tilted lines J of Exp Psy 20, 453-67. Carpenter RHS, Blakemore C, 1973 Interactions between orientations in human vision Exp Brain Res 18, 287-303. Tolhurst\nDJ, Thompson PG, 1975 Orientation illusions and after-effects: Inhibition between\nchannels Vis Res 15,967-72. Barlow HB, Foldiak P, 1989 Adaptation and decorrelation in the cortex The Computing Neuron, Durbin R, Miall C, Mitchison G, eds,\nAddison- Wesley, New York, NY.\n[6] Wehmeier U, Dong DW, Koch C, Van Essen DC, 1989 Modeling the mammalian\nvisual system Methods in Neuronal Modeling: From Synapses to Networks, Koch C,\nSegev I, eds, MIT Press, Cambridge, MA 335-60.\n[7] Dong DW, 1993 Associative Decorrelation Dynamics in Visual Cortex Lawrence\nBerkeley Laboratory Technical Report LBL-34491.\n[8] Dong DW, 1993 Anti-Hebbian dynamics and total recall of associative memory Proc\nWorld Congress on Neural Networks, Portland, 2, 275-9.\n[9] Campbell FW, Maffei L, 1971 The tilt after-effect: a fresh look Vis Res 11, 833-40.\n[10] Westheimer G, 1990 Simultaneous orientation contrast for lines in the human fovea\nVis Res 30, 1913-21.\n\n[11] Gilbert CD, Wiesel TN, 1990 The influence of contextual stimuli on the orientation\nselectivity of cells in primary visual cortex of the cat Vis Res 30,1689-701.\n[12] Atick JJ, Li Z, Redlich AN, 1993 What does post-adaptation color appearance reveal\nabout cortical color representation Vis Res 33, 123-9.\n\n\x0c'
p83157
sg94
S'A Connectionist Technique for Accelerated\nTextual Input: Letting a Network Do the Typing\n\nDean A. Pomerleau\npomerlea@cs.cmu.edu\nSchool of Computer Science\nCarnegie Mellon University\nPittsburgh, PA 15213\n\nAbstract\nEach year people spend a huge amount of time typing. The text people type\ntypically contains a tremendous amount of redundancy due to predictable\nword usage patterns and the text\'s structure. This paper describes a\nneural network system call AutoTypist that monitors a person\'s typing and\npredicts what will be entered next. AutoTypist displays the most likely\nsubsequent word to the typist, who can accept it with a single keystroke,\ninstead of typing it in its entirety. The multi-layer perceptron at the heart\nof Auto\'JYpist adapts its predictions of likely subsequent text to the user\'s\nword usage pattern, and to the characteristics of the text currently being\ntyped. Increases in typing speed of 2-3% when typing English prose and\n10-20% when typing C code have been demonstrated using the system,\nsuggesting a potential time savings of more than 20 hours per user per year.\nIn addition to increasing typing speed, AutoTypist reduces the number of\nkeystrokes a user must type by a similar amount (2-3% for English, 1020% for computer programs). This keystroke savings has the potential to\nsignificantly reduce the frequency and severity of repeated stress injuries\ncaused by typing, which are the most common injury suffered in today\'s\noffice environment.\n\n1 Introduction\nPeople in general, and computer professionals in particular, spend a huge amount of time\ntyping. Most of this typing is done sitting in front of a computer display using a keyboard as\nthe primary input device. There are a number of efforts using artificial neural networks and\nother techniques to improve the comfort and efficiency of human-computer communication\nusing alternative modalities. Speech recognition [Waibel et al., 1988], handwritten character\nrecognition [LeCun et al., 1989], and even gaze tracking [Baluja & Pomerleau, 1993] have\n\n\x0c1040\n\nDean Pomerleau\n\nthe potential to facilitate this communication. But these technologies are still in their infancy,\nand at this point cannot approach the speed and accuracy of even a moderately skilled typist\nfor textual input.\nIs there some way to improve the efficiency of standard keyboard-based human-computer\ncommunication? The answer is yes, there are several ways to make typing more efficient.\nThe first, called the Dvorak keyboard, has been around for over 60 years. The Dvorak\nkeyboard has a different arrangement of keys, in which the most common letters, E, T, S,\netc., are on the home row right under the typist\'s fingers. This improved layout requires the\ntypist\'s fingers to travel1116th as far, resulting in an average of20% increase in typing speed.\nUnfortunately, the de facto standard in keyboards is the inefficient QWERTY configuration,\nand people are reluctant to learn a new layout.\nThis paper describes another approach to improving typing efficiency, which can be used\nwith either the QWERTY or DVORAK keyboards. It takes advantage of the hundreds of\nthousands of computer cycles between the typist\'s keystrokes which are typically wasted\nwhile the computer idly waits for additional input. By spending those cycles trying to predict\nwhat the user will type next, and allowing the typist to accept the prediction with a single\nkeystroke, substantial time and effort can be saved over typing the entire text manUally.\nThere are actually several such systems available today, including a package called "Autocompletion" developed for gnu-emacs by the author, and an application called "Magic\nTypist" developed for the Apple Macintosh by Olduvai Software. Each of these maintains\na database of previously typed words, and suggests completions for the word the user is\ncurrently in the middle of typing, which can be accepted with a single keystroke. While reasonable useful, both have substantial drawbacks. These systems use a very naive technique\nfor calculating the best completion, simply the one that was typed most recently. In fact,\nexperiments conducted for this paper indicated that this "most recently used" heuristic is\ncorrect only about 40% of the time. In addition, these two systems are annoyingly verbose,\nalways suggesting a completion if a word has been typed previously which matches the\nprefix typed so far. They interrupt the user\'s typing to suggest a completion even if the\nword they suggest hasn\'t been typed in many days, and there are many other alternative\ncompletions for the prefix, making it unlikely that the suggestion will be correct. These\ndrawbacks are so severe that these systems frequently decrease the user\'s typing speed,\nrather than increase it.\nThe Auto\'JYpist system described in this paper employs an artificial neural network during the\nspare cycles between keystrokes to make more intelligent decisions about which completions\nto display, and when to display them.\n\n2 The Prediction Task\nTo operationalize the goal of making more intelligent decisions about which completions\nto display, we have defined the neural networks task to be the following: Given a list of\ncandidate completions for the word currently being typed, estimate the likelihood that the\nuser is actually typing each of them. For example, if the user has already types the prefix\n"aut", the word he is trying to typing could anyone of a large number of possibilities,\nincluding "autonomous", "automatic", "automobile" etc. Given a list of these possibilities\ntaken from a dictionary, the neural network\'s task is to estimate the probability that each of\nthese is the word the user will type.\nA neural network cannot be expected to accurately estimate the probability for a particular\ncompletion based on a unique representation for each word, since there are so many words\n\n\x0cA Connectionist Technique for Accelerated Textual Input\n\nATTRIBUTE\nabsolute age\nrelative age\n\nabsolute frequency\nrelative frequency\ntyped previous\ntotal length\nremaining length\nspecial character match\n\ncapitalization match\n\n1041\n\nDESCRIPTION\ntime since word was last typed\nratio of the words age to age of the\nmost recently typed alternative\nnumber of times word has been typed\nin the past\nratio of the words frequency to that\nof the most often typed alternative\n1 if user has typed word previously,\nootherwise\nthe word\'s length, in characters\nthe number of characters left after the\nprefix to be typed for this word\nthe percentage of "special characters"\n(Le. not a-z) in this word relative to the\npercentage of special characters typed\nrecently\n1 if the capitalization of the prefix the\nuser has already typed matches the word\'s\nusual capitalization, 0 otherwise.\n\nTable 1: Word attributes used as input to the neural network for predicting word probabilities.\nin the English language, and there is only very sparse data available to characterize an\nindividual\'s usage pattern for any single word. Instead, we have chosen to use an input\nrepresentation that contains only those characteristics of a word that could conceivably have\nan impact on its probability of being typed. The attributes we employed to characterize each\ncompletion are listed in Table 1.\nThese are not the only possible attributes that could be used to estimate the probability of\nthe user typing a particular word. An additional characteristic that could be helpful is the\nword\'s part of speech (i.e. noun, verb, adjective, etc.). However this attribute is not typically\navailable or even meaningful in many typing situations, for instance when typing computer\nprograms. Also, to effectively exploit information regarding a word\'s part of speech would\nrequire the network to have knowledge about the context of the current text. In effect, it\nwould require at least an approximate parse tree of the current sentence. While there are\ntechniques, including connectionist methods [Jain, 1991], for generating parse trees, they\nare prone to errors and computationally expensive. Since word probability predictions in\nour system must occur many times between each key the user types, we have chosen to\nutilize only the easy to compute attributes shown in Table 1 to characterize each completion.\n\n3 Network Processing\nThe network architecture employed for this system is a feedforward multi-layer perceptron.\nEach of the networks investigated has nine input units, one for each of the attributes listed\nin Table 1, and a single output unit. As the user is typing a word, the prefix he has typed so\nfar is used to find candidate completions from a dictionary, which contains 20,000 English\nwords plus all words the user has typed previously. For each of these candidate completions,\nthe nine attributes in Table 1 are calculated, and scaled to the range of 0.0 to 1.0. These\nvalues become the activations of the nine units in the input layer. Activation is propagated\nthrough the network to produce an activation for the single output unit, representing the\n\n\x0c1042\n\nDean Pomerleau\n\nprobability that this particular candidate completion is the one the user is actually typing.\nThese candidate probabilities are then used to determine which (if any) of the candidates\nshould be displayed to the typist, using a technique described in a later section.\nTo train the network, the user\'s typing is again monitored. After the user finishes typing a\nword, for each prefix of the word a list of candidate completions, and their corresponding\nattributes, is calculated. These form the input training patterns. The target activation for\nthe single output unit on a pattern is set to 1.0 if the candidate completion represented by\nthat pattern is the word the user was actually typing, and 0.0 if the candidate is incorrect.\nNote that the target output activation is binary. As will be seen below, the actual output the\nnetwork learns to produce is an accurate estimate of the completion\'s probability. Currently,\ntraining of the network is conducted off-line, using a fixed training set collected while a\nuser types normally. Training is performed using the standard backpropagation learning\nalgorithm.\n\n4 Experiments\nSeveral tests were conducted to determine the ability of multi-layer perceptrons to perform\nthe mapping from completion attributes to completion probability. In each of the tests,\nnetworks were trained on a set of inputJoutputexemplars collected over one week of a single\nsubject\'s typing. During the training data collection phase, the subject\'s primary text editing\nactivities involved writing technical papers and composing email, so the training patterns\nrepresent the word choice and frequency distributions associated with these activities. This\ntraining set contained of 14,302 patterns of the form described above.\nThe first experiment was designed to determine the most appropriate network architecture\nfor the prediction task. Four architecture were trained on a 10,000 pattern subset of the\ntraining data, and the remaining 4,302 patterns were used for cross validation. The first of\nthe four architectures was a perceptron, with the input units connected directly to the single\noutput unit. The remaining three architectures had a single hidden layer, with three, six\nor twelve hidden units. The networks with hidden units were fully connected without skip\nconnections from inputs to output. Networks of three and six hidden units which included\nskip connections were tested, but did not exhibit improved performance over the networks\nwithout skip connections, so they are not reported.\nEach of the network architectures were trained four times, with different initial random\nweights. The results reported are those produced by the best set of weights from these\ntrials. Note that the variations between trials with a single architecture were small relative\nto the variations between architectures. The trained networks were tested on a disjoint set\nof 10,040 collected while the same subject was typing another technical paper.\nThree different performance metrics were employed to evaluate the performance of these\narchitectures on the test set. The first was the standard mean squared error (MSE) metric,\ndepicted in Figure 1. The MSE results indicate that the architectures with six and twelve\nhidden units were better able to learn the task than either the perceptron, or the network with\nonly three hidden units. However the difference appears to be relatively small, on the order\nof about 10%.\nMSE is not a very informative error metric, since the target output is binary (1 if the\ncompletion is the one the user was typing, 0 otherwise), but the real goal is to predict\nthe probability that the completion is correct. A more useful measure of performance is\nshown in Figure 2. For each of the four architectures, it depicts the predicted probability\nthat a completion is correct, as measured by the network\'s output activation value, vs. the\n\n\x0c1043\n\nA Connectionist Technique for Accelerated Textual Input\n\n0.095\n\n0.070 ......._ _\n\nPerceptron\n\n3 Hidden\nUnits\n\n6 Hidden\nUnits\n\n12 Hidden\nUnits\n\nFigure 1: Mean squared error for four networks on the task of predicting completion\nprobability.\n\nactual probability that a completion is correct. The lines for each of the four networks\nwere generated in the following manner. The network\'s output response on each of the\n10,040 test patterns was used to group the test patterns into 10 categories. All the patterns\nwhich represented completions that the network predicted to have a probability of between\no and 10% of being correct (output activations of 0.0-0.1) were placed in one category.\nCompletions that the network predicted to have a 10-20% change of being right were placed\nin the second category, etc. For each of these 10 categories, the actual likelihood that\na completion classified within the category is correct was calculated by determining the\npercent of the completions within that category that were actually correct.\nAs a concrete example, the network with 6 hidden units produced an output activation\nbetween 0.2 and 0.3 on 861 of the 10,040 test patterns, indicating that on these patterns\nit considered there to be a 20-30% chance that the completion each pattern represented\nwas the word the user was typing. On 209 of these 861 patterns in this category, the\ncompletion was actually the one the user was typing, for a probability of 24.2%. Ideally, the\nactual probability should be 25%, half way between the minimum and maximum predicted\nprobability thresholds for this category. This ideal classification performance is depicted as\nthe solid 45? line labeled "Target" in Figure 2. The closer the line for a given network matches\nthis 45? line, the more the network\'s predicted probability matches the actual probability\nfor a completion. Again, the networks with six and twelve hidden units outperformed the\nnetworks with zero and three hidden units, as illustrated by their much smaller deviations\nfrom the 45? line in Figure 2.\nThe output activations produced by the networks with six and twelve hidden units reflect\nthe actual probability that the completion is correct quite accurately. However prediction\naccuracy is only half of what is required to perform the final system goal, which recall was\nto identify as many high probability completions as possible, so they can be suggested to\nthe user without requiring him to manually type them. If overall accuracy of the probability\npredictions were the only requirement, a network could score quite highly by classifying\n\n\x0c1044\n\nDean Pomerleau\n\n1.00\n\ne\n-a\n\n0\n.....\n\n.....\n.D\n\nPerceptron\n\n0.80\n\n~\n\n~\n\n3 Hidden Units\n6 Hidden Units\n\n.D\n~\n\nn;;get\n\n0.60\n\n12 Hidden Units\n\n0.40\n\nu\n\n<\n\n0.20\n0.00\n\nFigure 2: Predicted vs.\narchitectures tested.\n\nactual probability of a completion being correct for the four\n\nevery pattern into the 10-20% category, since about 15% of the 10,040 completions in the\ntest set represent the word the user was typing at the time. But a constant prediction of\n10-20% probability on every alternative completion would not allow the system to identify\nand suggest to the user those individual completions that are much more likely than the other\nalternatives.\nTo achieve the overall system goal, the network must be able to accurately identify as many\nhigh probability completions as possible. The ability of each of the four networks to achieve\nthis goal is shown in Figure 3. This figures shows the percent of the 10,040 test patterns each\nof the four networks classified as having more than a 60% probability of being correct. The\n60% probability threshold was selected because it represents a level of support for a single\ncompletion that is significantly higher than the support for all the others. As can be seen in\nFigure 3, the networks with hidden units again significantly outperformed the perceptron,\nwhich was able to correctly identify fewer than half as many completions as highly likely.\n\n5\n\nAuto1)rpist System Architecture and Performance\n\nThe networks with six and twelve hidden units are able to accurately identify individual\ncompletions that have a high probability of being the word the user is typing. In order\nto exploit this prediction ability and speed up typing, we have build an X-window based\napplication called AutoTypist around the smaller of the two networks. The application\nserves as the front end for the network, monitoring the user\'s typing and identifying likely\ncompletions for the current word between each keystroke. If the network at the core of\nAutoTypist identifies a single completion that it is both significantly more probably than all\nthe rest, and also longer than a couple characters, it will momentarily display the completion\nafter the current cursor location in whatever application the user is currently typing 1? If the\ndisplayed completion is the word the user is typing, he can accept it with a single keystroke\n(The criterion for displaying a completion, and the human interface for AutoTypist, are somewhat\nmore sophisticated than this description. However for the purposes of this paper, a high level\ndescription is sufficient.\n\n\x0cA Connectionist Technique for Accelerated Textual Input\n\n1045\n\nPercent of\n6.0\nPatterns Classified 5 .0\nas over 60%\n4.0\nProbable\n3.0\n\n2.0\n1.0\n\nPerceptron\n\n3 Hidden\nUnits\n\n6 Hidden\nUnits\n\n12 Hidden\nUnits\n\nFigure 3: Percent of candidate completions classified as having more than a 60% chance of\nbeing correct for the four architectures tested.\n\nand move on to typing the next word. If the displayed completion is incorrect, he can\ncontinue typing and the completion will disappear.\nQuantitative results with the fully integrated Auto1Ypist system, while still preliminary, are\nvery encouraging. In a two week trial with two subjects, who could type at 40 and 60 wpm\nwithout AutoTypists, their typings speeds were improved by 2.37% and 2.21 % respectively\nwhen typing English text. Accuracy improvements during these trials were even larger,\nsince spelling mistakes become rare when AutoTypist is doing a significant part of the\ntyping automatically. When writing computer programs, speed improvements of 12.93%\nand 18.47% were achieved by the two test subjects. This larger speedup was due to the\nfrequent repetition of variable and function names in computer programs, which Auto1Ypist\nwas able to expedite. Not only is computer code faster to produce with AutoTypist, it is\nalso easier to understand. AutoTypist encourages the programmer to use long, descriptive\nvariable and function names, by making him type them in their entirety only once. On\nsubsequent instances of the same name, the user need only type the first few characters and\nthen exploitAutoTypist\'s completion mechanism to type the rest. These speed improvements\nwere achieved by subjects who are already relatively proficient typists. Larger gains can\nbe expected for less skilled typists, since typing an entire word with a single keystroke will\nsave more time when each keystroke takes longer.\nPerhaps an even more significant benefit results from the reduced number of keystrokes\nAuto1Ypist requires the user to type. During the test trials described above, the two test\nsubjects had to strike an average of 2.89% fewer keys on the English text, and 16.42% fewer\nkeys on the computer code than would have been required to type the text out in its entirety.\nClearly this keystroke savings has the potential to benefit typists who suffer from repeated\nstress injuries brought on by typing.\nUnfortunately it is impossible to quantitatively compare these results with those of the other\ncompletion-based typing aids described in the introduction, since the other systems have\nnot been quantitatively evaluated. Subjectively, Auto1Ypist is far less disturbing than the\n\n\x0c1046\n\nDean Pomerleau\n\nalternatives, since it only displays a completion when there is a very good chance it is the\ncorrect one.\n\n6\n\nFuture Work\n\nFurther experiments are required to verify the typing speed improvements possible with\nAutoTypist, and to compare it with alternative typing improvement systems. Preliminary\nexperiments suggest a network trained on the word usage patterns of one user can generalize\nto that of other users, but it may be necessary to train a new network for each individual\ntypist. Also, the experiments conducted for this paper indicate that a network trained on\none type of text, English prose, can generalize to text with quite different word frequency\npatterns, C language computer programs. However substantial prediction improvements,\nand therefore typing speedup, may be possible by training separate networks for different\ntypes of text. The question of how to rapidly adapt a single network, or perhaps a mixture\nof expert networks, to new text types is one which should be investigated.\nEven without these extensions, AutoTypist has the potential to greatly improve the comfort\nand efficiency of the typing tasks. For people who type English text two hours per workday,\neven the conservative estimate of a 2% speedup translates into 10 hours of savings per\nyear. The potential time savings for computer programming is even more dramatic. A\nprogrammer who types code two hours per workday could potentially save between 52\nand 104 hours in a single year by using AutoTypist. With such large potential benefits,\ncommercial development of the AutoTypist system is also being investigated.\nAcknowledgements\n\nI would like to thank David Simon and Martial Hebert for their helpful suggestions, and for\nacting as willing test subjects during the development of this system.\n\nReferences\n[Baluja & Pomerleau, 1993] Baluja, S. and Pomerleau, D.A. (1993) Non-Intrusive Gaze\nTracking Using Artificial Neural Networks. In Advances in Neural Information Processing Systems 6, San Mateo, CA: Morgan Kaufmann Publishers.\n[Jain,1991] Jain, A.N. (1991) PARSEC: A connectionist learning architecture for parsing\nspoken language. Carnegie Mellon University School of Computer Science Technical\nReport CMU-CS-91-208.\n[LeCun et al., 1989] LeCun, Y., Boser, B., Denker, 1.S., Henderson, D., Howard, R.E.,\nHubbard, W., and Jackel, L.D. (1989) Backpropagation applied to handwritten zip\ncode recognition. Neural Computation 1(4).\n[Waibel et al., 1988] Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., Lang, K. (1988)\nPhoneme recognition: Neural Networks vs. Hidden Markov Models. Proceedings from\nInt. Conf on Acoustics, Speech and Signal Processing, New York, New York.\n\n\x0c'
p83158
sg96
S'Connectionist Speaker Normalization\nwith Generalized\nResource Allocating Networks\n\nCesare Furlanello\nIstituto per La Ricerca\nScientifica e Tecnologica\nPovo (Trento), Italy\nfurlan?lirst. it\n\nDiego Giuliani\nIstituto per La Ricerca\nScientifica e Tecnologica\nPovo (Trento), Italy\ngiuliani?lirst.it\n\nEdmondo Trentin\nIstituto per La Ricerca\nScientifica e Tecnologica\nPovo (Trento), Italy\ntrentin?lirst.it\n\nAbstract\nThe paper presents a rapid speaker-normalization technique based\non neural network spectral mapping. The neural network is used\nas a front-end of a continuous speech recognition system (speakerdependent, HMM-based) to normalize the input acoustic data from\na new speaker. The spectral difference between speakers can be\nreduced using a limited amount of new acoustic data (40 phonetically rich sentences). Recognition error of phone units from the\nacoustic-phonetic continuous speech corpus APASCI is decreased\nwith an adaptability ratio of 25%. We used local basis networks of\nelliptical Gaussian kernels, with recursive allocation of units and\non-line optimization of parameters (GRAN model). For this application, the model included a linear term. The results compare\nfavorably with multivariate linear mapping based on constrained\northonormal transformations.\n\n1\n\nINTRODUCTION\n\nSpeaker normalization methods are designed to minimize inter-speaker variations,\none of the principal error sources in automatic speech recognition. Training a speech\nrecognition system on a particular speaker (speaker-dependent or SD mode) generally gives better performance than using a speaker-independent system, which is\n\n\x0c868\n\nCesare Furlanello. Diego Giuliani. Edmondo Trentin\n\ntrained to recognize speech from a generic user by averaging over individual differences. On the other hand, performance may be dramatically worse when a SD\nsystem "tailored" on the acoustic characteristics of a speaker (the reference speaker)\nis used by another one (the new or target speaker). Training a SD system for any\nnew speaker may be unfeasible: collecting a large amount of new training data\nis time consuming for the speaker and unacceptable in some applications. Given\na pre-trained SD speech recognition system, the goal of normalization methods is\nthen to reduce to a few sentences the amount of training data required from a new\nspeaker to achieve acceptable recognition performance. The inter-speaker variation\nof the acoustic data is reduced by estimating a feature vector transformation between the acoustic parameter space of the new speaker and that of the reference\nspeaker (Montacie et al., 1989; Class et al., 1990; Nakamura and Shikano, 1990;\nHuang, 1992; Matsukoto and Inoue, 1992). This multivariate transformation, also\ncalled spectral mapping given the type of features considered in the parameterization of speech data, provides an acoustic front-end to the recognition system.\nSupervised speaker normalization methods require that the text of the training utterances required from the new speaker is known, while arbitrary utterances can\nbe used by unsupervised methods (Furui and Sondhi, 1991). Good performance\nhave been achieved with spectral mapping techniques based on MSE optimization\n(Class et al., 1990; Matsukoto and Inoue, 1992). Alternative approaches presented\nestimation of the spectral normalization mapping with Multi-Layer Perceptron neural networks (Montacie et al., 1989; Nakamura and Shikano, 1990; Huang, 1992;\nWatrous, 1994).\nThis paper introduces a supervised speaker normalization method based on neural\nnetwork regression with a generalized local basis model of elliptical kernels (Generalized Resource Allocating Network: GRAN model). Kernels are recursively allocated\nby introducing the heuristic procedure of (Platt, 1991) within the generalized RBF\nschema proposed in (Poggio and Girosi, 1989). The model includes a linear term\nand efficient on-line optimization of parameters is achieved by an automatic differentiation technique. Our results compare favorably with normalization by affine\nlinear transformations based on orthonormal constrained pseudoinverse. In this paper, the normalization module was integrated and tested as an acoustic front-end for\nspeaker-dependent continuous speech recognition systems. Experiments regarded\nphone units recognition with Hidden Markov Model (HMM) recognition systems.\nThe diagram in Figure 1 outlines the general structure of the experiment with\nGRAN normalization modules. The architecture is independent from the specific\nspeech recognition system and allows comparisons between different normalization\ntechniques. The GRAN model and a general procedure for data standardization are\ndescribed in Section 2 and 3. After a discussion of the spectral mapping problem\nin Section 4, the APASCI corpus used in the experiments and the characteristics\nof the acoustic data are described in Section 5. The recognition system and the\nexperiment set-up are detailed in Sections 6-8. Results are presented and discussed\nin Section 9.\n\n\x0cConnectionist Speaker Normalization with Generalized Resource Allocating Networks\n\nDataBase:\nreference phrase\n\n869\n\nphraseS\n\n(Yj } j - I ? ...? ]\n\nDynamic Time Warping\nTraining\n\nfx)\n\nI(Xi(t), Yj(t}}\n-\'\n\nTest\n\ni-I ?...? I\n\n\'-------------------"1\n\nNeural Network\nsupervised training\n\n:\n\nGRAN normalizati\n\nFeature Extraction\n\nSpeech Signal\ncorresponding to phrase S\nuttered by a new speaker\n\nOutput\n\nFigure 1: System overview\n\n2\n\nTHE GRAN MODEL\n\nFeedforward artificial neural networks can be regarded as a convenient realization\nof general functional superpositions in terms of simpler kernel functions (Barron\nand Barron, 1988). With one hidden layer we can implement a multivariate superposition f(z) = Ef=o cxjKj(z,wj) where Kj is a function depending on an\ninput vector z and a parameter vector Wj, a general structure which allows to realize flexible models for multivariate regression. We are interested in the schema:\ny = H K(x) + Ax + b with input vector x E Rd 1 and estimated output vector y E R 2 . K = (Kj) is a n-dimensional vector of local kernels, H is the\nd2 x n real matrix of kernel coefficients, b E R d 2 is an offset term and A is a\nd2 x d1 linear term. Implemented kernels are Gaussian, Hardy multiquadrics, inverse of Hardy multiquadrics and Epanenchnikov kernels, also in the NadarayaWatson normalized form (HardIe, 1990). The kernel allocation is based on a\nrecursive procedure: if appropriate novelty conditions are satisfied for the example (x\', y/), a new kernel Kn+1 is allocated and the new estimate Yn+l becomes\nYn+l (x) = Yn(X) + Kn+1 (llx - x\'llw)(y\' - Yn(X)) (HardIe, 1990). Global properties and rates of convergence for recursive kernel regression estimates are given in\n(Krzyzak, 1992). The heuristic mechanism suggested by (Platt, 1991) has been\nextended to include the optimization of the weighted metrics as requested in the\ngeneralized versions of RBF networks of (Poggio and Girosi, 1989). Optimization\nregards kernel coefficients, locations and bandwidths, the offset term, the coefficient matrix A if considered, and the W matrix defining the weighted metrics in\nthe input space: IIxll~ = xtwtWx. Automatic differentiation is used for efficient\non-line gradient-descent procedure w.r. t. different error functions (L2, L1, entropy\nfit), with different learning rates for each type of parameters.\n\n\x0c870\n\nCesare FurLanello, Diego GiuLiani, Edmondo Trentin\n\nIj;-::=<p\nX -----------+" Y\n\nTJx\n\nTJy\n\n-1\n\nTJy\n\nx -----------" Y\nFigure 2: Commutative diagram for the speaker normalization problem. The spectral mapping <p between original spaces X and Y is estimated by Ij; = TJy 1 . ip . TJx,\nobtained by composition of the neural GRAN mapping ip between PCA spaces X\nand Y with the two invertible PCA transformations TJx and TJy.\n\n3\n\nNETWORKS AND PCA TRANSFORMATIONS\n\nThe normalization module is designed to estimate a spectral mapping between the\nacoustic spaces of two different speakers. Inter-speaker variability is reflected by\nsignificant differences in data distribution in these multidimensional spaces (we considered 8 dimensions); in particular it is important to take into account global data\nanisotropy. More generally, it is also crucial to decorrelate the features describing\nthe data. A general recipe is to apply the well-known Principal Component Analysis (PCA) to the data, in this case implemented from standard numerical routines\nbased on Singular Value Decomposition of the data covariance matrices. The network was applied to perform a mapping between the new feature spaces obtained\nfrom the PCA transformations, mean translation included (Figure 2).\n\n4\n\nTHE SPECTRAL MAPPING PROBLEM\n\nA sound uttered by a speaker is generally described by a sequence offeature vectors\nobtained from the speech signal via short-time spectral analysis (Sec. 5). The spectral representations of the same sequence of sounds uttered by two speakers are subject to significant variations (e.g. differences between male and female speakers, regional accents, ... ). To deal with acoustic differences, a suitable transformation (the\nspectral mapping) is seeked which performs the "best" mapping between the corresponding spectra oftwo speakers. Let Y = (Yl, Y2, ... , YJ) and X = (x 1, X2, ... , XI) be\nthe spectral feature vector sequences of the same sentence uttered by two speakers,\ncalled respectively the reference and the new speaker. The desired mapping is performed by a function <pC Xi) such that the transformed vector sequence obtained from\nX = (Xi) approximates as close as possible the spectral vector sequence Y = (Yi).\nTo eliminate time differences between the two acoustic realizations, a time warping\nfunction has to be determined yielding pairs C(k) = (i(k),j(k))k=1.. .K of corresponding indexes of feature vectors in X and Y, respectively. The desired spectral\n\n\x0cConnectionist Speaker Normalization with Generalized Resource Allocating Networks\n\n87 J\n\nmapping r,o(Xi) is the one which minimizes Ef=l d(Yj(k)\' r,o(Xi(k?)) where d(?,?) is a\ndistorsion measure in the acoustic feature space. To estImate the transformation, a\nset of supervised pairs (Xi(k), Yj(k?) is considered. In summary, the training material\nconsidered in the experiments consisted of a set of vector pairs obtained by applying\nthe Dynamic Time Warping (DTW) algorithm (Sakoe and Chiba, 1978) to a set\nof phrases uttered by the reference and the new speaker.\n\n5\n\nTHE APASCI CORPUS\n\nThe experiments reported in this paper were performed on a portion of APASCI,\nan italian acoustic-phonetic continuous speech corpus. For each utterance, text\nand phonetic transcriptions were automatically generated (Angelini et al., 1994).\nThe corpus consists of two portions. The first part, for the training and validation of speaker independent recognition systems, consists of a training set (2140\nutterances), a development set (900 utterances) and a test set (860 utterances).\nThe sets contain, respectively, speech material from 100 speakers (50 males and 50\nfemales), 36 speakers (18 males and 18 females) and 40 speakers (20 males and 20\nfemales). The second portion of the corpus is for training and validation of speaker\ndependent recognition systems. It consists of speech material from 6 speakers (3\nmales and 3 females). Each speaker uttered 520 phrases, 400 for training and 120\nfor test. Speech material in the test set was acquired in different days with respect\nto the training set. A subset of 40 utterances from the training material forms the\nadaptation training set, to be used for speaker adaptation/normalization purposes.\nFor this application, each signal in the corpus was processed to obtain its parametric\nrepresentation. The signal was preemphasized using a filter with transfer function\nH(z)\n1 - 0.95 X z-l, and a 20 ms Hamming window is then applied every 10\nms. For each frame, the normalized log-energy as well as 8 Mel Scaled Cepstral\nCoefficients (MSCC) based on a 24-channel filter-bank were computed. Normalization of log-energy was performed by subtracting the maximum log-energy value in\nthe sentence; for each Mel coefficient, normalization was performed by subtracting\nthe mean value of the whole utterance. For both MSCC and the log-energy, the\nfirst order derivatives as well as the second order derivatives were computed. For\neach frame, all the computed acoustic parameters were combined in a single feature\nvector with 27 components.\n\n=\n\n6\n\nTHE RECOGNITION SYSTEM\n\nFor each of the 6 speakers, a SD HMM recognition system was trained with the 400\nutterances available in the APASCI corpus; the systems were bootstrapped with\ngender dependent models trained on the gender dependent speech material (1000\nutterances for male and 1140 utterances for female). A set of 38 context independent\nacoustic-phonetic units was considered. Left-to-right HMMs with three and four\nstates were adopted for short (i.e. p,t,k,b,d,g) and long (e.g. a,i,u,Q,e) sounds\nrespectively. Silence, pause and breath were modeled with a single state ergodic\nmodel. The output distribution probabilities were modeled with mixtures of 16\ngaussian probability densities, diagonal covariance matrixes. Transitions leaving\nthe same state shared the same output distribution probabilities.\n\n\x0c872\n\nCesare Furlanello, Diego Giuliani, Edmondo Trentin\n\nTable 1: Phone Recognition Rate (Unit Accuracy %) without normalization\n\n7\n\nTRAINING THE NORMALIZATION MODULES\n\nA set of 40 phrases was considered for each pair (new, re f erence) of speakers to train\nthe normalization modules. In order to take into account alternative pronunciation,\ninsertion or deletion of phonemes, pauses between words and other phenomena, the\nautomatic phonetic transcription and segmentation available in APASCI was used\nfor each utterance. Given two utterances corresponding to the same phrase, we considered only their segments having the same phonetic transcription. To determine\nthese segments the DTW algorithm was applied to the phonetic transcription of the\ntwo utterances. The DTW algorithm was applied a second time to the obtained\nsegments and the resulting optimal alignment paths gave the desired set of vector\npairs. The DTW algorithm was applied only to the 8 MSCC and the other acoustic\nparameters were left unmodified.\nWe trained networks with 8 inputs and 8 outputs. The model included a linear\nterm: first the linear term was fit to the data, and then the rest of the expansion\nwas estimated by fitting the residuals of the linear regression. The networks grew\nup to 50 elliptical gaussian kernels using dynamic allocation. Kernel coefficients,\nlocations and bandwidths were optimized using different learning rates for 10 epochs\nw.r.t the Ll norm, which proved to be more efficient than the usual L2 norm.\n\n8\n\nTHE RECOGNITION EXPERIMENTS\n\nExperiments concerned continuous phone recognition without any lexical and\nphonetical constraint (no phone statistic was used).\nFor all the couples\n(new, reference) of speakers in the database, a recognition experiment was performed using 90 (of the 120 available) test utterances from the new speaker with\nthe SD recognition system previously trained for the reference speaker. On average the test sets consisted of 4770 phone units. The experiments were repeated\ntransforming the test data with different normalization modules and performance\ncompared. Results are expressed in terms of insertions (Ins), deletions (Del) and\nsubstitutions (Sub) of phone units made by the recognizer. Unit Accuracy (U A)\nand Percent Correct (PC) performance indicators are respectively defined w.r.t.\nthe total number of units nunih as U A = 100 (1 - (Ins + Del + Sub)/nunit.) and\nPC = 100 (1 - (Del + Sub)/nunit.). In Table 1 the baseline speaker dependent\nperformance for the 6 speaker dependent systems is reported. Row labels indicate the speaker reference model while column labels identify whose target acous-\n\n\x0cConnectionist Speaker Normalization with Generalized Resource Allocating Networks\n\n873\n\nTable 2: Phone Recognition Rate (Unit Accuracy %) with NN normalization\n\ntic data are used. Thus U A and PC entries in the main diagonal are for the\nsame speaker who trained the system while the remaining entries relate to performance obtained with new speakers. We also considered the adaptability ratios for\na U A and P PC (Montacie et al., 1989): Pa (aRT - aRT )/(aRR - aRT) and\nPp = (PRT - PRT )/(PRR - PRT) where aRT indicate accuracy for reference speaker\nR and target T without normalization, aRR is the speaker dependent baseline accuracy and apex n indicates normalization. The same notation applies to the percent\ncorrect adaptability ratio pp.\n\n=\n\n9\n\n=\n\n=\n\nRESULTS AND CONCLUSIONS\n\nNormalization experiments have been performed with the set-up described in the\nprevious Section. The phone recognition rates obtained with normalization modules\nbased on the GRAN model are reported in Table 2 in terms of Unit Accuracy (dee\nTable 1 for the baseline performance). In Table 3 the performance of the GRAN\nmodel (NN) and constrained orthonormal linear mapping (LIN) are compared with\nthe baseline performance (SD: no adaptation) in terms of both Unit Accuracy and\nPercent Correct. The network shows an improvement, as evidenced by the variation\nin the Pa and Pp values. Results are reported averaging performance over all the\npairs (new,reference) of speakers (Total column), and considering pairs of speakers\nof the same gender and of different genders (Female: only female subjects, Male:\nonly males, Dill: different genders). An analysis of the adaptability ratios shows\nthat the effect of the network normalization is higher than with the linear network\nfor all the 3 subgroups of pairs: p~N = 0.20 vs p~IN = 0.16 for the Female\ncouples and liN = 0.16 vs p~IN = 0.15 for the Male couples. The improvement is\nhigher (p~N = 0.28, p~IN = 0.24) for speaker of different genders. Although these\npreliminary experiments show only a minor improvement of performance achieved\nby the network with respect to linear mappings, we expect that the selectivity of\nthe network could be exploited using acoustic contexts and code dependent neural\nnetworks.\nAcknowledgements\nThis work has been developed within a grant of the "Programma Nazionale di\nRicerca per la Bioelettronica" assigned by the Italian Ministry of University and\nTechnologic Research to Elsag Bailey. The authors would like to thank B. Angelini,\nF. Brugnara, B. Caprile, R. De Mori, D. Falavigna, G. Lazzari and P. Svaizer.\n\n\x0c874\n\nCesare Furlanello, Diego Giuliani, Edmondo Trentin\n\nTable 3: Phone Recognition Rate (%) in terms of both Unit Accuracy, Percent\nCorrect, and adaptability ratio p.\n\nReferences\n\nAngelini, B., Brugnara, F., Falavigna, D., Giuliani, D., Gretter, R., and Omologo,\nM. (September 1994). Speaker Independent Continuous Speech Recognition Using\nan Acoustic-Phonetic Italian Corpus. In Proc. of ICSLP, pages 1391-1394.\nBarron, A. R. and Barron, R. L. (1988). Statistical learning networks: a unifying\nview. In Symp. on the Interface: Statistics and Computing Science, Reston, VI.\nClass, F., Kaltenmeier, A., Regel, P., and Troller, K. (1990). Fast speaker adaptation for speech recognition system. In Proc. of ICASSP 90, pages 1-133-136.\nFurui, S. and Sondhi, M. M., editors (1991). Advances in Speech Signal Processing.\nMarcel Dekker and Inc.\nHardIe, W. (1990). Applied nonparametric regression, volume 19 of Econometric\nSociety Monographs. Cambridge University Press, New York.\nHuang, X. D. (1992). Speaker normalization for speech recognition. In Proc. of\nICASSP 92, pages 1-465-468.\nKrzyzak, A. (1992). Global convergence of the recursive kernel regression estimates\nwith applications in classification and nonlinear system estimation. IEEE Transactions on Information Theory, 38(4):1323-1338.\nMatsukoto, H. and Inoue, H. (1992). A piecewise linear spectral mapping for supervised speaker adaptation. In Proc. of ICASSP 92, pages 1-449-452.\nMontacie, C., Choukri, K., and Chollet, G. (1989). Speech recognition using temporal decomposition and multi-layer feed-forward automata. In Proc. of ICASSP\n89, pages 1-409-412.\nNakamura, S. and Shikano, K. (1990). A comparative study of spectral mapping\nfor speaker adaptation. In Proc. of ICASSP 90, pages 1-157-160.\nPlatt, J. (1991). A resource-allocating network for function interpolation. Neural\nComputation, 3(2):213-225.\nPoggio, T. and Girosi, F. (1989). A theory of networks for approximation and\nlearning. A.1. Memo No. 1140, MIT.\nSakoe, H. and Chiba, S. (1978). Dynamic programming algorithm optimization for\nspoken word recognition. IEEE-A SSP, 26(1):43-49.\nWatrous, R. (1994). Speaker normalization and adaptation using second-order connectionist networks. IEEE Trans. on Neural Networks, 4(1):21-30.\n\n\x0c'
p83159
sg48
S'A Critical Comparison of Models for\nOrientation and Ocular Dominance\nColumns in the Striate Cortex\nE. Erwin\nBeckman Institute\nUniversity of Illinois\nUrbana, IL 61801, USA\n\nK. Obermayer\nTechnische Fakultat\nU niversitat Bielefeld\n33615 Bielefeld, FRG\n\nK. Schulten\nBeckman Institute\nUniversity of Illinois\nUrbana, IL 61801, USA\n\nAbstract\nMore than ten of the most prominent models for the structure\nand for the activity dependent formation of orientation and ocular dominance columns in the striate cort(>x have been evaluated.\nWe implemented those models on parallel machines, we extensively\nexplored parameter space, and we quantitatively compared model\npredictions with experimental data which were recorded optically\nfrom macaque striate cortex.\nIn our contribution we present a summary of our results to date.\nBriefly, we find that (i) despite apparent differences, many models\nare based on similar principles and, consequently, make similar predictions, (ii) certain "pattern models" as well as the developmental\n"correlation-based learning" models disagree with the experimental data, and (iii) of the models we have investigated, "competitive\nHebbian" models and the recent model of Swindale provide the\nbest match with experimental data.\n\n1\n\nModels and Data\n\nThe models for the formation and structure of orientation and ocular dominance\ncolumns which we have investigated are summarized in table 1. Models fall into\ntwo categories: "Pattern models" whose aim is to achieve a concise description of\nthe observed patterns and "developmental models" which are focussed on the pro-\n\n\x0c94\n\nE. Erwin, K. Obermayer, K. Schulten\n\nClass\nPattern\nModels\n\nType\nStructural\nModels\nSpectral\nModels\n\nDevelop.\nModels\n\nCorrelation\nBased Learning\nCompetl bve\nHebbian\nOther\n\nModel\n1. Icecube\n2. Pinwheel\n3. Gotz\n4. Baxter\n5. ROJer\n6. Niebur\n7. Swindale\n8. Linsker\n9. Miller\n10. ~UM-h\n11 . SOM-I\n12. EN\n13. Tanaka\n14. Yuille\n\nReference\nHubel and Wiesel 1977 [~I\nBraitenberg and Braitenberg 1979 161\nGotz 1987 (8)\nBaxter and Dow 1989 11)\nROJer and Schwartz 1990 J20)\nNiebur and Worgotter 1993 (15)\nSwindale 1992a (21)\nLinsker 1986c J12]\nMiller 1989, 1994 113, 14)\nUbennayer, et. al. 1990 P~J\nObermayer, et. al. 1992(17)\nDurbin and Mitchison 1990 (7)\nTanaka 1991 [22J\nYuille, et. al. 1992 (23)\n\nTable 1: Models of visual cortical maps which have been evaluated.\n\ncesses underlying their formation. Pattern models come in two varieties, "structural\nmodels" and "spectral models", which describe orientation and ocular dominance\nmaps in real and in Fourier space, respectively. Developmental models fall into the\ncategories "correlations based learning", "competitive Hebbian" learning and a few\nmiscellaneous models.\nModels are compared with data obtained from macaque striate cortex through optical imaging [2, 3, 4, 16]. Data were recorded from the representation of the parafovea\nfrom the superficial layers of cortex. In the following we will state that a particular\nmodel reproduces a particular feature of the experimental data (i) if there exists a\nparameter regime where the model generates appropriate patterns and (ii) if the\nphenomena are robust. We will state that a particular model does not reproduce a\ncertain feature (i) if we have not found an appropriate parameter regime and (ii) if\nthere exists either a proof or good intuitive reasons that a lllodel cannot reproduce\nthis feature.\nOne has to keep in mind, though, that model predictions are compared with a fairly\nspecial set of data. Ocular dominance patterns, e.g., are known to vary between\nspecies and even between different regions within area 17 of an individual. Consequently, a model which does not reproduce certain featurE\'S of ocular dominance\nor orientation colulllns in the macaque may well describE\' those patterns in other\nspecies. Interspecies differences, however, are not. the focus of this contribution;\nresults of corresponding modelling studies will be reported E\'lsewhere.\n\n2\n\nExamples of Organizing Principles and Model Predictions\n\nIt has been suggested t.hat the most important principles underlying the pattern of\norientation and ocular dominance are "continuity" and "diversity" [7. 19, 21]. Continuity, because early image processing is often local in fE\'atnre space, and diversity,\nbecause, e.g., the visual system may want to avoid perceptual scotomata. The continuity and diversity principles underlie almost all dE\'scriptive and developmental\n\n\x0cA Critical Comparison of Models for Orientation and Ocular Dominance Columns\n\n95\n\nFigure 1: Typical patterns of orientation preferences as they are predicted by six\nof the models list.ed in Table 1. Orientation preferences are coded by gray values,\nwhere black - whit.e denotes preferences for vertical _ horizont.al - vertical. Top\nrow (left to right): Models 7, 11, 9. Bottom row (left to right) Models 5, 12, 8.\n\nmodels, but. maps which comply with t.hese principles often differ in qualitat.ive ways:\nThe icecube model, e.g., obeys bot.h principles but. contains no singularities in the\norient.ation preference map and no branching of ocular dominance bands. Figure 1\nshows orientat.ion maps generated by six different. algorithms taken from Tab. 1.\nAlthough all pat.t.erns are consist.ent. wit.h the continuit.y and diversity const.raints,\ncloser comparison reveals differences. Thus additional element.s of organization must\nbe considered.\nIt has been suggested that maps are characterized by local correlations and global\ndisorder. Figure 2 (left) shows as an exam pIe two- point correlation functions of\norientation maps. The autocorrelation function [17] of one of the Cartesian coordinat.es of t.he orientation vector is plotted as a function of cortical distance. The\nfact. that all correlation functions decay indicates that the orientation map exhibits\nglobal disorder. Global disorder is predicted by all models except. the early pattern models 6, 8 and 9. Figure 2 (right) shows the corresponding power spectra.\nBandpass-like spectra which are typical for the experiment.al data [16] are well predicted by models 10- 12. Interestingly, they are not predicted by model 9, which\nalso fails reproducing the Mexican-hat shaped correlation functions (bold lines),\nand model 13.\nBased on the fact that. experimental maps are characterized by a bandpass-like\npower spectrum it has been suggested that orientation maps may be organized\n\n\x0c96\n\nE. Erwin, K. Obermayer, K. Schulten\n\n1.0 . . . _ - - - - - - - - - - - - - ,\n\n--\n\n1,0\n\n" \'0\n\n.~\n\n?\n\n1,6\n\nS...\n\n0,4\n\n~\n\n-0.5\n\n.................- _ __....-_\n10\n20\n30\n\n~----:."_\n\no\n\ndistance (normalized)\n\n_4\n\n40\n\n1,8\n\n&\n\n0,2\n\n0,0\n\n0\n\n5\n\n10\n\n15\n\n20\n\ndistance (normalized)\n\nFigure 2: Left: Spatial autocorrelation functions for one of the cartesian coordinates of the orientat.ion vector. Aut.ocorrelation functions were averaged over all\ndirections. Right: Complex power spectra of orientation maps. Power was averaged over all directions of the wave vector. Modelnumhers as in Tab. 1.\n\naccording to four principles [15]: continuity, diversity, homogeneity and isotropy.\nIf those principles are implemented using bandpass filtered noise the resulting\nmaps [15, 21] indeed share many properties with the experimental data. Above\nprinciples alone, however, are not sufficient: (i) There are models such as model ?5\nwhich are based on those principles but generate different patterns, (ii) homogeneity and isotropy are hardly ever fulfilled ([16] and next paragraph), and (iii) those\nprinciples cannot. account for correlations between maps of various response properties [16].\nMaps of orientation and ocular dominance in the macaque are anisotropic, i.e. ,\nthere exist preferred directions along which orientation and ocular dominance slabs\nalign [16]. Those anisotropies can emerge due to different mechanisms: (i) spontaneous symmetry breaking, (ii) model equations, which are not rotation invariant,\nand (iii) appropriately chosen boundary conditions. Figure 3 illustrates mechanisms (ii) and (iii) for model 11. Bot.h mechanisms indeed predict anisotropic\npat.terns, however, preferred directions of orientation and ocular dominance align in\nboth cases (fig. 3, left and center). This is not true for the experimental data, where\npreferred directions tend to be orthogonal [16]. Ort.hogonal preferred directions can\nbe generated by llsing different neighborhood funct.ions for different components of\nthe feature vector (fig. 3, right). However, this is not a satisfactory solution, and\nthe issue of anisotropies is still unsolved.\nThe pattern of orientation preference in the area 17 of the macaque exhibits four\nlocal elements of organization: linear zones, singularit.ies, saddle point.s and fractures [16]. Those element.s are correctly predict.ed by most. of the pat.t,ern models,\nexcept models 1- 3, and they appear in the maps generated by models 10- 14. Interestingly\' models 9 and 13 predict very few linear zones, which is related to the\nfact. that those models generate orientat.ion maps with lowpass-like power spect.ra.\nAnother important property of orientation maps is that orientation preferences and\ntheir spatial layout across cortex are not correlated which each other. One conse-\n\n\x0cA Critical Comparison of Models for Orientation and Ocular Dominance Columns\n\n:.~ .\n.~;\n\n+\n\n+\n\n+\n\n\' .~,!-.\n\n+\n\n97\n\n.\n\n. 4-\n\n+\n\nFigure 3: Anisotropic orientation and ocular dominance maps generated by model\n11. The figure shows Fourier spectra [17] of orientation (top row) and ocular dominance maps (bottom row). Left: Maps generated with an elliptic neighborhood\nfunction (case (ii), see text); Center: Maps generated using circular input layers and an elliptical cortical sheet (case (iii), see text), Right: Maps generated\nwith different, elliptic neighborhood functions for orientation preference and ocular\ndominance. \'+\' symbols indicate the locations of the origin.\nquence is that there exist singularities, near which the curl of the orientation vector\nfield does not vanish (fig. 4, left). This rules out a class of pattern models where the\norientation map is derived from the gradient of a potential function, model 5. Figure 4 (right) shows another consequence of this property. In those figures cortical\narea is plotted against the angular difference between the iso-orientation lines and\nthe local orientation preference. The even distribution found in the experimental\ndata is correctly predicted by models 1,6, 7 and 10-12. Model 8, however, predicts\npreference for large difference angles while model 9 - over a wide range of parameters\n- predicts preference for small difference angles (bold lines).\nFinally, let us consider correlations between the patterns of orientation preference\nand ocular dominance. Among the more prominent relationships present in macaque\ndata are [3, 16,21]: (i) Singularities are aligned with the centers of ocular dominance\nbands, (ii) fractures are either aligned or run perpendicular, and (iii) iso-orientation\nbands in linear zones intersect ocular dominance bands at approximately right angles. Those relationships are readily reproduced only by models 7 and 10- 12. For\nmodel 9 reasonable orientation and ocular dominance patterns have not been generated at the same time. It would seem as if the parameter regime where reasonable\norientation columns emerge is incompatible with the parameter regime where ocular\ndominance patterns are formed.\n\n\x0c98\n\nE. Erwin, K. Obermayer, K. Schulten\n\ne\'"\n\'0"\n\nC+-I\n\n0J.5\n\n0.10\n\nu\n\nbO\n\n\'"\n\nO.\n=\n~\n\n8.\n\n0. . +----.--_ _-....-_--1\n\n03060\n\n90\n\ndifference angle ( degrees)\nFigure 4: Left: This singularity is an example of a feature in the experimental\ndata which is not allowed by model 5. The arrows indicat.e orientation vectors,\nwhose angular component is twice the value of the local orientation preference.\nRight: Percentage of area as a function of the angular difference bet.ween preferred\norient.ation and t.he local orientation gradient vector. Model numbers as in Table 1.\n\n3\n\nThe Current Status of the Model Comparison Project\n\nLack of space prohibit.s a detailed discussion of our findings hut we have summarized\nthe current status of our project in Tables 2 and 3. Given the models list.ed in\nTab. 1 and given the properties of t.he orientation and ocular dominance patt.erns\nin macaque striate cortex listed in Tables 2 and 3 it is models 7 and 10-12 which\ncurrently are in best agreement with the data. Those models, however, are fairly\nabstract. and simplified, and they cannot easily be extended to predict receptive\nfield structure. Biological realism and predictions about. receptive fields are the\nadvantages of models 8 and 9. Those models, however, cannot account for the\nobserved orientation patterns. It. would, therefore, be of high interest, if elements\nof both approaches could be combined to achieve a better description of the dat.a.\nThe main conclusion, however, is that there are now enough data available to allow\na better evaluation of model approaches than just by visual comparison of the\ngenerated pat.terns. It. is our hope, that future studies will address at least those\npropert.ies of t.he patterns which are known and well described, some of which are\nlist.ed in Tables 2 and 3. In case of developmental models more stringent tests\nrequire experiments which (i) monitor the actual time-course of pattern formation,\nand which (ii) study pattern development under experimentally modified conditions\n(deprivation experiments). Currently there is not enough data available to constrain\nmodels but the experiments are under way [5, 10, 11, 18].\n\nAcknowledgements\nVVe are very much indebted to Drs. Linsker, Tanaka and Yuille for sharing modelling\ndata. E.E. thanks t.he Beckman Institute for support.. K.O. thanks ZiF (Universitat\nBielefeld) for it.s hospitality. Computing time on a CM-2 and a CM-5 was made\navailable by NCSA.\n\n\x0cA Critical Comparison of Models for Orientation and Ocular Dominance Columns\n\nno.\n\n1\n2\n3\n4\n5\n6\n\n7\n8\n9\n10\n11\n\n12\n13\n14\n\ndisorder\n\nbandpass\n\nlinear\nzones\n\n-\n\n+\n+\n+\n+\n+\n+\n+\n+\n\n+\n+\n+\n+\n+\n+\n+\n\n+2\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n\n-\n\n+\n+\n+\n-\n\n?\n\n-\n\n+\n+\n+\n?\n\nProperties of OR Maps\nsaddle sing.\nfracto indep.\npoints ?1/2\ncoord.\n\n-\n\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n\n-\n\n+\n+2\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n\n-\n\n-\n\n+1\n+1\n+1\n+\n+\n+1\n+1\n+1\n+1\n+\n\n+\n\n-\n\nhigh\nspec.\nn\nn\nn\nn\n\n+\n+\n\n-\n\n-\n\n+\n\n-\n\nn\n\n-/+\n+\n+\n+\n+\n\n+\n+\n+\n+\n+\n\n?\n\nn\n\n99\n\namsotropy\n\nn\nn\n\nURbias\nn\nn\nn\nn\nn\nn\nn\nn\nn\n\n+\n+\n+\n\n+\n+\n+\n\nn\nn\n\nn\nn\n\n+\nn\nn\n\n+\n+\n+\n+\n\nTable 2: Evaluation of orientation (OR) map models. Properties of the experimental maps include (left to right): global disorder; bandpass-like power spectra; the\npresence of linear zones in roughly 50% of the map area; the presence of saddle\npoints, singularities (?1/2 with equal densities), and fractures; independence between cortical and orientation preference coordinates; a distribution favoring high\nvalues of orientation specificity; global anisotropy; and a possible orientation bias.\nSymbols: \'+\': There exists a parameter regime in which a model generates maps\nwith this property; \'-\': The model cannot reproduce this property; "n\': The model\nmakes no predictions; "?\': Not enough data available. 1 Models agree with the data\nonly if one assumes that fractures are loci of rapid orientation change rather than\nreal discontinuities. 20 ne of several cases.\n\nReferences\n[1] W. T. Baxter and B. M. Dow. Bioi. Cybern. , 61:171-182, 1989.\n[2] G. G. Blasdel. J. Neurosci., 12:3115-3138, 1992.\n[3] G. G. Blasdel. J. Neurosci., 12:3139-3161,1992.\n\n[4] G. G. Blasdel and G. Salama. Nature, 321:579- 585, 1986.\n[5] T. Bonhoeffer, D. Kim, and W. Singer. Soc. Neurosci. Abs., 19:1800, 1993.\n[6] V. Braitenberg and C. Braitenberg. Bioi. Cybern., 33:179- 186, 1979.\n[7] R. Durbin and G. Mitchison. Nature, 343:341-344, 1990.\n[8] K. G. Gotz. Bioi. Cybern., 56:107-109, 1987.\n[9] D. Rubel and T. N. Wiesel. Proc. Roy. Soc. Lond. B, 198:1-59, 1977.\n[10] D. Rubel, T. N. Wiesel, and S. LeVay. Phil. Trans. Roy. Soc. Lond. B, 278:377409, 1977.\n\n[11] D. Kim and T. Bonhoeffer. Soc. Neurosci. Abs., 19:1800, 1993.\n[12] R. Linsker. Proc. Nat. Acad. Sci., USA, 83:8779-8783, 1986.\n\n\x0c100\n\nE. Erwin, K. Obermayer, K. Schulten\n\nno.\n\nsegregation\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n+\nn\n\n+\nn\n\n+\nn\n\n+\nn\n\n+\n+\n+\n+\n+\n+\n\nProperties of OD Maps\nstraani so- ODdisorder tropy bias bismus\nn\n+\n+\nn\nn\nn\nn\nn\nn\n+\nn\nn\nn\nn\nn\n+\n+\nn\nn\nn\nn\nn\n+\n+\n+\nn\nn\nn\nn\n\n+\n+\n+\n+\n+\n+\n\n+\n+\n+\n+\n+\n+\n\n+\n+\n+\n+\n+\n+\n\n+\n+\n+\n+\n+\nn\n\nCorrelations Between OR and OD\nsing.\nspec.\nglobal\nlocal\northog. orthog. vs.OD vs.OD\n+:.l\n+M\nn\nn\nn\nn\nn\n+2\nn\nn\n+\nn\nn\nn\nn\n\n_I\n\n+1\n\n+I ,O!\n\n_I\n\nn\n\nn\n\nn\n\nn\n\n-\n\n+\n\n+\n\n+\n\nn\n\nn\n\n?1\n\n?1\n+\n+2\n+1,2\n\nn\nn\nn\nn\nn\n\nn\nn\n\nn\n\nn\n\n?1\n+\n+2\n+1,2\n\n?1\n+\n+2\n+1 ,2\n\nn\nn\n\nn\nn\n\nTable 3: Left: Evaluation of ocular dominance (OD) map models. Properties of\nthe experimental maps include (left to right): Segregated bands of eye dominance;\nglobal disorder; bandpass-like power spectra; global anisotropy; a bias to the representation of one eye; and OD-patterns in animals with strabismus. Right: Evalu\nation of correlations between OD and OR. Experimental maps show (left to right):\nLocal and global orthogonality between OR and OD slabs; singularities preferably\nin monocular regions, and lower OR specificity in monocular regions. 1 Authors\ntreated OD and OR in independent models, but we consider a combined version.\n2 Correlations are stronger than in the experimental data.\nM\n\n[13] K. D. Miller. J. Neurosci., 14:409- 441, 1994.\n[14] K. D. Miller, J. B. Keller, and M. P. Stryker. Science, 245:605-615, 1989.\n[15] E. Niebur and F. Worgotter. In F. H. Eeckman and J. M. Bower, Computation\nand Neura.l Systems, pp. 409-413. Kluwer Academic Publishers, 1993.\n[16] K. Obermayer and G. G. Blasdel. J. Neurosci., 13:4114-4129, 1993.\n[17] K. Obermayer, G. G. Blasdel, and K. Schulten. Phys. Rev. A, 45:7568-7589,\n1992.\n[18] K Obermayer, L. Kiorpes, and G. G. Blasdel. In J. D. Cowan at al., Advances\nin Neural Information Processing Systems 6. Morgan Kaufmann, 1994. 543550.\n[19] K. Obermayer, H. Ritter, and K. Schulten.\n87:8345- 8349, 1990.\n\nProc. Nat. Acad. Sci., USA,\n\n[20] A. S. Rojer and E. L. Schwartz. Bioi. Cybern., 62:381 - 391, 1990.\n[21] N. V. Swindale. Bioi. Cybern., 66:217-230, 1992.\n[22] S. Tanaka. Bioi. Cybern., 65:91- 98, 1991.\n[23] A. L. Yuille, J. A. Kolodny, and C. W. Lee. TR 91-3, Harvard Robotics\nLaboratory, 1991.\n\n\x0c'
p83160
sg99
S'Interference in Learning Internal\nModels of Inverse Dynamics in Humans\n\nReza Shadmehr; Tom Brashers-Krug, and Ferdinando Mussa-lvaldi t\nDept. of Brain and Cognitive Sciences\nM. I. T., Cambridge, MA 02139\nreza@bme.jhu.edu, tbk@ai.mit.edu, sandro@parker.physio.nwu.edu\n\nAbstract\nExperiments were performed to reveal some of the computational\nproperties of the human motor memory system. We show that\nas humans practice reaching movements while interacting with a\nnovel mechanical environment, they learn an internal model of the\ninverse dynamics of that environment. Subjects show recall of this\nmodel at testing sessions 24 hours after the initial practice. The\nrepresentation of the internal model in memory is such that there\nis interference when there is an attempt to learn a new inverse\ndynamics map immediately after an anticorrelated mapping was\nlearned. We suggest that this interference is an indication that\nthe same computational elements used to encode the first inverse\ndynamics map are being used to learn the second mapping. We\npredict that this leads to a forgetting of the initially learned skill.\n\n1\n\nIntroduction\n\nIn tasks where we use our hands to interact with a tool, our motor system develops\na model of the dynamics of that tool and uses this model to control the coupled\ndynamics of our arm and the tool (Shadmehr and Mussa-Ivaldi 1994). In physical\nsystems theory, the tool is a mechanical analogue of an admittance, mapping a force\nas input onto a change in state as output (Hogan 1985). In this framework, the\n?Currently at Dept. Biomedical Eng, Johns Hopkins Univ, Baltimore, MD 21205\ntCurrently at Dept. Physiology, Northwestern Univ Med Sch (M211), Chicago, IL 60611\n\n\x0c1118\n\nReza Shadmehr, Tom Brashers-Krug, Ferdinando Mussa-Ivaldi\n\nFigure 1: The experimental setup. The robot is\na very low friction planar mechanism powered by\ntwo torque motors that act on the shoulder and\nelbow joints. Subject grips the end-point of the\nrobot which houses a force transducer and moves\nthe hand to a series of targets displayed on a monitor facing the subject (not shown) . The function of\nthe robot is to produce novel force fields that the\nsubject learns to compensate for during reaching\nmovements.\n\nmodel developed by the motor control system during the learning process needs to\napproximate an inverse of this mapping . This inverse dynamics map is called an\ninternal model of the tool.\nWe have been interested in understanding the representations that the nervous\nsystem uses in learning and storing such internal models. In a previous work we\nmeasured the way a learned internal model extrapolated beyond the training data\n(Shadmehr and Mussa-Ivaldi 1994). The results suggested that the coordinate system of the learned map was in intrinsic (e.g., joint or muscles based) rather than in\nextrinsic (e.g., hand based) coordinates. Here we present a mathematical technique\nto estimate the input-output properties of the learned map. We then explore the\nissue of how the motor memory might store two maps which have similar inputs\nbut different outputs.\n\n2\n\nQuantifying the internal model\n\nIn our paradigm, subjects learn to control an artificial tool: the tool is a robot\nmanipulandum which has torque motors that can be programmed to produce a\nvariety of dynamical environments (Fig. 1). The task for the subject is to grasp\nthe end-effector and make point to point reaching movements to a series of targets.\nThe environments are represented as force fields acting on the subject\'s hand, and a\ntypical case is shown in Fig. 2A. A typical experiment begins with the robot motors\nturned off. In this "null" environment subjects move their hand to the targets in a\nsmooth, straight line fashion. When the force field is introduced, the dynamics of the\ntask change and the hand trajectory is significantly altered (Shadmehr and MussaIvaldi 1994). With practice (typically hundreds of movements), hand trajectories\nreturn to their straight line path. We have suggested that practice leads to formation\nof an internal model which functions as an inverse dynamics mapping, i.e., from a\ndesired trajectory (presumably in terms of hand position and velocity, Wolpert et\nal. 1995) to a prediction of forces that will be encountered along the trajectory. We\ndesigned a method to quantify these forces and estimate the output properties of\nthe internal model.\nIf we position a force transducer at the interaction point between the robot and the\nsubject, we can write the dynamics of the four link system in Fig. 1 in terms of the\n\n\x0cInterference in Learning Internal Models of Inverse Dynamics in Humans\n\nfollowing coupled vector differential equations:\nIr(P)P + Gr(p,p)p = E(p,p)\n\nIII (q)q + GII(q, q)q\n\n= C(q, q, q*(t?\n\n1119\n\n+ J\'{ F\n\n(1)\n\n- f; F\n\n(2)\n\nwhere I and G are inertial and Corriolis/centripetal matrix functions, E is the\ntorque field produced by the robot\'s motors, i.e., the environment, F is the force\nmeasured at the handle of the robot, C is the controller implemented by the motor\nsystem of the subject, q*(t) is the reference trajectory planned by the motor system\nof the subject, J is the Jacobian matrix describing the differential transformation\nof coordinates from endpoint to joints, q and p are joint positions of the subject\nand the robot, and the subscripts sand r denote subject or robot matrices.\n\n?\n\nIn the null environment, i.e., E = in Eq. (1), a solution to this coupled system\nis q = q*(t) and the arm follows the reference trajectory (typically a straight hand\npath with a Gaussian tangential velocity profile). Let us name the controller which\naccomplishes this task C = Co in Eq. (2). When the robot motors are producing a\nforce field E # 0, it can be shown that the solution is q = q*(t) if and only if the\nnew controller in Eq. (2) is C = C1 = Co + f[ J;T E. The internal model composed\nby the subject is C 1 - Co, i.e., the change in the controller after some training\nperiod. We can estimate this quantity by measuring the change in the interaction\nforce along a given trajectory before and after training. If we call these functions\nFo and FI, then we have:\nFo(q, q, ij, q*(t?\nJ;T(Co - IlIq - Gllq)\n(3)\n\nFI(q,q,ij,q*(t?\nJII-T(Co+f;J;TE-Illq-Gllq)\n(4)\nThe functions Fo and FI are impedances of the subject\'s arm as viewed from the\ninteraction port. Therefore, by approximating the difference FI - F o, we have an\nestimate of the change in the controller. The crucial assumption is that the reference\ntrajectory q*(t) does not change during the training process.\nIn order to measure Fo, we had the subjects make movements in a series of environments. The environments were unpredictable (no opportunity to learn) and\ntheir purpose was to perturb the controller about the reference trajectory so we\ncould measure Fo at neighboring states. Next, the environment in Fig. 2A was\npresented and the subject given a practice period to adapt. After training, FI was\nestimated in a similar fashion as Fo. The difference between these two functions was\ncalculated along all measured arm trajectories and the results were projected onto\nthe hand velocity space. Due to computer limitations, only 9 trajectories for each\ntarget direction were used for this approximation. The resulting pattern of forces\nwere interpolated via a sum of Gaussian radial basis functions, and are shown in\nFig. 2B. This is the change in the impedance of the arm and estimates the inputoutput property of the internal model that was learned by this subject. We found\nthat this subject, which provided some of best results in the test group, learned to\nchange the effective impedance of his arm in a way that approximated the imposed\nforce field. This would be a sufficient condition for the arm to compensate for the\nforce field and allow the hand to follow the desired trajectory. An alternate strategy\nmight have been to simply co-contract arm muscles: this would lead to an increased\nstiffness and an ability to resist arbitrary environmental forces. Figure 2B suggests\nthat practice led to formation of an internal model specific to the dynamics of the\nimposed force field.\n\n\x0cReza Shadmehr, Tom Brashers-Krug, Ferdinando Mussa-Ivaldi\n\n1120\n\nA\n-200\n\n0\n\n200\n\n..... _<...-)\n\n...\n\nB\n\n",",,-<...-)\n\nFigure 2: Quantification of the change in impedance of a subject\'s arm after learning a\nforce field. A: The force field produced by the robot during the training period. B: The\nchange in the subject\'s arm impedance after the training period, i.e., the internal model.\n\n2.1\n\nFormation of the internal model in long-term memory\n\nHere we wished to determine whether subjects retained the internal model in longterm motor memory. We tested 16 naive subjects. They were instructed to move\nthe handle of the robot to a sequence of targets in the null environment. Each\nmovement was to last 500 ? 50 msec . They were given visual feedback on the\ntiming of each movement. After 600 movements, subjects were able to consistently\nreach the targets in proper time. These trajectories constituted a baseline set.\nSubjects returned the next day and were re-familiarized with the timing of the\ntask. At this point a force field was introduced and subjects attempted to perform the exact task as before: get to the target in proper time. A sequence of 600\ntargets was given. When first introduced, the forces perturbed the subject\'s trajectories, causing them to deviate from the straight line path. As noted in previous\nwork (Shadmehr and Mussa-Ivaldi 1994), these deviations decreased with practice.\nEventually, subject\'s trajectories in the presence of the force field came to resemble\nthose of the baseline, when no forces were present. The convergence of the trajectories to those performed at baseline is shown for all 16 subjects in Fig. 3A. The\ntiming performance of the subjects while moving in the field is shown in Fig. 3B.\nIn order to determine whether subjects retained the internal model of the force\nfield in long-term memory, we had them return the next day (24 to 30 hours later)\nand once again be tested on a force field. In half of the subjects, the force field\npresented was one that they had trained on in the previous day (call this field 1).\nIn the other half, it was a force field which was novel to the subjects, field 2. Field\n2 had a correlation value of -1 with respect to field 1 (i.e., each force vector in\nfield 2 was a 180 degree rotation of the respective vector in field 1). Subjects who\nwere tested on a field that they had trained on before performed significantly better\n(p < 0.01) than their initial performance (Fig. 4A), signifying retention. However,\nthose who were given a field that was novel performed at naive levels (Fig. 4B).\nThis result suggested that the internal model formed after practice in a given field\nwas (1) specific to that field: performance on the untrained field was no better than\n\n\x0c1121\n\nInterference in Learning Internal Models of Inverse Dynamics in Humans\n\n0.9\n0.85\n\n?-; 0.9\n\n.~ 0.8\n\n~\n\n:?\n~ 0.75\n\n8\n\n0.7\n\ni\n\n08\n\n~\n\n0.7\n\n0.85\n\nA\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\n0.6\n\nB\n\n0\n\n100\n\nMovemen1 N!mber\n\n200\n\n300\n\n400\n\n500\n\n600\n\nMovement Number\n\nFigure 3: Measures of performance during the training period (600 movements) for 16\nnaive subjects. Short breaks (2 minutes) were given at intervals of 200 movements. A :\nMean ? standard error (SE) of the correlation coefficient between hand trajectory in a\nnull environment (called baseline trajectories, measured before exposure to the field) , and\ntrajectory in the force field. Hand trajectories in the field converge to that in the null field\n(i.e. , become straight, with a bell shaped velocity profile). B: Mean ? SE of the movement\nperiod to reach a target. The goal was to reach the target in 0.5 ? 0.05 seconds.\n\nI\n\n0.8\n\n1.,\n\n1\n\n0.75\n\n., 0.9\n\nE\ni=\n\nA\n\nI:: \\~ , ~ \',\n:::;; 0.6\n\n;\n\n~iIJIJ\n,l,ll,lI"\n0)\n"T\' I,.", 1f11T\'1\n\nY!1y~\n\no\n\n100\n\nE\ni=\n\n200\n\n300\n\n400\n\n500\n\n600\n\n0.7\n\n~ 0.65\nE\n\n~\n\n0.8\n\n:::;;\n\n0.55\n\nB\n\n0\n\nMovement Number\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\nMovement Number\n\nFigure 4: Subjects learned an internal model specific to the field and retained it in longterm memory. A: Mean ? standard error (SE) of the movement period in the force field\n(called field 1) during initial practice session (upper trace) and during a second session\n24-30 hours after the initial practice (lower trace). B: Movement period in a different\ngroup of subjects during initial training (dark line) in field 1 and test in an anti-correlated\nfield (called field 2) 24-30 hours later (gray line).\n\nperformance recorded in a separate set of naive subjects who were given than field\nin their initial training day; and (2) could be retained, as evidenced by performance\nin the following day.\n2.2\n\nInterference effects of the motor memory\n\nIn our experiment the "tool" that subjects learn to control is rather unusual , nevertheless, subjects learn its inverse dynamics and the memory is used to enhance\nperformance 24 hours after its initial acquisition. We next asked how formation\nof this memory affected formation of subsequent internal models. In the previous\nsection we showed that when a subject returns a day after the initial training, although the memory of the learned internal model is present , there is no interference\n(or decrement in performance) in learning a new, anti-correlated field . Here we\nshow that when this temporal distance is significantly reduced, the just learned\n\n\x0c1122\n\nReza Shadmehr, Tom Brashers-Krug, Ferdinanda Mussa-Ivaldi\n\n200\n\n300\n\n400\n\nMovement Number\n\nFigure 5: Interference in sequential learning of two uncorrelated force fields: The lower\ntrace is the mean and standard error of the movement periods of a naive group of subjects\nduring initial practice in a force field (called field 1). The upper trace is the movement period of another group of naive subjects in field 1, 5 minutes after practicing 400 movements\nin field 2, which was anti-correlated with field 1.\n\nmodel interferes with learning of a new field.\nSeven new subjects were recruited. They learned the timing of the task in a null\nenvironment and in the following day were given 400 targets in a force field (called\nfield 1). They showed improvement in performance as before. After a short break\n(5-10 minutes in which they walked about the lab or read a magazine), they were\ngiven a new field: this field was called field 2 and was anti-correlated with respect\nto field 1. We found a significant reduction (p < 0.01) in their ability to learn field\n2 (Fig. 5) when compared to a subject group which had not initially trained in field\n1. In other words, performance in field 2 shortly after having learned field 1 was\nsignificantly worse than that of naives. Subjects seemed surprised by their inability\nto master the task in field 2. In order to demonstrate that field 2 in isolation was\nno more difficult to learn than field 1, we had a new set of subjects (n = 5) initially\nlearn field 2, then field 1. Now we found a very large decrement in learn ability of\nfield 1.\nOne way to explain the decrement in performance shown in Fig. 5 is to assume that\nthe same "computational elements" that represented the internal model of the first\nfield were being used to learn the second field.! In other words, when the second field\nwas given, because the forces were opposite to the first field, the internal model was\nbadly biased against representing this second field: muscle torque patterns predicted\nfor movement to a given target were in the wrong direction.\nIn the connectionist literature this is a phenomenon called temporal interference\n(Sutton 1986). As a network is trained, some of its elements acquire large weights\nand begin to dominate the input-output transformation. When a second task is\npresented with a new and conflicting map (mapping similar inputs to different outputs), there are large errors and the network performs more poorly than a "naive"\nnetwork. As the network attempts to learn the new task, the errors are fed to each\nelement (i.e., pre-synaptic input). This causes most activity in those elements that\n1 Examples of computational elements used by the nervous system to model inverse\ndynamics of a mechanical system were found by Shidara et al. (1993), where it was shown\nthat the firing patterns of a set of Purkinje cells in the cerebellum could be reconstructed\nby an inverse dynamic representation of the eye.\n\n\x0cInterference in Learning Internal Models of Inverse Dynamics in Humans\n\n1123\n\nhad the largest synaptic weight. If the learning algorithm is Hebbian , i.e., weights\nchange in proportion to co-activation of the pre- and the post-synaptic element,\nthen the largest weights are changed the most , effectively causing a loss of what\nwas learned in the first task . Therefore, from a computational stand point, we\nwould expect that the internal model of field 1 as learned by our subjects should be\ndestroyed by learning of field 2. Evidence for "catastrophic interference" in these\nsubjects is presented elsewhere in this volume (Brashers-Krug et al. 1995).\nThe phenomenon of interference in sequential learning of two stimulus-response\nmaps has been termed proactive interference or negative transfer in the psychological\nliterature. In humans, interference has been observed extensively in verbal tasks\ninvolving short-term declarative memory (e.g., tasks involving recognition of words\nin a list or pairing of non-sense syllables, Bruce 1933, Melton and Irwin 1940,\nSears and Hovland 1941). It has been found that interference is a function of the\nsimilarity of the stimulus-response maps in the two tasks: if the stimulus in the new\nlearning task requires a response very different than what was recently learned, then\nthere is significant interference. Interestingly, it has been shown that the amount of\ninterference decreases with increased learning (or practice) on the first map (Siipola\nand Israel 1933).\nIn tasks involving procedural memory (which includes motor learning, Squire 1986),\nthe question of interference has been controversial: Although Lewis et al. (1949)\nreported interference in sequential learning of two motor tasks which involved moving levers in response to a set of lights, it has been suggested that the interference\nthat they observed might have been due to cognitive confusion (Schmidt 1988).\nIn another study, Ross (1974) reported little interference in subjects learning her\nmotor tasks.\nWe designed a task that had little or no cognitive components. We found that\nshortly after the acquisition of a motor memory, that memory strongly interfered\nwith learning of a new, anti-correlated input-output mapping. However, this interference was not significant 24 hours after the memory was initially acquired . One\npossible explanation is that the initial learning has taken place in a temporary and\nvulnerable memory system. With time and/or practice, the information in this\nmemory had transferred to long-term storage (Brashers-Krug et al. 1995) .\nBrain imaging studies during motor learning suggest that as subjects become more\nproficient in a motor task, neural fields in the motor cortex display increases in\nactivity (Grafton et al. 1992) and new fields are recruited (Kawashima et al. 1994) .\nIt has been reported that when a subject attempts to learn two new motor tasks\nsuccessively (in this case the tasks consisted of two sequences of finger movements),\nthe neural activity in the motor cortex is lower for the second task , even when the\norder ofthe tasks is reversed (Jezzard et al. 1994). It remains to be seen whether this\ndecrement in neural activity in the motor cortex is correlated with the interference\nobserved when subjects attempt to learn two different input-output mappings in\nsuccession (Gandolfo et al. 1994) .\nReferences\nBrashers-Krug T , Shadmehr R, Todorov E (1995) Catastrophic interference in human\nmotor learning. Adv Neural Inform Proc Syst, vol 7, in press.\n\n\x0c1124\n\nReza Shadmehr, Tom Brashers-Krug, Ferdinando Mussa-Ivaldi\n\nBruce RW (1933) Conditions of transfer of training. J Exp Psychol 16:343-361.\nFrench, R. (1992) Semi-distributed Representations and Catastrophic Forgetting in Connectionist Networks, Connection Science 4:365-377.\nGrafton ST et al. (1992) Functional anatomy of human procedural learning determined\nwith regional cerebral blood flow and PET. J Neurosci 12:2542-2548.\nGandolfo F, Shadmehr R, Benda B, Bizzi E (1994) Adaptive behavior ofthe monkey motor\nsystem to virtual environments. Soc Neurosci Abs 20(2):1411.\nHogan N (1985) Impedance control: An approach for manipulation: Theory. J Dynam\nSys Meas Cont 107:1-7.\nJezzard P et al. (1994) Practice makes perfect: A functional MRI study oflong term motor\ncortex plasticity. 2nd Ann Soc. Magnetic Res., p. 330.\nKawashima R, Roland PE, O\'Sullivan BT (1994) Fields in human motor areas involved\nin preparation for reaching, actual reaching, and visuomotor learning: A PET study. J\nNeurosci 14:3462-3474.\nLewis D, Shephard AH, Adams JA (1949) Evidences of associative interference in psychomotor performance. Science 110:271-273.\nMelton AW, Irwin JM (1940) The influence of degree of interpolated learning on retroactive\ninhibition and the overt transfer of specific responses. Amer J Psychol 53:173-203.\nRoss D (1974) Interference in discrete motor tasks: A test of the theory. PhD dissertation,\nDept. Psychology, Univ. Michigan, Ann Arbor.\nSchmidt RA (1988) Motor Control and Learning: A Behavioral Emphasis. Human Kinetics\nBooks, Champaign IL, pp. 409-411.\nSears RR, Hovland CI (1941) Experiments on motor conflict. J Exp Psychol 28:280-286.\nShadmehr R, Mussa-Ivaldi FA (1994) Adaptive representation of dynamics during learning\nof a motor task. J Neuroscience, 14(5):3208- 3224.\nShidara M, Kawano K, Gomi H, Kawato M (1993) Inverse dynamics model eye movement\ncontrol by Purkinje cells in the cerebellum. Nature 365:50-52.\nSiipola EM, Israel HE (1933) Habit interference as dependent upon stage oftraining. Amer\nJ Psychol 45:205-227.\nSquire LR (1986) Mechanisms of memory. Science 232:1612-1619.\nSutton RS (1986) Two problems with backpropagation and other steepest-descent learning\nprocedures for networks. Proc 8th Cognitive Sci Soc, pp. 823-831.\nWolpert DM, Ghahramani Z, Jordan MI (1995) Are arm trajectories planned in kimenatic\nor dynamic coordinates? An adaptation stUdy. Exp Brain Res, in press.\n\n\x0c'
p83161
sg313
S'Active Learning with Statistical Models\n\nDavid A. Cohn, Zoubin Ghahramani, and Michael I. Jordan\ncohnQpsyche.mit.edu. zoubinQpsyche.mit.edu. jordan~syche.mit.edu\nDepartment of Brain and Cognitive Sciences\nMassachusetts Institute of Technology\nCambridge, MA 02139\n\nAbstract\nFor many types of learners one can compute the statistically "optimal" way to select data. We review how these techniques have\nbeen used with feedforward neural networks [MacKay, 1992; Cohn,\n1994] . We then show how the same principles may be used to select\ndata for two alternative, statistically-based learning architectures:\nmixtures of Gaussians and locally weighted regression. While the\ntechniques for neural networks are expensive and approximate, the\ntechniques for mixtures of Gaussians and locally weighted regression are both efficient and accurate.\n\n1\n\nACTIVE LEARNING - BACKGROUND\n\nAn active learning problem is one where the learner has the ability or need to\ninfluence or select its own training data. Many problems of great practical interest\nallow active learning, and many even require it.\nWe consider the problem of actively learning a mapping X - Y based on a set of\ntraining examples {(Xi,Yi)}~l\' where Xi E X and Yi E Y. The learner is allowed\nto iteratively select new inputs x (possibly from a constrained set), observe the\nresulting output y, and incorporate the new examples (x, y) into its training set.\nThe primary question of active learning is how to choose which x to try next.\nThere are many heuristics for choosing x based on intuition, including choosing\nplaces where we don\'t have data, where we perform poorly [Linden and Weber,\n1993], where we have low confidence [Thrun and Moller, 1992], where we expect it\n\n\x0c706\n\nDavid Cohn, Zoubin Ghahramani, Michael I. Jordon\n\nto change our model [Cohn et aI, 1990], and where we previously found data that\nresulted in learning [Schmidhuber and Storck, 1993].\nIn this paper we consider how one may select x "optimally" from a statistical\nviewpoint. We first review how the statistical approach can be applied to neural\nnetworks, as described in MacKay [1992] and Cohn [1994]. We then consider two\nalternative, statistically-based learning architectures: mixtures of Gaussians and\nlocally weighted regression. While optimal data selection for a neural network is\ncomputationally expensive and approximate, we find that optimal data selection for\nthe two statistical models is efficient and accurate.\n\n2\n\nACTIVE LEARNING - A STATISTICAL APPROACH\n\nWe denote the learner\'s output given input x as y(x). The mean squared error of\nthis output can be expressed as the sum of the learner\'s bias and variance. The\nvariance 0\'3 (x) indicates the learner\'s uncertainty in its estimate at x. 1 Our goal\nwill be to select a new example x such that when the resulting example (x, y) is\nadded to the training set, the integrated variance IV is minimized:\n\nIV =\n\nJ0\'3\n\nP (x)dx.\n\n(1)\n\nHere, P(x) is the (known) distribution over X. In practice, we will compute a\nMonte Carlo approximation of this integral, evaluating 0\'3 at a number of random\npoints drawn according to P(x).\nSelecting x so as to minimize IV requires computing 0-3, the new variance at x given\n(x, y). Until we actually commit to an x, we do not know what corresponding y we\nwill see, so the minimization cannot be performed deterministically.2 Many learning\narchitectures, however, provide an estimate of PWlx) based on current data, so we\ncan use this estimate to compute the expectation of 0-3. Selecting x to minimize\nthe expected integrated variance provides a solid statistical basis for choosing new\nexamples.\n\n2.1\n\nEXAMPLE: ACTIVE LEARNING WITH A NEURAL\nNETWORK\n\nIn this section we review the use of techniques from Optimal Experiment Design\n(OED) to minimize the estimated variance of a neural network [Fedorov, 1972;\nMacKay, 1992; Cohn, 1994] . We will assume we have been given a learner y = fwO,\na training set {(Xi, yd}f;l and a parameter vector til that maximizes a likelihood\nmeasure. One such measure is the minimum sum squared residual\n\n52 =\n\n~\nm\n\nf\n\n(Yi - Y(Xi))2.\n\ni=l\n\nlUnless explicitly denoted, fI and O\'~ are functions of x. For simplicity, we present our\nresults in the univariate setting. All results in the paper extend easily to the multivariate\ncase.\n2This contrasts with related work by Plutowski and White [1993], which is concerned\nwith filtering an existing data set.\n\n\x0cActive Learning with Statistical Models\n\n707\n\nThe estimated output variance of the network is\n\nO\'~ ~ S2\ny\n\n(Oy(X ?) T(02 S2) (Oy(X?)\nOW 2\n\now\n\n-1\n\nOW\n\nThe standard OED approach assumes normality and local linearity. These assumptions allow replacing the distribution P(ylx) by its estimated mean y(x) and\nvariance\nThe expected value of the new variance, iT~, is then:\n\nS2.\n\n-2)...... 2\n(O\'g\n...... O\'g -\n\nx)\nS2O\'~(x,\n+ O\'~(x)\'\n\n[MacKay, 1992].\n\n(2)\n\nwhere we define\n\n_(\n\n0\' y\n\n_) =\nx, x -\n\nS2 (OY(X?)T\n(02S2)-1\n(Oy(X?)\now\now2\now?\n\nFor empirical results on the predictive power of Equation 2, see Cohn [1994] .\nThe advantages of minimizing this criterion are that it is grounded in statistics,\nand is optimal given the assumptions. Furthermore, the criterion is continuous\nand differentiable. As such, it is applicable in continuous domains with continuous\naction spaces, and allows hillclimbing to find the "best" x.\nFor neural networks, however, this approach has many disadvantages. The criterion\nrelies on simplifications and strong assumptions which hold only approximately.\nComputing the variance estimate requires inversion of a Iwl x Iwl matrix for each\nnew example, and incorporating new examples into the network requires expensive\nretraining. Paass and Kindermann [1995] discuss an approach which addresses some\nof these problems.\n\n3\n\nMIXTURES OF GAUSSIANS\n\nThe mixture of Gaussians model is gaining popularity among machine learning practitioners [Nowlan, 1991; Specht, 1991; Ghahramani and Jordan, 1994]. It assumes\nthat the data is produced by a mixture of N Gaussians gi, for i = 1, ... , N. We\ncan use the EM algorithm [Dempster et aI, 1977] to find the best fit to the data,\nafter which the conditional expectations of the mixture can be used for function\napproximation.\nFor each Gaussian gi we will denote the estimated input/output means as JLx,i and\nJLy,i and estimated covariances as O\';,i\'\nand O\'xy,i. The conditional variance of\ny given x may then be written\n\nO\';,i\n\nWe will denote as ni the (possibly fractional) number of training examples for which\ngi takes responsibility:\n\n\x0cDavid Cohn, Zoubin Ghahramani, Michael I. Jordon\n\n708\n\nFor an input x, each 9i has conditional expectation\n\nYi = J.Ly,i\nA\n\nxy -,i ( X + -0-2\n0-\n\nx,i\n\nJ.Lx,i ) ,\n\no-~\n\n.=\n\ny,J\n\n2\n\nYi\n\n0.\n~\n\nand variance (1\'~,i:\n\n.)2) .\n\n1 + x - J.Lx,~\n\n((\n\nn\'\n\n0- 2 .\nXI\'\n\nt\n\nThese expectations and variances are mixed according to the prior probability that\n9i has of being responsible for x:\n\n. = h.( ) _\nh,_~x-\n\nP(xli)\n.\n2:j=l P(xlj)\nN\n\nFor input x then, the conditional expectation\nvariance may be written:\n\nY of the\n\nresulting mixture and its\n\nN\n\nY=\n\nL hi Yi,\ni:::l\n\nIn contrast to the variance estimate computed for a neural network, here o-~ can be\ncomputed efficiently with no approximations.\n\n3.1\n\nACTIVE LEARNING WITH A MIXTURE OF GAUSSIANS\n\nWe want to select x to minimize ( Cr~). With a mixture of Gaussians, the model\'s\nestimated distribution of ii given x is explicit:\n\nP(ylx)\n\nN\n\nN\n\ni=l\n\ni=l\n\n= L hiP(ylx, i) = L hiN(Yi(X), o-;lx,i(X)),\n\n=\n\nwhere hi hi (x). Given this, calculation of ( Cr~) is straightforward: we model the\nchange in each 9i separately, calculating its expected variance given a new point\nsampled from P(ylx, i) and weight this change by hi. The new expectations combine\nto form the learner\'s new expected variance\n(3)\nwhere the expectation can be computed exactly in closed form:\n\n\x0cActive Learning with Statistical Models\n\n4\n\n709\n\nLOCALLY WEIGHTED REGRESSION\n\nWe consider here two forms of locally weighted regression (LWR): kernel regression\nand the LOESS model [Cleveland et aI, 1988]. Kernel regression computes y as an\naverage of the Yi in the data set, weighted by a kernel centered at x. The LOESS\nmodel performs a linear regression on points in the data set, weighted by a kernel\ncentered at x. The kernel shape is a design parameter: the original LOESS model\nuses a "tricubic" kernel; in our experiments we use the more common Gaussian\nhi(x) == hex - Xi) = exp( -k(x - xd 2),\nwhere k is a smoothing constant. For brevity, we will drop the argument x for hi(x),\nand define n = L:i hi. We can then write the estimated means and covariances as:\n\nL:ihiXi\n2\nL:i hi(Xi- x )2\n, Ux =\n, Uxy = Lihi(Xi-X)(Yi-J.Ly)\nn\nn\nn\n_ L:i hiYi 2 _ Li hi(Yi - J.Ly)2 2 _ 2 u;y\nJ.Ly , Uy , Uyl x - Uy - - 2 .\nn\nn\n~\nWe use them to express the conditional expectations and their estimated variances:\nJ.Lx =\n\nkernel:\nLOESS:\n\nY\n\n,_\nY - J.Ly\n\n= J.Ly,\n\n+ ~( X q2\n\n%\n\n4.1\n\nu\n= -1!..\n2\n\nu?y\n\n),...? __\nJ.Lx,\nY\nV\n\n(4)\n\nn\n\nu;lx (1 + (x n\n\nJ.Lx)2)\n\nu;\n\n(5)\n\nACTIVE LEARNING WITH LOCALLY WEIGHTED\nREGRESSION\n\nAgain we want to select x to minimize (iT~) . With LWR, the model\'s estimated\ndistribution of y given x is explicit:\n\nP(ylx) = N(y(x), u;lxCx))\nThe estimate of (iT~) is also explicit. Defining\nkernel, the learner\'s expected new variance is\n\n1.\nkerne.\n\nh as the weight assigned to x by the\n\n(-2)\n_ (iT~)\nuy - --n+h\n\nwhere the expectation can be computed exactly in closed form:\n\n(6)\n\n\x0c710\n\n5\n\nDavid Cohn, Zoubin Ghahramani, Michael 1. Jordon\n\nEXPERIMENTAL RESULTS\n\nBelow we describe two sets of experiments demonstrating the predictive power of\nthe query selection criteria in this paper. In the first set, learners were trained on\ndata from a noisy sine wave. The criteria described in this paper were applied to\npredict how a new training example selected at point x would decrease the learner\'s\nvariance. These predictions, along with the actual changes in variance when the\ntraining points were queried and added, are plotted in Figure 1.\n\no.\n\n- .- - . _. - predicted change\n- - actual change\n\n-0.5\n\no.\n\n- ._.-. - .? predicted change\n- - actual\n.\n\n9"8rl97".\n\\\n-" i\n\n~.\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n-0.2\n\n,"\n\n0.2\n\n.-.- -.-.- predicted change\n- - actual change\n\n.\n0.4\n\n0.6\n\n0.8\n\nFigure 1: The upper portion of each plot indicates each learner\'s fit to noisy sinusoidal data. The lower portion of each plot indicates predicted and actual changes\nin the learner\'s average estimated variance when x is queried and added to the\ntraining set, for x E [0,1]. Changes are not plotted to scale with learners\' fits.\n\nIn the second set of experiments, we a:pplied the techniques of this paper to learning\nthe kinematics of a two-joint planar arm (Figure 2; see Cohn [1994] for details).\nBelow, we illustrate the problem using the LOESS algorithm.\nAn example of the correlation between predicted and actual changes in variance\non this problem is plotted in Figure 2. Figure 3 demonstrates that this correlation may be exploited to guide sequential query selection. We compared a\nLOESS learner which selected each new query so as to minimize expected variance\n\n\x0cActive Learning with Statistical Models\n\n711\n\nwith LOESS learners which selected queries according to various heuristics. The\nvariance-minimizing learner significantly outperforms the heuristics in terms of both\nvariance and MSE.\n0 .025r--..---...,......-~---.----...---,---.",\n\no\no\n\n0\n\n0.02\no\n\n~\n\nc::\n\n0.015\no\n\n.~\n\n~\n\n0.01\n\ntil\n\n~\n\n"iii\n\n0.005\n\n::I\n\n~\n\n0\n-0.005\no\n\n-?$.01 -0.005\n\n0\n0.005 0,01 0.015\npredicted delta variance\n\n0.02\n\n0.025\n\nFigure 2: (left) The arm kinematics problem. (right) Predicted vs. actual changes\nin model variance for LOESS on the arm kinematics problem. 100 candidate points\nare shown for a model trained with 50 initial random examples. Note that most\nof the potential queries produce very little improvement , and that the algorithm\nsuccessfully identifies those few that will help most.\n\n0.2\n0.1\n3\nVarianceO.04\n\nMSE\n\n0.02\n\n0.01\n\n0.3\n\n0.004\n\n0.1\n50 100 150 200 250 300 350 400 450 500\ntraining examples\n\n50 100 150200 250 300 350 400 450 500\ntraining examples\n\nFigure 3: Variance and MSE for a LOESS learner selecting queries according to\nthe variance-minimizing criterion discussed in this paper and according to several\nheuristics . "Sensitivity" queries where output is most sensitive to new data, "Bias"\nqueries according to a bias-minimizing criterion, ?Support" queries where the model\nhas the least data support. The variance of "Random" and "Sensitivity" are off the\nscale. Curves are medians over 15 runs with non-Gaussian noise.\n\n\x0c712\n\n6\n\nDavid Cohn. Zouhin Ghahramani. Michael 1. Jordon\n\nSUMMARY\n\nMixtures of Gaussians and locally weighted regression are two statistical models\nthat offer elegant representations and efficient learning algorithms. In this paper\nwe have shown that they also offer the opportunity to perform active learning in an\nefficient and statistically correct manner. The criteria derived here can be computed\ncheaply and, for problems tested, demonstrate good predictive power.\nAcknowledgements\n\nThis work was funded by NSF grant CDA-9309300, the McDonnell-Pew Foundation,\nATR Human Information Processing Laboratories and Siemens Corporate Research.\nWe thank Stefan Schaal for helpful discussions about locally weighted regression .\nReferences\nW. Cleveland, S. Devlin, and E. Grosse. (1988) Regression by local fitting. Journal of\nEconometrics 37:87-114.\nD. Cohn, 1. Atlas and R. Ladner. (1990) Training Connectionist Networks with Queries\nand Selective Sampling. In D. Touretzky, ed., Advances in Neural Information Processing\nSystems 2, Morgan Kaufmann.\nD. Cohn. (1994) Neural network exploration using optimal experiment design. In J . Cowan\net al., eds., Advances in Neural Information Processing Systems 6. Morgan Kaufmann.\nA. Dempster, N. Laird and D. Rubin. (1977) Maximum likelihood from incomplete data\nvia the EM algorithm. J. Royal Statistical Society Series B, 39:1-38.\nV. Fedorov. (1972) Theory of Optimal Experiments. Academic Press, New York.\nZ. Ghahramani and M. Jordan. (1994) Supervised learning from incomplete data via an\nEM approach. In J. Cowan et al., eds., Advances in Neural Information Processing Systems\n6. Morgan Kaufmann.\nA. Linden and F. Weber. (1993) Implementing inner drive by competence reflection. In\nH. Roitblat et al., eds., Proc. 2nd Int. Conf. on Simulation of Adaptive Behavior, MIT\nPress, Cambridge.\nD. MacKay. (1992) Information-based objective functions for active data selection, Neural\nComputation 4( 4): 590-604.\nS. Nowlan. (1991) Soft Competitive Adaptation: Neural Network Learning Algorithms\nbased on Fitting Statistical Mixtures. CMU-CS-91-126, School of Computer Science,\nCarnegie Mellon University, Pittsburgh, PA.\nPaass, G., and Kindermann, J . (1995) . Bayesian Query Construction for Neural Network\nModels. In this volume.\nM. Plutowski and H. White (1993). Selecting concise training sets from clean data. IEEE\nTransactions on Neural Networks, 4, 305-318.\nS. Schaal and C. Atkeson. (1994) Robot Juggling: An Implementation of Memory-based\nLearning. Control Systems Magazine, 14(1):57-71.\nJ. Schmidhuber and J . Storck. (1993) Reinforcement driven information acquisition in\nnondeterministic environments. Tech. Report, Fakultiit fiir Informatik, Technische Universitiit Munchen.\nD. Specht. (1991) A general regression neural network. IEEE Trans. Neural Networks,\n2(6):568-576.\nS. Thrun and K. Moller. (1992) Active exploration in dynamic environments. In J. Moody\net aI., editors, Advances in Neural Information Processing Systems 4. Morgan Kaufmann.\n\n\x0c'
p83162
sg44
S'A Rapid Graph-based Method for\nArbitrary Transformation-Invariant\nPattern Classification\nAlessandro Sperduti\nDipartimento di Informatica\nUniversita di Pisa\nCorso Italia 40\n56125 Pisa, ITALY\n\nDavid G. Stork\nMachine Learning and Perception Group\nRicoh California Research Center\n2882 Sand Hill Road # 115\nMenlo Park, CA USA 94025-7022\n\nperso~di.unipi.it\n\nstork~crc.ricoh.com\n\nAbstract\nWe present a graph-based method for rapid, accurate search\nthrough prototypes for transformation-invariant pattern classification. Our method has in theory the same recognition accuracy as\nother recent methods based on \'\'tangent distance" [Simard et al.,\n1994], since it uses the same categorization rule. Nevertheless ours\nis significantly faster during classification because far fewer tangent distances need be computed. Crucial to the success of our\nsystem are 1) a novel graph architecture in which transformation\nconstraints and geometric relationships among prototypes are encoded during learning, and 2) an improved graph search criterion,\nused during classification. These architectural insights are applicable to a wide range of problem domains. Here we demonstrate that\non a handwriting recognition task, a basic implementation of our\nsystem requires less than half the computation of the Euclidean\nsorting method.\n\n1\n\nINTRODUCTION\n\nIn recent years, the crucial issue of incorporating invariances into networks for pattern recognition has received increased attention, most especially due to the work of\n\n\x0c666\n\nAlessandro Sperduti, David G. Stork\n\nSimard and his colleagues. To a regular hierachical backpropagation network Simard\net al. [1992] added a Jacobian network, which insured that directional derivatives\nwere also learned. Such derivatives represented directions in feature space corresponding to the invariances of interest, such as rotation, translation, scaling and\neven line thinning. On small training sets for a function approximation problem,\nthis hybrid network showed performance superior to that of a highly tuned backpropagation network taken alone; however there was negligible improvement on\nlarge sets. In order to find a simpler method applicable to real-world problems,\nSimard, Le Cun & Denker [1993] later used a variation of the nearest neighbor\nalgorithm, one incorporating "tangent distance" (T-distance or D T ) as the classification metric - the smallest Euclidean distance between patterns after the optimal\ntransformation. In this way, state-of-the-art accuracy was achieved on an isolated\nhandwritten character task, though at quite high computational complexity, owing\nto the inefficient search and large number of Euclidean and tangent distances that\nhad to be calculated.\nWhereas Simard, Hastie & Saeckinger [1994] have recently sought to reduce this\ncomplexity by means of pre-clustering stored prototypes, we here take a different\napproach, one in which a (graph) data structure formed during learning contains\ninformation about transformations and geometrical relations among prototypes.\nNevertheless, it should be noted that our method can be applied to a reduced\n(clustered) training set such as they formed, yielding yet faster recognition. Simard\n[1994] recently introduced a hierarchical structure of successively lower resolution\npatterns, which speeds search only if a minority of patterns are classified more\naccurately by using the tangent metric than by other metrics. In contrast, our\nmethod shows significant improvement even if the majority or all of the patterns\nare most accurately classified using the tangent distance.\nOther methods seeking fast invariant classification include Wilensky and\nManukian\'s scheme [1994]. While quite rapid during recall, it is more properly\nconsidered distortion (rather than coherent transformation) invariant. Moreover,\nsome transformations such as line thinning cannot be naturally incorporated into\ntheir scheme. Finally, it appears as if their scheme scales poorly (compared to\ntangent metric methods) as the number of invariances is increased.\nIt seems somewhat futile to try to improve significantly upon the recognition accuracy of the tangent metric approach - for databases such as NIST isolated\nhandwritten characters, Simard et al. [1993] reported accuracies matching that\nof humans! Nevertheless, there remains much that can be done to increase the\ncomputational efficiency during recall. This is the problem we address.\n\n2\n\nTRANSFORMATION INVARIANCE\n\nIn broad overview, during learning our method constructs a labelled graph data\nstructure in which each node represents a stored prototype (labelled by its category)\nas given by a training set, linked by arcs representing the T-distance between them.\nSearch through this graph (for classification) takes advantage of the graph structure\nand an improved search criterion. To understand the underlying computations, we\nmust first consider tangent space.\n\n\x0cGraph-Based Method for Arbitrary Transformation-Invariant Pattern Classification\n\n667\n\nFigure 1: Geometry of tangent space. Here, a three-dimensional feature space\ncontains the "current" prototype, Pc, and the subspace consisting of all patterns\nobtainable by performing continuous transformations of it (shaded). Two candidate\nprototypes and a test pattern, T, as well as their projections onto the T-space of\nPc are shown. The insert (above) shows the progression of search through the\ncorresponding portion of the recognition graph. The goal is to rapidly find the\nprototype closest to T (in the T-distance sense), and our algorithm (guided by the\nminimum angle OJ in the tangent space) finds that P 2 is so closer to T than are\neither PI or Pc (see text).\n\nFigure 1 illustrates geometry of tangent space and the relationships among the fundamental entities in our trained system. A labelled ("current") trained pattern is\nrepresented by Pc, and the (shaded) surface corresponds to patterns arising under\ncontinuous transformations of Pc. Such transformations might include rotation,\ntranslation, scaling, line thinning, etc. Following Simard et al. [1993], we approximate this surface in the vicinity of Pc by a subspace - the tangent space or T -space\nof Pc - which is spanned by "tangent" vectors, whose directions are determined by\ninfinitessimally transforming the prototype Pc. The figure shows an ortho-normal\nbasis {TVa, TV b}, which helps to speed search during classification, as we shall see.\nA test pattern T and two other (candidate) prototypes as well as their projections\nonto the T-space of Pc are shown.\n\n\x0c668\n\n3\n\nAlessandro Sperduti, David G. Stork\n\nTHE ALGORITHMS\n\nOur overall approach includes constructing a graph (during learning), and searching\nit (for classification). The graph is constructed by the following algorithm:\n\nGraph construction\nInitialize N = # patterns; k = # nearest neighbors; t = # invariant transformations\nBegin Loop For each prototype Pi (i = 1 ~ N)\n? Compute a t-dimensional orthonormal basis for the T -space of Pi\n? Compute ("one-sided") T-distance of each of the N - 1 prototypes\nP j (j i- i) using Pi\'S T-space\n? Represent Pj.l (the projection of P j onto the T-space of Pi) in the\ntangent orthonormal frame of Pi\n? Connect Pi to each of its k T-nearest neighbors, storing their associated normalized projections Ph\nEnd Loop\nDuring classification, our algorithm permits rapid search through prototypes. Thus\nin Figure 1, starting at Pc we seek to find another prototype (here, P2) that is\ncloser to the test point T . After P2 is so chosen, it becomes the current pattern,\nand the search is extended using its T-space. Graph search ends when the closest\nprototype to T is found (Le., closest in a T-distance sense).\nWe let D~ denote the current minimum tangent distance. Our search algorithm is:\n\nGraph search\nInput Test\nInitialize\n?\n?\n?\nDo\n\npattern T\nChoose initial candidate prototype, Po\nSetPc~Po\n\nSet D~ ~ DT(P c , T), i.e., the T-distance ofT from Pc\nT.L?P~\n\n? For each prototype P j connected to Pc compute cos(Oj) = IT.Ll.L\n? Sort these prototypes by increasing values of OJ and put them into a\ncandidate list\n? Pick P j from the top of the candidate list\n? In T-space of Pj, compute DT(P j , T)\nIf DT(P j , T) < D~ then Pc ~ P j and D~ ~ DT(P j , T)\notherwise mark P j as a "failure" (F), and pick next prototype from\nthe candidate list\nUntil Candidate list empty\n\nReturn D~ or the category label of the optimum prototype found\n\n\x0cGraph-Based Method for Arbitrary Transformation-Invariant Pattern Classification\n\nDr\n\n4.91\n\n3.70\n\n3.61\n\n3.03\n\n669\n\n2.94\n\nFigure 2: The search through the "2" category graph for the T-nearest stored\nprototype to the test pattern is shown (N = 720 and k = 15 nearest neighbors).\nThe number of T-distance calculations is equal to the number of nodes visited plus\nthe number offailures (marked F); Le., in the case shown 5 + 26 = 31. The backward\nsearch step attempt is thwarted because the middle node has already been visited\n(marked M). Notice in the prototypes how the search is first a downward shift, then\na counter-clockwise rotation - a mere four steps through the graph.\nFigure 2 illustrates search through a network of "2" prototypes. Note how the Tdistance of the test pattern decreases, and that with only four steps through the\ngraph the optimal prototype is found.\nThere are several ways in which our search technique can be incorporated into a\nclassifier. One is to store all prototypes, regardless of class, in a single large graph\nand perform the search; the test pattern is classified by the label of the optimal\nprototype found. Another, is to employ separate graphs, one for each category, and\nsearch through them (possibly in parallel); the test is classified by the minimum\nT-distance prototype found. The choice of method depends upon the hardware\nlimitations, performance speed requirements, etc. Figure 3 illustrates such a search\nthrough a "2" category graph for the closest prototype to a test pattern "5." We\nreport below results using a single graph per category, however.\n\n3.1\n\nComputational complexity\n\nIf a graph contains N prototypes with k pointers (arcs) each, and if the patterns are\nof dimension m, then the storage requirement is O(N((t + 1) . m 2 + kt)). The time\ncomplexity of training depends upon details of ortho-normalization, sorting, etc.,\nand is of little interest anyway. Construction is more than an order of magnitude\nfaster than neural network training on similar problems; for instance construction\nof a graph for N = 720 prototypes and k = 100 nearest neighbors takes less than\n\n\x0cAlessandro Sperduti, David G. Stork\n\n670\n\n[ZJ[ZJ[2J[2]\nDr\n\n5.10\n\n5.09\n\n5.01\n\n4.93\n\n4.90\n\nFigure 3: The search through a "2" category graph given a "5" test pattern. Note\nhow the search first tries to find a prototype that matches the upper arc of the\n"5," and then one possessing skew or rotation. For this test pattern, the minimum\nT-distance found for the "5" category (3.62) is smaller than the one found for the\n"2" category shown here (4.22), and indeed for any other category. Thus the test\npattern is correctly classified as a "5."\n\n20 minutes on a Sparc 10.\nThe crucial quantity of interest is the time complexity for search. This is, of course,\nproblem related, and depends upon the number of categories, transformation and\nprototypes and their statistical properties (see next Section). Worst case analyses\n(e.g., it is theoretically conceivable that nearly all prototypes must be visited) are\nirrelevant to practice.\nWe used a slightly non-obvious search criterion at each step, the function cos(Oj),\nas shown in Figure 1. Not only could this criterion be calculated very efficiently\nin our orthonormal basis (by using simple inner products), but it actually led to\na slightly more accurate search than Euclidean distance in the T-space - perhaps\nthe most natural choice of criterion. The angle OJ seems to guide the "flow" of the\nsearch along transformation directions toward the test point.\n\n4\n\nSimulations and results\n\nWe explored the search capabilities of our system on the binary handwritten digit\ndatabase of Guyon, et al. [1991J. We needed to scale all patterns by a linear factor\n(0.833) to insure that rotated versions did not go outside the 16 x 16 pixel grid. As\nrequired in all T-space methods, the patterns must be continuous valued (Le., here\ngrayscale); this was achieved by convolution with a spatially symmetric Gaussian\nhaving a = .55 pixels. We had 720 training examples in each of ten digit categories;\nthe test set consisted of 1320 test patterns formed by transforming independent\nprototypes in all meaningful combinations of the t = 6 transformations (four spatial\ndirections and two rotation senses).\nWe compared the Euclidean sorting method of Simard et al. [1993J to our graph\n\n\x0cGraph-Based Method for Arbitrary Transformation-Invariant Pattern Classification\n\n1.00\n\n671\n\n______-----:::::::::::::==---10. 6\n?\n\n0.4 u\n\n.c\n\n~\n\'"\n\nu\n\n0.2\n\n...\'\n\n\',-.\n-\n\nerror\n\n---~.\n\no\n50\n\n100\n\n150\n\n200\n\n"\n\n.. - ................ --\n\n250\n\n300\n\n350\n\n~\n\ne\n\n~\n\n0\n400\n\nComputational complexity\n(equivalent number of T-distance calculations)\n\nFigure 4: Comparison of graph-based (heavy lines) and standard Euclidean sorting\nsearches (thin lines). Search accuracy is the percentage of optimal prototypes found\non the full test set of 1320 patterns in a single category (solid lines). The average\nsearch error is the per pattern difference between the global optimum T -distance and\nthe one actually found, averaged over the non-optimal prototypes found through the\nsearch (dashed lines). Note especially that for the same computational complexity,\nour method has the same average error, but that this average is taken over a much\nsmaller number of (non-optimal) prototypes. For a given criterion search accuracy,\nour method requires significantly less computation. For instance, if 90% of the\nprototypes must be found for a requisite categorization accuracy (a typical value\nfor asymptotically high recognition accuracy), our graph-based method requires less\nthan half the computation of the Euclidean sorting method.\n\nbased method using the same data and transformations, over the full range of\nrelevant computational complexities. Figure 4 summarizes our results. For our\nmethod, the computational complexity is adjusted by the number of neighbors\ninspected, k. For their Euclidean sorting method, it is adjusted by the percentage\nof Euclidean nearest neighbors that were then inspected for T -distance. We were\nquite careful to employ as many computational tricks and shortcuts on both methods\nwe could think of. Our results reflect fairly on the full computational complexity,\nwhich was dominated by tangent and Euclidean distance calculations.\nWe note parenthetically that many of the recognition errors for both methods could\nbe explained by the fact that we did not include the transformation of line thinning\n(solely because we lacked the preprocessing capabilities); the overall accuracy of\nboth methods will increase when this invariance is also included.\n\n5\n\nCONCLUSIONS AND FUTURE WORK\n\nWe have demonstrated a graph-based method using tangent distance that permits search through prototypes significantly faster than the most popular current\napproach. Although not shown above, ours is also superior to other tree-based\n\n\x0c672\n\nAlessandro Sperduli. David G. Stork\n\nmethods, such as k-d-trees, which are less accurate. Since our primary concern was\nreducing the computational complexity of search (while matching Simard et al.\'s\naccuracy), we have not optimized over preprocessing steps, such as the Gaussian\nkernel width or transformation set. We note again that our method can be applied\nto reduced training sets, for instance ones pruned by the method of Simard, Hastie\n& Saeckinger [1994]. Simard\'s [1994] recent method - in which low-resolution\nversions of training patterns are organized into a hierarchical data structure so\nas to reduce the number of multiply-accumulates required during search - is in\nsome sense "orthogonal" to ours. Our graph-based method will work with his lowresolution images too, and thus these two methods can be unified into a hybrid\nsystem.\nPerhaps most importantly, our work suggests a number of research avenues. We\nused just a single ("central") prototype Po to start search; presumably having\nseveral candidate starting points would be faster. Our general method may admit\ngradient descent learning of parameters of the search criterion. For instance, we can\nimagine scaling the different tangent basis vectors according to their relevance in\nguiding correct searches as determined using a validation set. Finally, our approach\nmay admit elegant parallel implementations for real-world applications.\nAcknowledgements\n\nThis work was begun during a visit by Dr. Sperduti to Ricoh CRC. We thank I.\nGuyon for the use of her database of handwritten digits and Dr. K. V. Prasad for\nassistance in image processing.\nReferences\n1. Guyon, P. Albrecht, Y. Le Cun, J. Denker & W. Hubbard. (1991) "Comparing\n\ndifferent neural network architectures for classifying handwritten digits," Proc. of\nthe Inter. Joint Conference on Neural Networks, vol. II, pp. 127-132, IEEE Press.\nP. Simard. (1994) "Efficient computation of complex distance metrics using hierarchical filtering," in J. D. Cowan, G. Tesauro and J. Alspector (eds.) Advances in\nNeural Information Processing Systems-6 Morgan Kaufmann pp. 168-175.\nP. Simard, B. Victorrio, Y. Le Cun & J. Denker. (1992) "Tangent Prop - A formalism for specifying selected invariances in an adaptive network," in J. E. Moody, S.\nJ . Hanson and R. P. Lippmann (eds.) Advances in Neural Information Processing\nSystems-4 Morgan Kaufmann pp. 895-903.\nP. Y. Simard, Y. Le Cun & J. Denker. (1993) "Efficient Pattern Recognition Using\na New Transformation Distance," in S. J. Hanson, J. D. Cowan and C. L. Giles\n(eds.) Advances in Neural Information Processing Systems-5 Morgan Kaufmann\npp.50-58.\nP. Y. Simard, T. Hastie & E. Saeckinger. (1994) "Learning Prototype Models for\nTangent Distance," Neural Networks for Computing Snowbird, UT (April, 1994).\nG. D. Wilensky & N. Manukian. (1994) "Nearest Neighbor Networks: New Neural\nArchitectures for Distortion-Insensitive Image Recognition," Neural Networks for\nComputing Snowbird, UT (April, 1994).\n\n\x0c'
p83163
sg149
S'Ocular Dominance and Patterned Lateral\nConnections in a Self-Organizing Model of the\nPrimary Visual Cortex\nJoseph Sirosh and Risto Miikkulainen\n\nDepartment of Computer Sciences\nUniversity of Texas at Austin, Austin, \'IX 78712\nemail:\n\nsirosh.risto~cs.utexas.edu\n\nAbstract\nA neural network model for the self-organization of ocular dominance and\nlateral connections from binocular input is presented. The self-organizing\nprocess results in a network where (1) afferent weights of each neuron organize into smooth hill-shaped receptive fields primarily on one of the retinas, (2) neurons with common eye preference form connected, intertwined\npatches, and (3) lateral connections primarily link regions of the same eye\npreference. Similar self-organization of cortical structures has been observed experimentally in strabismic kittens. The model shows how patterned lateral connections in the cortex may develop based on correlated\nactivity and explains why lateral connection patterns follow receptive field\nproperties such as ocular dominance.\n\n1 Introduction\nLateral connections in the primary visual cortex have a patterned structure that closely\nmatches the response properties of cortical cells (Gilbert and Wiesel 1989; Malach et al.1993).\nFor example, in the normal visual cortex, long-range lateral connections link areas with similar orientation preference (Gilbert and Wiesel 1989). Like cortical response properties, the\nconnectivity pattern is highly plastic in early development and can be altered by experience\n(Katz and Callaway 1992). In a cat that is brought up squint-eyed from birth, the lateral connections link areas with the same ocular dominance instead of orientation (Lowel and Singer\n1992). Such patterned lateral connections develop at the same time as the orientation selectivity and ocular dominance itself (Burkhalter et al.1993; Katz and Callaway 1992). Together,\n\n\x0c110\n\nJoseph Sirosh, Risto Miikkulainen\n\nthese observations suggest that the same experience-dependent process drives the development of both cortical response properties and lateral connectivity.\nSeveral computational models have been built to demonstrate how orientation preference,\nocular dominance, and retinotopy can emerge from simple self-organizing processes (e.g.\nGoodhill1993; Miller 1994; Obermayer et al.1992; von der Malsburg 1973). These models\nassume that the neuronal response properties are primarily determined by the afferent connections, and concentrate only on the self-organization of the afferent synapses to the cortex. Lateral interactions between neurons are abstracted into simple mathematical functions\n(e.g. Gaussians) and assumed to be uniform throughout the network; lateral connectivity is not\nexplicitly taken into account. Such models do not explicitly replicate the activity dynamics\nof the visual cortex, and therefore can make only limited predictions about cortical function.\nWe have previously shown how Kohonen\'s self-organizing feature maps (Kohonen 1982)\ncan be generalized to include self-organizing lateral connections and recurrent activity dynamics (the Laterally Interconnected Synergetically Self-Organizing Map (LISSOM); Sirosh\nand Miikkulainen 1993, 1994a), and how the algorithm can model the development of ocular dominance columns and patterned lateral connectivity with abstractions of visual input.\nLISSOM is a low-dimensional abstraction of cortical self-organizing processes and models a\nsmall region of the cortex where all neurons receive the same input vector. This paper shows\nhow realistic, high-dimensional receptive fields develop as part of the self-organization, and\nscales up the LISSOM approach to large areas of the cortex where different parts of the cortical network receive inputs from different parts of the receptor surface. The new model shows\nhow (1) afferent receptive fields and ocular dominance columns develop from simple retinal images, (2) input correlations affect the wavelength of the ocular dominance columns and\n(3) lateral connections self-organize cooperatively and simultaneously with ocular dominance\nproperties. The model suggests new computational roles for lateral connections in the cortex,\nand suggests that the visual cortex maybe maintained in a continuously adapting equilibrium\nwith the visual input by co adapting lateral and afferent connections.\n\n2\n\nThe LISSOM Model of Receptive Fields and Ocular Dominance\n\nThe LISSOM network is a sheet of interconnected neurons (figure 1). Through afferent connections, each neuron receives input from two "retinas". In addition, each neuron has reciprocal excitatory and inhibitory lateral connections with other neurons. Lateral excitatory connections are short-range, connecting only close neighbors. Lateral inhibitory connections run\nfor long distances, and may even implement full connectivity between neurons in the network.\nNeurons receive afferent connections from broad overlapping patches on the retina called\nanatomical receptive fields, or RFs. The N x N network is projected on to each retina of\nR x R receptors, and each neuron is connected to receptors in a square area of side s around\nthe projections. Thus, neurons receive afferents from corresponding regions of each retina.\nDepending on the location of the projection, the number of afferents to a neuron from each\nretina could vary from\nx ~s (at the comers) to s x s (at the center).\n\nts\n\nThe external and lateral weights are organized through an unsupervised learning process. At\neach training step, neurons start out with zero activity. The initial response TJij of neuron (i, j)\n\n\x0cOcular Dominance and Patterned Lateral Connections\n\nLoft _ . .\n\n111\n\nfllgIIl Roll . .\n\nFigure 1: The Receptive-Field LISSOM architecture. The afferent and lateral connectionsof a single\nneuron in the liSSOM network are shown. All connection weights are positive.\n\nis based on the scalar product\nTJij\n\n=\n\n(T\n\n(L\n\neabJJij ,ab\n\n+\n\na,b\n\nL\n\n(1)\n\neCdJJij,Cd) ,\n\nc,d\n\nwhere eab and ecd are the activations of retinal receptors (a, b) and (c, d) within the receptive\nfields of the neuron in each retina, JJij,ab and JJij,cd are the corresponding afferent weights,\nand (T is a piecewise linear approximation of the familiar sigmoid activation function. The\nresponse evolves over time through lateral interaction. At each time step, the neuron combines the above afferent activation I:: eJJ with lateral excitation and inhibition:\nTJij(t)\n\n=\n\n(T\n\n(L eJJ + L\n"Ie\n\nEij,kITJkl(t -\n\n1) - L\n"Ii\n\nk,1\n\nIij,klTJkl(t -\n\n1)) ,\n\n(2)\n\nk,1\n\nwhere Eij,kl is the excitatory lateral connection weight on the connection from neuron (k, l)\nto neuron (i, j), Iij,kl is the inhibitory connection weight, and TJkl (t - 1) is the activity of\nneuron (k, I) during the previous time step. The constants "Ie and "Ii determine the relative\nstrengths of excitatory and inhibitory lateral interactions. The activity pattern starts out diffuse and spread over a substantial part of the map, and converges iteratively into stable focused\npatches of activity, or activity bubbles. After the-activity has settled, typically in a few iterations of equation 2, the connection weights of each neuron are modified. Both afferent and\nlateral weights adapt according to the same mechanism: the Hebb rule, normalized so that the\nsum of the weights is constant:\n(\n\nWij,mn t\n\nr ) _\n\n+ vt\n\n-\n\n+\n\nWij,mn(t)\nCtTJijXmn\n\'""\n( )\nwmn [Wij ,mn t\nCtTJijXmn\n\n+\n\n1\'\n\n(3)\n\nwhere TJij stands for the activity of neuron (i, j) in the final activity bubble, Wij,mn is the afferent or lateral connection weight (JJ, E or I), Ct is the learning rate for each type of connection\n(Ct a for afferent weights, Ct E for excitatory, and Ct I for inhibitory) and X mn is the presynaptic\nactivity for afferent, TJ for lateral).\n\n(e\n\n\x0cJoseph Sirosh, Risto Miikkulainen\n\n112\n\n"\n(a) Random Initial Weights\n\n(b) Monocular RF\n\n(c) Binocular RF\n\nFigure 2: Self-organization of the afferent input weights into receptive fields. The afferent weights\nof a neuron at position (42,39) in a 60 x 60 network are shown before (a) and after self-organization\n(b). This particular neuron becomes monocular with strong connections to the right eye, and weak connections to the left. A neuron at position (38, 23) becomes binocular with appoximately equal weights\nto both eyes (c).\nBoth excitatory and inhibitory lateral connections follow the same Hebbian learning process and strengthen by correlated activity. The short-range excitation keeps the activity of\nneighboring neurons correlated, and as self-organization progresses, excitation and inhibition strengthen in the vicinity of each neuron. At longer distances, very few neurons have\ncorrelated activity and therefore most long-range connections become weak. Such weak connections are eliminated, and through weight normalization, inhibition concentrates in a closer\nneighborhood of each neuron. As a result, activity bubbles become more focused and local,\nweights change in smaller neighborhoods, and receptive fields become better tuned to local\nareas of each retina.\nThe input to the model consists of gaussian spots of "light" on each retina:\nt\n_\n((x\n<"x,y - exp -\n\n- xd 2 + (y - Yi)2)\nu2\n\n(4)\n\nwhere ex,y is the activation of receptor (x, V), u 2 is a constant determining the width of the\nspot, and (Xi,Yi): 0 ~ xi, Yi < R its center. At each input presentation, one spot is randomly\nplaced at (Xi ,Yi) in the left retina, and a second spot within a radius of p x RN of (Xi, yd\nin the right retina. The parameter p E [0, 1] specifies the spatial correlations between spots\nin the two retinas, and can be adjusted to simulate different degrees of correlations between\nimages in the two eyes.\n\n3\n\nSimulation results\n\nTo see how correlation between the input from the two eyes affects the columnar structures\nthat develop, several simulations were run with different values of p. The afferent weights of\nall neurons were initially random (as shown in figure 2a), with the total strength to both eyes\nbeing equal.\nFigures 2b,c show the final afferent receptive fields of two typical neurons in a simulation\nwith p = 1. In this case, the inputs were uncorrelated, simulating perfect strabismus. In\nthe early stages of such simulation, some of the neurons randomly develop a preference for\none eye or the other. Nearby neurons will tend to share the same preference because lateral\n\n\x0cOcular Dominance and Patterned Lateral Connections\n\n(a) Connections of a Monocular Neuron\n\n113\n\n(b) Connections of a Binocular Neuron\n\nFigure 3: Ocular dominance and lateral connection patterns. The ocular dominance of a neuron is\nmeasured as the difference in total afferent synaptic weight from each eye to the neuron. Each neuron\nis labeled with a grey-scale value (black ~ white) that represents continuously changing eye preference from exclusive left through binocular to exclusive right. Small white dots indicate the lateral input\nconnections to the neuron marked with a big white dot. (a) The surviving lateral connections of a left\nmonocular neuron predominantly link areas of the same ocular dominance. (b) The lateral connections\nof a binocular neuron come from both eye regions.\n\nexcitation keeps neural activity partially correlated over short distances. As self-organization\nprogresses, such preferences are amplified, and groups of neurons develop strong weights to\none eye. Figure 2b shows the afferent weights of a typical monocular neuron.\nThe extent of activity correlations on the network detennines the size of the monocular neuronal groups. Farther on the map, where the activations are anticorrelated due to lateral inhibition, neurons will develop eye preferences to the opposite eye. As a result, alternating\nocular dominance patches develop over the map, as shown in figure 3. 1 In areas between ocular dominance patches, neurons will develop approximately equal strengths to both eyes and\nbecome binocular, like the one shown in figure 2e.\nThe width and number of ocular dominance columns in the network (and therefore, the wavelength of ocular dominance) depends on the input correlations (figure 4). When inputs in the\ntwo eyes become more correlated (p < 1), the activations produced by the two inputs in the\nnetwork overlap closely and activity correlations become shorter range. By Hebbian adaptation, lateral inhibition concentrates in the neighborhood of each neuron, and the distance at\nwhich activations becomes anticorrelated decreases. Therefore, smaller monocular patches\ndevelop, and the ocular dominance wavelength decreases. Similar dependence was very recently observed in the cat primary visual cortex (LoweI1994). The LISSOM model demonstrates that the adapting lateral interactions and recurrent activity dynamics regulate the wavelength, and suggests how these processes help the cortex develop feature detectors at a scale\n1 For a thorough treatment of the mathematical principles underlying the development of ocular dominance columns, see (GoodhillI993; Miller et al.1989; von der Malsburg and Singer 1988).\n\n\x0c114\n\nJoseph Sirosh, Risto Miikkulainen\n\n-0\n-0\n\n(a) Strabismic case\n\n(b ) Normal case\n\nFigure 4: Ocular dominance wavelength in strabismic and normal models. In the strabismic case,\nthere are no between-eye correlations (p = 1), and broad ocular dominance columns are produced (a) .\nWith normal, partial between-eye correlations (p = 0.45 in this example), narrower stripes are formed\n(b). As a result, there are more ocular dominance columns in the normal case and the ocular dominance\nwavelength is smaller.\n\nthat matches the input correlations.\nAs eye preferences develop, left or right eye input tends to cause activity only in the left or\nright ocular dominance patches. Activity patterns in areas of the network with the same ocular dominance tend to be highly correlated because they are caused by the same input spot.\nTherefore, the long-range lateral connections between similar eye preference areas become\nstronger, and those between opposite areas weaker. After the weak lateral connections are\neliminated, the initially wide-ranging connections are pruned, and eventually only connect\nareas of similar ocular dominance as shown in figure 3. Binocular neurons between ocular\ndominance patches will see some correlated activity in both the neigbboring areas, and maintain connections to both ocular dominance columns (figure 3b).\nThe lateral connection patterns shown above closely match observations in the primary visual cortex. Lowel and Singer (1992) observed that when between-eye correlations are abolished in kittens by surgically induced strabismus, long-range lateral connections primarily\nlink areas of the same ocular dominance. However, binocular neurons, located between ocular dominance columns, retained connections to both eye regions. The receptive field model\nconfinns that such patterned lateral connections develop based on correlated neuronal activity,\nand demonstrates that they can self-organize simultaneously with ocular dominance columns.\nThe model also predicts that the long-range connections have an inhibitory function.\n\n4 Discussion\nIn LISSOM, evolving lateral interactions and dynamic activity patterns are explicitly modeled. Therefore, LISSOM has several novel properties that set it apart from other selforganizing models of the cortex.\nPrevious models (e.g. Goodhill1993; Milleret al.1989; Obermayer et al.1992; von der Malsburg 1973) have concentrated only on forming ordered topographic maps where clusters of\nadjacent neurons assume similar response properties such as ocular dominance or orientation\npreference. The lateral connections in LISSOM, in addition, adapt to encode correlations be-\n\n\x0cOcular Dominance and Patterned Lateral Connections\n\n115\n\ntween the responses. 2 This property can be potentially very useful in models of cortical function. While afferent connections learn to detect the significant features in the input space (such\nas ocularity or orientation), the lateral connections can learn correlations between these features (such as Gestalt principles), and thereby form a basis for feature grouping.\nAs an illustration, consider a single spot of light presented to the left eye. The spot causes disjoint activity patterns in the left-eye-dominant patches. How can these multiple activity patterns be recognized as representing the same spatially coherent entity? As proposed by Singer\net al. (1990), the long-range lateral connections between similar ocular dominance columns\ncould synchronize cortical activity, and form a coherently firing assembly of neurons. The\nspatial coherence of the spot will then be represented by temporal coherence of neural activity. LISSOM can be potentially extended to model such feature binding.\nEven after the network has self-organized, the lateral and afferent connections remain plastic\nand in a continuously-adapting dynamic equilibrium with the input. Therefore, the receptive\nfield properties of neurons can dynamically readapt when the activity correlations in the network are forced to change. For example, when a small area of the cortex is set inactive (or\nlesioned), the sharply-tuned afferent weight profiles of the neurons surrounding that region\nexpand in size, and neurons begin to respond to the stimuli that previously activated only the\nlesioned area (Sirosh and Miikkulainen 1994b, 1994c). This expansion of receptive fields is\nreversible, and when the lesion is repaired, neurons return to their original tuning. Similar\nchanges occur in response to retinal lesions as well. Such dynamic expansions of receptive\nfields have been observed in the visual cortex (Pettet and Gilbert 1992). The LISSOM model\ndemonstrates that such plasticity is a consequence of the same self-organizing mechanisms\nthat drive the development of cortical maps.\n\n5\n\nConclusion\n\nThe LISSOM model shows how a single local and unsupervised self-organizing process can\nbe responsible for the development of both afferent and lateral connection structures in the primary visual cortex. It suggests that this same developmental mechanism also encodes higherorder visual information such as feature correlations into the lateral connections. The model\nforms a framework for future computational study of cortical reorganization and plasticity, as\nwell as dynamic perceptual processes such as feature grouping and binding.\nAcknowledgments\n\nThis research was supported in part by National Science Foundation under grant #IRI9309273. Computer time for the simulations was provided by the Pittsburgh Supercomputing\nCenter under grants IRI930005P and TRA940029P.\n\nReferences\nBurkhalter, A., Bernardo, K. L., and Charles, V. (1993). Development of local circuits in\nhuman visual cortex. Journalo/Neuroscience, 13:1916-1931.\nGilbert, C. D., and Wiesel, T. N. (1989). Columnar specificity of intrinsic horizontal and\ncorticocortical connections in cat visual cortex. Journal 0/ Neuroscience, 9:2432-2442.\n2Tbe idea was conceived by von der Malsburg and Singer (1988), but not modeled.\n\n\x0c116\n\nJoseph Sirosh, Risto Miikkulainen\n\nGoodhill, G. (1993). Topography and ocular dominance: a model exploring positive correlations. Biological Cybernetics, 69:109-118.\nKatz, L. C., and Callaway, E. M. (1992). Development of local circuits in mammalian visual\ncortex. Annual Review o/Neuroscience, 15:31-56.\nKohonen, T. (1982). Self-organized formation of topologically correct feature maps. Biolog-\n\nical Cybernetics, 43:59-69.\nLowel, S. (1994). Ocular dominance column development: Strabismus changes the spacing\nof adjacent columns in cat visual cortex. Journal 0/ Neuroscience, 14(12):7451-7468.\nLowel, S., and Singer, W. (1992). Selection of intrinsic horizontal connections in the visual\ncortex by correlated neuronal activity. Science, 255:209-212.\nMalach, R., Amir, Y., Harel, M., and Grinvald, A (1993). Relationship between intrinsic\nconnections and functional architecture revealed by optical imaging and in vivo targeted\nbiocytin injections in the primate striate cortex. Proceedings o/the National Academy\n\no/Sciences, USA,90:10469-10473.\nMiller, K. D. (1994). A model for the development of simple cell receptive fields and the\nordered arrangement of orientation columns through activity-dependent competition between on- and off-center inputs. Journalo/Neuroscience, 14:409-441.\nMiller, K. D., Keller, 1. B., and Stryker, M. P. (1989). Ocular dominance column development:\nAnalysis and simulation. Science, 245:605-615.\nObermayer, K., Blasdel, G. G., and Schulten, K. J. (1992). Statistical-mechanical analysis of\nself-organization and pattern formation during the development of visual maps. Physical\n\nReview A, 45:7568-7589.\nPettet, M. W., and Gilbert, C. D. (1992). Dynamic changes in receptive-field size in cat primary visual cortex. Proceedings o/the NationalAcademy 0/ Sciences, USA,89:83668370.\nSinger, W., Gray, C., Engel, A, Konig, P., Artola, A, and Bracher, S. (1990). Formation of\ncortical cell assemblies. In Cold Spring Harbor Symposia on Quantitative Biology, Vol.\nLV, 939-952. Cold Spring Harbor, NY: Cold Spring Harbor Laboratory.\nSirosh, J., and Miikkulainen, R. (1993). How lateral interaction develops in a self-organizing\nfeature map. In Proceedings o/the IEEE International Conference on Neural Networks\n(San Francisco, CA), 1360--1365. Piscataway, NJ: IEEE.\nSirosh, J., and Miikkulainen, R. (1994a). Cooperative self-organization of afferent and lateral\nconnections in cortical maps. Biological Cybernetics, 71(1):66--78.\nSirosh, 1., and Miikkulainen, R. (1994b). Modeling cortical plasticity based on adapting lateral interaction. In The Neurobiologyo/Computation: Proceedings o/the Annual ComputationalNeuroscience Meeting. Dordrecht; Boston: Kluwer. In Press.\nSirosh, J., and Miikkulainen, R. (1994c). A neural network model oftopographic reorganization following cortical lesions. In Proceedings o/the World Congress on Computational\nMediCine, Public Health and BioteChnology (Austin, TX). World Scientific. In Press.\nvon der Malsburg, C. (1973). Self-organization of orientation-sensitive cells in the striate\ncortex. Kybernetik, 15:85-100.\nvon der Malsburg, C., and Singer, W. (1988). Principles of cortical network organization. In\nRakic, P., and Singer, W., editors, Neurobiology 0/Neocortex, 69-99. New York: Wiley.\n\n\x0c'
p83164
sg429
S'618\n\nNEURAL NETWORKS FOR MODEL\nMATCHING AND PERCEPTUAL\nORGANIZATION\nGene Gindi\nEE Department\nYale University\nNew Haven, CT 06520\n\nEric Mjolsness\nCS Department\nYale University\nNew Haven, CT 06520\n\nP. Anandan\nCS Department\nYale University\nNew Haven, CT 06520\n\nABSTRACT\nWe introduce an optimization approach for solving problems in computer vision that involve multiple levels of abstraction. Our objective\nfunctions include compositional and specialization hierarchies. We cast\nvision problems as inexact graph matching problems, formulate graph\nmatching in terms of constrained optimization, and use analog neural\nnetworks to perform the optimization. The method is applicable to perceptual grouping and model matching. Preliminary experimental results\nare shown.\n\n1\n\nIntroduction\n\nThe minimization of objective functions is an attractive way to formulate and solve\nvisual recognition problems. Such formulations are parsimonious, being expressible\nin several lines of algebra, and may be converted into artificial neural networks\nwhich perform the optimization. Advantages of such networks including speed,\nparallelism, cheap analog computing, and biological plausibility have been noted\n[Hop field and Tank, 1985].\nAccording to a common view of computational vision, recognition involves the construction of abstract descriptions of data governed by a data base of models. Abstractions serve as reduced descriptions of complex data useful for reasoning about\nthe objects and events in the scene. The models indicate what objects and properties\nmay be expected in the scene. The complexity of visual recognition demands that\nthe models be organized into compositional hierarchies which express object-part\nrelationships and specialization hierarchies which express object-class relationships.\nIn this paper, we describe a methodology for expressing model-based visual recognition as the constrained minimization of an objective function. Model-specific\nobjective functions are used to govern the dynamic grouping of image elements into\nrecognizable wholes. Neural networks are used to carry out the minimization.\n?This work was supported in part by AFOSR grant F49620-88-C-002S, and by DARPA grant\nDAAAlS-87-K-OOOl, by ONR grant N00014-86-0310.\n\n\x0cModel Matching and Perceptual Organization\n\nPrevious work on optimization in vision has typically been restricted to computations occuring at a single of level of abstraction and/or involving a single model\n[Barrow and Popplestone, 1971,Hummel and Zucker, 1983,Terzopoulos, 1986]. For\nexample, surface interpolation schemes, even when they include discontinuities\n[Terzopoulos, 1986] do not include explicit models for physical objects whose surface\ncharacteristics determine the expected degree of smoothness. By contrast, heterogeneous and hierarchical model-bases often occur in non-optimization approaches\nto visual recognition [Hanson and Riseman, 1986] including some which use neural\nnetworks [Ballard, 1986]. We attempt to obtain greater express ability and efficiency\nby incorporating hierarchies of abstraction into the optimization paradigm.\n\n2\n\nCasting Model Matching as Optimization\n\nWe consider a type of objective function which, when minimized by a neural\nnetwork, is capable of expressing many of the ideas found in Frame systems\nin Artificial Intelligence [Minsky, 1975]. These "Frameville" objective functions\n[Mjolsness et al., 1988,Mjolsness et al., 1989] are particularly well suited to applications in model-based vision, with frames acting as few-parameter abstractions of\nvisual objects or perceptual groupings thereof. Each frame contains real-valued parameters, pointers to other frames, and pointers to predefined models (e.g. models\nof objects in the world) which determine what portion of the objective function acts\nupon a given frame.\n\n2.1\n\nModel Matching as Graph Matching\n\nModel matching involves finding a match between a set of frames, ultimately derived\nfrom visual data, and the predefined static models. A set of pointers represent\nobject-part relationships between frames, and are encoded as a graph or sparse\nmatrix called ina. That is, inaij = 0 unless frame j is "in" frame i as one of its\nparts, in which case inaij\n1 is a "pointer" from j to i. The expected objectpart relationships between the corresponding models is encoded as a fixed graph\nor sparse matrix INA. A form of inexact graph-matching is required: ina should\nfollow INA as much as is consistent with the data.\nA sparse match matrix M (0 < Meti < 1) of dynamic variables represents the\ncorrespondence between model a and frame i. To find the best match between the\ntwo graphs one can minimize a simple objective function for this match matrix, due\nto Hopfield [Hopfield, 1984] (see also [Feldman et al., 1988,Malsburg, 1986]), which\njust counts the number of consistent rectangles (see Figure 1a):\n\n=\n\nE(M)\n\n= - ~~INAet~inaijMaiM~j.\net{3\n\n(1)\n\nij\n\nThis expression may be understood as follows: For model a and frame i, the match\nvalue M eti is to be increased if the neighbors of a (in the INA graph) match to the\nneighbors of i (in the ina graph).\n\n619\n\n\x0c620\n\nMjolsness, Gindi and Anandan\nM\n\n1??\n\nINA t.2\n\nData\nside\n\nModel\nside\n\nFigure 1: (a) Examples of Frameville rectangle rule. Shows the rectangle relationship between frames (triangles) representing a wing of a plane, and the plane\nitself. Circles denote dynamic variables, ovals denote models, and triangles denote\nframes. For the plane and wing models, the first few parameters of a frame are\ninterpreted as position, length, and orientation. (b) Frameville sibling competition among parts. The match variables along the shaded lines (M3,9 and M 2,7)\nare suppressed in favor of those along the solid lines (M2,9 and M 3,7)\'\nNote that E(M) as defined above can be trivially minimized by setting all the elements of the match matrix to unity. However, to do so will violate additional\nsyntactic constraints of the form h(M) 0 which are imposed on the optimization,\neither exactly (Platt and Barr, 1988] or as penalty terms (Hopfield and Tank, 1985]\n~h2(M) added to the objective function. Originally the syntactic constraints\nsimply meant that each frame should match one model and vice versa, as in\n(Hopfield and Tank, 1985]. But in Frameville, a frame can match both a model\nand one of its specializations (described later), and a single model can match any\nnumber of instances or frames. In addition one can usually formulate constraints\nstating that if a model matches a frame then two distinct parts of the same model\nmust match two distinct part frames and vice-versa. \\Ve have found the following\n\n=\n\n\x0cModel Matching and Perceptual Organization\n\nformulation to be useful:\n\n~ INAa{3Mai - ~ inaijM{3j\na\n\n"\'p, i\n\n(2)\n\n0, Va,j\n\n(3)\n\n0,\n\nj\n\nE inaijMai - E 1NAa{3M{3j\nJ\n\n{3\n\nwhere the first sum in each equation is necessary when several high-level models\n(or frames) share a part. (It turns out that the first sums can be forced to zero\nor one by other constraints.) The resulting competition is illustrated in Figure lb.\nAnother constraint is that M should be binary-valued, i.e.,\n(4)\n\nbut this constraint can also be handled by a special "analog gain" term in\nthe objective function [Hopfield and Tank, 1985] together with a penalty term\nc Eai Mai(l - Mai).\nIn Frameville, the ina graph actually becomes variable, and is determined by a dynamic grouping or "perceptual organization" process. These new variables require\nnew constraints, starting with inaij (1 - inaij) = 0, and including many high-level\nconstraints which we now formulate.\n\n2.2\n\nFranles and Objective Functions\n\nFrames can be considered as bundles ~ of real-valued parameters Fip, where p\nindexes the different parameters of a frame. For efficiency in computing complex\narithmetic relationships, such as those involved in coordinate transformations, an\nanalog representation of these parameters is used. A frame contains no information\nconcerning its match criteria or control flow; instead, the match criteria are expressed as objective functions and the control flow is determined by the partiCUlar\nchoice of a minimization technique.\nIn Figure la, in order for the rectangle (1,4,9,2) to be consistent, the parameters\nF 4p and F 9p should satisfy a criterion dictated by models 1 and 2, such as a restriction on the difference in angles appropriate for a mildly swept back wing. Such a\nconstraint results in the addition of the following term to the objective function:\n\nL\n\nlNA a{3 inaij MaiM{3j Ha{3(~, Pj)\n\n(5)\n\ni,j,a,{3\n\nwhere Ha{3(~, Fj) measures the deviation of the parameters of the data frames from\nthat demanded by the models. The term H can express coordinate transformation\narithmetic (e.g. H a{3(Xi, Xj) = 1/2[xi - Xj - D.x a{3]2), and its action on a frame f;.\nis selectively controlled or "gated" by M and ina variables. This is a fundamental\nextension of the distance metric paradigm in pattern recognition; because of the\ncomplexity of the visual world, we use an entire database of distance metrics H a {3.\n\n621\n\n\x0c622\n\nMjolsness, Gindi and Anandan\n\nFigure 2: Frameville specialization hierarchy. The plane model specializes\nalong 154. links to a propeller plane or a jet plane and correspondingly the wing\nmodel specializes to prop-wing or jet-wing. Sibling match variables M 6 ,4 and M 4 ,4\ncompete as do M7,9 and M S ,9. The winner in these competitions is determined by\nthe consistency of the appropriate rectangles, e.g. if the 4-4-9-5 rectangle is more\nconsistent than the 6-4-9-7 rectangle, then the jet model is favored over the prop\nmodel.\nWe index the models (and, indirectly, the data base of H metrics) by introducing\na static graph of pointers I54. OI {j to act as both a specialization hierarchy and a\ndiscrimination network for visual recognition. A frame may simultaneously match\nto a model and just one of its specializations:\n\nMcxi -\n\nL I54.cx{jMf3i = o.\n\n(6)\n\nf3\n\nAs a result, 154. siblings compete for matches to a given frame (see Figure 2); this\ncompetition allows the network to act as a discrimination tree.\nFrameville networks have great expressive power, but have a potentially serious\nproblem with cost: for n data frames and m models there may be O(nm + 71 2 )\nneurons widely interconnected but sparsely activated. The number of connections\nis at most the number of monomials in the polynomial objective function, namely\nn2 m/, where / is the fan-out of the INA graph. One solution to the cost problem, used in the line grouping experiments reported in [Mjolsness et al., 1989], is to\nrestrict the flexibility of the frame system by setting most M and ina neurons to\nzero permanently. The few remaining variables can form an efficient data structure\n\n\x0cModel Matching and Perceptual Organization\n\nsuch as a pyramid in vision. A more flexible solution might enforce the sparseness\nconstraints on the M and ina neurons during minimization, as well as at the fixed\npoint. Then large savings could result from using "virtual" neurons (and connections) which are created and destroyed dynamically. This and other cost-cutting\nmethods are a subject of continuing research.\n\n3\n\nExperimental Results\n\nWe describe here experiments involving the recognition of simple stick figures.\n(Other experiments involving the perceptual grouping of lines are reported in\n[Mjolsness et al., 1989].) The input data (Figure 3(a)) are line segments parameterized by location x, y and orientation (), corresponding to frame parameters Fjp\n(p\n1,2,3). As seen in Figure 3(b), there are two high-level models, "T" and\n"L" junctions, each composed of three low-level segments. The task is to recognize\ninstances of "T", "L", and their parts, in a translation-invariant manner.\nThe parameter check term H cx {3 of Equation 5 achieves translation invariance by\nchecking the location and orientation of a given part relative to a designated main\npart and is given by:\n\n=\n\nHa{3(~, ff;)\n\n= I)Fip -\n\nFjp - ~;{3)2\n\n(7)\n\nP\n\nHere Fjp and Fip are the slots of a low-level segment frame and a high-level main\npart, respectively, and the quantity ~~{3 is model information that stores coordinate\ndifferences. (Rotation invariance can also be formulated if a different parameterization is used.) It should be noted that absence of the main part does not preclude\nrecognition of the high-level model.\nWe used the unconstrained optimization technique in [Hopfield and Tank, 1985] and\nachieved improved results by including terms demanding that at most one model\nmatch a given frame, and that at most one high-level frame include a given low-level\nframe as its part [Mjolsness et al., 1989].\nFigure 3(c) shows results of attempts to recognize the junctions in Figure 3(a).\nWhen initialized to random values, the network becomes trapped in unfavorable\nlocal minima of the fifth-order objective function. (But with only a single high-level\nmodel in the database, the system recognizes a shape amid noise.) If, however, the\nnetwork is given a "hint" in the form of an initial state with mainparts and high-level\nmatches set correctly, the network converges to the correct state.\nThere is a great deal of unexploited freedom in the design of the model base and\nits objective functions; there may be good design disciplines which avoid introducing spurious local minima. For example, it may be possible to use ISA and INA\nhierarchies to guide a network to the desired local minimum.\n\n623\n\n\x0c624\n\nMjolsness, Gindi and Anandan\n\n+ +\n\n+++++++t\n+ + + + + + +\n? + +\n,+++++++\n? + +\n+ + + + + + + +\n7\n? + + + + +\n+ + + + +\n5+++++ ,+++++\n4 + + + + +\n+ + +\n3 + + + + + t + + + + +\n2 + + + + +\n+ + + + +\n1 + + + + + +--t--+ + + +\no-;r+ + + + + + +--t-+\' +\n\n10\n\no\n\n1\n\n2\n\n3\n\n4\n\n5\n\nI\n\n7\n\n?\n\n9\n\n10\n\nMpj\n\nE; 0000.00000\n\n1\n\n2\n\n3\n\n123\n1 ? ? ? 0000000\n20000 ? ? ?000\n30000000000\ninaij\n\nB .000000000\n8 00000.0000\nE3 0.00000000\nE1 00.0000000\nB 000000l1li000\n9 0000000000\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n12345678910\n\nFigure 3: (a) Input data consists of unit-length segments oriented horizontally or\nvertically. The task is translation-invariant recognition of three segments forming a\nliT" junction (e.g. sticks 1,2,3) or an "L" (e.g. sticks 5,6,7) amid extraneous noise\nsticks. (b) Structure of network. Models occur at two levels. INA links are\nshown for a liT". Each frame has three parameters: position x, y and orientation\ne. Also shown are some match and ina links. The bold lines highlight a possible\nconsistency rectangle. (c) Experhnental result. The value of each dynamical\nvariable is displayed as the relative area of the shaded portion of a circle. Matrix\nM{jj indicates low-level matches and MOti indicates high-level matches. Grouping\nof low-level to high-level frames is indicated by the ina matrix. The parameters of\nthe high-level frames are displayed in the matrix Fip of linear analog neurons. (The\nparameters of the low-level frames, held fixed, are not displayed.) The few neurons\ncircumscribed by a square, corresponding to correct matches for the main parts of\neach model, are clamped to a value near unity. Shaded circles indicate the final\ncorrect state.\n\n\x0cModel Matching and Perceptual Organization\n\n4\n\nConclusion\n\nFrameville provides opportunities for integrating all levels of vision in a uniform notation which yields analog neural networks. Low-level models such as fixed convolution filters just require analog arithmetic for frame parameters, which is provided.\nHigh-level vision typically requires structural matching, also provided. Qualitatively\ndifferent models may be integrated by specifying their interactions, H cx /3.\nAcknowledgements\n\nWe thank J. Utans, J. Ockerbloom and C. Garrett for the Frameville simulations.\nReferences\n[1] Dana Ballard. Cortical connections and parallel processing: structure and function.\nBehavioral and Brain Sciences, vol 9:67-120, 1986.\n[2] Harry G. Barrow and R. J. Popplestone. Relational descriptions in picture processing.\nIn D. Mitchie, editor, Machine Intelligence 6, Edinborough University Press, 1971.\n[3] Jerome A. Feldman, Mark A. Fanty, and Nigel H. Goddard. Computing with structured neural networks. IEEE Computer, 91, March 1988.\n[4] Allen R. Hanson and E. M. Riseman. A methodology for the development of general\nknowledge-based vision systems. In M. A. Arbib and A. R. Hanson, editors, Vision,\nBrain, and Cooperative Computation, MIT Press, 1986.\n[5] J. J. Hopfield. Personal communication. October 1984.\n[6] J. J. Hopfield and D. W. Tank. \'Neural\' computation of decisions in optimization\nproblems. Biological Cybernetics, vol. 52:141-152, 1985.\n[7] Robert A. Hummel and S. W. Zucker. On the foundations of relaxation labeling\nprocesses. IEEE Transactions on PAMI, vol. PAMI-5:267-287, May 1983.\n[8] Marvin L. Minsky. A framework for representing knowledge. In P. H. Winston, editor,\nThe Psychology of Computer Vision, McGraw-Hill, 1975.\n[9] Eric Mjolsness, Gene Gindi, and P. Anandan. Optimization in Model Matching and\nPerceptual Organization: A First Look. Technical Report YALEU /DCS/RR-634,\nYale University, June 1988.\n[10] Eric Mjolsness, Gene Gindi, and P. Anandan. Optimization in Model Matching and\nPerceptual Organization. Neural Computation, to appear.\n[11] John C. Platt and Alan H. Barr. Constraint methods for flexible models. Computer\nGraphics, 22(4), August 1988. Proceedings of SIGGRAPH \'88.\n[12] Demitri Terzopoulos. Regularization of inverse problems involving discontinuities.\nIEEE Transactions on PAMI, vol. PAMI-8:413-424, 1986.\n[13] Christoph von der Malsburg and Elie Bienenstock. Statistical coding and short-term\nsynaptic plasticity: a scheme for knowledge representation in the brain. In Disordered\nSystems and Biological Organization, pages 247-252, Springer-Verlag, 1986.\n\n625\n\n\x0c'
p83165
sg102
S'186\n\nAN APPLICATION OF THE PRINCIPLE OF\nMAXIMUM INFORMATION PRESERVATION\nTO LINEAR SYSTEMS\nRalph Linsker\nIBM T. J. Watson Research Center, Yorktown Heights, NY 10598\n\nABSTRACT\nThis paper addresses the problem of determining the weights for a\nset of linear filters (model "cells") so as to maximize the\nensemble-averaged information that the cells\' output values jointly\nconvey about their input values, given the statistical properties of\nthe ensemble of input vectors. The quantity that is maximized is the\nShannon information rate, or equivalently the average mutual\ninformation between input and output. Several models for the role\nof processing noise are analyzed, and the biological motivation for\nconsidering them is described. For simple models in which nearby\ninput signal values (in space or time) are correlated, the cells\nresulting from this optimization process include center-surround\ncells and cells sensitive to temporal variations in input signal.\n\nINTRODUCTION\nI have previously proposed [Linsker, 1987, 1988] a principle of "maximum\n\ninformation preservation," also called the "infomax" principle, that may account for\ncertain aspects of the organization of a layered perceptual network. The principle\napplies to a layer L of cells (which may be the input layer or an intermediate layer\nof the network) that provides input to a next layer M. The mapping of the input\nsignal vector L onto an output signal vector M, f:L ~ M, is characterized by a\nconditional probability density function ("pdf") p(MI L). The set S of allowed\nmappings I is specified. The input pdf PL(L) is also given. (In the cases considered\nhere, there is no feedback from M to L.) The infomax principle states that a\nmapping I should be chosen for which the Shannon information rate [Shannon,\n1949]\nR(j) ==\n\nf dL PL(L) f dM p(MI L) 10g[P(MI L)/PM(M)]\n\n(1)\n\nis a maximum (over allIin the set S). Here PM(M) == fdLPL(L)P(MIL) is the pdf\nof the output signal vector M. R is identical to the average mutual information\nbetween Land M.\n\n\x0c187\n\nMaximum Infonnation Preservation to Linear Systems\n\nTo understand better how the info max principle may be applied to biological systems\nand complex synthetic networks, it is useful to solve the infomax optimization\nproblem explicitly for simpler systems whose properties are nonetheless biologically\nmotivated. This paper therefore deals with the practical computation of infomax\nsolutions for cases in which the mappings! are constrained to be linear.\n\nINFOMAX SOLUTIONS FOR A SET OF LINEAR FILTERS\nWe consider the case of linear model "neurons" with multivariate Gaussian input\nand additive Gaussian noise. There are N input (L) cells and N\' output (M) cells.\nThe input column vector L = (Lt,~, ... ,LNF is randomly selected from an\nN-dimensional Gaussian distribution having mean zero. That is,\n(2)\n\nwhere QL is the covariance matrix of the input activities, Q6\n(Superscript T denotes the matrix transpose.)\n\n= JdL PL(L)LjL\n\nj\n\n?\n\nTo specify the set S of allowed mappings !:L .... M, we define a processing model\nthat includes a description of (i) how noise enters during processing, (ii) the\nindependent variables over which we are to maximize R, and (iii) any constraints\non their values. Figure 1 shows several such models. We shall analyze the simplest,\nthen explain the motivation for the more complex models and analyze them in turn.\nModel A -- Additive noise of constant variance\n\nIn Model A of Fig. 1 the output signal value of the nth M cell is:\n(3)\n\nThe noise components "11 are independently and identically distributed (fli.i.d. ")\nrandom variables drawn from a Gaussian distribution having a mean of zero and\nvariance B.\nEach mapping !:L .... M is characterized by the values of the {Cnj } and the noise\nparameter B. The elements of the covariance matrix of the output activities are\n(using Eqn. 3)\n(4)\n\nwhere\n\n~nm\n\n= 1 if n =\n\nm and 0 otherwise.\n\nEvaluating Eqn. 1 for this processing model gives the information rate:\nR(j) = (1/2) In Det W(j)\n\n(5)\n\nwhere ~m = Q:!\'/ B. (R is the difference of two entropy terms. See [Shannon,\n1949], p.57, for the entropy of a Gaussian distribution.)\n\n\x0c188\n\nLinsker\n\nIf the components Cni of the C matrix are allowed to be arbitrarily large, then the\n\ninformation rate can be made arbitrarily large, and the effects of noise become\narbitrarily small. One way to limit C is to impose a "resource constraint" on each\nM cell. An example of such a constraint is ~jqj = 1 for all n. One can then attempt\ndirectly, using numerical methods, to maximize Eqn. 5 over all allowed C for given\nB. However, when some additional conditions (below) are satisfied, further\nanalytical progress can be made.\nSuppose the NL-cells are uniformly spaced along the line interval [0,1] with periodic\nboundary conditions, so that cell N is next to cell 1. [The analysis can be extended\nto a two- (or higher-) dimensional array in a straightforward manner.] Suppose also\nthat (for given N) the covariance Q6 of the input values at cells i and j is a function\nQL(Sj) only of the displacement s\'J from i to j. (We deal with the periodicity by\ndefining Sab = b - a - Ya~ and choosing the integer Yab such that\n-N/2 S Sab < N/2.) Then QL is a Toeplitz matrix, and its eigenvalues {Ak} are the\ncomponents of the discrete Fourier transform ("F.T.") of QL(S):\nAk = ~sQL(s) exp( -2~ks/N), (-N/2) S k\n\n< N/2.\n\n(6)\n\nWe now impose two more conditions: (1) N\' = N. This simplifies the resulting\nexpressions, but is otherwise inessential, as we shall discuss. (2) We constrain each\nM cell to have the same arrangement of C-values relative to the M cell\'s position.\nThat is, Cnj is to be a function C(Sni) only of the displacement Sni from n to i. This\nconstraint substantially reduces the computational demands. We would not expect\n\n(S,C)\n\nL?I\n\nL?I\n\nFigure 1.\n\n(D)\n\nFour processing models (A)-(D): Each diagram shows a single\nM cell (indexed by n) having output activity Mn. Inputs {LJ may\nbe common to many M cells. All noise contributions (dotted\nlines) are uncorrelated with one another and with {LJ. GC =\ngain control (see text).\n\n\x0cMaximum Information Preservation to Linear Systems\n\nit to hold in general in a biologically realistic model -- since different M cells should\nbe allowed to develop different arrangements of weights -- although even then it\ncould be used as an Ansatz to provide a lower bound on R. The section,\n"Temporally-correlated input patterns," deals with a situation in which it is\nbiologically plausible to impose this constraint.\nUnder these conditions, (Q:!\') is also a Toeplitz matrix. Its eigenvalues are the\ncomponents of the F.T. of QM(snm). For N\' = N these eigenvalues are (B + A~k) ,\nwhere Zk = ICkl2 and Ck == ~sC(s) exp( -2\'TT~ks/N) is the F.T. of C(s). [This\nexpression for the eigenvalues is obtained by rewriting Eqn. 4 as:\nQM(snm) = B8n_m.o + ~j.jC(snJQL(Sj)C(sm) ,and taking the F.T. of both sides.]\nTherefore\nR = (1/2)~k In[l\n\n+ AJcZk/ B].\n\n(7)\n\nWe want to maximize R subject to ~sC(S)2 = 1, which is equivalent to ~Zk = N .\nUsing the Lagrange multiplier method, we maximize A == R + 11-(~Zk - N) over all\nnonnegative {Zk}\' Solving for (JA/ (JZk = 0 and requiring Zk ~ 0 for all k gives the\nsolution:\nZk\n\n= max[( -1/211-)\n\n(8)\n\n- (B/Ak)\' 0],\n\nwhere (given B) 11- is chosen such that\n\n~Zk =\n\nN.\n\nNote that while the optimal {Zk} are uniquely determined, the phases of the {ck} are\ncompletely arbitrary [except that since the {C(s)} are real, we must have Ck * = c_ k\nfor all k]. The {C(s)} values are therefore not uniquely determined. Fig. 2a shows\ntwo of the solutions for .an example in which QL(S) = exp[ - (s/ So)2] with So = 6,\nN=N\'=64, and B.:..:.l. Both solutions have ZO.?1..... ?6=5.417, 5.409, 5.378,\n5.306, 5.134,4.689,3.376, and all other Zk == O. Setting all Ck phases to zero yields\nthe solid curve; a particular random choice of phases yields the dotted dHve. We\nshall later see that imposing locality conditions on the {C(s)} (e.g., penalizing\nnonzero C(s) for large Is I) can remove the phase ambiguity.\nOur solution (Eqn. 8) can be described in terms of a so-called "water-filling"\nanalogy: If one plots B /Ak versus k, then Zk is the depth of "water" at k when one\n"pours" into the "vessel" defined by the B / Ak curve a total quantity of "water" that\ncorresponds to ~Zk = N and brings the "water level" to ( -1/211-).\nLet us contrast this problem with two other problems to which the "water-filling"\nanalogy has been applied in the information-theory literature. In our notation, they\nare:\n1.\n\nGiven a transfer function {C(s)} and the noise variance B, how should a given\ntotal input signal power ~Ak be apportioned among the various wavenumbers\nk so as to maximize the information rate R [Gallager, 1968]? Our problem is\ncomplementary to this: we fix the input signal properties and seek an optimal\ntransfer function subject to constraints.\n\n189\n\n\x0c190\n\nLinsker\n\n2.\n\nRate-distortion (R-D) calculation [Berger, 1971]: Given a distortion measure\n(that defines a "distance" between the actual input signal and an estimate of it\nthat can be reconstructed from the channel\'s output), and the input power\nspectrum p. k}, what choice of {Zk} minimizes the average distortion for given\ninformation rate (or minimizes the required rate for given distortion)? In the\nR-D problem there is a process of reconstruction, and a given measure for\nassessing the "goodness" of reconstruction. In contrast, in our network there\nis no reconstruction of the input signal, and no criterion of the "goodness" of\nsuch a hypothetical reconstruction is provided.\n\nNote also that infomax optimization is not the same as computing which channel\n(that is, which mapping !:L .... M) selected from an allowed set has the maximum\ninformation-theoretic capacity. In that problem, one is free to encode the inputs\nbefore transmission so as to make optimal use of (Le., "achieve the capacity of") the\nchannel. In our case, there is no such pre-encoding; the input ensemble is prescribed\n(by the environment or by the output of an earlier processing stage) and we need to\nmaximize the channel rate for that ensemble.\nThe simplifying condition that N = N\' (above) is unnecessarily restrictive. Eqn. 7\ncan be easily generalized to the case in which N is a mUltiple of N\' and the N\' M cells\nare uniformly spaced on the unit interval. Moreover, in the limit that 1/N\' is much\nsmaller than the correlation length scale of QL, it can be shown that R is unchanged\nwhen we simultaneously increase N\' and B by the same factor. (For example, two\nadjacent M cells each having noise variance 2B jointly convey the same information\n\nc\n\nc\n\nc\n(0)\n\n(b)\nl\n\n.\'..- s\n-10\n\\\n\n,:\n\\,/\n\nFigure 2.\n\nExample infomax solutions C(s) for locally-correlated\ninputs: (a) Model A; region of nonnegligible C(s) extends over\nall s; phase ambiguity in Ck yields non unique C(s) solutions, two\nof which are shown. See text for details. (b) Models C (solid\ncurve) and D (dotted curve) with Gaussian g(S)-l favoring short\nconnections; shows center-surround receptive fields, more\npronounced in Model D. (c) "Temporal receptive field" using\nModel D for temporally correlated scalar input to a single M cell;\nC(s) is the weight applied to the input signal that occurred s time\nsteps ago. Spacing between ordinate marks is 0.1; ~ C(S)2 = 1 in\neach case.\n\n\x0cMaximum Information Preservation to Linear Systems\n\nabout L as one M cell having noise variance B.) For biological applications we are\nmainly interested in cases in which there are many L cells [so that C(s) can be\ntreated as a function of a continuous variable] and many M cells (so that the effect\nof the noise process is described by the single parameter B/ N).\nThe analysis so far shows two limitations of Model A. First, the constraint\n~iqi = 1 is quite arbitrary. (It certainly does not appear to be a biologically natural\nconstraint to impose!) Second, for biological applications we are interested in\npredicting the favored values of {C(s)}, but the phase ambiguity prevents this. In\nthe next section we show that a modified noise model leads naturally, without\narbitrary constraints on ~iqi\' to the same results derived above. We then turn to a\nmodel that favors local connections over long-range ones, and that resolves the\nphase ambiguity issue.\nModel B -- Independent noise on each input line\n\nIn Model B of Fig. 1 each input Li to the nth M cell is corrupted by Li.d. Gaussian\nnoise V l1i of mean zero and variance B. The output is\n(9)\n\nSince each V ni is independent of all other noise terms (and of the inputs {Li }), we find\n(10)\nWe may rewrite the last term as\nthen R = (1/2) In DetWwhere\n\nB~l1m (~iqy!2 (~jc;)l/2.\n\nThe information rate is\n(11)\n\nDefine c\' ni == Cl1i(~kqk)-1/2 ; then J?,.m = ~lIm + (~,.jc\'lIiQbC\' mj)/ B. Note that this is\nidentical (except for the replacement C ~ C\') to the expression following Eqn. (5),\nin which QM was given by Eqn. (4). By definition, the {C\' nil satisfy ~iC\';i = 1 for\nall n. Therefore, the problem of maximizing R for this model (with no constraints\non ~jq;) is identical to the problem we solved in the previous section.\nModel C -- Favoring of local connections\n\nSince the arborizations of biological cells tend to be spatially localized in many cases,\nwe are led to consider constraints or cost terms that favor localization. There are\nvarious ways to implement this. Here we present a way of modifying the noise\nprocess so that the infomax principle itself favors localized solutions, without\nrequiring additional terms unrelated to information transmission.\nModel C of Fig. 1 is the same as Model B, except that now the longer connections\nare "noisier" than the shorter ones. That is, the variance of VIIi is <V;i> = B~(sn;)\nwhere g(s) increases with 1s I. [Equivalently, one could attenuate the signal on the\n(i ~ n) line by g(sll;) 1/2 and have the same noise variance Bo on all lines.]\n\n191\n\n\x0c192\n\nLinsker\n\nThis change causes the last term of Eqn. 10 to be replaced by\nUnder the conditions discussed earlier (Toeplitz QL and QM, and N\n\nBo8I1m~g(SIl)qi .\n\n= N), we derive\n(12)\n\nRecall that the {ck } are related to {C(s)} by a Fourier transform (see just before Eqn.\n7). To cotppute which choice of IC(s)} maximizes R for a given problem, we used\na gradient ascent algorithm several times, each time using a different random set of\ninitial I C(s)} values. For the problems whose solutions are exhibited in Figs. 2b and\n2c, multiple starting points usually yielded the same solution to within the error\ntolerance specified for the algorithm [apart from an arbitrary factor by which all of\nthe C(s)\'s can be multiplied without affecting R], and that solution had the largest\nR of any obtained for the given problem. That is, a limitation sometimes associated\nwith gradient ascent algorithms -- namely, that they may yield multiple "solutions"\nthat are local, but far from global, maxima -- did not appear to be a difficulty in these\ncases.\nFig. 2b (solid curve) shows the infomax solution for an example having\nQL(S) = exp[ - (S/sO)2] and g(s) = exp[(s/s.)2] with So = 4, s. = 6, N = N = 32,\nand Bo = 0.1. There is a central excitatory peak flanked by shallow inhibitory\nsidelobes (and weaker additional oscillations). (As noted, the negative of this\nsolution, having a central inhibitory region and excitatory sidelobes, gives the same\nR.) As Bo is increased (a range from 0.001 to 20 was studied), the peak broadens,\nthe sidelobes become shallower (relative to the peak), and the receptive fields of\nnearby M cells increasingly overlap. This behavior is an example of the\n"redundancy-diversity" tradeoff discussed in [Linsker, 1988].\nModel D -- Bounded output variance\n\nOur previous models all produce output values Mn whose variance is not explicitly\nconstrained. More biologically realistic cells have limited output variance. For\nexample, a cell\'s firing rate must lie between zero and some maximum value. Thus,\nthe output of a model nonlinear cell is often taken to be a sigmoid function of\n(~iCII;L)?\n\nWithin the context of linear cell models, we can capture the effect of a bounded\noutput variance by using Model D of Fig. 1. We pass the intermediate output\n~iClIi(Li + VIIi) through a gain control QC that normalizes the output variance to\nunity, then we add a final (Li.d. Gaussian) noise term V\'II of variance R.. That is,\n(13)\n\nWithout the last term, this model wo~ld be identical to Model C, since mUltiplying\nboth the signal and the VIIi noise by the same factor GC would not affect R. The last\nterm in effect fixes the number of output values that can be discriminated (Le., not\nconfounded with each other by the noise process V\'II) to be of order Rl1!2.\nThe information rate for this model is derived to be (cf. Eqn. 12):\n\n\x0cMaximum Information Preservation to Linear Systems\n\n(14)\n\nwhere V( C) is the variance of the intermediate output before it is passed through\nGC:\n(15)\n\nFig. 2b (dotted curve) shows the infomax solution (numerically obtained as above)\nfor the same QL(S) and g(s) functions and parameter values as were used to generate\nthe solid curve (for Model C), but with the new parameter Bl = 0.4. The effect of\nthe new Bl noise process in this case is to deepen the inhibitory sidelobes (relative\nto the central peak). The more pronounced center-surround character of the\nresulting M cell dampens the response of the cell to differences (between different\ninput patterns) in the spatially uniform component of the input pattern. This\nresponse property allows the L .... M mapping to be info max-optimal when the\ndynamic range of the cells\' output response is constrained.? (A competing effect can\ncomplicate the analysis: If Bl is increased much further, for example to 50 in the\ncase discussed, the sidelobes move to larger s and become shallower. This behavior\nresembles that discussed at the end of the previous section for the case of increasing\nBo; in the present case it is the overall noise level that is being increased when Bl\nincreases and Bo is kept constant.)\nTemporaUy-correlated input patterns\nLet us see how infomax can be used to extract regularities in input time series, as\ncontrasted with the spatially-correlated input patterns discussed above. We consider\na single M cell that, at each discrete time denoted by n, can process inputs {LJ from\nearlier times i ~ n (via delay !ines, for example). We use the same Model D as\nbefore. There are two differences: First, we want g(s) = 00 for all s > 0 (input lines\nfrom future times are "infinitely noisy"). [A technical point: Our use of periodic\nboundary conditions, while computationally convenient, means that the input value\nthat will occur s time steps from now is the same value that occurred (N - s) steps\nago. We deal with this by choosing g(s) to equal 1 at s = 0, to increase as\ns .... -N/2 (going into the past), and to increase further as s decreases from +N/2\nto 1, corresponding to increasingly remote past times. The periodicity causes no\nunphysical effects, provided that we make g(s) increase rapidly enough (or make N\nlarge enough) so that C(s) is negligible for time intervals comparable to N.] Second,\nthe fact that C,,; is a function only of s\'" is now a consequence of the constancy of\nconnection weights C(s) of a single M cell with time, rather than merely a convenient\nAnsatz to facilitate the infomax computation for a set of many M cells (as it was in\nprevious sections).\nThe\n\ninfomax solution is shown in Fig. 2c for an example having\nQL(S) = exp[ - (S/So)2]; g(s) = exp[ -t(s}/s.J with t(s} = s for s ~ 0 and\nt(s} = s - N for s ~ 1; So = 4, Sl = 6, N = 32, Bo = 0.1, and Bl = 0.4. The result is\nthat the "temporal receptive field" of the M cell is excitatory for recent times, and\n\n193\n\n\x0c194\n\nLinsker\n\ninhibitory for somewhat more remote times (with additional weaker oscillations).\nThe cell\'s output can be viewed approximately as a linear combination of a smoothed\ninput and a smoothed first time derivative of the input, just as the output of the\ncenter-surround cell of Fig. 2b can be viewed as a linear combination of a smoothed\ninput and a smoothed second spatial derivative of the input. As in Fig. 2b, setting\nBI = 0 (not shown) lessens the relative inhibitory contribution.\n\nSUMMARY\nTo gain insight into the operation of the principle of maximum information\npreservation, we have applied the principle to the problem of the optimal design of\nan array of linear filters under various conditions. The filter models that have been\nused are motivated by certain features that appear to be characteristic of biological\nnetworks. These features include the favoring of short connections and the\nconstrained range of output signal values. When nearby input signals (in space or\ntime) are correlated, the infomax-optimal solutions for the cases studied include (1)\ncenter-surround cells and (2) cells sensitive to temporal variations in input. The\nresults of the mathematical analysis presented here apply also to arbitrary input\ncovariance functions of the form QL( I i - j I). We have also presented more general\nexpressions for the information rate, which can be used even when QL is not of this\nform. The cases discussed illustrate the operation of the infomax principle in some\nrelatively simple but instructive situations. The analysis and results suggest how the\nprinciple may be applied to more biologically realistic networks and input ensembles.\nReferences\nT. Berger, Rate Distortion Theory (Prentice-Hall, Englewood Cliffs, N.J., 1971),\nchap. 4.\nR. G. Gallager, Information Theory and Reliable Communication (John Wiley and\nSons, N.Y., 1968), p. 388.\nR. Linsker, in: Neural Information Processing Systems (Denver, Nov. 1987), ed.\nD. Z. Anderson (Amer. Inst. of Physics, N.Y.), pp. 485-494.\nR. Linsker, Computer 21 (3) 105-117 (March 1988).\nC. E. Shannon and W. Weaver, The Mathematical Theory of Communication (Univ.\nof Illinois Press, Urbana, 1949).\n\n\x0c'
p83166
sg104
S'653\n\nAN ADAPTIVE NETWORK THAT LEARNS\nSEQUENCES OF TRANSITIONS\nC. L. Winter\nScience Applications International Corporation\n5151 East Broadway, Suite 900\nTucson, Auizona 85711\n\nABSTRACT\nWe describe an adaptive network, TIN2, that learns the transition\nfunction of a sequential system from observations of its behavior. It\nintegrates two subnets, TIN-I (Winter, Ryan and Turner, 1987) and\nTIN-2. TIN-2 constructs state representations from examples of\nsystem behavior, and its dynamics are the main topics of the paper.\nTIN-I abstracts transition functions from noisy state representations\nand environmental data during training, while in operation it produces\nsequences of transitions in response to variations in input. Dynamics\nof both nets are based on the Adaptive Resonance Theory of Carpenter\nand Grossberg (1987). We give results from an experiment in which\nTIN2 learned the behavior of a system that recognizes strings with an\neven number of l\'s .\n\nINTRODUCTION\nSequential systems respond to variations in their input environment with sequences of\nactivities. They can be described in two ways. A black box description characterizes a\nsystem as an input-output function, m = B(u), mapping a string of input symbols, ll,\ninto a single output symbol, m. A sequential automaton description characterizes a\nsystem as a sextuple (U, M, S, SO, f, g) where U and M are alphabets of input and output\nsymbols, S is a set of states, sO is an initial state and f and g are transition and output\nfunctions respectively. The transition function specifies the current state, St, as a\nfunction of the last state and the current input, Ut,\n(1)\n\nIn this paper we do not discuss output functions because they are relatively simple. To\nfurther simplify discussion, we restrict ourselves to binary input alphabets, although the\nneural net we describe here can easily be extended to accomodate more complex alphabets.\n\n\x0c654\n\nWinter\n\nA common engineering problem is to identify and then simulate the functionality of a\nsystem from observations of its behavior. Simulation is straightforward when we can\nactually observe the internal states of a system, since then the function f can be specified\nby learning simple associations among internal states and external inputs. In robotic\nsystems, for instance, internal states can often be characterized by such parameters as\nstepper motor settings, strain gauge values, etc., and so are directly accessible. Artificial\nneural systems have peen found useful in such simulations because they can associate\nlarge, possibly noisy state space and input variables with state and output variables (Tolat\nand Widrow, 1988; Winter, Ryan and Turner, 1987).\nUnfortunately, in many interesting cases we must base simulations on a limited set of\nexamples of a system\'s black box behavior because its internal workings are\nunobservable. The black box description is not, by itself, much use as a simulation tool\nsince usually it cannot be specified without resorting to infinitely large input-output\ntables. As an alternative we can try to develop a sequential automaton description of the\nsystem by observing regularities in its black box behavior. Artificial neural systems can\ncontribute to the development of physical machines dedicated to system identification\nbecause i) frequently state representations must be derived from many noisy input\nvariables, ii) data must usually be processed in continuous time and iii) the explicit\ndynamics of artificial neural systems can be used as a framework for hardware\nimplementations.\nIn this paper we give a brief overview of a neural net, TIN2, which learns and processes\nstate transitions from observations of correct black box behavior when the set of\nobservations is large enough to characterize the black box as an automaton. The TIN2\nnet is based on two component networks. Each uses a modified adaptive resonance circuit\n(Carpenter and Grossberg, 1987) to associate heterogeneous input patterns. TIN-1\n(Winter, Ryan and Turner, 1987) learns and executes transitions when given state\nrepresentations. It has been used by itself to simulate systems for which explicit state\nrepresentations are available (Winter, 1988a). TIN-2 is a highly parallel, continuous time\nimplementation of an approach to state representation first outlined by Nerode (1958).\nNerode\'s approach to system simulation relies upon the fact that every string, l!. moves a\nmachine into a particular state, s(y). once it has been processed. The s(y) state can be\ncharacterized by putting the system initially into s(u) (by processing y) and then\npresenting a set of experimental strings. (~1 .... , ~n)\' for further processing.\nExperiments consist of observing the output mi = BUt?~i) where ? indicates\nconcatenation. A state can then be represented by the entries in a row of a state\ncharacterization table, C (Table 1). The rows of C are indexed by strings, lI, its columns\nare indexed by experiments. Wi. and its entries are mi. In Table 1 annotations in\nparentheses indicate nodes (artificial neurons) and subnetworks of TIN-2 equivalent to the\ncorresponding C table entry. During experimentation C expands as states are\n\n\x0cAdaptive Network That Learns Sequences of Transitions\n\ndistinguished from one another. The orchestration of experiments, their selection, the\nTABLE 1. C Table Constructed by TIN-2\n\nA.\n\nA.\n1\n0\n10\n\n1 (Node 7)\n\no(Node 6)\no(Node 1)\no(Node 3)\n\no(Assembly 1)\n\n1 (Assembly 2)\n\no(Node 2)\no(Node 9)\n\no(Node 5)\n\n1 (Node 6)\no(Node 2)\n\n1 (Node 1)\no(Node 4)\no(Node 0)\n\nrole of teachers and of the environment have been investigated by Arbib and Zeiger\n(1969), Arbib and Manes (1974), Gold (1972 and 1978) and Angluin (1987) to name a\nfew. TIN-2 provides an architecture in which C can be embedded and expanded as\nnecessary. Collections of nodes within TIN-21earn to associate triples (mi, 11, ~i) so that\ninputting II later results in the output of the representation (m 1, ... , mn)n of the state\nassociated with 11.\n\nTIN-2\nTIN-2 is composed of separate assemblies of nodes whose dynamics are such that each\nassembly comes to correspond to a column in the state characterization table C. Thus we\ncall them column-assemblies. Competition among column-assemblies guarantees that\nnodes of only one assembly, say the ith, learn to respond to experimental pattern ~i\'\nHence column-assemblies can be labelled ~1\' ~2 and so on, but since labelings are not\nassigned ahead of time, arbitrarily large sets of experiments can be learned.\nThe theory of adaptive resonance is implemented in TIN-2 column-assemblies through\npartitioned adaptive resonance circuits (cf. Ryan, Winter and Turner, 1987). Adaptive\nresonance circuits (Grossberg and Carpenter, 1987; Ryan and Winter, 1987) are composed\nof four collections of nodes: Input, Comparison (FI), Recognition (F2) and Reset. In\nTIN-2 Input, Comparison and Reset are split into disjoint m,.u and ~ partitions. The net\nruns in either training or operational mode, and can move from one to the other as\nrequired. The training dynamics of the circuit are such that an F2 node is stimulated by\nthe overall triple (m. n,~, but can be inhibited by a mismatch with any component.\nDuring operation input of.u recalls the state representation s(u) = (m 1.... , mn)n\'\nNode activity for the kth FI partition, FI ,k\' k = m, u,\n\nW,\n\nis governed by\n\n(2)\nHere t < 1 scales time, Ii,k is the value of the ith input node of partition k, xi,k is\n\n655\n\n\x0c656\n\nWinter\n\nactivity in the corresponding node of FI and f is a sigmoid function with range [0. 1].\nThe elements of I are either 1. -lor O. The dynamics of the TIN-2 circuit are such that 0\nindicates the absence of a symbol, while 1 and -1 represent elements of a binary alphabet.\nThe adaptive feedback filter. T. is a matrix (Tji) whose elements. after training. are also\n1.-1 orO.\nActivity, yj. in the jth F2 node is driven by\n\n+ L meFl,m Bmj h(xm)] - 4[ ~*j f(YTl) + Ruj + Rw] .\n\n(3)\n\nThe feedforward fllter B is composed of matrices (Buj)\' (Bmj) and (Bw) whose elements\nare normalized to the size of the patterns memorized. Note that (Bw) is the same for\nevery node in a given column-assembly. i.e. the rows of (Bw) are all the same. Hence all\nnodes within a column-assembly learn to respond to the same experimental pattern. w.\nand it is in this sense that an assembly evolves to become equivalent to a column in table\nC. During training the sum ~*j f(YTl) in (3) runs through the recognition nodes of all\nTIN-2 column-assemblies. Thus. during training only one F2 node. say the Jth. can be\nactive at a time across all assemblies. In operation. on the other hand. we remove\ninhibition due to nodes in other assemblies so that at any time one node in each\ncolumn-assembly can be active. and an entire state representation can be recalled.\nThe Reset terms Ru,j and Rw in (3) actively inhibit nodes of F2 when mismatches\nbetween memory and input occur. Ruj is specific to the jth F2 node.\ndRujldt = -Ruj + f(Yj) f(v 1I1u II - II ?I.u II) .\n\n(4)\n\nRw affects all F2 nodes in a column-assembly and is driven by\ndRw/dt = -Rw + [LjeF2 f(Yj)] f(v IIlw II-II fI.w II).\n\n(5)\n\nv < 1 is a vigilance parameter (Carpenter and Grossberg. 1987): for either (4) or (5) R > 0\nat equilibrium just when the intersection between memory and input. PI T n I. is\n\n=\n\nrelatively small, i.e. R > 0 when v 11111 > II PI II. When the system is in operation. we\nfix Rw = 0 and input the pattern Iw = O. To recall the row in table C indexed by 11, we\ninput 11 to all column-assemblies. and at equilibrium xi.m = Lje F2 Tjif(Yj). Thus xi,m\nrepresents the memory of the element in C corresponding to 11 and the column in C with\nthe same label as the column-assembly. Winter (1988b) discusses recall dynamics in\nmore detail.\n\n\x0c657\n\nAdaptive Network That Learns Sequences of Transitions\n\nAt equilibrium in either training or operational mode only the winning F2 node has YJ *-\n\nO. so LjTjif(Yj) = TJi in (2). Hence xi.k = 0 if TJi = -li.k. i.e. if memory and input\nmismatch; IXi.kl = 2 if TJi = Ii,k. i.e. when memory and input match; and IXi.kl = 1 if\nTJ.i =O. Ii.k *- 0 or ifTJ.i *- O. Ii.k = O. The F1 output function h in (3) is defined so\nthat hex) = 1 if x> 1. hex) = -1 if x < -1 and hex) = 0 if -1 S x S 1. The output pattern\n~1 = (h(x1) ..... h(xnl? reflects IJ (\'\\ Ik. as h(xi) *- 0 only if TJi = Ii.k.\nThe adaptive filters (Buj) and (Bmj) store normalized versions of those patterns on FI.u\nand F1.m which have stimulated the jth F2 node. The evolution of Bij for u E FI.u or\nF1 m is driven by\n\n?\n\n(6)\n\nOn the other hand (Bw) stores a normalized version of the experiment w which labels the\nentire column-assembly. Thus all nodes in a column-assembly share a common memory\nof~.\n\n(7)\n\nwhere w E F1 w .\n\n?\n\nThe feedback mters (Tuj). (Tmj) and (Tw) store exact memories of patterns on partitions\nofFI:\n(8)\n\nfor i\n\nE\n\nFI.u ? F1.m ? and\n\n(9)\n\nfor i E FI.w\' In operation long-term memory modification is suspended.\n\nEXPERIMENT\nHere we report partial results from an experiment in which TlN-2 learns a state\ncharacterization table for an automaton that recognizes strings containing even numbers of\n\n.\n\n.\n\n\x0c658\n\nWinter\n\nboth I\'s and O\'s. More details can be found in Winter (1988b). For notational\nconvenience in this section we will discuss patterns as if they were composed of l\'s and\nO\'s, but be aware that inside TIN-2 every 0 symbol is really a -1. Data is provided in the\nform of triples eM, ll, YD by a teacher; the data set for this example is given in Table 1.\nData were presented to the net in the order shown. The net consisted of three\ncolumn-assemblies. Each F2 collection contained ten nodes. Although the strings that\ncan be processed by an automaton of this type are in principle arbitrarily long, in practice\nsome limitation on the length of training strings is necessary if for no other reason than\nthat the memory capacity of a computer is finite. For this simple example Input and F I\npartitions contain eight nodes, but in order to have a special symbol to represent A..\nstrings are limited to at most six elements. With this restriction the A. symbol can be\ndistinguished from actual input strings through vigilance criteria. Other solutions to the\nproblem of representing A. are being investigated, but for now the special eight bit\nsymbol, 00000011, is used to represent A. in the strings A.-yt.\nThe net was trained using fast-learning (Carpenter and Grossberg, 1987): a triple in Table\n1 was presented to the net. and all nodes were allowed to come to their equilibrium values\nwhere they were held for about three long-term time units before the next triple was\npresented. Consider the processing that follows presentation of (0, 1,0) the first datum\nin Table 1. The net can obtain equivalents to two C table entries from (0, 1,0): the entry\nin row 1l = 10, column Yi. = A. and the entry in row II =1, column w = O. The string 10\nand the membership value 0 were displayed on the A. assembly\'s input slabs, and in this\ncase the 3rd F2 node learned the association among the two patterns. When the pattern\n(0, 1, 0) was input to other column-assemblies, one F2 node (in this case the 9 th in\ncolumn-assembly 1) learned to associate elements of the triple. Of course a side effect of\nthis was that column-assembly 1 was labelled by Yi.. = 0 thereafter. When (1. 1, 1) was\ninput next, node 9 in column-assembly 1 tried to respond to the new triple, all nodes in\ncolumn-assembly 1 were then inhibited by a mismatch on Yi.., and finally node 1 on\ncolumn-assembly 2 learned (1, 1, 1). From that point on column-assembly 2 was\nlabelled by 1.\n\nLEARNING TRANSITIONS\nThe TIN-I net (Winter. Ryan and Turner, 1987) is composed of i) a partitioned adaptive\nresonance circuit with dynamics similar to (2) - (9) for learning state transitions and ii) a\nControl Circuit which forces transitions once they have been learned. Transitions are\nunique in the sense that a previous state and current input completely determine the\ncurrent state. The partitioned adaptive resonance circuit has three input fields: one for the\nprevious state, one for the current input and one for the next state. TIN-l\'s F2 nodes\nlearn transitions by associating patterns in the three input fields. Once trained. TIN-l\nprocesses strings sequentially. bit-by-bit.\n\n\x0cAdaptive Network That Learns Sequences of Transitions\n\n1L~r--T-\'N---2-~:t ~\n\nTIN-l\n\nTIN-2\n\nI~ u.eu\n\nFigure 1. Training TIN2.\nThe architecture of TIN2, the net that integrates TIN-2 and TIN-I. is shown in Figure 1.\nThe system resorts to the TIN-2 nets only to learn transitions. If TIN-2 has learned a C\ntable in which examples of all transitions appear, TIN-I can easily learn the automaton\'s\nstate transitions. A C table contains an example of a transition from state si to state Sj\nforced by current input u, if it contains i) a row labelled by a string lli which leaves the\nautomaton in si after processing and ii) a row labelled by the string lltu which leaves the\nautomaton in Sj. To teach TIN-l the transition we simply present lli to the lower TIN-2\nin Figure I, llieu to the upper TIN-2 net and u to TIN-I.\n\nCONCLUSIONS\nWe have described a network, TIN-2, which learns the equivalent of state characterization\ntables (Gold, 1972). The principle reasons for developing a neural net implementation are\ni) neural nets are intrinsically massively parallel and so provide a nice model for systems\nthat must process large data sets, ii) although in the interests of brevity we have not\nstressed the point, neural nets are robust against noisy data, iii) neural nets like the\npartitioned adaptive resonance circuit have continuous time activity dynamics and so can\nbe synchronized with other elements of a larger real-time system through simple scaling\nparameters, and iv) the continuous time dynamics and precise architectural specifications\nof neural nets provide a blueprint for hardware implementations.\nWe have also sketched a neural net, TIN2, that learns state transitions by integrating\nTIN-2 nets with the TIN-I net (Winter, Ryan and Turner, 1987). When a complete state\ncharacterization table is available from TIN-2, TIN2 can be taught transitions from\nexamples of system behavior. However, the ultimate goal of a net like this lies in\ndeveloping a system that "or,rates acceptably" with a partial state characterization table.\nTo operate acceptably TIN must perform transitions correctly when it can, recognize\nwhen it cannot, signal for new data when it is required and expand the state charcterization\ntaole when it must. Happily TIN2 already provides the first two capabilities, and\ncombinations of TIN2 with rule-based controllers and with auxiliary control networks are\ncurrently being explored as approachws to satisfy the latter (Winter, 1988b).\nNets like TIN2 may eventually prove useful as control elements in physical machines\nbecause sequential automata can respond to unpredictable environments with a wide range\nof behavior. Even very simple automata can repeat activities and make decisions based\nupon environmental variations. Currently, most physical machines that make decisions\nare dedicated to a single task; applying one to a new task requires re-programming by a\n\n659\n\n\x0c660\n\nWinter\n\nskilled technician. A programmer must, furthermore, determine a priori precisely which\nmachine state - environment associations are significant enough to warrant insertion in\nthe control structure of a given machine. TIN2, on the other hand, is trained, not\nprogrammed, and can abstract significant associations from noisy input. It is a "blank\nslate" that learns the structure of a particular sequential machine from examples.\n\nReferences\nD. Angluin, "Learning Regular Sets from Queries and Counterexamples", Information\nand Computation, 75 (2), 1987.\nM. A. Arbib and E. G. Manes, "Machines in a Category: an Expository Introduction",\nSIAM Review, 16 (2), 1974.\nM. A. Arbib and H. P. Zeiger, "On the Relevance of Abstract Algebra to Control\nTheory", Automatica, 5, 1969.\nG. Carpenter and S. Grossberg, "A Massively Parallel Architecture for a Self-Organizing\nNeural Pattern Recognition Machine", Comput. Vision Graphics Image Process. 37 (54),\n1987.\nE. M. Gold, "System Identification Via State Characterization", Automatica, 8, 1972.\nE. M. Gold, "Complexity of Automaton Identification from Given Data", Info. and\nControl, 37, 1978.\nA. Neroda, "Linear Automaton Transformations", Proc. Am. Math. Soc., 9, 1958.\nT. W. Ryan and C. L. Winter, "Variations on Adaptive Resonance", in Proc. 1st IntI.\nConf. on Neural Networks, IEEE, 1987.\nT. W. Ryan, C. L. Winter and C. J. Turner, "Dynamic Control of an Artificial Neural\nSystem: the Property Inheritance Network", Appl. Optics, 261 (23) 1987.\nV. V. Tolat and B. Widrow, "An Adaptive Neural Net Controller with Visual Inputs",\nNeural Networks, I, S upp I, 1988.\nC. L. Winter, T. W. Ryan and C. J. Turner, "TIN: A Trainable Inference Network", in\nProc. 1st Inti. Conf. on Neural Networks, 1987.\nC. L. Winter, "An Adaptive Network that Flees Pursuit", Neural Networks, I, Supp.l,\n1988a.\nC. L. Winter, "TIN2: An Adaptive Controller", SAIC Tech. Rpt., SAIC, 5151 E.\nBroadway, Tucson, AZ, 85711, 1988b.\n\n\x0cPart V\nImplementation\n\n\x0c'
p83167
sg106
S'394\n\nSTORING COVARIANCE BY THE ASSOCIATIVE\nLONG?TERM POTENTIATION AND DEPRESSION\nOF SYNAPTIC STRENGTHS IN THE HIPPOCAMPUS\nPatric K. Stanton? and Terrence J. Sejnowski t\nDepartment of Biophysics\nJohns Hopkins University\nBaltimore, MD 21218\nABSTRACT\n\nIn modeling studies or memory based on neural networks, both the selective\nenhancement and depression or synaptic strengths are required ror effident storage\nor inrormation (Sejnowski, 1977a,b; Kohonen, 1984; Bienenstock et aI, 1982;\nSejnowski and Tesauro, 1989). We have tested this assumption in the hippocampus,\na cortical structure or the brain that is involved in long-term memory. A brier,\nhigh-frequency activation or excitatory synapses in the hippocampus produces an\nincrease in synaptic strength known as long-term potentiation, or LTP (BUss and\nLomo, 1973), that can last ror many days. LTP is known to be Hebbian since it\nrequires the simultaneous release or neurotransmitter from presynaptic terminals\ncoupled with postsynaptic depolarization (Kelso et al, 1986; Malinow and Miller,\n1986; Gustatrson et al, 1987). However, a mechanism ror the persistent reduction or\nsynaptic strength that could balance LTP has not yet been demonstrated. We studied the associative interactions between separate inputs onto the same dendritic\ntrees or hippocampal pyramidal cells or field CAl, and round that a low-frequency\ninput which, by itselr, does not persistently change synaptic strength, can either\nincrease (associative LTP) or decrease in strength (associative long-term depression\nor LTD) depending upon whether it is positively or negatively correlated in time\nwith a second, high-frequency bursting input. LTP or synaptic strength is Hebbian,\nand LTD is anti-Hebbian since it is elicited by pairing presynaptic firing with postsynaptic hyperpolarization sufficient to block postsynaptic activity. Thus, associative LTP and associative LTO are capable or storing inrormation contained in the\ncovariance between separate, converging hippocampal inputs?\n\n?Present address: Dep~ents of NeW\'Oscience and Neurology, Albert Einstein College\nof Medicine, 1410 Pelham Parkway South, Bronx, NY 10461 USA.\ntPresent address: Computational Neurobiology Laboratory, The Salk Institute, P.O. Box\n85800, San Diego, CA 92138 USA.\n\n\x0cStoring Covariance by Synaptic Strengths in the Hippocampus\n\nINTRODUCTION\nAssociative LTP can be produced in some hippocampal neuroos when lowfrequency. (Weak) and high-frequency (Strong) inputs to the same cells are simultaneously activated (Levy and Steward, 1979; Levy and Steward, 1983; Barrionuevo and\nBrown, 1983). When stimulated alone, a weak input does not have a long-lasting effect\non synaptic strength; however, when paired with stimulation of a separate strong input\nsufficient to produce homo synaptic LTP of that pathway, the weak pathway is associatively potentiated. Neural network modeling studies have predicted that, in addition to\nthis Hebbian form of plasticity, synaptic strength should be weakened when weak and\nstrong inputs are anti-correlated (Sejnowski, 1977a,b; Kohonen, 1984; Bienenstock et al,\n1982; Sejnowski and Tesauro, 1989). Evidence for heterosynaptic depression in the hippocampus has been found for inputs that are inactive (Levy and Steward, 1979; Lynch et\nal, 1977) or weakly active (Levy and Steward, 1983) during the stimulation of a strong\ninput, but this depression did not depend on any pattern of weak input activity and was\nnot typically as long-lasting as LTP.\nTherefore, we searched for conditions under which stimulation of a hippocampal\npathway, rather than its inactivity, could produce either long-term depression or potentiation of synaptic strengths, depending on the pattern of stimulation. The stimulus paradigm that we used, illustrated in Fig. I, is based on the finding that bursts of stimuli at 5\nHz are optimal in eliciting LTP in the hippocampus (Larson and Lynch, 1986). A highfrequency burst (S\'IRONG) stimulus was applied to Schaffer collateral axons and a lowfrequency (WEAK) stimulus given to a separate subicular input coming from the opposite side of the recording site, but terminating on dendrites of the same population of CAl\npyramidal neurons. Due to the rhythmic nature of the strong input bursts, each weak\ninput shock could be either superimposed on the middle of each burst of the strong input\n(IN PHASE), or placed symmetrically between bursts (OUT OF PHASE).\n\nRESULTS\nExtracellular evoked field potentials were recorded from the apical dendritic and\nsomatic layers of CAl pyramidal cells. The weak stimulus train was first applied alone\nand did not itself induce long-lasting changes. The strong site was then stimulated alone,\nwhich elicited homosynaptic LTP of the strong pathway but did not significantly alter\namplitude of responses to the weak input. When weak and strong inputs were activated\nIN PHASE, there was an associative LTP of the weak input synapses, as shown in Fig.\n2a. Both the synaptic excitatory post-synaptic potential (e.p.s.p.) (Ae.p.s.p. = +49.8 ?\n7.8%, n=20) and population action potential (&Pike = +65.4 ? 16.0%, n=14) were\nsignificantly enhanced for at least 60 min up to 180 min following stimulation.\nIn contrast, when weak and strong inputs were applied OUT OF PHASE, they elicited an associative long-term depression (LTO) of the weak input synapses, as shown in\nFig. 2b. There was a marked reduction in the population spike (-46.5 ? 11.4%, n=10)\nwith smaller decreases in the e.p.s.p. (-13.8 ? 3.5%, n=13). Note that the stimulus patterns applied to each input were identical in these two experiments, and only the relative\n\n395\n\n\x0c396\n\nStanton and Sejnowski\n\nphase of the weak and strong stimuli was altered. With these stimulus patterns. synaptic\nstrength could be repeatedly enhanced and depressed in a single slice. as illustrated in Fig\n2c. As a control experiment to determine whether information concerning covariance\nbetween the inputs was actually a determinant of plasticity. we combined the in phase\nand out of phase conditions, giving both the weak input shocks superimposed on the\nbursts plus those between the bursts. for a net frequency of 10 Hz. This pattern. which\nresulted in zero covariance between weak and strong inputs. produced no net change in\nweak input synaptic strength measmed by extracellular evoked potentials. Thus. the assoa\n\nb\nA.SSOCIA.TIVE STIMULUS PA.RA.DIGMS\nPOSJTIVE.LY CORKELA TED ? "IN PHASE"\n\n~K~~ _I~__~I____~I____~I_\nSI1IONG,NJO\\IT\n\n. u.Jj1l 11l. -1---1&1111.....\n11 ---1&1\n111.....\n11 ---,I~IIII\n\nNEGATIVELY CORRELATED? \'our OF PHASE"\nW[AKIN\'lTf\n\nSTIONG \'N\'\'\'\'\n\n~I\n\n11111\n\n--,-;\n\n11111\n\n11111\n\nFigure 1. Hippocampal slice preparation and stimulus paradigms. a: The in vitro hippocampal slice showing recording sites in CAl pyramidal cell somatic (stratum pyramidale) and dendritic (stratum radiatum) layers. and stimulus sites activating Schaffer collateral (STRONG) and commissural (WEAK) afferents. Hippocampal slices (400 Jlm\nthick) were incubated in an interface slice chamber at 34-35 0 C. Extracellular (1-5 M!l\nresistance, 2M NaCI filled) and intracellular (70-120 M 2M K-acetate filled) recording electrodes. and bipolar glass-insulated platinum wire stimulating electrodes (50 Jlm\ntip diameter). were prepared by standard methods (Mody et al, 1988). b: Stimulus paradigms used. Strong input stimuli (STRONG INPUT) were four trains of 100 Hz bursts.\nEach burst had 5 stimuli and the interburst interval was 200 msec. Each train lasted 2\nseconds for a total of 50 stimuli. Weak input stimuli (WEAK INPUT) were four trains of\nshocks at 5 Hz frequency. each train lasting for 2 seconds. When these inputs were IN\nPHASE. the weak single shocks were superimposed on the middle of each burst of the\nstrong input. When the weak input was OUT OF PHASE. the single shocks were placed\nsymmetrically between the bursts.\n\nn.\n\n\x0cStoring Covariance by Synaptic Strengths in the Hippocampus\n\nciative LTP and LTD mechanisms appear to be balanced in a manner ideal for the\nstorage of temporal covariance relations.\nThe simultaneous depolarization of the postsynaptic membrane and activation of\nglutamate receptors of the N-methyl-D-aspartate (NMDA) subtype appears to be necessary for LTP induction (Collingridge et ai, 1983; Harris et al, 1984; Wigstrom and Gustaffson, 1984). The SJ?read of current from strong to weak synapses in the dendritic tree,\nd\n\nASSOCIATIVE\n\nLON(;.TE~\n\nI\'OTENTIATION\n\nLONG-TE~\n\nDE,/tESSION\n\n-\n\n!!Ll!!!!.\n\nb\n\nASSOCIATIVE\n\nI\n\n11111\n\n?\n\n11111.\nI\n\nc\n\ne...\n\nI\n\nI\n\nI\n\nI\n\nFigure 2. mustration of associative long-term potentiation (LTP) and associative longterm depression (LTD) using extracellular recordings. a: Associative LTP of evoked\nexcitatory postsynaptic potentials (e.p.s.p.\'s) and population action potential responses in\nthe weak inpuL Test responses are shown before (Pre) and 30 min after (post) application of weak stimuli in phase with the coactive strong input. b: Associative LTD of\nevoked e.p.s.p.\'s and population spike responses in the weak input. Test responses are\nshown before (Pre) and 30 min after (post) application of weak stimuli out of phase with\nthe coactive strong input. c: Time course of the changes in population spike amplitude\nobserved at each input for a typical experiment. Test responses from the strong input (S,\nopen circles), show that the high-frequency bursts (5 pulses/l00 Hz, 200 msec interburst\ninterval as in Fig. 1) elicited synapse-specific LTP independent of other input activity.\nTest responses from the weak input (W. filled circles) show that stimulation of the weak\npathway out of phase with the strong one produced associative LTD (Assoc LTD) of this\ninput. Associative LTP (Assoc LTP) of the same pathway was then elicited following in\nphase stimulation. Amplitude and duration of associative LTD or LTP could be increased\nby stimulating input pathways with more trains of shocks.\n\n397\n\n\x0c398\n\nStanton and Sejnowski\n\ncoupled with release of glutamate from the weak inputs, could account for the ability of\nthe strong pathway to associatively potentiate a weak one (Kelso et al, 1986; Malinow\nand Miller, 1986; Gustaffson et al, 1987). Consistent with this hypothesis, we find that\nthe NMDA receptor antagonist 2-amino-S-phosphonovaleric acid (APS, 10 J.1M) blocks\ninduction of associative LTP in CAl pyramidal neurons (data not shown, n=S). In contrast, the application of APS to the bathing solution at this same concentration had no\nsignificant effect on associative LTD (data not shown, n=6). Thus, the induction of LTD\nseems to involve cellular mechanisms different from associative LTP.\nThe conditions necessary for LTD induction were explored in another series of\nexperiments using intracellular recordings from CAl pyramidal neurons made using\nstandard techniques (Mody et al, 1988). Induction of associative LTP (Fig 3; WEAK\nS+W IN PHASE) produced an increase in amplitude of the single cell evoked e.p.s.p. and\na lowered action potential threshold in the weak pathway, as reported previously (Barrionuevo and Brown, 1983). Conversely, the induction of associative LTD (Fig. 3;\nWEAK S+W OUT OF PHASE) was accompanied by a long-lasting reduction of e.p.s.p.\namplitude and reduced ability to elicit action potential firing. As in control extracellular\nexperiments, the weak input alone produced no long-lasting alterations in intracellular\ne.p.s.p.\'s or firing properties, while the strong input alone yielded specific increases of\nthe strong pathway e.p.s.p. without altering e.p.s.p. \'s elicited by weak input stimulation.\n\nPRE\n\n30 min POST\nS+W OUT OF PHASE\n\n30 min POST\nS+W IN PHASE\n\nFigure 3. Demonstration of associative LTP and LTD using intracellular recordings from\na CAl pyramidal neuron. Intracellular e.p.s.p.\'s prior to repetitive stimulation (pre), 30\nmin after out of phase stimulation (S+W OUT OF PHASE), and 30 min after subsequent in phase stimuli (S+W IN PHASE). The strong input (Schaffer collateral side,\nlower traces) exhibited LTP of the evoked e.p.s.p. independent of weak input activity.\nOut of phase stimulation of the weak (Subicular side, upper traces) pathway produced a\nmarked, persistent reduction in e.p.s.p. amplitude. In the same cell, subsequent in phase\nstimuli resulted in associative LTP of the weak input that reversed the LTD and enhanced\namplitude of the e.p.s.p. past the original baseline. (RMP = -62 mY, RN = 30 MO)\n\n\x0cStoring Covariance by Synaptic Strengths in the Hippocampus\n\nA weak stimulus that is out of phase with a strong one anives when the postsynaptic neuron is hyperpolarized as a consequence of inhibitory postsynaptic potentials and\nafterhyperpolarization from mechanisms intrinsic to pyramidal neurons. This suggests\nthat postsynaptic hyperpolarization coupled with presynaptic activation may trigger L\'ID.\nTo test this hypothesis, we injected current with intracellular microelectrodes to hyperpolarize or depolarize the cell while stimulating a synaptic input. Pairing the injection of\ndepolarizing current with the weak input led to LTP of those synapses (Fig. 4a; STIM;\n\na\n\nPRE\n\n? ?IDPOST\nS\'I1M ? DEPOL\n\n~l"V\nlS.,.c\n\nr\n," i\n\nCOI\'ITROL\n\n-Jj\n\nb\n\nI\n\n--" \\\n\n"----\n\n(W.c:ULVllj\n\nPRE\n\nlOlIIin POST\nSTlM ? HYPERPOL\n\nFigure 4. Pairing of postsynaptic hyperpolarization with stimulation of synapses on CAl\nhippocampal pyramidal neurons produces L\'ID specific to the activated pathway, while\npairing of postsynaptic depolarization with synaptic stimulation produces synapsespecific LTP. a: Intracellular evoked e.p.s.p.\'s are shown at stimulated (STIM) and\nunstimulated (CONTROL) pathway synapses before (Pre) and 30 min after (post) pairing a 20 mY depolarization (constant current +2.0 nA) with 5 Hz synaptic stimulation.\nThe stimulated pathway exhibited associative LTP of the e.p.s.p., while the control,\nunstimulated input showed no change in synaptic strength. (RMP = -65 mY; RN = 35\nMfl) b: Intracellular e.p.s.p. \'s are shown evoked at stimulated and control pathway\nsynapses before (Pre) and 30 min after (post) pairing a 20 mV hyperpolarization (constant current -1.0 nA) with 5 Hz synaptic stimulation. The input (STIM) activated during\nthe hyperpolarization showed associative LTD of synaptic evoked e.p.s.p.\'s, while\nsynaptic strength of the silent input (CONTROL) was unaltered. (RMP =-62 mV; RN =\n38M!l)\n\n399\n\n\x0c400\n\nStanton and Sejnowski\n\n+64.0 -9.7%, n=4), while a control input inactive during the stimulation did not change\n(CONTROL), as reported previously (Kelso et al, 1986; Malinow and Miller, 1986; Gustaffson et al, 1987). Conversely, prolonged hyperpolarizing current injection paired with\nthe same low-frequency stimuli led to induction of LTD in the stimulated pathway (Fig.\n4b; STIM; -40.3 ? 6.3%, n=6). but not in the unstimulated pathway (CONTROL). The\napplication of either depolarizing current, hyperpolarizing current, or the weak 5 Hz\nsynaptic stimulation alone did not induce long-term alterations in synaptic strengths.\nThus. hyperpolarization and simultaneous presynaptic activity supply sufficient conditions for the induction of LTD in CAl pyramidal neurons.\n\nCONCLUSIONS\nThese experiments identify a novel fono of anti-Hebbian synaptic plasticity in the\nhippocampus and confirm predictions made from modeling studies of information storage\nin neural networks. Unlike previous reports of synaptic depression in the hippocampus,\nthe plasticity is associative, long-lasting, and is produced when presynaptic activity\noccurs while the postsynaptic membrane is hyperpolarized. In combination with Hebbian\nmechanisms also present at hippocampal synapses. associative LTP and associative LTD\nmay allow neurons in the hippocampus to compute and store covariance between inputs\n(Sejnowski, 1977a,b; Stanton and Sejnowski. 1989). These finding make temporal as\nwell as spatial context an important feature of memory mechanisms in the hippocampus.\nElsewhere in the brain, the receptive field properties of cells in cat visual cortex\ncan be altered by visual experience paired with iontophoretic excitation or depression of\ncellular activity (Fregnac et al, 1988; Greuel et al, 1988). In particular, the chronic hyperpolarization of neurons in visual cortex coupled with presynaptic transmitter release leads\nto a long-teno depression of the active. but not inactive, inputs from the lateral geniculate\nnucleus (Reiter and Stryker, 1988). Thus. both Hebbian and anti-Hebbian mechanisms\nfound in the hippocampus seem to also be present in other brain areas, and covariance of\nfiring patterns between converging inputs a likely key to understanding higher cognitive\nfunction.\nThis research was supported by grants from the National Science Foundation and\nthe Office of Naval research to TJS. We thank Drs. Charles Stevens and Richard Morris\nfor discussions about related experiments.\n\nRererences\nBienenstock, E., Cooper. LN. and Munro. P. Theory for the development of neuron\nselectivity: orientation specificity and binocular interaction in visual cortex. J. Neurosci. 2. 32-48 (1982).\nBarrionuevo, G. and Brown, T.H. Associative long-teno potentiation in hippocampal\nslices. Proc. Nat. Acad. Sci. (USA) 80, 7347-7351 (1983).\nBliss. T.V.P. and Lomo, T. Long-lasting potentiation of synaptic ttansmission in the dentate area of the anaesthetized rabbit following stimulation of the perforant path. J.\nPhysiol. (Lond.) 232. 331-356 (1973).\n\n\x0cStoring Covariance by Synaptic Strengths in the Hippocampus\n\nCollingridge, GL., Kehl, SJ. and McLennan, H. Excitatory amino acids in synaptic\ntransmission in the Schaffer collateral-commissural pathway of the rat hippocampus. J.\nPhysiol. (Lond.) 334, 33-46 (1983).\nFregnac, Y., Shulz, D., Thorpe, S. and Bienenstock, E. A cellular analogue of visual cortical plasticity. Nature (Lond.) 333, 367-370 (1988).\nGreuel. J.M.. Luhmann. H.J. and Singer. W. Pharmacological induction of usedependent receptive field modifications in visual cortex. Science 242,74-77 (1988).\nGustafsson, B., Wigstrom, H., Abraham, W.C. and Huang. Y.Y. Long-term potentiation\nin the hippocampus using depolarizing current pulses as the conditioning stimulus to\nsingle volley synaptic potentials. J. Neurosci. 7, 774-780 (1987).\nHarris. E.W., Ganong, A.H. and Cotman, C.W. Long-term potentiation in the hippocampus involves activation of N-metbyl-D-aspartate receptors. Brain Res. 323, 132137 (1984).\nKelso, S.R.. Ganong, A.H. and Brown, T.H. Hebbian synapses in hippocampus. Proc.\nNatl. Acad. Sci. USA 83, 5326-5330 (1986).\nKohonen. T. Self-Organization and Associative Memory. (Springer-Verlag. Heidelberg,\n1984).\nLarson. J. and Lynch. G. Synaptic potentiation in hippocampus by patterned stimulation\ninvolves two events. Science 232, 985-988 (1986).\nLevy. W.B. and Steward, O. Synapses as associative memory elements in the hippocampal formation. Brain Res. 175,233-245 (1979).\nLevy. W.B. and Steward, O. Temporal contiguity requirements for long-term associative\npotentiation/depression in the hippocampus. Neuroscience 8, 791-797 (1983).\nLynch. G.S., Dunwiddie. T. and Gribkoff. V. Heterosynaptic depression: a postsynaptic\ncorrelate oflong-term potentiation. Nature (Lond.) 266. 737-739 (1977).\nMalinow. R. and Miller, J.P. Postsynaptic hyperpolarization during conditioning reversibly blocks induction of long-term potentiation Nature (Lond.)32.0. 529-530 (1986).\nMody. I.. Stanton. PK. and Heinemann. U. Activation of N-methyl-D-aspartate\n(NMDA) receptors parallels changes in cellular and synaptic properties of dentate\ngyrus granule cells after kindling. J. Neurophysiol. 59. 1033-1054 (1988).\nReiter, H.O. and Stryker, M.P. Neural plasticity without postsynaptic action potentials:\nLess-active inputs become dominant when kitten visual cortical cells are pharmacologically inhibited. Proc. Natl. Acad. Sci. USA 85, 3623-3627 (1988).\nSejnowski, T J. and Tesauro, G. Building network learning algorithms from Hebbian\nsynapses, in: Brain Organization and Memory JL. McGaugh, N.M. Weinberger, and\nG. Lynch, Eds. (Oxford Univ. Press, New York, in press).\nSejnowski, TJ. Storing covariance with nonlinearly interacting neurons. J. Math. Biology 4, 303-321 (1977).\nSejnowski, T. J. Statistical constraints on synaptic plasticity. J. Theor. Biology 69, 385389 (1977).\nStanton, P.K. and Sejnowski, TJ. Associative long-term depression in the hippocampus:\nEvidence for anti-Hebbian synaptic plasticity. Nature (Lond.), in review.\nWigstrom, H. and Gustafsson, B. A possible correlate of the postsynaptic condition for\nlong-lasting potentiation in the guinea pig hippocampus in vitro. Neurosci. Lett. 44,\n327?332 (1984).\n\n401\n\n\x0c'
p83168
sg108
S'133\n\nTRAINING MULTILAYER PERCEPTRONS WITH THE\nEXTENDED KALMAN ALGORITHM\nSharad Singhal and Lance Wu\nBell Communications Research, Inc.\nMorristown, NJ 07960\n\nABSTRACT\nA large fraction of recent work in artificial neural nets uses\nmultilayer perceptrons trained with the back-propagation\nalgorithm described by Rumelhart et. a1. This algorithm\nconverges slowly for large or complex problems such as\nspeech recognition, where thousands of iterations may be\nneeded for convergence even with small data sets. In this\npaper, we show that training multilayer perceptrons is an\nidentification problem for a nonlinear dynamic system which\ncan be solved using the Extended Kalman Algorithm.\nAlthough computationally complex, the Kalman algorithm\nusually converges in a few iterations. We describe the\nalgorithm and compare it with back-propagation using twodimensional examples.\n\nINTRODUCTION\nMultilayer perceptrons are one of the most popular artificial neural net\nstructures being used today. In most applications, the "back propagation"\nalgorithm [Rllmelhart et ai, 1986] is used to train these networks. Although\nthis algorithm works well for small nets or simple problems, convergence is\npoor if the problem becomes complex or the number of nodes in the network\nbecome large [Waibel et ai, 1987]. In problems sllch as speech recognition,\ntens of thousands of iterations may be required for convergence even with\nrelatively small elata-sets. Thus there is much interest [Prager anel Fallsiele,\n1988; Irie and Miyake, 1988] in other "training algorithms" which can\ncompute the parameters faster than back-propagation anel/or can handle much\nmore complex problems.\nIn this paper, we show that training multilayer perceptrons can be viewed as\nan identification problem for a nonlinear dynamic system. For linear dynamic\nCopyright 1989. Bell Communications Research. Inc.\n\n\x0c134\n\nSinghal and Wu\n\nsystems with white input and observation noise, the Kalman algorithm\n[Kalman, 1960] is known to be an optimum algorithm. Extended versions of\nthe Kalman algorithm can be applied to nonlinear dynamic systems by\nlinearizing the system around the current estimate of the parameters.\nAlthough computationally complex, this algorithm updates parameters\nconsistent with all previously seen data and usually converges in a few\niterations. In the following sections, we describe how this algorithm can be\napplied to multilayer perceptrons and compare its performance with backpropagation using some two-dimensional examples.\n\nTHE EXTENDED KALMAN FILTER\nIn this section we briefly outline the Extended Kalman filter. Mathematical\nderivations for the Extended Kalman filter are widely available in the\nliterature [Anderson and Moore, 1979; Gelb, 1974] and are beyond the scope\nof this paper.\nConsider a nonlinear finite dimensional discrete time system of the form:\n\nx(n+l) = In(x(n? + gn(x(n?w(n),\nden) = hn(x(n?+v(n).\n\n(1)\n\nHere the vector x (n) is the state of the system at time n, w (n) is the input,\nden) is the observation, v(n) is observation noise and In(\'), gn(\'), and h n(\')\nare nonlinear vector functions of the state with the subscript denoting possible\ndependence on time. We assume that the initial state, x (0), and the\nsequences {v (n)} and {w (n)} are independent and gaussian with\n\nE [x (O)]=x(O), E {[x (O)-x (O)][x (O)-i(O?)I} = P(O),\nE [w (n)] = 0, E [w (n )w t (l)] = Q (n )Onl\'\nE[v(n)] = 0, E[v(n)vt(l)] = R(n)onb\n\n(2)\n\nwhere Onl is the Kronecker delta. Our problem is to find an estimate i (n +1)\nof x (n +1) given d (j) , O<j <n. We denote this estimate by i (n +11 n).\nIf the nonlinearities in (1) are sufficiently smooth, we can expand them llsing\nTaylor series about the state estimates i (n In) and i (n In -1) to obtain\n\nIn(x(n? = I" (i(n In? + F(n)[x(n)-i(n In)] + ...\ngn(x(n? = gil (i(n In? + ... = C(n) + ...\nhn(x(n? = hll(i(n In-I? + J-f1(n)[x(n)-i(n In-1)] +\nwhere\nC(ll) = gn(i(n Ill?,\ndin (x)\nF (ll ) = - - - .-ax\n\nx = .i (II III)\n\n, I-P\n\ndh ll (x)\n(n ) = --.,--Ox\n\n(3)\nx=i(IIII1-1)\n\ni.e. G (n) is the value of the function g" (.) at i (n In) and the ij th\ncomponents of F (n) and H\' (n) are the partial derivatives of the i th\ncomponents of f II ( . ) and hll (-) respectively with respect to the j th component\nof x (n) at the points indicated. Neglecting higher order terms and assuming\n\n\x0cTraining Multilayer Perceptrons\n\nknowledge of i (n In) and i (n In-I), the system in (3) can be approximated\nas\nx(n+l) = F(n)x(n) + G(n)w(n)\nz (n ) = HI (n )x (n )+ v (n) + y (n ),\n\n+ u(n)\n\n(4)\n\nn>O\n\nwhere\n\n(5)\n\nu(n) = /n(i(n In? - F(n)i(n In)\ny(n) = hn(i(n In-I? - H1(n)i(n In-1).\n\nIt can be shown [Anderson and Moore, 1979] that the desired estimate\n\ni (n + 11 n) can be obtained by the recursion\n\ni(n+1In) =/n(i(n In?\ni(n In) = i(n In-I) + K(n)[d(n) - hn(i(n In-1?]\nK(n) = P(n In-I)H(n)[R(n)+HI(n)P(n In-I)H(n)tl\nP(n+Iln) = F(n)P(n In)FI(n) + G(n)Q(n)G1(n)\nP(n In) = P(n In-I) - K(n)HI(n)P(n In-I)\n\n(6)\n(7)\n(8)\n(9)\n(10)\n\nwith P(11 0) = P (0). K (n) is known as the Kalman gain. In case of a linear\nsystem, it can be shown that P(n) is the conditional error covariance matrix\nassociated with the state and the estimate i (n +1/ n) is optimal in the sense\nthat it approaches the conditional mean E [x (n + 1) Id (0) ... d (n)] for large\nn . However, for nonlinear systems, the filter is not optimal and the estimates\ncan only loosely be termed conditional means.\n\nTRAINING MULTILAYER PERCEPTRONS\nThe network under consideration is a L layer perceptronl with the i th input\nof the k th weight layer labeled as :J-l(n), the jth output being zjk(n) and the\nweight connecting the i th input to the j th output being (}i~j\' We assume that\nthe net has m inputs and I outputs. Thresholds are implemented as weights\nconnected from input nodes 2 with fixed unit strength inputs . Thus, if there\nare N (k) nodes in the k th node layer, the total number of weights in the\nsystem is\nL\n\nM = ~N(k-l)[N(k)-l].\n\n(11)\n\nk=1\n\nAlthough the inputs and outputs are dependent on time 11, for notational\nbrevity, we wil1 not show this dependence unless explicitly needed .\nl.\n\nWe use the convention that the number of layers is equal to the number of weight layers . Thus\nwe have L layers of Wl\'iglrls labeled 1 ?\nL and I ~ + I layer s of /lodes (including the input and\noutput nodes) labeled O ? . . L . We will refer to the kth weight layer or the kth node layer\nunless the cont ext is clear.\n\n2.\n\nWe adopt the convention that the 1st input node is the threshold. i.e.\nthe j th output node from the k th weight layer.\n\nlit., is\n\nthe threshold for\n\n135\n\n\x0c136\n\nSinghal and Wu\n\nIn order to cast the problem in a form for recursive estimation, we let the\nweights in the network constitute the state x of the nonlinear system, i.e.\n\nx = [Ob,Ot3 ... 0k(O),N(l)]t.\n\n(12)\n\nThe vector x thus consists of all weights arranged in a linear array with\ndimension equal to the total number of weights M in the system. The system\nmodel thus is\n\nx(n+l)=x(n) n>O,\nden) = zL(n) + v(n) = hn(x(n),zO(n))\n\n+ v(n),\n\n(13)\n(14)\n\nwhere at time n, zO(n) is the input vector from the training set, d (n) is the\ncorresponding desired output vector, and ZL (n) is the output vector\nproduced by the net. The components of h n (.) define the nonlinear\nrelationships between the inputs, weights and outputs of the net. If r(?) is the\nnonlinearity used, then ZL (n) = h n(x (n ),zO(n)) is given by\n\nzL(n) = r{(OL)tr{(OL-l)tr ... r{(OlyzO(n)}? .. }}..\n\n(15)\n\nwhere r applies componentwise to vector arguments. Note that the input\nvectors appear only implicitly through the observation function h n ( . ) in (14).\nThe initial state (before training) x (0) of the network is defined by populating\nthe net with gaussian random variables with a N(x(O),P(O)) distribution where\n(0) and P (0) reflect any apriori knowledge about the weights. In the absence\nof any such knowledge, a N (0,1/f. I) distribution can be used, where f. is a\nsmall number and I is the identity matrix. For the system in (13) and (14),\nthe extended Kalman filter recursion simplifies to\n\nx\n\ni(I1+1) = i(n) + K(n)[d(n) - hn(i(n),zO(n))]\nK (n) = P(n)H (n )[R (n )+H\' (n )P(n )H(n )]-1\nPen +1) = P(n) - K (n )Ht (n)P (n)\n\n(16)\n(17)\n(18)\n\nwhere P(n) is the (approximate) conditional error covariance matrix .\nNote that (16) is similar to the weight update equation in back-propagation\nwith the last term [ZL - h n (x ,ZO)] being the error at the output layer.\nHowever, unlike the delta rule used in back-propagation, this error is\npropagated to the weights through the Kalman gain K (n) which updates each\nweight through the entire gradient matrix H (n) and the conditional error\ncovariance matrix P (n ). In this sense, the Kalman algorithm is not a local\ntraining algorithm . However, the inversion required in (17) has dimension\nequal to the llumber of outputs I, 110t the number of weights M, and thus\ndoes not grow as weights arc added to the problem.\n\nEXAMPLES AND RESULTS\nTo evaluale the Olltpul and the convergence properties of the extended\nKalman algorithm. we constructed mappings using two-dimensional inputs\nwith two or four outputs as shown in Fig. 1. Limiting the input vector to 2\ndimensions allows liS to visualize the decision regiolls ohtained by the net and\n\n\x0cTraining Multilayer Perceptrons\n\nto examine the outputs of any node in the net in a meaningful way. The xand y-axes in Fig. 1 represent the two inputs, with the origin located at the\ncenter of the figures. The numbers in the figures represent the different\noutput classes.\n\n2\n-\n\n-\n\n1\n\nt------+-----I\n\n1\n\n2\n\nI\n(a) REGIONS\n\n(b) XOR\n\nFigure 1. Output decision regions for two problems\n\nThe training set for each example consisted of 1000 random vectors uniformly\nfilling the region . The hyperbolic tangent nonlinearity was used as the\nnonlinear element in the networks. The output corresponding to a class was\nset to 0.9 when the input vector belonged to that class, and to -0.9 otherwise.\nDuring training, the weights were adjusted after each data vector was\npresented. Up to 2000 sweeps through the input data were used with the\nstopping criteria described below to examine the convergence properties. The\norder in which data vectors were presented was randomized for each sweep\nthrough the data. In case of back-propagation, a convergence constant of 0.1\nwas used with no "momentum" factor. In the Kalman algorithm R was set to\nI ?e-k / 50 , where k was the iteration number through the data. Within each\niteration, R was held constant.\nThe Stopping Criteria\n\nTraining was considered complete if anyone of the following\nsatisfied:\n\ncon~itions\n\nwas\n\na.\n\n2000 sweeps through the input data were used,\n\nh.\n\nthe RMS (root mean squared) error at the output averaged over all\ntraining data during a sweep fell below a threshold 11\' or\n\nc.\n\nthe error reduction 8 after the i th sweep through the data fell below a\nthreshold I::., where 8; = !3b;_1 + (l-,B) Iei-ei_l I. Here !3 is some\npositive constant less than unity, and ei is the error defined in b.\nIn our simulations we set ;3 = 0.97, II = 10-2 and 12 = 10- 5 ?\n\n137\n\n\x0c138\n\nSinghal and Wu\n\nExample 1 - Meshed, Disconnected Regions:\n\nFigure l(a) shows the mapping with 2 disconnected, meshed regions\nsurrounded by two regions that fill up the space. We used 3-layer perceptrons\nwith 10 hidden nodes in each hidden layer to Figure 2 shows the RMS error\nobtained during training for the Kalman algorithm and back-propagation\naveraged over 10 different initial conditions. The number of sweeps through\nthe data (x-axis) are plotted on a logarithmic scale to highlight the initial\nreduction for the Kalman algorithm. Typical solutions obtained by the\nalgorithms at termination are shown in Fig. 3. It can be seen that the Kalman\nalgorithm converges in fewer iterations than back-propagation and obtains\nbetter solutions.\n1\n\n0.8\nAverage 0.6\nRMS\nError 0.4\n\nbackprop\n\n0.2\nKalman\n\n0\n2\n\n1\n\n5\n\n10\n\n20\n50 100 200\nNo. of Iterations\n\n500 10002000\n\nFigure 2. Average output error during training for Regions problem using the\nKalman algorithm and backprop\n\nI\n\nI\n\n(a)\n(b)\nFigure 3. Typical solutions for Regions problem using (a) Kalman algorithm\nand (h) hackprop.\n\n\x0cTraining Multilayer Perceptrons\n\nExample 2 - 2 Input XOR:\n\nFigure 1(b) shows a generalized 2-input XOR with the first and third\nquadrants forming region 1 and the second and fourth quadrants forming\nregion 2. We attempted the problem with two layer networks containing 2-4\nnodes in the hidden layer. Figure 4 shows the results of training averaged\nover 10 different randomly chosen initial conditions. As the number of nodes\nin the hidden layer is increased, the net converges to smaller error values.\nWhen we examine the output decision regions, we found that none of the nets\nattempted with back-propagation reached the desired solution. The Kalman\nalgorithm was also unable to find the desired solution with 2 hidden nodes in\nthe network. However, it reached the desired solution with 6 out of 10 initial\nconditions with 3 hidden nodes in the network and 9 out of 10 initial\nconditions with 4 hidden nodes. Typical solutions reached by the two\nalgorithms are shown in Fig. 5. In all cases, the Kalman algorithm converged\nin fewer iterations and in all but one case, the final average output error was\nsmaller with the Kalman algorithm.\n1\n0.8\nAverage 0.6\nRMS\nError 0.4\nKalman 3 nodes\n\n0.2\n\nKalman 4 nodes\n\n0\n\n1\n\n2\n\n5\n\n10\n\n50 100 200\n20\nNo. of Iterations\n\n500 10002000\n\nFigure 4. Average output error during training for XOR problem using the\nKalman algorithm and backprop\n\nCONCLUSIONS\nIn this paper, we showed that training feed-forward nets can be viewed as a\nsystem identification problem for a nonlinear dynamic system. For linear\ndynamic systems, the Kalman tllter is known to produce an optimal estimator.\nExtended versions of the Kalman algorithm can be used to train feed-forward\nnetworks. We examined the performance of the Kalman algorithm using\nartifkially constructed examples with two inputs and found that the algorithm\ntypically converges in a few iterations. We also llsed back-propagation on the\nsame examples and found that invariably, the Kalman algorithm converged in\n\n139\n\n\x0c140\n\nSinghal and Wu\n\nl\n2\n\n1\n\n~\n\n1\n\n2\n\nI\n\n2\n\nI\n\nI\n\n(a)\n\n(b)\n\nFigure 5. Typical solutions for XOR problem using (a) Kalman algorithm and\n\n(b) backprop.\nfewer iterations. For the XOR problem, back-propagation failed to converge\non any of the cases considered while the Kalman algorithm was able to find\nsolutions with the same network configurations.\nReferences\n\n[1]\n\nB. D. O. Anderson and J. B. Moore, Optimal Filtering, Prentice Hall,\n1979.\n\n[2]\n\nA. Gelb, Ed., Applied Optimal Estimation, MIT Press, 1974.\n\n[3]\n\nB. Irie, and S. Miyake, "Capabilities of Three-layered Perceptrons,"\nProceedings of the IEEE International Conference on Neural Networks,\nSan Diego, June 1988, Vol. I, pp. 641-648.\n\n[4]\n\nR. E. Kalman, "A New Approach to Linear Filtering and Prediction\nProblems," 1. Basic Eng., Trans. ASME, Series D, Vol 82, No.1, 1960,\npp.35-45.\n\n[5]\n\nR. W. Prager and F. Fallside, "The Modified Kanerva Model for\nAutomatic Speech Recognition," in 1988 IEEE Workshop on Speech\nRecognition, Arden House, Harriman NY, May 31-Jllne 3,1988.\n\n[6]\n\nD. E. Rumelharl, G. E. Hinton and R. J. Williams, "Learning Internal\nRepresentations by Error Propagation," in D. E. Rllmelhart and\nJ. L. McCelland (Eds.), Parallel Distributed Processing: Explorations in\nthe Microstructure oj\' Cognition. Vol 1: Foundations. MIT Press, 1986.\n\n[7J\n\nA. Waibel, T. Hanazawa, G. Hinton, K. Shikano and K . Lang\n"Phoneme Recognition Using Time-Delay Neural Networks," A 1R\ninternal Report TR-I-0006, October 30, 1987.\n\n\x0c'
p83169
sg110
S'2\n\nCONSTRAINTS ON ADAPTIVE NETWORKS\nFOR MODELING HUMAN GENERALIZATION\nM. Pavel\n\nMark A. Gluck\n\nVan Henkle\n\nDepartm?1Il of Psychology\n\nStanford University\nStanford. CA 94305\n\nABSTRACT\nThe potential of adaptive networks to learn categorization rules and to\nmodel human performance is studied by comparing how natural and\nartificial systems respond to new inputs, i.e., how they generalize. Like\nhumans, networks can learn a detenninistic categorization task by a\nvariety of alternative individual solutions. An analysis of the constraints imposed by using networks with the minimal number of hidden\nunits shows that this "minimal configuration" constraint is not\nsufficient to explain and predict human performance; only a few solutions were found to be shared by both humans and minimal adaptive\nnetworks. A further analysis of human and network generalizations\nindicates that initial conditions may provide important constraints on\ngeneralization. A new technique, which we call "reversed learning",\nis described for finding appropriate initial conditions.\n\nINTRODUCTION\nWe are investigating the potential of adaptive networks to learn categorization tasks and\nto model human performance. In particular we have studied how both natural and\nartificial systems respond to new inputs, that is, how they generalize. In this paper we\nfirst describe a computational technique to analyze generalizations by adaptive networks.\nFor a given network structure and a given classification problem, the technique\nenumerates all possible network solutions to the problem. We then report the results of\nan empirical study of human categorization learning. The generalizations of human subjects are compared to those of adaptive networks. A cluster analysis of both human and\nnetwork generalizations indicates, significant differences between human perfonnance\nand possible network behaviors. Finally, we examine the role of the initial state of a network for biasing the solutions found by the network. Using data on the relations between\nhuman subjects\' initial and final performance during training, we develop a new technique, called "reversed learning", which shows some potential for modeling human\nlearning processes using adaptive networks. The scope of our analyses is limited to generalizations in deterministic pattern classification (categorization) tasks.\n\n\x0cModeling Human Generalization\n\nThe basic difficulty in generalization is that there exist many different classification rules\n("solutions") that that correctly classify the training set but which categorize novel\nobjects differently. The number and diversity of possible solutions depend on the\nlanguage defining the pattern recognizer. However, additional constraints can be used in\nconjunction with many types of pattern categorizers to eliminate some, hopefully\nundesirable, solutions.\nOne typical way of introducing additional constraints is to minimize the representation.\nFor example minimizing the number of equations and parameters in a mathematical\nexpression, or the number of rules in a rule-based system would assure that some\nidentification maps would not be computable. In the case of adaptive networks, minimizing the size of adaptive networks, which reduces the number of possible encoded functions, may result in improved generalization perfonnance (Rumelhart, 1988).\nThe critical theoretical and applied questions in pattern recognition involve characterization and implementation of desirable constraints. In the first part of this paper we\ndescribe an analysis of adaptive networks that characterizes the solution space for any\nparticular problem.\n\nANALYSES OF ADAPTIVE NETWORKS\nFeed-forward adaptive networks considered in this paper will be defined as directed\ngraphs with linear threshold units (LTV) as nodes and with edges labeled by real-valued\nweights. The output or activations of a unit is detennined by a monotonic nonlinear function of a weighted sum of the activation of all units whose edges tenninate on that unit\nThere are three types of units within a feed-forward layered architecture: (1) Input units\nwhose activity is determined by external input; (2) output units whose activity is taken as\nthe response; and (3) the remaining units, called hidden units. For the sake of simplicity\nour discussion will be limited to objects represented by binary valued vectors.\nA fully connected feed-forward network with an unlimited number of hidden units can\ncompute any boolean function. Such a general network, therefore, provides no constraints on the solutions. Therefore, additional constraints must be imposed for the network to prefer one generalization over another. One such constraint is minimizing the\nsize of the network. In order to explore the effect of minimizing the number of hidden\nunits we first identify the minimal network architecture and then examine its generalizations.\nMost of the results in this area have been limited to finding bounds on the expected\nnumber of possible patterns that could be classified by a given network (e.g. Cover, 1965;\nVolper and Hampson, 1987; Valiant, 1984; Baum & Haussler, 1989). The bounds found\nby these researchers hold for all possible categorizations and are, therefore, too broad to\nbe useful for the analysis of particular categorization problems.\nTo determine the generalization behavior for a particular network architecture, a specific\n\n3\n\n\x0c4\n\nGluck, Pavel and Henkle\n\ncategorization problem and a training set it is necessary to find find all possible solutions\nand the corresponding generalizations. To do this we used a computational (not a simulation) procedure developed by Pavel and Moore (1988) for finding minimal networks\nsolving specific categorization problems. Pavel and Moore (1988) defined two network\nsolutions to be different if at least one hidden unit categorized at least one object in the\ntraining set differently. Using this definition their algorithm finds all possible different\nsolutions. Because finding network solutions is NP-complete (Judd, 1987), for larger\nproblems Pavel and Moore used a probabilistic version of the algorithm to estimate the\ndistribution of generalization responses.\nOne way to characterize the constraints on generalization is in terms of the number of\npossible solutions. A larger number of possible solutions indicates that generalizations\nwill be less predictable. The critical result of the analysis is that, even for minimal networks. the number of different network solutions is often quite large. Moreover. the\nnumber of solutions increases rapidly with increases in the number of hidden units. The\napparent lack of constraints can also be demonstrated by finding the probability that a\nnetwork with a randomly selected hidden layer can solve a given categorization problem.\nThat is, suppose that we se~t n different hidden units, each unit representing a linear\ndiscriminant fwction. The activations of these random hidden wits can be viewed as a\nttansformation of the input patterns. We can ask what is the probability that an output\nunit can be found to perfonn the desired dichotomization. A typical example of a result\nof this analysis is shown in Figure 1 for the three-dimensional (3~) parity problem. In\nthe minimal configuration involving three hidden units there were 62 different solutions\nto the 3D parity problem. The rapid increase in probability (high slope of the curve in\nFigure 1) indicates that adding a few more hidden units rapidly increases the probability\nthat a random hidden layer will solve the 3D parity problem.\n100\n\n,,\nII\n,,\n\n80\n\n"\n\n,\n,,,\n\n!;\n\n-\'\n\ni\n\n...... -.\n\n,\n,,\n\n10\n\n?gz\n\n~.\n\n,,\n,, ,\n\n40\n\n~\n20\n\n-----\n\n,~\n,,\n\n0\n0\n\n2\n\n4\n\n6\nHIOOENUNITS\n\nFigure 1 1be proportion of solutions\n\n?\n\nEXPERIMENT\n3D PARITY\n\n10\n\n12\n\n3D parity problem (solid line) and the\nexperimental task (dashed line) as a function of the number of hidden units.\nto\n\nThe results of a more detailed analysis of the generalization performance of the minimal\nnetworks will be discussed following a description of a categorization experiment with\n\n\x0cModeling Human Generalization\n\nhuman subjects.\n\nHUMAN CATEGORIZATION EXPERIMENT\nIn this experiment human subjects learned to categorize objects which were defined by\nfour dimensional binary vectors. Of the 24 possible objects, subjects were trained to classify a subset of 8 objects into two categories of 4 objects each. The specific assignments\nof objects into categories was patterned after Medin et aI. (1982) and is shown in Figure\n2. Eight of the patterns are designated as a training set and the remaining eight comprise\nthe test seL The assignment of the patterns in the training set into two categories was\nsuch that there were many combinations of rules that could be used to correctly perfonn\nthe categorization. For example, the first two dimensions could be used with one other\ndimension. The training patterns could also be categorized on the basis of an exclusive\nor (XOR) of the last two dimensions. The type of solution obtained by a human subject\ncould only be determined by examining responses to the test set as well as the training\nseL\nTRAINING SET\n\n1 10\n1 11\n101\nX. 101\n\nX1\nDIMENSIONS ~\n~\nCATEGORY\n\n1\n0\n0\n0\n\n001\n000\n101\no1 0\n\nTEST SET\n\n0\n1\n0\n1\n\nAAAA BBBB\n\n000 1\n001 0\no1 0 1\no10 1\n??? ?\n\n1 10 1\n1 1 10\no1 0 1\no1 0 1\n????\n\nFigllTe 2. PattemI to be clulmed. (Adapted from Medin et aI .? 1982).\n\nIn the actual experiments, subjects were asked to perform a medical diagnosis for each\npattern of four symptoms (dimensions). The experimental procedure will be described\nhere only briefly because the details of this experiment have been described elsewhere in\ndetail (pavel, Gluck, Henkle, 1988). Each of the patterns was presented serially in a randomized order. Subjects responded with one of the categories and then received feedback. The training of each individual continued until he reached a criterion (responding\ncorrectly to 32 consecutive stimuli) or until each pattern had been presented 32 times.\nThe data reported here is based on 78 subjects, half (39) who learned the task to criterion\nand half who did DOL\nFollowing the training phase, subjects were tested using all 16 possible patterns. The\nresults of the test phase enabled us to determine the generalizations perfonned by the\nsubjects. Subjects\' generalizations were used to estimate the "functions" that they may\nhave been using. For example, of the 39 criterion subjects, 15 used a solution that was\nconsistent with the exclusive-or (XOR) of the dimensions x 3 and X4.\nWe use "response profiles" to graph responses for an ensemble of functions, in this case\nfor a group of subjects. A response profile represents the probability of assigning each\n\n5\n\n\x0c6\n\nGluck, Pavel and Henkle\n\npattern to category "A". For example, the response profile for the XOR solution is\nshown in Figure 3A. For convenience we define the responses to the test set as the "generalization profile". The response profile of all subjects who reached the criterion is\nshown in Figure 3D. The responses of our criterion subjects to the training set were basically identical and correct The distribution of subjects\' genezalization profiles reflected\nin the overall generalization profile are indicative of considerable individual differences\n\n/I)\n\nz\n\na::\n\nloll\n\n~\nC\n\n~\n\n1001\n\n1001\n\n0110\n1101\n\n0110\n\n1110\n1011\n\n1110\n\n1101\n\n-=:===::--\n\n1011~\n\n0100\n0011\n\n/I)\n\nz\n\n0000\n0101\n1010\n0001\n\na::\n\nloll\n\n~\nC\n\n~\n\n0100\n0011~\n\n0000\n0101\n1010\n0001\n\n0010\n1000\n\n0010\n1000\n\n0111\n1100\n\n0111\n1100\n1111\n\n1111\n00\n\n02\n\n04\n\n06\n\n08\n\n10\n\n12\n\nPROPORTION " .. -\n\nr---\n\n.\n\n00\n\n02\n\n04\n\n06\n\n01\n\n10\n\n12\n\nPROPORTION " It.-\n\nFigwe 3. (A) Response profile of the XOR solution. and (B) a proportion of\nthe response "A" to all patterns for human subjects (dark bars) and minimal\nnetworks (light bars). The lower 8 patterns are from the training set and the\nupper 8 patterns from the test set.\n\nMODEliNG THE RESPONSE PROFILE\n\nOne of our goals is to model subjects\' distribution of categorizations as represented by\nthe response profile in Figure 3D. We considered three natural approaches to such\nmodeling: (1) Statistical/proximity models, (2) Minimal disjunctive normal forms\n(DNF), and (3) Minimal two-layer networks.\nThe statistical approach is based on the assumption that the response profile over subjects\nrepresents the probability of categorizations performed by each subject Our data are not\nconsistent with that assumption because each subject appeared to behave deterministically. The second approach, using the minimal DNF is also not a good candidate because\nthere are only four such solutions and the response profile over those solutions differs\nconsiderably from that of the SUbjects. Turning to the adaptive network solutions, we\nfound all the solutions using the linear programming technique described above (pavel &\nMoore, 1988). The minimal two-layer adaptive network that was capable of solving the\ntraining set problem consisted of two hidden units. The proportion of solutions as a\n\n\x0cModeling Human Generalization\n\nfunction of the number of hidden units is shown in Figure 1 by the dashed line.\nFor the minimal network there were 18 different solutions. These 18 solutions had 8 different individual generalization profiles. Assuming that each of the 18 network solution\nis equally likely. we computed the generalization profile for minimal network shown in\nFigure 3B. The response profile for the minimal network represents the probability that a\nrandomly selected minimal network will assign a given pattern to category "A". Even\nwithout statistical testing we can conclude that the generalization profiles for humans and\nnetworks are quite different. It is possible. however. that humans and minimal networks\nobtain similar solutions and that the differences in the average responses are due to the\nparticular statistical sampling assumption used for the minimal networks (i.e. each solution is equally likely). In order to determine the overlap of solutions we examined the\ngeneralization profiles in more detail.\n\nCLUSTERING ANALYSIS OF GENERALIZATION PROFILES\nTo analyze the similarity in solutions we defined a metric on generalization profiles. The\nHamming distance between two profiles is equal to the number of patterns that are\ncategorized differently. For example. the distance between generalization profile ?? A A\nB A B B B B" and "A A B B B B A B" is equal to two. because the two profiles differ\non only the fourth and seventh pattern. Figure 4 shows the results of a cluster analysis\nusing a hierarchical clustering procedure that maximizes the average distance between\nclusters.\n\n?c\n\n?c\n?c\n?c\n\nc\n\n??\n??c\nc\nc\n\n???\n?\n\n3\n\nc\n~\n\n?\n?\n\n???\n\no\n\n??? ???\nc? ~\nc\n\n3~\n\n? ?c\n~\n\n? =\n?\n\n??c c??\n\n;\n\n~\n\n??~ ??~\n??\n\n?~\nI\n?\n??\n\nc\n\n?~\n??\nI\n\n!?\n??c\n\nc\n\n~\n\n??\n??\n\nFiglll\'ll 4. Results of hierarchical clustering for human (left) and network\n(right) generalization profiles.\n\nIn this graph the average distance between any two clusters is shown by the value of the\nlowest common node in the tree. The clustering analysis indicates that humans and\n\n7\n\n\x0c8\n\nGluck, Pavel and Henkle\n\nnetworks obtained widely different generalization profiles. Only three generalization\nprofiles were found to be common to human and networks. This number of common\ngeneralizations is to be expected by chance if the human and network solutions are\nindependent Thus, even if there exists a learning algorithm that approximates the human\nprobability distribution of responses, the minimal network would not be a good model of\nhuman perfonnance in this task.\nIt is clear from the previously described network analysis that somewhat larger networks\nwith different constraints could account for human solutions. In order to characterize the\nadditional constraints, we examined subjects\' individual strategies to find out why individual subjects obtained different solutions.\n\nANALYSIS OF HUMAN LEARNING STRATEGIES\nHuman learning strategies that lead to preferences for particular solutions may best be\nmodeled in networks by imposing constraints and providing hints (Abu-Mostafa 1989).\nThese include choosing the network architecture and a learning rule, constraining connectivity, and specifying initial conditions. We will focus on the specification of initial\nconditions.\n\n30\n\nCI ..CONSISTENT\n?\n\nCONSISTENT\n\n20\n\n10\n\no\nlOR\n\nNON lOR\n\nNO CRrTERION\n\nSUBJECT TYPES\nFiglU\'e 5. The number of consistent or non-stable responses (black) and the\n\nnwnber of stable incorrect responses (light) for XOR, Non-XOR criterion su~\njeers, and for those who never reached criterion.\n\nOur effort to examine initial conditions was motivated by large differences in learning\ncurves (Pavel et al., 1988) between subjects who obtained the XOR solutions and those\nwho did not The subjects who did not obtain the XOR solutions would perfonn much\nbetter on some patterns (e.g. 0001) then the XOR subjects, but worse on other patterns\n(e.g. 10(0). We concluded that these subjects during the first few trials discovered rules\n\n\x0cModeling Human Generalization\n\nthat categorized most of the training patterns correctly but failed on one or two training\npatterns.\nWe examined the sequences of subjects\' responses to see how well they adhered to\n"incorrect" rules. We designated a response to a pattern as stable if the individual\nresponded the same way to that pattern at least four times in a row. We designated a\nresponse as consistent if the response was stable and correct The results of the analysis\nare shown in Figure 5. These results indicate that the subjects who eventually achieved\nthe XOR solution were less likely to generate stable incorrect solutions. Another important result is that those subjects who never learned the correct responses to the training\nset were not responding randomly. Rather, they were systematically using incorrect\nrules. On the basis of these results, we conclude that subjects\' initial strategies may be\nimportant detenninants of their final solutions.\n\nREVERSED LEARNING\nFor simplicity we identify subjects\' initial conditions by their responses on the first few\ntrials. An important theoretical question is whether or not it is possible to find a network\nstructure, initial conditions and a learning rule such that the network can represent both\nthe initial and final behavior of the subject In order to study this problem we developed\na technique we call ""reversed leaming". It is based on a perturbation analysis of feedforward networks. We use the fact that the error surface in a small neighborhood of a\nminimum is well approximated by a quadratic surface. Hence, a well behaved gradient\ndescent procedure with a starting point in the neighborhood of the minimum will find that\n\'minimum.\nThe reversed learning procedure consists of three phases. (1) A network is trained to a\nfinal desired state of a particular individual, using both the training and the test patterns.\n(2) Using only the training patterns, the network is then trained to achieve the initial state\nof that individual subject closest to the desired final state (3) The network is trained with\nonly the training patterns and the solution is compared to the subject\'s response profiles.\nOur preliminary results indicate that this procedure leads in many cases to initial conditions that favor the desired solutions. We are currently investigating conditions for\nfinding the optimal initial states.\n\nCONCLUSION\nThe main goal of this study was to examine constraints imposed by humans (experimentally) and networks (linear programming) on learning of simple binary categorization\ntasks. We characterize the constraints by analyzing responses to novel stimuli. We\nshowed that. like the humans, networks learn the detenninistic categorization task and\nfind many, very different. individual solutions. Thus adaptive networks are better models\nthan statistical models and DNF rules. The constraints imposed by minimal networks,\nhowever, appear to differ from those imposed by human learners in that there are only a\nfew solutions shared between human and adaptive networks. After a detailed analysis of\n\n9\n\n\x0c10\n\nGluck, Pavel and Henkle\n\nthe human learning process we concluded that initial conditions may provide imPOl\'Wlt\nconstraints. In fact we consider the set of initial conditions as .powerful "hints" (AbuMostafa, 1989) which reduces the number of potential solutions. without reducing the\ncomplexity of the problem. We demonstrated the potential effectiveness of these constraints using a perturbation technique. which we call reversed learning, for finding\nappropriate initial conditions.\n\nAcknowledgements\nThis work was supported by research grants from the National Science Foundation\n(BNS-86-18049) to Gordon Bower and Mark Gluck. and (IST-8511589) to M. Pavel. and\nby a grant from NASA Ames (NCC 2-269) to Stanford University. We thank Steve Sloman and Bob Rehder for useful discussions and their comments on this draft\n\nReferences\nAbu-Mostafa, Y. S. Learning by example with hints. NIPS. 1989.\nBaum, E. B.? & Haussler. D. What size net gives vaUd generalization? NIPS, 1989.\nCover. T. (June 1965). Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. IEEE Transactions on Electronic\nComputers. EC-14. 3. 326-334.\nJudd. J. S. Complexity of connectionist learning with various node functions. Presented at\nthe First IEEE International Conference on Neural Networks. San Diego, June\n1987.\nMedin. D. L.? Altom. M. W.? Edelson. S. M.? & Freko. D. (1982). Correlated symptoms\nand simulated medical classification. Journal of Experimental Psychology: Learning. Memory. & Cognition, 8(1).37-50.\nPavel. M.? Gluck, M. A.? & Henkle. V. Generalization by humans and multi-layer adaptive networks. Submitted to Tenth Annual Conference of the Cognitive Science\nSociety. August 17-19, 1988.\nPavel. M.? & Moore, R. T. (1988). Computational analysis of solutions of two-layer\nadaptive networks. APL Technical Repon, Dept. of Psychology. Stanford University.\nValiant, L. G. (1984). A theory of the learnable. Comm. ACM. 27.11.1134-1142.\nVolper. D. J?? & Hampson. S. E. (1987). Learning and using specific instances. Biological\nCybernetics, 56?.\n\n\x0c'
p83170
sg63
S'323\n\nNEURAL NETWORK RECOGNIZER FOR\nHAND-WRITTEN ZIP CODE DIGITS\nJ. S. Denker, W. R. Gardner, H. P. Graf, D. Henderson, R. E. Howard,\nW. Hubbard, L. D. Jackel, H. S. Baird, and I. Guyon\nAT &T Bell Laboratories\nHolmdel, New Jersey 07733\n\nABSTRACT\nThis paper describes the construction of a system that recognizes hand-printed\ndigits, using a combination of classical techniques and neural-net methods. The\nsystem has been trained and tested on real-world data, derived from zip codes seen\non actual U.S. Mail. The system rejects a small percentage of the examples as\nunclassifiable, and achieves a very low error rate on the remaining examples. The\nsystem compares favorably with other state-of-the art recognizers. While some of\nthe methods are specific to this task, it is hoped that many of the techniques will\nbe applicable to a wide range of recognition tasks.\n\nMOTIVATION\nThe problem of recognizing hand-written digits is of enormous practical and the~\nretical interest [Kahan, Pavlidis, and Baird 1987; Watanabe 1985; Pavlidis 1982].\nThis project has forced us to formulate and deal with a number of questions ranging from the basic psychophysics of human perception to analog integrated circuit\ndesign.\nThis is a topic where "neural net" techniques are expected to be relevant, since\nthe task requires closely mimicking human performance, requires massively parallel\nprocessing, involves confident conclusions based on low precision data, and requires\nlearning from examples. It is also a task that can benefit from the high throughput\npotential of neural network hardware.\nMany different techniques were needed. This motivated us to compare various classical techniques as well as modern neural-net techniques. This provided valuable\ninformation about the strengths, weaknesses, and range of applicability of the numerous methods.\nThe overall task is extremely complex, so we have broken it down into a great\nnumber of simpler steps. Broadly speaking, the recognizer is divided into the preprocessor and the classifier. The two main ideas behind the preprocessor are (1) to\nremove meaningless variations (i.e. noise) and (2) to capture meaningful variations\n(i.e . salient features).\nMost of the results reported in this paper are based on a collection of digits taken\nfrom hand-written Zip Codes that appeared on real U.S. Mail passing through the\n\n\x0c324\n\nDenker, et al\n\n(j{OL3l-/.jGIJ\nOI~.?4--:;\n\nJI<=t\n\n~789\n\nd/~~\'f.!Jd,7\n\n012dL/-S-67\n\n87\n\nget\n\nFigure 1: Typical Data\nBuffalo, N.Y. post office. Details will be discussed elsewhere [Denker et al., 1989].\nExamples of such images are shown in figure 1. The digits were written by many\ndifferent people, using a great variety of writing styles and instruments, with widely\nvarying levels of care.\nImportant parts of the task can be handled nicely by our lab\'s custom analog\nneural network VLSI chip [Gra! et aI., 1987; Gra! & deVegvar, 1987], allowing us\nto perform the necessary computations in a reasonable time. Also, since the chip\nwas not designed with image processing in mind, this provided a good test of the\nchips\' versatility.\n\nTHE PREPROCESSOR\nAcquisition\nThe first step is to create a digital version of the image. One must find where on\nthe envelope the zip code is, which is a hard task in itself (Wang and Srihari 1988].\nOne must also separate each digit from its neighbors. This would be a relatively\nsimple task if we could assume that a character is contiguous and is disconnected\nfrom its neighbors, but neither of these assumptions holds in practice. It is also\ncommon to find that there are meaningless stray marks in the image.\nAcquisition, binarization, location, and preliminary segmentation were performed\nby Poetal Service contractors. In some images there were extraneous marks, so we\ndeveloped some simple heuristics to remove them while preserving, in most cases,\nall segments of a split character.\n\nScaling and Deskewing\nAt this point, the size of the image is typically 40 x 60 pixels, although the scaling\nroutine can accept images that are arbitrarily large, or as small as 5 x 13 pixels. A\ntranslation and scale factor are then applied to make the image fit in a rectangle\n\n\x0cNeural Network Recognizer for Hand-Written Zip Code Digits\n\n20 x 32 pixels. The character is centered in the rectangle, and just touches either\nthe horizontal or vertical edges, whichever way fits. It is clear that any extraneous\nmarks must be removed before this step, lest the good part of the image be radically\ncompressed in order to make room for some wild mark. The scaling routine changes\nthe horizontal and vertical size of the image by the same factor, so the aspect ratio\nof the character is preserved.\nAs shown in figure 1, images can differ greatly in the amount of skew, yet be\nconsidered the same digit. This is an extremely significant noise source. To remove\nthis noise, we use the methods of [Casey 1970]; see also [Naylor 1971]. That is, we\ncalculate the XY and YY moments of the image, and apply a linear transformation\nthat drives the XY moment to zero. The transformation is a pure shear, not a\nrotation, because we find that rotation is much less common than skew.\nThe operations of scaling and deskewing are performed in a single step. This yields\na speed advantage, and, more importantly, eliminates the quantization noise that\nwould be introduced by storing the intermediate images as pixel maps, were the\ncalculation carried out in separate steps.\n\nSkeletonization\nFor the task of digit recognition, the width of the pen used to make the characters is\ncompletely meaningless, and is highly variable. It is important to remove this noise\nsource. By deleting pixels at the boundaries of thick strokes. After a few iterations\nof this process, each stroke will be as thin as possible. The idea is to remove as\nmany pixels as possible without breaking the connectivity. Connectivity is based\non the 8 nearest neighbors.\nThis can be formulated as a pattern matching problem - we search the image\nlooking for situations in which a pixel should be deleted. The qecisions can be\nexpressed as a convolution, using a rather small kernel, since the identical decision\nprocess is repeated for each location in the image, and the decision depends on the\nconfiguration of the pixel\'s nearest and next-nearest neighbors.\nFigure 2 shows an example of a character before (e) and after (I) skeletonization.\nIt also shows some of the templates we use for skeletonization, together with an\nindication of where (in the given image) that template was active. To visualize the\nconvolution process, imagine taking a template, laying it over the image in each\npossible place, and asking if the template is "active" in that place. (The template\nis the convolution kernel; we use the two terms practically interchangeably.) The\nportrayal of the template uses the following code: Black indicates that if the corresponding pixel in the image is ON, it will contribute +1 to the activity level of\nthis template. Similarly, gray indicates that the corresponding pixel, if ON, will\ncontribute -5, reducing the activity of this template. The rest of the pixels don\'t\nmatter. If the net activity level exceeds a predetermined threshold, the template\nis considered active at this location. The outputs of all the skeletonizer templates\n\n325\n\n\x0c326\n\nDenker, et al\n\na)\n\nb)\n\nc)\n\nd)\n\nFigure 2: Skeletonization\nare eombined in a giant logieal OR, that is, whenever any template is aetive, we\neonelude that the pixel presently under the eenter of the template should be deleted.\n\nThe skeletonization eomputation involves six nested loops:\nfor each iteration I\nfor all X in the image (horizontal eoordinate)\nfor all Y in the image (vertical eoordinate)\nfor all T in the set of template shapes\nfor all P in the template (horizontal)\nfor all Q in the template (vertical)\ncompare image element(X +P, Y+Q)\nwith template(T) element(P, Q)\n\nThe inner three loops (the loops over T, P, and Q) are performed in parallel, in\na single cyde of our special-purpose ehip. The outer three loops (1, X, and Y)\nare performed serially, calling the ehip repeatedly. The X and Y loops eould be\nperformed in parallel with no change in the algorithms. The additional parallelism\nwould require a proportionate increase in hardware.\n\n\x0cNeural Network Recognizer for Hand-Written Zip Code Digits\n\nThe purpose of template a is to detect pixels at the top edge of a thick horizontal\nline. The three "should be OFF" (light grey shade in figure 2) template elements\nenforce the requirement that this should be a boundary, while the three "should be\nON" (solid black shade in figure 2) template elements enforce the requirement that\nthe line be at least two pixels wide.\nTemplate b is analogous to template a, but rotated 90 degrees. Its purpose is to\ndetect pixels at the left edge of a thick vertical line.\nTemplate c is similar to, but not exactly the same as, template a rotated 180 degrees.\nThe distinction is necessary because all templates are applied in parallel. A stroke\nthat is only two pixels thick ?must not be attacked from both sides at once, lest it be\nremoved entirely, changing the connectivity of the image . Previous convolutional\nline-thinning schemes [Naccache 1984] used templates of size 3 x 3, and therefore\nhad to use several serial sub-stages. For parallel operation at least 3 x 4 kernels are\nneeded, and 5 x 5 templates are convenient, powerful, and flexible.\n\nFeature Maps\nHaving removed the main sources of meaningless variation, we turn to the task of\nextracting the meaningful information. It is known from biological studies [Hubel\nand Wiesel 1962] that the human vision system is sensitive to certain features that\noccur in images, particularly lines and the ends of lines. We therefore designed\ndetectors for such features. Previous artificial recognizers [\\Vatanabe 1985] have\nused similar feature extractors.\nOnce again we use a convolutional method for locating the features of interest - we\ncheck each location in the image to see if each particular feature is present there.\nFigure 3 shows some of the templates we use, and indicates where they become\nactive in an example image. The feature extractor templates are 7 x 7 pixels slightly larger than the skeletonizer templates.\nFeature b is designed to detect the right-hand end of (approximately) horizontal\nstrokes. This can be seen as follows: in order for the template to become active\nat a particular point, the image must be able to touch the "should be ON" pixels\nat the center of the template without touching the surrounding horseshoe-shaped\ncollection of "\'must be OFF" pixels. Essentially the only way this can happen is at\nthe right-hand end of a stroke. (An isolated dot in the image will also activate this\ntemplate, but the images, at this stage, are not supposed to contain dots). Feature\nd detects (approximately) horizontal strokes.\nThere are 49 different feature extractor templates. The output of each is stored\nseparately. These outputs are called feature maps, since they show what feature(s)\noccurred where in the image. It is possible, indeed likely, that several different\nfeatures will occur in the same place.\nWhereas the outputs of all the skeletonizer templates were combined in a very simple\nway (a giant OR), the outputs of the feature extractor templates are combined in\n\n327\n\n\x0c328\n\nDenker, et al\na)\n\nc)\n\nb)\n\n?\nI\n\n~------~.~\n\n~.~------~\nFigure 3: Feature Extraction\n\nvarious artful ways. For example, feature" and a similar one are O~d to form a\nsingle combined feature that responds to right-hand ends in general. Certain other\nfeatures are ANDed to form detectors for arcs (long curved strokes). There are 18\ncombined features, and these are what is passed to the next stage.\nWe need to create a compact representation, but starting from the skeletonized\nimage, we have, instead, created 18 feature maps of the same size. Fortunately, we\ncan now return to the theme of removing meaningless variation.\nIf a certain image contains a particular feature (say a left-hand stroke end) in the\nupper left corner, it is not really necessary to specify the location of that feature\nwith great precision. To recognize the Ihope of the feature required considerable\nprecision at the input to the convolution, but the pOlitiora of the feature does not\nrequire so much precision at the output of the convolution. We call this Coarse\nBlocking or Coarse Coding of the feature maps. We find that 3 x 5 is sufficent\nresolution.\n\nCLASSIFIERS\nIf the automatic recognizer is unable to classify a particular zip code digit, it may\nbe possible for the Post Office to determine the correct destination by other means.\nThis is costly, but not nearly so costly as a misclassification (substitution error) that\ncauses the envelope to be sent to the wrong destination. Therefore it is critically\n\n\x0cNeural Network Recognizer for Hand-Written Zip Code Digits\n\nimportant for the system to provide estimates of its confidence, and to reject digits\nrather than misclassify them.\nThe objective is not simply to maximize the number of classified digits, nor to\nminimize the number of errors . The objective is to minimize the cost of the whole\noperation, and this involves a tradeoff between the rejection rate and the error rate.\n\nPreliminary Inves tigations\nSeveral different classifiers were tried, including Parzen Windows, K nearest neighbors, highly customized layered networks, expert systems, matrix associators, feature spins, and adaptive resonance. We performed preliminary studies to identify\nthe most promising methods. We determined that the top three methods in this\nlist were significantly better suited to our task than the others, and we performed\nsystematic comparisons only among those three.\n\nClassical Clustering Methods\nWe used two classical clustering techniques, Parzen Windows (PW) and K Nearest Neighbors (KNN), which are nicely described in Duda and Hart [1973]. In\nthis application, we found (as expected) that they behaved similarly, although PW\nconsistently outperformed KNN by a small margin. These methods have many\nadvantages, not the least of which is that they are well motivated and easily understood in terms of standard Bayesian inference theory. They are well suited to\nimplementation on parallel computers and/or custom hardware. They provide excellent confidence information.\nUnlike modern adaptive network methods, PW and KNN require no "learning\ntime", Furthermore the performance was reproducible and responded smoothly to\nimprovements in the preprocessor and increases in the size of the training set. This\nis in contrast to the "noisy" performance of typical layered networks. This is convenient, indeed crucial, during exploratory work .\n\nAdaptive Network Methods\nIn the early phases of the project, we found that neural network methods gave\nrather mediocre results. Later, with a high-performance preprocessor, plus a large\ntraining database, we found that a layered network gave the best results, surpassing\neven Parzen Windows. We used a network with two stages of processing (i.e., two\nlayers of weights), with 40 hidden units and using a one-sided objective function (as\nopposed to LMS) as described in [Denker and Wittner 1987]. The main theoretical\nadvantage of the layered network over the classical methods is that it can form\n"higher order" features - conjunctions and disjunctions of the features provided\nby our feature extractor. Once the network is trained, it has the advantage that the\nclassification of each input is very rapid compared to PW or KNN. Furthermore,\nthe weights represent a compact distillation of the training data and thus have a\nsmaller memory requirement. The network provides confidence information that is\n\n329\n\n\x0c330\n\nDenker, et al\n\njust as good as the classical methods. This is obtained by comparing the activation\nlevel of the most active output against the runner-up unit(s).\nTo check on the effectiveness of the preprocessing stages, we applied these three\nclassification schemes (PW, KNN, and the two-layer network) on 256-bit vectors\nconsisting of raw bit maps of the images - with no skeletonization and no feature\nextraction. For each classification scheme, we found the error rate on the raw bit\nmaps was at least a factor of 5 greater than the error rate on the feature vectors,\nthus clearly demonstrating the utility of feature extraction.\n\nTESTING\nIt is impossible to compare the performance of recognition systems except on identical databases. Using highly motivated "friendly" writers, it is possible to get a\ndataset that is so clean that practically any algorithm would give outstanding results. On the other hand, if the writers are not motivated to write clearly, the result\nwill be not classifiable by machines of any sort (nor by humans for that matter).\nIt would have been much easier to classify digits that were input using a mouse or\nbitpad, since the lines in the such an image have zero thickness, and stroke-order\ninformation is available. It would also have been much easier to recognize digits\nfrom a single writer.\nThe most realistic test data we could obtain was provided by the US Postal Service.\nIt consists of approximately 10,000 digits (1000 in each category) obtained from the\nzip codes on actual envelopes. The data we received had already been binarized\nand divided into images of individual digits, rather than multi-digit zip codes, but\nno further processing had been done.\nOn this data set, our best performance is as follows: if 14% of the images are rejected\nas unclassifiable, only 1% of the remainder are misclassified. If no images are rejected, approximately 6% are misclassified. Other groups are working with the same\ndataset, but their results have not yet been published. Informal communications\nindicate that our results are among the best.\n\nCONCLUSIONS\nWe have obtained very good results on this very difficult task. Our methods include\nlow-precision and analog processing, massively parallel computation, extraction of\nbiologically-motivated features, and learning from examples. We feel that this is,\ntherefore, a fine example of a Neural Information Processing System. We emphasize that old-fashioned engineering, classical pattern recognition, and the latest\nlearning-from-examples methods were all absolutely necessary. Without the careful\nengineering, a direct adaptive network attack would not succeed, but by the same\ntoken, without learning from a very large database, it would have been excruciating\nto engineer a sufficiently accurate representation of the probability space.\n\n\x0cNeural Network Recognizer for Hand-Written Zip Code Digits\n\nAcknowledgements\nIt is a pleasure to acknowledge useful discussions with Patrick Gallinari and technical assistance from Roger Epworth. We thank Tim Barnum of the U.S. Postal\nService for making the Zip Code data available to us.\nReferences\n1. R. G. Casey, "Moment Normalization of Handprinted Characters", IBM J.\nRes. Develop., 548 (1970)\n2. J. S. Denker et al., "Details of the Hand-Written Character Recognizer", to\nbe published (1989)\n3. R. O. Duda and P. E. Hart, Pattern Classification and Scene Analysis,\nJohn Wiley and Sons (1973)\n4. E. Gullichsen and E. Chang, "Pattern Classification by Neural Network: An\nExperimental System for Icon Recognition", Proc. IEEE First Int. Conf. on\nNeural Networks, San Diego, IV, 725 (1987)\n5. H. P. Graf, W. Hubbard, L. D. Jackel, P.G.N. deVegvar, "A CMOS Associative\nMemory Chip", Proc. IEEE First Int. Conf. on Neural Networks, San Diego,\n111,461 (1987)\n6. H.P Graf and P. deVegvar, "A CMOS Implementation of a Neural Network\nModel", Proc. 1987 Stanford Conf. Advanced Res. VLSI, P. Losleben (ed.)\nMIT Press, 351 (1987)\n7. D. H. Hubel and T. N. Wiesel, "Receptive fields, binocular interaction and\nfunctional architecture in the cat\'s visual cortex", J. Physiology 160, 106\n(1962)\n8. S. Kahan, T. Pavlidis, and H. S. Baird, "On the Recognition of Printed Characters of Any Font and Size", IEEE Transactions on Pattern Analysis and\nMachine Intelligence, PAMI-9, 274 (1987)\n9. N. J. Naccache and R. Shinghal, \'\'SPTA: A Proposed Algorithm for Thinning\nBinary Patterns", IEEE Trans. Systems, Man, and Cybernetics, SMC-14,\n409 (1984)\n10. W. C. Naylor, "Some Studies in the Interactive Design of Character Recognition Systems", IEEE Transactions on Computers, 1075 (1971)\n11. T. Pavlidis, Algorithms for Graphics and Image Processing, Computer\nScience Press (1982)\n12. C. Y. Suen, M. Berthod, and S. Mori, "Automatic Recognition of Handprinted\nCharacters - The State of the Art", Proceedings of the IEEE 68 4, 469\n(1980).\n13. C-H. Wang and S. N. Srihari, "A Framework for Object Recognition in a Visually Complex Environment and its Application to Locating Address Blocks\non Mail Pieces", IntI. J. Computer Vision 2, 125 (1988)\n14. S. Watanabe, Pattern Recognition, John Wiley and Sons, New York (1985)\n\n331\n\n\x0c'
p83171
sg52
S'728\n\nDIGITAL REALISATION OF SELF-ORGANISING MAPS\nNigel M. Allinson\n\nM~rtin J. Johnson\nDepartment of Electronics\nUniversity of York\nYork\nY015DD\nEngland\n\nKevin J. Moon\n\nABSTRACT\nA digital realisation of two-dimensional self-organising feature\nmaps is presented.\nThe method is based on subspace\nWeight vector\nclassification using an n-tuple technique.\napproximation and orthogonal projections to produce a winnertakes-all network are also discussed. Over one million effective\nbinary weights can be applied in 25ms using a conventional\nmicrocomputer. Details of a number of image recognition tasks,\nincluding character recognition and object centring, are\ndescribed.\n\nINTRODUCTION\nBackground\n\nThe overall aim of our work is to develop fast and flexible systems for image\nrecognition, usually for commercial inspection tasks. There is an urgent need for\nautomatic learning systems in such applications, since at present most systems\nemploy heuristic classification techniques. This approach requires an extensive\ndevelopment effort for each new application, which exaggerates implementation\ncosts; and for many tasks, there are no clearly defined features which can be\nemployed for classification. Enquiring of a human expert will often only produce\n"good" and "bad" examples of each class and not the underlying strategies which\nhe may employ. Our approach is to model in a quite abstract way the perceptual\nnetworks found in the mammalian brain for vision. A back-propagation network\ncould be employed to generalise about the input pattern space, and it would find\nsome useful representations. However, there are many difficulties with this\napproach, since the network structure assumes nothing about the input space and\nit can be difficult to bound complicated feature clusters using hyperplanes. The\nmammalian brain is a layered structure, and so another model may be proposed\nwhich involves the application of many two-dimensional feature maps. Each map\ntakes information from the output of the preceding one and performs some type of\nclustering analysis in order to reduce the dimensionality of the input information.\nFor successful recognition, similar patterns must be topologically close so that\n\n\x0cDigi tal Realisation of Self-Organising Maps\n\nnovel patterns are in the same general area of the feature map as the class they\nare most like. There is therefore a need for both global and local ordering\nprocesses within the feature map. The process of global ordering in a topological\nmap is termed, by Kohonen (1984), as self-organisation.\nIt Is important to realize that all feedforward networks perform only one function,\nnamely the labelling of areas in a pattern space. This paper concentrates on a\ntechnique for realising large, fast, two-dimensional feature maps using a purely\ndigital implementation.\n\nFigure 1. Unbounded Feature Map of Local Edges\nSelf Organisation\nGlobal ordering needs to adapt the entire neural map, but local ordering needs\nonly local information. Once the optimum global organisation has been found,\nthen only more localised ordering can improve the topological organisation. This\nprocess is the basis of the Kohonen clustering algorithm, where the specified area\n\n729\n\n\x0c730\n\nJohnson, Allinson and Moon\n\nof adaption decreases with time to give an increasing local ordering. It has been\nshown that this approach gives optimal ordering at global and local levels (Oja,\n1983). It may be considered as a dimensionality reduction algorithm, and can be\nused as a vector quantiser.\nAlthough Kohonen\'s self-organising feature maps have been successfully applied\nto speech recognition (Kohonen, 1988; Tattersall et aI., 1988), there has been little\nInvestigation in their application for image recognition. Such feature maps can be\nused to extract various image primitives, such as textures, localised edges and\nterminations, at various scales of representations (Johnson and Allinson, 1988).\nAs a simple example, a test image of concentric circles is employed to construct a\nsmall feature map of localised edges (Figure 1). The distance measure used is the\nnormalised dot product since in general magnitude information is unimportant.\nUnder these conditions, each neuron output can be considered a similarity\nmeasure of the directions between the input pattern and the synaptic weight\nvector. This map shows that similar edges have been grouped together and that\ninverses are as far from each other as possible.\n\nDIGITAL IMPLEMENTATION\nSub-Space Classification\nAlthough a conventional serial computer is normally thought of as only performing\none operation at a time, there is a task which it can successfully perform involving\nparallel computation. The action of addressing memory can be thought of as a\nhi&JhlY parallel process, since it involves the comparison of a word, W, with a set ~\n2 others where N is the number of bits in W. It is, in effect, performing 2\nparallel computations - each being a single match. This can be exploited to speed\nup the simulation of a network by using a conversion between conventional\npattern space labelling and binary addressing.\nFigure 2 shows how the labelling of two-dimensional pattern space is equivalent to\nthe partitioning of the same space by the decision regions of a multiple layer\nperceptron. If each quantised part of the space is labelled with a number for each\nclass then all that is necessary is for the pattern to be used as an address to give\nthe stored label (i.e. the response) for each class. These labels may form a cluster\nof any shape and so multiple layers are not required to combine regions.\nThe apparent flaw in the above suggestion is that for anything other than a trivial\nproblem, the labelling of every part of pattern space is impractical. For example a\n32 x 32 input vector would require a memory of 2 1024 words per unit! What is\nneeded is a coding system which uses some basic assumptions about patterns in\norder to reduce the memory requirements. One assumption which can be made\nis that patterns will cI uster together into various classes. As early as 1959, a\nmethod known as the n-tuple technique was used for pattern recognition (Bledsoe\nand Browning, 1959). This technique takes a number of subspaces of the pattern\n\n\x0cDigital Realisation of Self-Organising Maps\n\nPERCEPTRON\nc1/c2\n\nx2\n\nx1\n\n~=C1\n\n=c2\n\nI\' I,\' ,I,\' ~I,J::-I\' 1.-1-1?\' 1\' 1? I?\'\nI.;:I~\'I~I-::I \' I,\nI? ~~t:\'\n\nl\n\nI? ~ ~~\n\n~~~\n\n....t It ..\n,- II ?\n\n010 I. \' .\' I,\n\n? ? ~\'I\nf\n\n.\n\n.ltl\n\n~.\n\nI- I.\n\n??\n\n,\n\nI-\n\nI_\n\n.?\n~~\n\nI .? ~\nI"~ ~~~ ~~~~\n~\ni\' I, I.\'\n\nI? ?\n? I?\n\nx2\n\n~\n\n"I.\'\n\n~\n\n~\n\n~~\n\n\'.\nII ? .\'\n\nI~. ,\n1:\\ ,\n\n~~\n\n1--1\' ~~\n, 1,\' 1\' ~\nI\'\n?\nI,\' , I"\n1411. Itili ?\n1.\'1,\ni" "\n\n1\'1 ?\n\nI~r.-r~\n\n,\n\n\'\n\n~\n\n~ .\'\n~\n\n.\n\nLABELING\nThe labeling of a quantized\nsubspace is equivalent to\nthe partitioning of pattern\nspace by the multi-layer\nperceptron.\n\nI,\'\nI?\n\ni? - It It 11111.\n?1? 1?1?1-\n\n? = Class 1 0\n\n= Class 2\n\nFigure 2. Comparison of Perceptron and Sub-Space Classification\nspace and uses the sum of the resultant labels as the overall response. This gives\na set of much smaller memories and inherent in the coding method is that similar\npatterns will have identical labels.\nFor example, assume a 16 bit pattern - 0101101001010100. Taking a four-bit\nsample from this, say bits 0-3, giving 0100. This can be used to address a 16 word\nmemory to produce a single bit. If this bit is set to 1, then it is in effect labelling all\npatterns with 0100 as their first four bits; that is 4096 patterns of the form\nxxxxxxxxxxxx0100. Taking a second sample, namely bits 4-7 (0101). This labels\nxxxxxxxx0101xxxx patterns, but when added to the first sample there will be 256\npatterns labelled twice (namely, xxxxxxxx01010100) and 7936 (Le. 8192-256)\nlabelled once.\nThe third four-bit sample produces 16 patterns (namely,\n\n731\n\n\x0c732\n\nJohnson, Allinson and Moon\n\nxxx(101001010100) labelled three times. The fourth sample produces only one\npattem 0101101001010100, which has been labelled four times. If an input pattern\nis applied which differs from this by one bit, then this will now be labelled three\ntimes by the samples; if it differs by two bits, it will either be labelled two or three\ntimes depending on whether the changes were in the same four-bit sample or not.\nThus a distance measure is implicit in the coding method and reflects the\nassumed clustering of patterns. Applying this approach to the earlier problem of a\n32 x 32 binary input vector and taking 128 eight-bit samples results in a distance\nmeasure between 0 and 128 and uses 32K bits of memory per unit.\nWeight Vector Approximation\nIt is possible to make an estimate of the approximate weight vector for a particular\nsample from the bit table. For simplicity, consider a binary image from which t\nsamples are taken to form a word, w, where\nt-1\nw = xo + 2x 1 + .... + 2 ~-1\nThis word can be used to address a vector W. Every bit in W[b] which is 1 either\nincreases the weight vector probability where the respective bit in the address is\nset, or decreases if it is clear. Hence, if BIT [w,i] is the ith bit of wand A[i] is the\ncontents of the memory {O, 1} then,\n\n2 t -1\n\nE\n\nW[b] =\ni= 0\n\nA[i] (2 BIT(b,i) -1)\n\nThis represents an approximate measure of the weight element. Table 1\ndemonstrates the principle for a four-bit sample memory. Given randomly\ndistributed inputs this binary vector is equivalent to the weight vector [2, 4, 0, -2].\nIf there is a large number of set bits in the memory for a particular unit then that\nwill always give a high response - that is, it will become saturated. However, if\nthere are too few bits set, this unit will not rfiSpond strongly to a general set of\npatterns. The number of bits must, therefore, be fixed at the start of training,\ndistributed randomly within the memory and only redistribution of these bits\nallowed. Set bits could be taken from any other sample, but some samples will be\nmore important than others. The proportion of 1\'s in an image should not be used\nas a measure, otherwise large uniform regions will be more significant than the\npattern detail. This is a form of magnitude independent operation similar to the\nuse of the normalised dot product applied in the analogue approach and so bits\nmay only be moved from addresses with the same number of set bits as the\ncurrent address.\n\n\x0cDigital Realisation of Self-Organising Maps\n\nTABLE 1. Weight Vector Approximation\nAddress\nX3 x2 x,\n\nXo\n\nA\n\nWeight change\nW3 W2 W, W0\n\nAddress\nx3 x2 x,\n\nXo\n\nA\n\nWeight change\nW3 W2 W, Wo\n\n+\n\n-\n\n0\n\n0\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n1\n\n0\n\n1\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n1\n\n0\n\n0\n\n1\n\n0\n\n1\n\n0\n\n0\n\na\n\n0\n\n1\n\n1\n\n1\n\n+ +\n\n1\n\n0\n\n1\n\n1\n\n0\n\n0\n\n1\n\n0\n\n0\n\n1\n\n-\n\n1\n\n1\n\n0\n\n0\n\n1\n\n+\n\n+\n\n-\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n1\n\n0\n\n1\n\n1\n\n+ +\n\n-\n\n0\n\n1\n\n1\n\n0\n\n1\n\n1\n\n1\n\n1\n\n0\n\n1\n\n+\n\n+ + -\n\n0\n\n1\n\n1\n\n1\n\n0\n\n1\n\n1\n\n1\n\n1\n\n1\n\n+\n\n+\n\n+\n\n2\n\n4\n\n0-2\n\n+\n\n+\n\n+\n\n-\n\nEquivalent weight vector\n\n+\n\n+\n\nOrthogonal Projections\nIn order to speed up the simulation further, instead of representing each unit by a\nsingle bit in memory, each unit can be represented by a combination of bits.\nHence many calculations can be effectively computed in parallel. The number of\nunits which require a 1 for a particular sample will always be relatively small, and\nhence these can be coded. The coding method employed is to split the binary\nword, W, into x and y fields. These projection fields address a two dimensional\nmap and so provide a fast technique of approximating the true content of the\nmemory. The x bits are summed separately to the y bits, and together they give a\ngood estimate of the unit co-ordinates with the most bits set in x and in y. This\nmap becomes, in effect, a winner-takes-all network. The reducing neighbourhood\nof adaption employed in the Kohonen algorithm can also be readily incorporated\nby applying an overall mask to this map during the training phase.\nThough only this output map is required during normal application of the system\nto image recognition tasks, it is possible to reconstruct the distribution of the twodimensional weight vectors. Figure 3, using the technique illustrated in Table 1,\nshows this weight vector map for the concentric circle test image applied\n\n733\n\n\x0c734\n\nJohnson, Allinson and Moon\n\nFigure 3. Reconstructed Feature Map of Local Edges\npreviously in the conventional analogue approach. This is a small digitised map\ncontaining 32 x 32 elements each with 16 x 16 input units and can be applied,\nusing a general purpose desktop microcomputer running at 4 mips, in a few\nmilliseconds.\n\nAPPLICATION EXAMPLES\nCharacter Recognition\nThough a long term objective remains the development of general purpose\ncomputer vision systems, with many layers of interacting feature maps together\nwith suitable pre- and post-processing, many commercial tasks require decisions\nbased on a constricted range of objects - that is their perceptual set is severely\nlimited. However, ease of training and speed of application are paramount. An\nexample of such an application involves the recognition of characters.\nFigures 4 and 5 show an input pattern of hand-drawn A\'s and B\'s. The network,\nusing the above digital technique, was given no information concerning the input\nimage and the input window of 32 x 32 pixels was placed randomly on the image.\n,The network took less than one minute to adapt and can be applied in 25 ms. This\nnetwork is a 32 x 32 feature map of 32 x 32 elements, thus giving over one million\neffective weights. The output map forms two distinct clusters, one for A\'s in the\ntop right corner of the map (Figure 4), and one for B\'s in the bottom left corner\n(Figure 5). If further characters are introduced in the input image then the output\nmap will, during the training phase, self-organise to incorporate them.\n\n\x0cDigital Realisation of Self-Organising Maps\n\nFigure 4. Trained Network Response for \'A\' in Input Window\n\nFigure 5. Trained Network Response for \'B\' in Input Window\n\n735\n\n\x0c736\n\nJohnson, Allinson and Moon\n\nCorrupted Images\nOnce the maximum response from the map is known, then the parts of the input\nwindow which caused it can be reconstructed to provide a form of ideal input\npattern. The reconstructed input pattern is shown in the figures beneath the input\nimage. This reconstruction can be employed to recognise occuluded patterns or\nto eliminate noise in subsequent input images.\n\nFigure 6. Trained Network Response for Corrupted \'A\' in Input Window.\nReconstructed Input Pattern Shown Below Test Image\nFigure 6 shows the response of the network, trained on the input image of Figures\n4 and 5, to a corrupted image of A\'s and B\'s. It has still managed to recognise the\ninput character as an A, but the reconstructed version shows that the extra noise\nhas been eliminated.\nObject Centring\nThe centering of an object within the input window permits the application of\nconformant mapping strategies, such as polar exponential grids, to be applied\nwhich yields scale and rotation invariant recognition. The same network as\nemployed in the previous example was used, but a target position for the\nmaximum network response was specified and the network was adapted half-way\nbetween this and the actual maximum response location.\n\n\x0cDigital Realisation of Self-Organising Maps\n\nFigure 7. Trained Network Response for Off-Centred Character. Input Window is\nLow-Pass Filtered as shown.\nFigure 7 shows such a network. When the response is in the centre of the output\nmap then an input object (character) is centred in the recognition window. In the\nexample shown, there is an off-centred response of the trained network for an offcentred character. This deviation is used to change the position of the input\nwindow. Once centering has been achieved, object recognition can occur.\n\nCONCLUSIONS\nThe application of unsupervised feature maps for image recognition has been\ndemonstrated. The digital realisation technique permits the application of large\nmaps. which can be applied in real time using conventional microcomputers. The\nuse of orthogonal projections to give a winner-take-all network reduces memorY\nrequirements by approximately 3D-fold and gives a computational cost of O(n 1/2),\nwhere n is the number of elements in the map. The general approach can be\napplied in any form of feedforward neural network.\nAcknowledgements\nThis work has been supported by the Innovation and Research Priming Fund of\nthe University of York.\n\n737\n\n\x0c738\n\nJohnson, Allinson and Moon\n\nReferences\nW. W. Bledsoe and I. Browning. Pattern Recognition and Reading by Machine.\nProc. East. Joint Compo Conf., 225-232 (1959).\nM. J. Johnson and N. M. Allinson. An Advanced Neural Network for Visual Pattern\nRecognition. Proc. UKIT 88, Swansea, 296-299 (1988).\nT. Kohonen. Self Organization and Associative Memory. Springer-Vertag, Bertin\n(1984).\nT. Kohonen. The \'Neural\' Phonetic Typewriter. Computer21,11-22 (1988).\nE. Oja.\n\nSubspace Methods of Pattern Recognition.\nLetchworth (1983).\n\nResearch Studies Press,\n\nG. D. Tattersall, P. W. Linford and R. Linggard. Neural Arrays for Speech\nRecognition. Br. Telecom Techno/. J. Q. 140-163 (1988).\n\n\x0c'
p83172
sg114
S'340\n\nBACKPROPAGATION AND ITS\nAPPLICATION TO HANDWRITTEN\nSIGNATURE VERIFICATION\nDorothy A. Mighell\nElectrical Eng. Dept.\nInfo. Systems Lab\nStanford University\nStanford, CA 94305\n\nTimothy S. Wilkinson\nElectrical Eng. Dept.\nInfo. Systems Lab\nStanford University\nStanford, CA 94305\n\nJoseph W. Goodman\nElectrical Eng. Dept.\nInfo. Systems Lab\nStanford University\nStanford, CA 94305\n\nABSTRACT\nA pool of handwritten signatures is used to train a neural network for the task of deciding whether or not a given signature is a\nforgery. The network is a feedforward net, with a binary image as\ninput. There is a hidden layer, with a single unit output layer. The\nweights are adjusted according to the backpropagation algorithm.\nThe signatures are entered into a C software program through the\nuse of a Datacopy Electronic Digitizing Camera. The binary signatures are normalized and centered. The performance is examined\nas a function of the training set and network structure. The best\nscores are on the order of 2% true signature rejection with 2-4%\nfalse signature acceptance.\n\nINTRODUCTION\nSignatures are used everyday to authorize the transfer of funds for millions of people.\nWe use our signature as a form of identity, consent, and authorization. Bank checks,\ncredit cards, legal documents and waivers all require the everchanging personalized\nsignature. Forgeries on such transactions amount to millions of dollars lost each\nyear. A trained eye can spot most forgeries, but it is not cost effective to handcheck\nall signatures due to the massive number of daily transactions. Consequently, only\ndisputed claims and checks written for large amounts are verified. The consumer\nwould certainly benefit from the added protection of automated verification. Neural\nnetworks lend themselves very well to signature verification. Previously, they have\nproven applicable to other signal processing tasks, such as character recognition\n{Fukishima, 1986} {Jackel, 1988}, sonar target classification {Gorman, 1986}, and\ncontrol- as in the broom balancer {Tolat, 1988}.\n\nHANDWRITING ANALYSIS\nSignature verification is only one aspect of the study of handwriting analysis.\nRecognition is the objective, whether it be of the writer or the characters. Writer\nrecognition can be further broken down into identification and verification. Identi-\n\n\x0cBackpropagation and Handwritten Signature Verification\n\nfication selects the author of a sample from among a group of writers. Verification\nconfirms or rejects a written sample for a single author. In both cases, it is the\nstyle of writing that is important.\nDeciphering written text is the basis of character recognition. In this task, linguistic\ninformation such as the individual characters or words are extracted from the text.\nStyle must be eliminated to get at the content. A very important application of\ncharacter recognition is automated reading of zip-codes in the post office {Jackel,\n1988}.\nData for handwriting analysis may be either dynamic or static. Dynamic data\nrequires special devices for capturing the temporal characteristics of the sample.\nFeatures such as pressure, velocity, and position are examined in the dynamic\nframework. Such analysis is usually performed on-line in real time.\nStatic analysis uses the final trace of the writing, as it appears on paper. Static\nanalysis does not require any special processing devices while the signature is being\nproduced. Centralized verification becomes possible, and the processing may be\ndone off-line.\nWork has been done in both static and dynamic analysis {Sato, 1982} {Nemcek,\n1974}. Generally, signature verification efforts have been more successful using\nthe dynamic information. It would be extremely useful though, to perform the\nverification using only the written signature. This would eliminate the need for\ncostly machinery at every place of business. Personal checks may also be verified\nthrough a static signature analysis.\n\nTASK\nThe handwriting analysis task with which this paper is concerned is that of signature verification using an off-line method to detect casual forgeries. Casual forgeries\nare non-professional forgeries, in which the writer does not practice reproducing\nthe signature. The writer may not even have a copy of the true signature. Casual\nforgeries are very important to detect. They are far more abundant, and involve\ngreater monetary losses than professional forgeries. This signature verification task\nfalls into the writer recognition category, in which the style of writing is the important variable. The off-line analysis allows centralized verification at a lower cost\nand broader use.\n\nHANDWRITTEN SIGNATURES\nThe signatures for this project were gathered from individuals to produce a pool\nof 80 true signatures and 66 forgeries. These are signatures, true and false, for one\nperson. There is a further collection of signatures, both true and false, for other\npersons, but the majority of the results presented will be for the one individual. It\nwill be clear when other individuals are included in the demonstration.\nThe signatures are collected on 3x5 index cards which have a small blue box as\n\n341\n\n\x0c342\n\nWilkinson, Mighell and Goodman\n\na guideline. The cards are scanned with a CCD array camera from Datacopy,\nand thresholded to produce binary images. These binary images are centered and\nnormalized to fit into a 128x64 matrix. Either the entire 128x64 image is presented\nas input, or a 90x64 image of the three initials alone is presented. It is also possible\nto present preprocessed inputs to the network.\n\nSOFTWARE SIMULATION\nThe type of learning algorithm employed is that of backpropagation. Both dwell\nand momentum are included. Dwell is the type of scheduling employed, in which\nan image is presented to the network, and the network is allowed to "dwell" on that\ninput for a few iterations while updating its weights. C. Rosenberg and T. Sejnowski\nhave done a few studies on the effects of scheduling on learning {Rosenberg, 1986}.\nMomentum is a term included in the change of weights equation to speed up learning\n{Rumelhart, 1986}.\nThe software is written in Microsoft C, and run on an IBM PC/AT with an 80287\nmath co-processor chip.\nIncluded in the simulation is a piece-wise linear approximation to the sigmoid transfer function as shown in Figure 1. This greatly improves the speed of calculation,\nbecause an exponential is not calculated. The non-linearity is kept to allow for\nlayering of the network. Most of the details of initialization and update are the\nsame as that reported in NetTalk {Sejnowski, 1986}.\n\nOUT\n\n~-111111::::+~----\'.\n\nIN\n\nFigure 1. Piece-wise linear transfer function.\nMany different nets were trained in this signature verification project, all of which\nwere feed-forward. The output layer most often consisted of a single output neuron,\nbut 5 output neurons have been used as well. If a hidden layer was used, then\nthe number of hidden units ranged from 2 to 53. The networks were both fullyconnected and partially-connected.\n\nSAMPLE RUN\nThe simplest network is that of a single neuron taking all 128x64 pixels as input,\nplus one bias. Each pixel has a weight associated with it, so that the total number\nof weights is 128x64 + 1 = 8193. Each white pixel is assigned an input value of + 1,\neach black pixel has a value of -1. The training set consists of 10 true signatures\n\n\x0cBackpropagation and Handwritten Signature Verification\n\nwith 10 forgeries. Figure 2a depicts the network structure of this sample run.\n\nOUT\n\nc::\n\n1\n\n0\n\n-"\nu\n\nCD\nCD\n\n0.5\n\n-\n\nf- ..\n\nCD\n\n-:J\n"-\n\nQ.\n\n0\n\n0.5\n\n0\nP(false\n\n~1~111J\n\n111.\n\n1\n\nacceptance)\n\n(b)\n\n"~~~mlla\n1\n\n1/\n\n(a)\n\n~\n\nen\n\nLL.\n\nC\n\n0\n\n0.5\n\nf\n\n0\n0\n(e)\n\n0.5\n\n1\n\nOutput Values\n\n(d)\nFigure 2. Sample run.\na) Network = one output neuron, one weight per pixel, fully connected. Training set = 10 true signatures + 10 forgeries.\nb) ROC plot for the sample run. (Probability of fa1se acceptance\nvs probability of true detection). Test set = 70 true signatures\n+ 56 forgeries.\nc) Clipped picture of the weights for the sample run. White\npositive weight, black = negative weight.\n\n=\n\nd) Cumulative distribution function for the true signatures (+) and\nfor the forgeries (0) of the sample run.\nThe network is trained on these 20 signatures until all signatures are classified\n\n343\n\n\x0c344\n\nWilkinson, Mighell and Goodman\n\ncorrectly. The trained network is then tested on the remaining 70 true signatures\nand 56 forgeries.\nThe results are depicted in Figures 2b and 2d. Figure 2b is a radar operating\ncharacteristic curve, or roc plot for short. In this presentation of data, the probability of detecting a true signature is plotted against the probability of accepting a\nforgery. Roc plots have been used for some time in the radar sciences as a means\nfor visualizing performance {Marcum, 1960}. A perfect roc plot has a right angle\nin the upper left-hand corner which would show perfect separation of true signatures from forgeries. The curve is plotted by varying the threshold for classification.\nEverything above the threshold is labeled a true signature, everything below the\nthreshold is labeled a forgery. The roc plot in Figure 2b is close to perfect, but\nthere is some overlap in the output values of the true signatures and forgeries. The\noverlap can be seen in the cumulative distribution functions (cdfs) for the true and\nfalse signatures as shown in Figure 2d. As seen in the cdfs, there is fairly good\nseparation of the output values. For a given threshold of 0.5, the network produces\n1% rejection of true signatures as false, with 4% acceptance of forgeries as being\ntrue. IT one lowers the threshold for classification down to 0.43, the true rejection\nbecomes nil, with a false acceptance of 7% . A simplified picture of the weights is\nshown in Figure 2c, with white pixels designating positive weights, and black pixels\nnegative weights.\n\nOTHER NETWORKS\nThe sample run above was expanded to include 2 and 3 hidden neurons with the\nsingle output neuron. The results were similar to the single unit network, implying\nthat the separation is linear.\nThe 128x64 input image was also divided into regions, with each region feeding into\na single neuron. In one network structure, the input was sectioned into 32 equally\nsized regions of 16x16 pixels. The hidden layer thus has 32 neurons, each neuron\nreceiving 16x16 + 1 inputs. The output neuron had 33 inputs. Likewise, the input\nimage was divided into 53 regions of 16x16 pixels, this time overlapping.\nFinally, only the initials were presented to the network. (Handwriting experts\nhave noted that leading strokes and separate capital letters are very significant in\nclassification {Osborn, 1929}.) In this case, two types of networks were devised.\nThe first had a single output neuron, the second had three hidden neurons plus one\noutput neuron. Each of the hidden neurons received inputs from only one initial,\nrather than from all three. The network with the single output neuron produced\nthe best results of all, with 2% true rejection and 2% false acceptance.\n\nIMPORTANCE OF FORGERIES IN THE TRAINING SET\nIn all cases, the networks performed much better when forgeries were included in the\ntraining set. When an all-white image is presented as the only forgery, performance\ndeteriorates significantly. When no forgeries are present, the network decides that\n\n\x0cBackpropagation and Handwritten Signature Verification\n\nall signatures are true signatures. It is therefore desirable to include actual forgeries\nin the training set, yet they may be impractical to obtain. One possibility for\navoiding ?the collection of forgeries is to use computer generated forgeries. Another\nis to distort the true signatures. A third is to use true signatures of other people as\nforgeries for the person in question. The attraction of this last option is that the\nmasquerading forgeries are already available for use.\n\nNETWORK WITHOUT FORGERIES\nTo test the use of true signatures of other people for forgeries, the following network\nis devised. Once again, the input is the 128x64 pixel image. The output layer is\ncomprised of five output neurons fully connected to the input image. The function\nof each output neuron is to be active when presented with a particular persons\'\nsignature. When a forgery is present, the output is to be low. Figure 3a depicts this\nnetwork. The training set has 50 true signatures, ten for each of five people. Each\nsignature has a desired output of true for one neuron, and false for the remaining\nfour neurons. Once the network is trained, it is tested on 210 true signatures and\n150 forgeries. Figures 3b and 3c record the results. At a threshold of 0.5, the true\nrejection is 3% and the false acceptance is 14%. Decreasing the threshold down to\n0.41 gives 0% true rejection and 28% false acceptance. These results are similar\nto the sample run, though not as good. This is a simple demonstration of the use\nof other true signatures as forgeries. More sophisticated techniques could improve\nthe discrimination. For instance, selecting names with similar lengths or spelling\nshould improve the classification.\n\nCONCLUSION\nAutomated signature verification systems would be extremely important in the\nbusiness world for verifying monetary transactions. Countless dollars are lost each\nday to instances of casual forgeries. An artificial neural network employing the\nbackpropagation learning algorithm has been trained on both true and false signatures for classification. The results have been very good: 2% rejection of genuine\nsignatures with 2% acceptance of forgeries. The analysis requires only the static\npicture of the signature, there by offering widespread use through centralized verification. True signatures of other people may substitute for the forgeries in the\ntraining set - eliminating the need for collecting non-genuine signatures.\n\n345\n\n\x0c346\n\nWilkinson, Mighell and Goodman\n\nJWG JTH TSW LDK ABH\n\n- r--::iif1l---------..\noC\n\n(a)\nlr-----------~~~--_=~\n\n1\n\n( ,)\n\nCD\n\nQ)\n\n.-\n\n(f.\n\nI\n\n~\n\n00.5\n\n"t:S U.5\n\no\n\nQ)\n\n::J\n\n~\n\no~----------~--------~\n\no\n\n0.5\n\nP(false\n\nacceptance)\n\n1\n\no~~----~~~--------~\n\no\n\n0.5\n\n1\n\nOutput Values\n\n(b)\n\n(c)\n\nFigure 3. Network without forgeries for 5 individuals.\na) Network\n5 output neurons, one for each individua~ as indicated by the initials. Training set = 10 true signatures for each\nindividual.\n\n=\n\nb) ROC plot for the network without forgeries.\n210 true signatures + 150 forgeries.\nTest set\n\n=\n\nc) Cumulative distribution function for the true signatures (+) and\nfor the forgeries (0) of the network without forgeries.\n\nReferenees\nK. Fukishima and S. Miyake, "Neocognitron: A biocybernetic approach to visual\npattern recognition Jt , in NHK Laboratorie~ Note, Vol. 336, Sep 1986 (NHK\nScience and Technical Research Laboratories, Tokyo).\n\n\x0cBackpropagation and Handwritten Signature Verification\n\nP. Gorman and T. J. Sejnowski, "Learned classification of sonar targets using a\nmassively parallel network", in the proceedings of the IEEE ASSP Oct 21,\n1986 DSP Workshop, Chatham, MA.\nL. D. Jackel, H. P. Graf, W. Hubbard, J. S. Denker, and D. Henderson, "An\napplication of neural net chips: handwritten digit recognition", in IEEE International Oonference on Neural Networks 1988, II 107-115.\nJ. T. Marcum, "A statistical theory of target detection by pulsed radar", in IRE\nTransactions in Information Theory, Vol. IT-6 (Apr.), pp 145-267, 1960.\nW. F. Nemcek and W. C. Lin, "Experimental investigation of automatic signature\nverification" in IEEE Transactions on Systems, Man, and Oybernetics, Jan.\n1974, pp 121-126.\nA. S. Osborn, Questioned Documents, 2nd edition (Boyd Printing Co, Albany NY)\n1929.\nC. R. Rosenberg and T. J. Sejnowski, "The spacing effect on NETtalk, a massively parallel network", in Proceedings of the Eighth Annual Oonference of\nthe Oognitive Science Society, (Hillsdale, New Jersey: Lawrence Erlbaum\nAssociates, 1986) 72-89.\nD. E. Rumelhart, G. E. Hinton, and R. J. Williams, "Learning internal representations by error propagation", in Parallel Distributed Processing: Explorations\nin the Microstructures of Oognition. Vol. 1: Foundations, edited by D. E.\nRumelhart & J. L. McClelland, (MIT Press, 1986).\nY. Sato and K. Kogure, "Online signature verification based on shape, motion,\nand writing pressure", in Proceedings of the 6th International Oonference on\nPattern Recognition, Vol. 2, pp 823-826 (IEEE NY) 1982.\nT. J. Sejnowski and C. R. Rosenberg, "NETtalk: A Parallel Network that Learns\nto Read Aloud", Johns Hopkins University Department of Electrical Engineering and Computer Science Technical Report JHU /EECS-86/01, (1986).\nV. V. Tolat and B. Widrow, "An adaptive \'broom balancer\' with visual inputs" , in\nIEEE International Oonference on Neural Networks 1988, II 641-647.\n\n347\n\n\x0c'
p83173
sg128
S'An experimental comparison\nof recurrent neural networks\nBill G. Horne and C. Lee Giles?\nNEe Research Institute\n4 Independence Way\nPrinceton, NJ 08540\n{horne.giles}~research.nj.nec.com\n\nAbstract\nMany different discrete-time recurrent neural network architectures have been proposed. However, there has been virtually no\neffort to compare these arch:tectures experimentally. In this paper\nwe review and categorize many of these architectures and compare\nhow they perform on various classes of simple problems including\ngrammatical inference and nonlinear system identification.\n\n1\n\nIntroduction\n\nIn the past few years several recurrent neural network architectures have emerged.\nIn this paper we categorize various discrete-time recurrent neural network architectures, and perform a quantitative comparison of these architectures on two problems: grammatical inference and nonlinear system identification.\n\n2\n\nRNN Architectures\n\nWe broadly divide these networks into two groups depending on whether or not the\nstates of the network are guaranteed to be observable. A network with observable\nstates has the property that the states of the system can always be determined from\nobservations of the input and output alone. The archetypical model in this class\n.. Also with UMIACS, University of Maryland, College Park, MD 20742\n\n\x0c698\n\nBill G. Horne, C. Lee Giles\n\nTable 1: Terms that are weighted in various single layer network architectures. Ui\nrepresents the ith input at the current time step, Zi represents the value of the lh\nnode at the previous time step.\nArchitecture\nFirst order\nHigh order\nBilinear\nQuadratic\n\nbias\nx\nx\n\nUi\n\nZi\n\nx\n\nx\n\nx\nx\n\nx\nx\n\nUiUj\n\nZiUj\n\nZiZj\n\nx\n\nx\nx\nx\n\nx\n\nwas proposed by Narendra and Parthasarathy [9]. In their most general model, the\noutput of the network is computed by a multilayer perceptron (MLP) whose inputs\nare a window of past inputs and outputs, as shown in Figure la. A special case of\nthis network is the Time Delay Neural Network (TDNN), which is simply a tapped\ndelay line (TDL) followed by an MLP [7]. This network is not recurrent since there\nis no feedback; however, the TDL does provide a simple form of dynamics that\ngives the network the ability model a limited class of nonlinear dynamic systems.\nA variation on the TDNN, called the Gamma network, has been proposed in which\nthe TDL is replaced by a set of cascaded filters [2]. Specifically, if the output of\none of the filters is denoted xj(k), and the output of filter i connects to the input\nof filter j, the output of filter j is given by,\n\nxj(k + 1) = I-\'xi(k) + (l-I-\')xj(k).\nIn this paper we only consider the case where I-\' is fixed, although better results can\nbe obtained if it is adaptive.\nNetworks that have hidden dynamics have states which are not directly accessible\nto observation. In fact, it may be impossible to determine the states of a system\nfrom observations of it\'s inputs and outputs alone. We divide networks with hidden dynamics into three classes: single layer networks, multilayer networks, and\nnetworks with local feedback.\nSingle layer networks are perhaps the most popular of the recurrent neural network\nmodels. In a single layer network, every node depends on the previous output of\nall of the other nodes. The function performed by each node distinguishes the\ntypes of recurrent networks in this class. In each of the networks, nodes can be\ncharacterized as a nonlinear function of a weighted sum of inputs, previous node\noutputs, or products of these values. A bias term may also be included. In this\npaper we consider first-order networks, high-order networks [5], bilinear networks,\nand Quadratic networks[12]. The terms that are weighted in each of these networks\nare summarized in Table 1.\nMultilayer networks consist of a feedforward network coupled with a finite set of\ndelays as shown in Figure lb. One network in this class is an architecture proposed\nby Robinson and Fallside [11], in which the feedforward network is an MLP. Another\npopular networks that fits into this class is Elman\'s Simple Recurrent Network\n(SRN) [3]. An Elman network can be thought of as a single layer network with an\nextra layer of nodes that compute the output function, as shown in Figure lc.\nIn locally recurrent networks the feedback is provided locally within each individual\n\n\x0cAn Experimental Comparison of Recurrent Neural Networks\n\n699\n\nMLP\n\nFigure 1: Network architectures: (a) Narendra and Parthasarathy\'s Recurrent Neural Network, (b) Multilayer network and (c) an Elman network.\n\nnode, but the nodes are connected together in a feed forward architecture. Specifically, we consider nodes that have local output feedback in which each node weights\na window of its own past outputs and windows of node outputs from previous layers.\nNetworks with local recurrence have been proposed in [1, 4, 10].\n\n3\n3.1\n\nExperimental Results\nExperimental methodology\n\nIn order to make the comparison as fair as possible we have adopted the following\nmethodology.\n?\n\n?\n\n?\n\nResources. We shall perform two fundamental comparisons. One in which the\nnumber of weights is roughly the same for all networks, another in which the\nnumber of states is equivalent. In either case, we shall make these numbers large\nenough that most of the networks can achieve interesting performance levels.\nNumber of weights. For static networks it is well known that the generalization\nperformance is related to the number of weights in the network. Although this\ntheory has never been extended to recurrent neural networks, it seems reasonable\nthat a similar result might apply. Therefore, in some experiments we shall try\nto keep the number of weights approximately equal across all networks.\nNumber of states. It can be argued that for dynamic problems the size of the\nstate space is a more relevant measure for comparison than the number of\nweights. Therefore, in some experiments we shall keep the number of states\nequal across all networks.\nVanilla learning. Several heuristics have been proposed to help speed learning\nand improve generalization of gradient descent learning algorithms. However,\nsuch heuristics may favor certain architectures. In order to avoid these issues,\nwe have chosen simple gradient descent learning algorithms.\nNumber of simulations. Due to random initial conditions, the recurrent\nneural network solutions can vary widely. Thus, to try to achieve a statistically\nsignificant estimation of the generalization of these networks, a large number of\nexperiments were run.\n\n\x0c700\n\nBill G. Horne, C. Lee Giles\n\no\n\nstan\n\n);::===:====,O\'l+------ll\no\no\n\nFigure 2: A randomly generated six state finite state machine.\n\n3.2\n\nFinite state machines\n\nWe chose two finite state machine (FSM) problems for a comparison of the ability of\nthe various recurrent networks to perform grammatical inference. The first problem\nis to learn the minimal, randomly generated six state machine shown in Figure 2.\nThe second problem is to infer a sixty-four state finite memory machine [6] described\nby the logic function\n\ny(k) = u(k - 3)u(k)\n\n+ u(k -\n\n3)y(k - 3) + u(k)u(k - 3)Y(k - 3)\n\nwhere u(k) and y(k) represent the input and output respectively at time k and x\nrepresents the complement of x.\nTwo experiments were run. In the first experiment all of the networks were designed\nsuch that the number of weights was less than, but as close to 60 as possible. In the\nsecond experiment, each network was restricted to six state variables, and if possible,\nthe networks were designed to have approximately 75 weights. Several alternative\narchitectures were tried when it was possible to configure the architecture differently\nand yield the same number of weights, but those used gave the best results.\nA complete set of 254 strings consisting of all strings of length one through seven is\nsufficient to uniquely identify both ofthese FSMs. For each simulation, we randomly\npartitioned the data into a training and testing set consisting of 127 strings each.\nThe strings were ordered lexographically in the training set.\nFor each architecture 100 runs were performed on each problem. The on-line Back\nPropagation Through Time (BPTT) algorithm was used to train the networks.\nVanilla learning was used with a learning rate of 0.5. Training was stopped at 1000\nepochs. The weights of all networks were initialized to random values uniformly\ndistributed in the range [-0.1,0.1]. All states were initialize to zeros at the beginning of each string except for the High Order net in which one state was arbitrarily\ninitialized to a value of 1.\nTable 2 summarizes the statistics for each experiment. From these results we draw\nthe following conclusions.\n?\n\n?\n\nThe bilinear and high-order networks do best on the small randomly generated\nmachine, but poorly on the finite memory machine. Thus, it would appear that\nthere is benefit to having second order terms in the network, at least for small\nfinite state machine problems.\nNarendra and Parthasarathy\'s model and the network with local recurrence do\nfar better than the other networks on the problem of inferring the finite memory\n\n\x0cAn Experimental Comparison of Recurrent Neural Networks\n\n701\n\nTable 2: Percentage classification error on the FSM experiment for (a) networks with\napproximately the same number of weights, (b) networks with the same number of\nstate variables. %P = The percentage of trials in which the training set was learned\nperfectly, #W = the number of weights, and #S = the number of states.\nF5M\n\nArchitecture t\n\nN&P\nTDNN\nRND\n\nGamma\nFirst Order\nHigh Order\nBilinear\nQuadratic\nMullilayer\nElman\nLocal\n\nN&P\nTDNN\nFMM\n\nGamma\nFirst Order\nHigh Order\nBilinear\nQuadratic\nMultilayer\nElman\nLocal\n\ntraining\nmean\n2 .8\n12.5\n19.6\n12.9\n0.8\n1.3\n12.9\n19 .4\n3.5\n2. 8\n0 .0\n6.9\n7.7\n4 .8\n5.3\n9 .5\n32.5\n36. 7\n12.0\n0.1\n\nerror\n( std)\n\n(M)\n(2.1)\n\n(H)\n(6.9)\n\n(1.5)\n(2 . 7)\n(13.4)\n(13 .6)\n\n~5.~~\n1.5\n~0 . 2 ~\n(2 .1 )\n(2 .2)\n(3 .0)\n\n(4.0)\n(10 .4)\n(10.8)\n(11.9)\n(12.5)\n\' (0.3)\n\ntesting error\n(std)\nmea.n\n16.9\n(8 .6)\n33.8\n(U)\n24 .8\n(3 .2)\n26 .5\n(9 .0)\n6 .2\n(6 .1 )\n5 .7\n(6 .1)\n17.7\n(14 .1)\n23 .4\n( 13.5)\n12.7\n~9.7.6!~\n26 .7\n0 .1\n15 .8\n15.7\n16 .0\n26 .0\n25 . 8\n40.5\n43 .5\n24 .9\n1.0\n\n~1 .~~\n\n(3 .2)\n(3.3)\n(6 .5)\n( 5. 1 )\n(7 .0)\n(7 .3)\n(8.5)\n(7 .9)\n( 3 .0)\n\n\'YoP\n22\n0\n0\n0\n60\n46\n12\n6\n27\n4\n99\n0\n0\n1\n1\n0\n0\n0\n5\n97\n\n#W\n56\n56\n56\n48\n50\n55\n45\n54\n55\n60\n\n#5\n8\n8\n8\n6\n5\n5\n3\n4\n6\n20\n\n56\n56\n56\n48\n50\n55\n45\n\n8\n8\n8\n6\n5\n5\n3\n4\n6\n20\n\n54\n\n55\n60\n\n(a)\nF5M\n\nArchitecture tt\n\nN&P\nTDNN\nRND\n\nGamma\nFirst Order\nHigh Order\nBilinear\nQuadratic\nMullilayer\nElman\nLocal\n\nN&P\nTDNN\nFMM\n\nGamma\nFirs t Order\nHigh Order\nBilinear\nQuadratic\nMullUayer\nElman\nLocal\n\ntra.lnlng\nmea.n\n4 .6\n11 . 7\n19.0\n12.9\n0 .3\n0 .6\n0 .2\n15. 4\n3.5\n13.9\n\n0 .1\n6 .8\n9 .0\n4 .8\n1.2\n2 .6\n12.6\n38.1\n12.8\n15 .3\n\nerror\n( std)\n\n( 8.~~\n( 2.0)\n\n(H)\n( 6.9)\n( 0 .5)\n( 0 .9)\n( 0 .5)\n(14 . 1)\n( 5.5 )\n\n( 405)\n\n( 0 .8)\n\n( 1.7)\n(2.9)\n(3 .0)\n\n( 1.7)\n( 402)\n(17.3)\n(12.6)\n\n~H.:~\n3 .8\n\ntestIng\nmea.n\n14.1\n34.3\n25 .2\n26 .5\n4 .6\n4 .4\n3.2\n19.9\n12.7\n20.2\n0 .3\n16.2\n14.9\n16.0\n25.1\n20 .3\n26.1\n42.8\n27.6\n22.2\n\nerror\n( std)\n(11 .3 )\n( 3 .9)\n(3.1)\n(9 .0)\n( 5 .1)\n\n( U)\n( 2 .6)\n\n(lU)\n( 9 .1)\n\n( 5.7)\n\n( 1.4)\n( 2 .9)\n(2 .8)\n(6 .5)\n( 5 .1)\n( 7 .2)\n(12 .8)\n( 9.2)\n(10 .7)\n\n( 409)\n\n\'YoP\n38\n0\n0\n0\n79\n55\n83\n16\n27\n0\n\n#W\n73\n73\nH\n48\nH\n78\n216\n76\n55\n26\n\n#5\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n\n97\n0\n0\n1\n31\n21\n13\n0\n8\n0\n\n73\n73\n73\n48\nH\n78\n216\n76\n55\n26\n\n6\n6\n6\n6\n6\n6\n6\n6\n6\n6\n\n(b)\ntThe TDNN and Gamma network both had 8 input taps and 4 hidden layer nodes. For\nthe Gamma network, I\' = 0.3 (RND) and I\' = 0.7 (FMM). Narendra and Parthasarathy\'s\nnetwork had 4 input and output taps and 5 hidden layer nodes. The High-order network\nused a "one-hot" encoding of the input values [5]. The multilayer network had 4 hidden\nand output layer nodes. The locally recurrent net had 4 hidden layer nodes with 5 input\nand 3 output taps, and one output node with 3 input and output taps.\nttThe TDNN, Gamma network, and N arendra and Parthasarathy\'s network all had 8\nhidden layer nodes. For the Gamma network, I\' = 0.3 (RND) and I\' = 0.7 (FMM). The\nHigh-order network again used a "one-hot" encoding of the input values. The multilayer\nnetwork had 5 hidden and 6 output layer nodes. The locally recurrent net had 3 hidden\nlayer nodes and one output layer node, all with only one input and output tap.\n\n\x0c702\n\nBill G. Horne, C. Lee Giles\n\nmachine when the number of states is not constrained. It is not surprising that\nthe former network did so well since the sequential machine implementation of\na finite memory machine is similar to this architecture [6]. However, the result\nfor the locally recurrent network was unexpected.\n? All of the recurrent networks do better than the TDNN on the small random\nmachine. However, on the finite memory machine the TDNN does surprisingly\nwell, perhaps because its structure is similiar to Narendra and Parthasarathy\'s\nnetwork which was well suited for this problem.\n? Gradient-based learning algorithms are not adequate for many of these architectures. In many cases a network is capable of representing a solution to a\nproblem that the algorithm was not able to find. This seems particularly true\nfor the Multilayer network.\n? Not surprisingly, an increase in the number of weights typically leads to overtraining. Although, the quadratic network, which has 216 weights, can consistently find solutions for the random machine that generalize well even though\nthere are only 127 training samples.\n? Although the performance on the training set is not always a good indicator of\'\ngeneralization performance on the testing set, we find that if a network is able\nto frequently find perfect solutions for the training data, then it also does well\non the testing data.\n3.3\n\nNonlinear system identification\n\nIn this problem, we train the network to learn the dynamics of the following set of\nequations proposed in [8]\n\nzl(k)\n\nZ2 ( k )\n1\n\n+\n\ny(k)\n\n+ 2z2(k)\n\nl+z~(k)\n\nzl(k+l)\n\n=\n\n+u\n\n(k)\n\nzl(k)Z2(k)\n+ u (k)\n1 + z~(k)\nzl(k) + z2(k)\n\nbased on observations of u( k) and y( k) alone.\nThe same networks that were used for the finite state machine problems were used\nhere, except that the output node was changed to be linear instead of sigmoidal\nto allow the network to have an appropriate dynamic range. We found that this\ncaused some stability problems in the quadratic and locally recurrent networks. For\nthe fixed number of weights comparison, we added an extra node to the quadratic\nnetwork, and dropped any second order terms involving the fed back output. This\ngave a network with 64 weights and 4 states. For the fixed state comparison,\ndropping the second order terms gave a network with 174 weights. The locally\nrecurrent network presented stability problems only for the fixed number of weights\ncomparison. Here, we used a network that had 6 hidden layer nodes and one output\nnode with 2 taps on the inputs and outputs each, giving a network with 57 weights\nand 16 states. In the Gamma network a value of l\' 0.8 gave the best results.\n\n=\n\nThe networks were trained with 100 uniform random noise sequences of length 50.\nEach experiment used a different randomly generated training set. The noise was\n\n\x0cAn Experimental Comparison of Recurrent Neural Networks\n\n703\n\nTable 3: Normalized mean squared error on a sinusoidal test signal for the nonlinear\nsystem identification experiment.\nArchi teet ure\nN&P\nTDNN\nGamma\nFirst Order\nHigh Order\nBilinear\nQuadratic\nMultilayer\nElman\nLocal\n\nFixed\n\n#\n\nweights\n\n0.101\n0.160\n0.157\n0.105\n1.034\n0.118\n0.108\n0.096\n0.115\n0.117\n\nFixed\n\n#\n\nstates\n\n0.067\n0.165\n0.151\n0.105\n1.050\n0.111\n0.096\n0.084\n0.115\n0.123\n\nuniformly distributed in the range [-2.0,2.0], and each sequence started with an\ninitial value of Xl(O) = X2(0) = O. The networks were tested on the response to\na sine wave of frequency 0.04 radians/second. This is an interesting test signal\nbecause it is fundamentally different than the training data.\nFifty runs were performed for each network. BPTT was used for 500 epochs with a\nlearning rate of 0.002. The weights of all networks were initialized to random values\nuniformly distributed in the range [-0.1,0.1].\nTable 3 shows the normalized mean squared error averaged over the 50 runs on the\ntesting set. From these results we draw the following conclusions.\n?\n\n?\n?\n\n?\n\n4\n\nThe high order network could not seem to match the dynamic range of its output\nto the target, as a result it performed much worse than the other networks. It is\nclear that there is benefit to adding first order terms since the bilinear network\nperformed so much better.\nAside from the high order network, all of the other recurrent networks performed\nbetter than the TDNN, although in most cases not significantly better.\nThe multilayer network performed exceptionally well on this problem, unlike the\nfinite state machine experiments. We speculate that the existence of target output at every point along the sequence (unlike the finite state machine problems)\nis important for the multilayer network to be successful.\nNarendra and Parthasarathy\'s architecture did exceptionally well, even though\nit is not clear that its structure is well matched to the problem.\n\nConclusions\n\nWe have reviewed many discrete-time recurrent neural network architectures and\ncompared them on two different problem domains, although we make no claim that\nany of these results will necessarily extend to other problems.\nNarendra and Parthasarathy\'s model performed exceptionally well on the problems\nwe explored. In general, single layer networks did fairly well, however it is important\nto include terms besides simple state/input products for nonlinear system identification. All of the recurrent networks usually did better than the TDNN except\n\n\x0c704\n\nBill G. Home, C. Lee Giles\n\non the finite memory machine problem. In these experiments, the use of averaging\nfilters as a substitute for taps in the TDNN did not seem to offer any distinct advantages in performance, although better results might be obtained if the value of\nJ.I. is adapted.\nWe found that the relative comparison of the networks did not significantly change\nwhether or not the number of weights or states were held constant. In fact, holding\none of these values constant meant that in some networks the other value varied\nwildly, yet there appeared to be little correlation with generalization.\nFinally, it is interesting to note that though some are much better than others,\nmany of these networks are capable of providing adequate solutions to two seemingly\ndisparate problems.\n\nAcknowledgements\nWe would like to thank Leon Personnaz and Isabelle Rivals for suggesting we perform the experiments with a fixed number of states.\n\nReferences\n[1] A.D. Back and A.C. Tsoi. FIR and IIR synapses, a new neural network architecture for time series modeling. Neural Computation, 3(3):375-385, 1991.\n[2] B. de Vries and J .C. Principe. The gamma model: A new neural model for\ntemporal processing. Neural Networks, 5:565-576, 1992.\n[3] J .L. Elman. Finding structure in time. Cognitive Science, 14:179-211, 1990.\n[4] P. Frasconi, M. Gori, and G. Soda. Local feedback multilayered networks.\nNeural Computation, 4:120-130, 1992.\n[5] C.L. Giles, C .B. Miller, et al. Learning and extracting finite state automata\nwith second-order recurrent neural networks. Neural Computation, 4:393-405,\n1992.\n[6] Z. Kohavi. Switching and finite automata theory. McGraw-Hill, NY, 1978.\n[7] K.J. Lang, A.H. Waibel, and G.E . Hinton. A time-delay neural network architecture for isolated word recognition. Neural Networks, 3:23-44, 1990.\n[8] K.S. Narendra. Adaptive control of dynamical systems using neural networks.\nIn Handbook of Intelligent Control, pages 141-183. Van Nostrand Reinhold,\nNY, 1992.\n[9] K.S. Narendra and K. Parthasarathy. Identification and control of dynamical\nsystems using neural networks. IEEE Trans. on Neural Networks, 1:4-27, 1990.\n[10] P. Poddar and K.P. Unnikrishnan. Non-linear prediction of speech signals\nusing memory neuron networks. In Proc. 1991 IEEE Work. Neural Networks\nfor Sig. Proc., pages 1-10. IEEE Press, 1991.\n[11] A.J. Robinson and F. Fallside. Static and dynamic error propagation networks\nwith application to speech coding. In NIPS, pages 632-641, NY, 1988. AlP.\n[12] R.L . Watrous and G.M. Kuhn . Induction of finite-state automata using\nsecond-order recurrent networks. In NIPS4, pages 309-316, 1992.\n\n\x0c'
p83174
sg130
S'Multidimensional Scaling and Data Clustering\n\nThomas Hofmann & Joachim Buhmann\nRheinische Friedrich-Wilhelms-U niversitat\nInstitut fur Informatik ill, Romerstra6e 164\nD-53117 Bonn, Germany\nemail:{th.jb}@cs.uni-bonn.de\n\nAbstract\nVisualizing and structuring pairwise dissimilarity data are difficult combinatorial optimization problems known as multidimensional scaling or pairwise data clustering.\nAlgorithms for embedding dissimilarity data set in a Euclidian space, for clustering\nthese data and for actively selecting data to support the clustering process are discussed\nin the maximum entropy framework. Active data selection provides a strategy to discover\nstructure in a data set efficiently with partially unknown data.\n\n1 Introduction\nGrouping experimental data into compact clusters arises as a data analysis problem in psychology, linguistics, genetics and other experimental sciences. The data which are supposed\nto be clustered are either given by an explicit coordinate representation (central clustering)\nor, in the non-metric case, they are characterized by dissimilarity values for pairs of data\npoints (pairwise clustering). In this paper we study algorithms (i) for embedding non-metric\ndata in a D-dimensional Euclidian space, (ii) for simultaneous clustering and embedding of\nnon-metric data, and (iii) for active data selection to determine a particular cluster structure\nwith minimal number of data queries. All algorithms are derived from the maximum entropy\nprinciple (Hertz et al., 1991) which guarantees robust statistics (Tikochinsky et al., 1984).\nThe data are given by a real-valued, symmetric proximity matrix D E R NXN , \'Dkl being\nthe pairwise dissimilarity between the data points k, l. Apart from the symmetry constraint\nwe make no further assumptions about the dissimilarities, i.e., we do not require D being a\nmetric. The numbers \'Dkl quite often violate the triangular inequality and the dissimilarity of\na datum to itself could be finite.\n\n2\n\nStatistical Mechanics of Multidimensional Scaling\n\nEmbedding dissimilarity data in a D-dimensional Euclidian space is a non-convex optimization problem which typically exhibits a large number of local minima. Stochastic search\nmethods like simulated annealing or its deterministic variants have been very successfulJy\n\n\x0c460\n\nThomas Hofmann. Joachim Buhmann\n\napplied to such problems. The question in multidimensional scaling is to find coordinates\n{Xi }i~1 in a D-dimensional Euclidian space with minimal embedding costs\nN\n\nH MDS\n\n[I Xi -\n\n1 \'"\'\n= 2N\nL.,\n\nXk 12 - \'Dik ]2 .\n\n(1)\n\ni,k=1\nWithout loss of generality we shift the center of mass in the origin <2::= I Xk = 0).\nIn the maximum entropy framework the coordinates {Xi} are regarded as random variables\nwhich are distributed according to the Gibbs distribution P ( { Xj} ) = exp( - f3 (H MDS - F). The\ninverse temperature f3 = 1/T controls the expected embedding costs (HMDS) (expectation values are denoted by (.). To calculate the free energy F for H MDS we approximate the coupling\nterm 2 2:~"k=1 \'DikxiXk/N ~ 2:[:1 xihi with the mean fields hi = 4 2:~= 1 \'Dik(Xk}/N.\nStandard t~chniques to evaluate the free energy F yield the equations\n\nJ J II\n\' 00\n\nZ(HMDS)\n\n00\n\ndy\n\nrv\n\n- \' 00\n\n- 00\n\nf)\n\nF(H MDS )\n\nD\n\nL\n\n2\n\ndR.d,d\' exp (-f3NF),\n\n(2)\n\nJ\n\n(3)\n\nd,d\'=1\nN\n\nR.~d\' - f3~ Lin\n\nd,d\'=1\n\ni=1\n\n00\n\ndXjexp (-f3f(Xi)) \'\n\n- 00\n\nN\n\nf(Xi)\n\nIXil4 -\n\n~IXiI2 L \'Dik + 4xTR.xi + xT (hi -\n\n4Y)?\nk=1\nThe integral in Eq. (2) is dominated by the absolute minimum of F in the limit N\nTherefore, we calculate the saddle point equations\n\n(4)\n~ 00.\n\nN\n\nR.\n\n=\n\n~L\n\n((Xjxf) + l(l x iI 2)I)\ni=1\nI Xi exp( -f3f(Xj)dx i\nI exp( -f3 f(Xj)dxi .\n\nand\n\n(5)\n\n0\n\n(6)\n\nEquation (6) has been derived by differentiating F with respect to hi. I denotes the D x D\nunit matrix. In the low temperature limit f3 ~ 00 the integral in (3) is dominated by the\nminimum of f(Xi) . Therefore, a new estimate of (Xi) is calculated minimizing f with respect\nto Xi. Since all explicit dependencies between the Xi have been eliminated, this minimization\ncan be performed independently for all i, 1 ~ i ~ N.\nIn the spirit of the EM algorithm for Gaussian mixture models we suggest the following\nalgorithm to calculate a meanfield approximation for the multidimensional scaling problem.\ninitialize (Xi)(O) randomly; t\nwhile\n\n2:::\':1 I(Xi )(t ) -\n\n(Xi)(t-I)I\n\nE- step: estimate\nM-step: calculate\n\n>\n\n(Xi) (t+l)\nn (t),\n\n= O.\nt:\n\nas a function of\n\nh~t) and determine\n\n(Xi)( t ) ,\ny (t)\n\nRY) ,\n\nsuch\n\nthat the centroid condition is satisfied.\n\ny(t ),\n\nh~ t)\n\n\x0cMultidimensional Scaling and Data Clustering\n\n461\n\nThis algorithm was used to determine the embedding of protein dissimilarity data as shown in\nFig. 1d. The phenomenon that the data clusters are arranged in a circular fashion is explained\nby the lack of small dissimilarity values. The solution in Fig. Id is about a factor of two\nbetter than the embedding found by a classical MDS program (Gower, 1966). This program\ndetermines a (N - 1)- space where the ranking of the dissimilarities is preserved and uses\nprinciple component analysis to project this tentative embedding down to two dimensions.\nExtensions to other MDS cost functions are currently under investigation.\n\n3\n\nMultidimensional Scaling and Pairwise Clustering\n\nEmbedding data in a Euclidian space precedes quite often a visual inspection by the data\nanalyst to discover structure and to group data into clusters. The question arises how both\nproblems, the embedding problem and the clustering problem, can be solved simultaneously.\nThe second algorithm addresses the problem to embed a data set in a Euclidian space such\nthat the clustering structure is approximated as faithfully as possible in the maximum entropy\nsense by the clustering solution in this embedding space. The coordinates in the embedding\nspace are the free parameters for this optimization problem.\nClustering of non-metric dissimilarity data, also called pairwise clustering (Buhmann, Hofmann, 1994a), is a combinatorial optimization problem which depends on Boolean assignments Miv E {a, I} of datum i to cluster lJ. The cost function for pairwise clustering with\nJ( clusters is\nIf\n1\nN\nN\n(7)\nE~:(M) =\n2 N\nMkvMlv\'Dkl with\nv=1 Pv\nk=! 1=1\n\nL\n\nLL\n\nIn the meanfield approach we approximate the Gibbs distribution P( Ej;) corresponding\nto the original cost function by a family of approximating distributions. The distribution\nwhich represents most accurately the statistics of the original problem is determined by\nthe minimum of the Kullback-Leibler divergence to the original Gibbs distribution. In the\npairwise clustering case we introduce potentials {Ekv } for the effective interactions, which\ndefine a set of cost functions with non-interacting assignments.\nK\n\nN\n\nL L Mk 1jEkl;.\n\n?<).; (M, {Ekv }) =\n\n(8)\n\nv=1 k=1\n\nThe optimal potentials derived from this minimization procedure are\n\n{?kv} = arg min \'DKL (PO(E~\' )IIP(E~)),\n\n(9)\n\n{?kv}\n\nwhere PO(E9{) is the Gibbs distribution corresponding to E~., and \'DKL(?II?) is the KLdivergence. This method is equivalent to minimizing an upper bound on the free energy\n(Buhmann, Hofmann, 1994b),\nF(E~:) ::; Fo(E~. )\n\n+ (VK)o,\n\nwith\n\nVA" = Ej; - ?~"\n\n(10)\n\n(\')0 denoting the average over all configurations of the cost function without interactions.\nCorrelations between assignment variables are statistically independent for PO( E9(), i.e.,\n(MkvA11v)0 = (M kv )0(A11v )0. The averaged potential VI\\, therefore, amounts to\nK\n\n(Vrd =\n\n1\n\nN\n\nLL\nv=1 k ,I=1\n\n(Mkl;) (Mlv) 2 vN\'Dk1 P\n\nK\n\nN\n\nL L(A1kv)Eklj,\nv=1 k=1\n\n(11)\n\n\x0c462\n\nThomas Hofmann. Joachim Buhmann\n\nthe subscript of averages being omitted for conciseness. The expected assignment variables\nare\n(12)\nMinimizing the upper bound yields\n(13)\nThe "optimal" potentials\n\n[i~\' =\n\n1\n\nN\n\nIN)\n\n(\n\nN L(Mkv ) \'Dik - 2 N L(M1v)D kl\n1v\nk=1\nPv\n1=1\nJ\n\n(14)\n\ndepend on the given distance matrix, the averaged assignment variables and the cluster\nprobabilities. They are optimal in the sense, that if we set\n(15)\nthe N * K stationarity conditions (13) are fulfilled for every i E {I, ... , N}, 11 E {I, ... , K}. A\nsimultaneous solution ofEq. (15) with (12) constitutes a necessary condition for a minimum\nof the upper bound for the free energy :F.\nThe connection between the clustering and the multidimensional scaling problem is established, if we restrict the potentials [iv to be of the form IXi - Yvf with the centroids\nYII = 2:~=1 Mkl/Xv/ 2::=1 Mkv. We consider the coordinates Xi as the variational parameters. The additional constraints restrict the family of approximating distributions, defined\nby ?9". to a subset. Using the chain rule we can calculate the derivatives of the upper bound\n(10), resulting in the exact stationary conditions for Xi,\nK\n\nN\n\n\'"\nco\n~ (Mia )(Mja ) (~Cia\n\nco\n-~Civ)Ya\n\na,v=1\n\n=\n\nK\n\njv ) x\n(MjoJ(M\nN\na,v=1\nPa\n\n\'~\n" \'~\n"\nj=1\n\nN ( (Xk - Ya) a(Mka)\n(~[ia - ~[ir/) [(Mia)! + ~\nOxi T)\n\n1(Xj - Ya),\n\n(16)\n\nwhere ~[iOt = ?ia - [tao The derivatives a(Mka) /Oxi can be exactly calculated, since they\nare given as the solutions of an linear equation system with N x K unknowns for every Xi. To\nreduce the computational complexity an approximation can be derived under the assumption\nay 0/ / aXj ~ O. In this case the right hand side of (16) can be set to zero in a first order\napproximation yielding an explicit formula for Xi,\nK\n\nKiXi\n\n~ ~ L(Miv) (11Yv1l\nv=1\n\nK\n2 -\n\n[tv) (Yv - L(Mia)Ya) ,\n\n(17)\n\na=1\n\nwith the covariance matrix Ki = ((yyT)j - (Y)i(Y)T) and (Y)i = 2:~=1 (Miv)Y v\'\nThe derived system of transcendental equations given by (12), (17) and the centroid condition explicitly reflects the dependencies between the clustering procedure and the Euclidian\nrepresentation. Solving these equations simultaneously leads to an efficient algorithm which\n\n\x0cMultidimensional Scaling and Data Clustering\n\na\n\n463\n\n.\n\n4tHB\n\nb\n\nHB\n\nHG,H~\n\nHA\n\nGGI\n\nGP~\n\n~\n\nGGI~\n\nMY\n\n~\n\nHBX,\nHF, HE\nGP\n\nHG~~~\n\n~\n\n~\nHBX,HF,HE\n~.\n\nGGGI\n???\n? [l}?,faitt\\tvJqJ~!;t\n\n. .?. ?. ,.\'?..,.? ... .\n~llt GP\n\nc\n\nGGI\n0\n\n420\n\n~GGG\nx\n\nx\n\nHAfo\n\n++\n\nRandom Selection\n380\n\n?re\n\n+\n+\n\nd\n\nx~\n\nHB\n+\n\n?\n\n340\n\nMY\n\n---+\n\nHG,HE,HF\n\n300\n\n# of selected Do,\n\nFigure 1: Similarity matrix of 145 protein sequences of the globin family (a): dark gray levels\ncorrespond to high similarity values; (b): clustering with embedding in two dimensions; (c):\nmultidimensional scaling solution for 2-dimensional embedding; (d): quality of clustering\nsolution with random and active data selection of \'D ik values. eKe has been calculated on the\nbasis of the complete set of \'Di k values.\ninterleaves the multidimensional scaling process and the clustering process and which avoids\nan artificial separation into two uncorrelated processes . The described algorithm for simultaneous Euclidian embedding and data clustering can be used for dimensionality reduction,\ne.g., high dimensional data can be projected to a low dimensional subspace in a nonlinear\nfashion which resembles local principle component analysis (Buhmann, Hofmann, 1994b).\nFigure (l) shows the clustering result for a real-world data set of 145 protein sequences. The\nsimilarity values between pairs of sequences are determined by a sequence alignment program\nwhich takes biochemical and structural information into account. The sequences belong to\ndifferent protein families like hemoglobin, myoglobin and other globins; they are abbreviated\nwith the displayed capital letters. The gray level visualization of the dissimilarity matrix with\ndark values for similar protein sequences shows the formation of distinct "squares" along the\nmain diagonal. These squares correspond to the discovered partition after clustering. The\nembedding in two dimensions shows inter-cluster distances which are in consistent agreement\nwith the similarity values of the data. In three and four dimensions the error between the\n\n\x0c464\n\nThomas Hofmann. Joachim Buhmann\n\ngiven dissimilarities and the constructed distances is further reduced. The results are in good\nagreement with the biological classification.\n\n4\n\nActive Data Selection for Data Clustering\n\nActive data selection is an important issue for the analysis of data which are characterized\nby pairwise dissimilarity values. The size of the distance matrix grows like the square of\nthe number of data \'points\'. Such a O(N2) scaling renders the data acquisition process\nexpensive. It is, therefore, desirable to couple the data analysis process to the data acquisition\nprocess, i.e., to actively query the supposedly most relevant dissimilarity values. Before\naddressing active data selection questions for data clustering we have to discuss the problem\nhow to modify the algorithm in the case of incomplete data.\nIf we want to avoid any assumptions about statistical dependencies, it is impossible to infer\nunknown values and we have to work directly with the partial dissimilarity matrix. Since the\ndata enters only in the (re-)ca1culation of the potentials in (14), it is straightforward to appropriately modify these equations. All sums are restricted to terms with known dissimilarities\nand the normalization factors are adjusted accordingly.\nAlternatively we can try to explicitly estimate the unknown dissimilarity values based on\na statistical model. For this purpose we propose two models, relying on a known group\nstructure of the data. The first model (I) assumes that all dissimilarities between a point\ni and points j belonging to a group G ~ are i.i.d. random variables with the probability\ndensity Pi/1 parameterized by eiw In this scheme a subset of the known dissimilarities of\ni and j to other points k are used as samples for the estimation of V ij . The selection\nof the specific subset is determined by the clustering structure. In the second model (II)\nwe assume that the dissimilarities between groups G v, G ~ are i.i.d. random variables with\ndensity PV/1 parameterized by e,IW The parameters ev~ are estimated on the basis of all\nknown dissimilarities {Vij E V} between points from G v and G~.\nThe assignments of points to clusters are not known a priori and have to be determined in the\nlight of the (given and estimated) data. The data selection strategy becomes self-consistent\nif we interpret the mean fields (.I"vfiv) of the clustering solution as posterior probabilities for\nthe binary assignment variables. Combined with a maximum likelihood estimation for the\nunknown parameters given the posteriors, we arrive at an EM-like iteration scheme with the\nE-step replaced by the clustering algorithm.\nThe precise form of the M-Step depends on the parametric form of the densities Pi~ or PI/~\'\nrespectively. In the case of Gaussian distributions the M-Step is described by the following\nestimation equations for the location parameters\n(I),\n\nwith 1T:j~ = 1+~vl\' ((Mil/){Mj~)\n\n+ (l\\tfi~)(Mjv)).\n\n(II),\n\n(18)\n\nCorresponding expressions are derived\n\nfor the standard deviations at) or a~\'~, respectively. In the case of non-normal distributions\nthe empirical mean might still be a good estimator of the location parameter, though not\nnecessarily a maximum likelihood estimator. The missing dissimilarities are estimated by\nthe following statistics, derived from the empirical means.\n- (I)\n\nDij\n\nK\n\n=\n\n\'""\n\n~ (l\\tfiv)(JVfj~)\n\n1/,11=)\n\ni\\[\n\n- (I)\n\nJ. i~mi~\n\nJY\n\n1~\n\n+ N jvmjv\n- (I)\n+ N.\n}V\n\n(I),\n\nD~~)\n!}\n\n= \'"" .".ij\nm- (I)\n"11/1\n~\n\n11-:5:~\n\n\'v~\n\n(II) ,\n\n(19)\n\n\x0cMultidimensional Scaling and Data Clustering\n\n465\n\n2600 r - - r -........-.----,--........-.-----,.,\n\nL,\n\'\\.,\n\n2400\n\n\\c,\n\n~---,-- ...\n\n\\\n\n2200\n\n""-!\n\n1\\\n\n\\t:\nAc t i ve Da t~:L--,\nSe 1 ec t ion\n\\ _______________________ _\n\n2000\n\no\n\n400\n\nBOO\n\n1200\n\n# of selected dissimilarities\n\nFigure 2: Similarity matrix of 54 word fragments generated by a dynamic programming\nalgorithm. The clustering costs in the experiment with active data selection requires only half\nas much data as a random selection strategy.\n\n=\n\nwith Nil\'\nE\'D.kE\'D(i11k11)\' For model (I) we have used a pooled estimator to exploit the\ndata symmetry. The iteration scheme finally leads to estimates (jill or (j\'lt\' respectively for the\nparameters and Dij for all unknown dissimilarities.\nCriterion for Active Data Selection: We will use the expected reduction in the variance of\nthe free energy Fo as a score, which should be maximized by the selection criterion. Fo is\ngiven by Fo(D) = -~ E;;:\', log E;~l exp( -{3?i/l(D)). If we query a new dissimilarity\nD ij the expected reduction of the variance of the free energy is approximated by\n\naFO]2 V [D .. _ D .. ]\n~ .. = 2 [aDij\ntJ\ntJ\nt)\n\n(20)\n\nThe partial derivatives can be calculated exactly by solving a system of linear equations with\nN x [ ..: unknowns. Alternatively a first order approximation in f /I = O( 1/ N P,/) yields\n(21)\n\nThis expression defines a relevance measure of Dij for the clustering problem since a Dij\nvalue contributes to the clustering costs only if the data i and j belong to the same cluster.\nEquation (21) summarizes the mean-field contributions aFo/aDij ~ a(H)o/aDjj .\nTo derive the final form of our scoring function we have to calculate an approximation of\nthe variance in Eq. (20) which measures the expected squared error for replacing the true\nvalue Dij with our estimate D ij . Since we assumed statistical independence the variances\nare additive V [Dij - Dij] = V [Dij] + V [Dij]. The total population variance is a sum\nof inner- and inter-cluster variances, that can be approximated by the empirical means and\nby the empirical variances instead of the unknown parameters of Pill or P\'lt\'. The sampling\nvariance of the statistics Dij is estimated under the assumption, that the empirical means ifl\'ill\n\n\x0c466\n\nThomas Hofmann, Joachim Buhmann\n\nor mVJ.l respectively are uncorrelated. This holds in the hard clustering limit. We arrive at\nthe following final expression for the variances of model (II)\n\nv [Vij-Dij ]\n\n~\n\nL1TYJl[(Dij-mvJl)2+(I+I: 1T:JJ.l\nV~Jl\n\nVk1EV\n\n1Tkl(j~Jl)l\nVJl\n\n(22)\n\nFor model (I) a slightly more complicated formula can be derived. Inserting the estimated\nvariances into Eq. (20) leads to the final expression for our scoring function.\nTo demonstrate the efficiency of the proposed selection strategy, we have compared the\nclustering costs achieved by active data selection with the clustering costs resulting from\nrandomly queried data. Assignments int the case of active selection are calculated with\nstatistical model (I). Figure 1d demonstrates that the clustering costs decrease significantly\nfaster when the selection criterion (20) is implemented. The structure of the clustering\nsolution has been completely inferred with about 3300 selected V ik values. The random\nstrategy requires about 6500 queries for the same quality. Analogous comparison results for\nlinguistic data are summarized in Fig. 2. Note the inconsistencies in this data set reflected by\nsmallVik values outside the cluster blocks (dark pixels) or by the large Vik values (white\npixels) inside a block.\nConclusion: Data analysis of dissimilarity data is a challenging problem in molecular biology, linguistics, psychology and, in general, in pattern recognition. We have presented\nthree strategies to visualize data structures and to inquire the data structure by an efficient\ndata selection procedure. The respective algorithms are derived in the maximum entropy\nframework for maximal robustness of cluster estimation and data embedding. Active data\nselection has been shown to require only half as much data for estimating a clustering solution\nof fixed quality compared to a random selection strategy. We expect the proposed selection\nstrategy to facilitate maintenance of genome and protein data bases and to yield more robust\ndata prototypes for efficient search and data base mining.\nAcknowledgement: It is a pleasure to thank M. Vingron and D. Bavelier for providing the\nprotein data and the linguistic data, respectively. We are also grateful to A. Polzer and H.J.\nWarneboldt for implementing the MDS algorithm. This work was partially supported by the\nMinistry of Science and Research of the state Nordrhein-Westfalen.\n\nReferences\nBuhmann, J., Hofmann, T. (l994a). Central and Pairwise Data Clustering by Competitive\nNeural Networks. Pages 104-111 of" Advances in Neural Infonnation Processing\nSystems 6. Morgan Kaufmann Publishers.\nBuhmann, J., Hofmann, T. (1994b). A Maximum Entropy Approach to Pairwise Data\nClustering. Pages 207-212 of" Proceedings of the International Conference on Pattern\nRecognition, Hebrew University, Jerusalem, vol. II. IEEE Computer Society Press.\nGower, J. C. (1966). Some distance properties of latent root and vector methods used in\nmultivariate analysis. Biometrika, 53, 325-328.\nHertz, J., Krogh, A., Palmer, R. G. (1991). Introduction to the Theory ofNeural Computation.\nNew York: Addison Wesley.\nTikochinsky, y, Tishby, N.Z., Levine, R. D. (1984). Alternative Approach to MaximumEntropy Inference. Physical Review A, 30, 2638-2644.\n\n\x0c'
p83175
sg132
S'Learning To Play the Game of Chess\n\nSebastian Thrun\nUniversity of Bonn\nDepartment of Computer Science III\nRomerstr. 164, 0-53117 Bonn, Germany\nE-mail: thrun@carbon.informatik.uni-bonn.de\n\nAbstract\nThis paper presents NeuroChess, a program which learns to play chess from the final\noutcome of games. NeuroChess learns chess board evaluation functions, represented\nby artificial neural networks. It integrates inductive neural network learning, temporal\ndifferencing, and a variant of explanation-based learning. Performance results illustrate\nsome of the strengths and weaknesses of this approach.\n\n1 Introduction\nThroughout the last decades, the game of chess has been a major testbed for research on\nartificial intelligence and computer science. Most oftoday\'s chess programs rely on intensive\nsearch to generate moves. To evaluate boards, fast evaluation functions are employed which\nare usually carefully designed by hand, sometimes augmented by automatic parameter tuning\nmethods [1]. Building a chess machine that learns to play solely from the final outcome of\ngames (win/loss/draw) is a challenging open problem in AI.\nIn this paper, we are interested in learning to play chess from the final outcome of games.\nOne of the earliest approaches, which learned solely by playing itself, is Samuel\'s famous\nchecker player program [10]. His approach employed temporal difference learning (in short:\nTO) [14], which is a technique for recursively learning an evaluation function . Recently,\nTesauro reported the successful application of TO to the game of Backgammon, using\nartificial neural network representations [16]. While his TO-Gammon approach plays grandmaster-level backgammon, recent attempts to reproduce these results in the context of Go\n[12] and chess have been less successful. For example, Schafer [11] reports a system just\nlike Tesauro\'s TO-Gammon, applied to learning to play certain chess endgames. Gherrity [6]\npresented a similar system which he applied to entire chess games. Both approaches learn\npurely inductively from the final outcome of games. Tadepalli [15] applied a lazy version\nof explanation-based learning [5, 7] to endgames in chess. His approach learns from the\nfinal outcome, too, but unlike the inductive neural network approaches listed above it learns\nanalytically, by analyzing and generalizing experiences in terms of chess-specific knowledge.\n\n\x0c1070\n\nSebastian Thrun\n\nThe level of play reported for all these approaches is still below the level of GNU-Chess, a\npublicly available chess tool which has frequently been used as a benchmark. This illustrates\nthe hardness of the problem of learning to play chess from the final outcome of games.\nThis paper presents NeuroChess, a program that learns to play chess from the final outcome\nof games. The central learning mechanisms is the explanation-based neural network (EBNN)\nalgorithm [9, 8]. Like Tesauro\'s TD-Gammon approach, NeuroChess constructs a neural\nnetwork evaluation function for chess boards using TO. In addition, a neural network version\nof explanation-based learning is employed, which analyzes games in terms of a previously\nlearned neural network chess model. This paper describes the NeuroChess approach, discusses several training issues in the domain of chess, and presents results which elucidate\nsome of its strengths and weaknesses.\n\n2\n\nTemporal Difference Learning in the Domain of Chess\n\nTemporal difference learning (TO) [14] comprises a family of approaches to prediction in\ncases where the event to be predicted may be delayed by an unknown number of time steps.\nIn the context of game playing, TD methods have frequently been applied to learn functions\nwhich predict the final outcome of games. Such functions are used as board evaluation\nfunctions.\nThe goal of TO(O), a basic variant of TO which is currently employed in the NeuroChess\napproach, is to find an evaluation function, V, which ranks chess boards according to their\ngoodness: If the board S is more likely to be a winning board than the board Sf, then\nV(s) > V(Sf). To learn such a function, TO transforms entire chess games, denoted by\na sequence of chess boards So, SI, s2, . . . , StunaJ\' into training patterns for V. The TO(O)\nlearning rule works in the following way. Assume without loss of generality we are learning\nwhite\'s evaluation function. Then the target values for the final board is given by\n{\n\nI,\n0,\n-1,\n\nif Stu.?tI is a win for white\nif StUnaJ is a draw\nif StonaJ is a loss for white\n\nand the targets for the intermediate chess boards So, SI , S2, . .. , Stu.?tI-2 are given by\nVt.1fget( St)\nI? V (St+2)\n\n=\n\n(1)\n\n(2)\n\nThis update rule constructs V recursively. At the end of the game, V evaluates the final\noutcome of the game (Eq. (l In between, when the assignment of V -values is less obvious,\nV is trained based on the evaluation two half-moves later (Eq. (2?. The constant I (with\no ~ I ~ 1) is a so-called discount factor. It decays V exponentially in time and hence\nfavors early over late success. Notice that in NeuroChess V is represented by an artificial\nneural network, which is trained to fit the target values vtarget obtained via Eqs. (l) and (2)\n(cj [6, 11, 12, 16]).\n\n?.\n\n3\n\nExplanation-Based Neural Network Learning\n\nIn a domain as complex as chess, pure inductive learning techniques. such as neural network Back-Propagation, suffer from enormous training times. To illustrate why, consider\nthe situation of a knight fork. in which the opponent\'s knight attacks our queen and king\nsimultaneously. Suppose in order to save our king we have to move it, and hence sacrifice\nour queen. To learn the badness of a knight fork, NeuroChess has to discover that certain\nboard features (like the position of the queen relative to the knight) are important, whereas\n\n\x0cLearning to Play the Game of Chess\n\n1071\n\nFigure 1: Fitting values and slopes in EBNN: Let V be the target function for which three\nexamples (s\\, V(S\\)), (S2\' V(S2)), and (S3, V(S3)) are known. Based on these points the\nS2)OS2, and a~;:3) are\nlearner might generate the hypothesis V\'. If the slopes a~;:I),\nalso known, the learner can do much better: V".\n\nar\n\nothers (like the number of weak pawns) are not. Purely inductive learning algorithms such\nas Back-propagation figure out the relevance of individual features by observing statistical\ncorrelations in the training data. Hence, quite a few versions of a knight fork have to be\nexperienced in order to generalize accurately. In a domain as complex as chess, such an\napproach might require unreasonably large amounts of training data.\nExplanation-based methods (EBL) [5, 7, 15] generalize more accurately from less training\ndata. They rely instead on the availability of domain knowledge, which they use for explaining\nand generalizing training examples. For example, in the explanation of a knight fork, EBL\nmethods employ knowledge about the game of chess to figure out that the position of the\nqueen is relevant, whereas the number of weak pawns is not. Most current approaches to\nEBL require that the domain knowledge be represented by a set of symbolic rules. Since\nNeuroChess relies on neural network representations, it employs a neural network version\nof EBL, called explanation-based neural network learning (EBNN) [9]. In the context of\nchess, EBNN works in the following way: The domain-specific knowledge is represented\nby a separate neural network, called the chess model M. M maps arbitrary chess boards St\nto the corresponding expected board St+2 two half-moves later. It is trained prior to learning\nV, using a large database of grand-master chess games. Once trained, M captures important\nknowledge about temporal dependencies of chess board features in high-quality chess play.\nEBNN exploits M to bias the board evaluation function V. It does this by extracting slope\nconstraints for the evaluation function V at all non-final boards, i.e., all boards for which V\nis updated by Eq. (2). Let\nwith\n\nt E\n\n{a, 1,2, ... , tlioa\\ - 2}\n\ndenote the target slope of V at St, which, because\nEq. (2), can be rewritten as\n\noV target ( St)\n\n=\n\n\'Y.\n\noV( St+2) OSt+2\n._OSt+2\nOSt\n\nvtarget ( St)\n\n(3)\n\nis set to \'Y V (St+2) according\n(4)\n\nusing the chain rule of differentiation. The rightmost term in Eq. (4) measures how infinitesimal small changes of the chess board St influence the chess board St+2. It can be\napproximated by the chess model M:\n\novtarget(St)\nOSt\n\n~\n\n\'Y.\n\nOV(St+2) oM(st)\n.\nOSt+2\nOSt\n\n(5)\n\nThe right expression is only an approximation to the left side, because M is a trained neural\n\n\x0cSebastian Thrun\n\n1072\n\n~\n\nbmrd at time\n\nf\n\n(W"T"~)\n\n~\n\nboard attime 1+ I\n(black to move)\n\n~\n\nboard at time 1+2\n\n(w"\'?ro~)\n\npredictive model network M\n\n165 hidden unit,\n\nV(1+2)\n\nFigure 2: Learning an evaluation function in NeuroChess. Boards are mapped into a\nhigh-dimensionalJeature vector, which forms the input for both the evaluation network V\nand the chess model M. The evaluation network is trained by Back-propagation and the\nTD(O) procedure. Both networks are employed for analyzing training example in order to\nderive target slopes for V.\nnetwork and thus its first derivative might be erroneous. Notice that both expressions on\nthe right hand side of Eq. (5) are derivatives of neural network functions, which are easy to\ncompute since neural networks are differentiable.\nThe result of Eq . (5) is an estimate of the slope of the target function V at 8t . This slope\nadds important shape information to the target values constructed via Eq. (2). As depicted in\nFig. 1, functions can be fit more accurately if in addition to target values the slopes of these\nvalues are known. Hence, instead of just fitting the target values vtarget ( 8t), NeuroChess also\nfits these target slopes. This is done using the Tangent-Prop algorithm [13].\nThe complete NeuroChess learning architecture is depicted in Fig. 2. The target slopes\nprovide a first-order approximation to the relevance of each chess board feature in the\ngoodness of a board position. They can be interpreted as biasing the network V based on\nchess-specific domain knowledge, embodied in M . For the relation ofEBNN and EBL and\nthe accommodation of inaccurate slopes in EBNN see [8].\n\n4\n\nTraining Issues\n\nIn this section we will briefly discuss some training issues that are essential for learning good\nevaluation functions in the domain of chess. This list of points has mainly been produced\nthrough practical experience with the NeuroChess and related TD approaches. It illustrates\nthe importance of a careful design of the input representation, the sampling rule and the\n\n\x0cLearning to Play the Game of Chess\n\n1073\n\nparameter setting in a domain as complex as chess.\nSampling. The vast majority of chess boards are, loosely speaking, not interesting. If, for\nexample, the opponent leads by more than a queen and a rook, one is most likely to loose.\nWithout an appropriate sampling method there is the danger that the learner spends most\nof its time learning from uninteresting examples. Therefore, NeuroChess interleaves selfplay and expert play for guiding the sampling process. More specifically, after presenting\na random number of expert moves generated from a large database of grand-master games,\nNeuroChess completes the game by playing itself. This sampling mechanism has been found\nto be of major importance to learn a good evaluation function in a reasonable amount of time.\nQuiescence. In the domain of chess certain boards are harder to evaluate than others. For\nexample, in the middle of an ongoing material exchange, evaluation functions often fail to\nproduce a good assessment. Thus, most chess programs search selectively. A common\ncriterion for determining the depth of search is called quiescence. This criterion basically\ndetects material threats and deepens the search correspondingly. NeuroChess\' search engine\ndoes the same. Consequently, the evaluation function V is only trained using quiescent\nboards.\nSmoothness. Obviously, using the raw, canonical board description as input representation is\na poor choice. This is because small changes on the board can cause a huge difference in value,\ncontrasting the smooth nature of neural network representations. Therefore, NeuroChess\nmaps chess board descriptions into a set of board features . These features were carefully\ndesigned by hand.\nDiscounting. The variable \'Y in Eq. (2) allows to discount values in time. Discounting has\nfrequently been used to bound otherwise infinite sums of pay-off. One might be inclined to\nthink that in the game of chess no discounting is needed, as values are bounded by definition.\nIndeed, without discounting the evaluation function predicts the probability for winning-in\nthe ideal case. In practice, however, random disturbations of the evaluation function can\nseriously hurt learning, for reasons given in [4, 17]. Empirically we found that learning\nfailed completely when no discount factor was used. Currently, NeuroChess uses \'Y = 0.98.\nLearning rate. TO approaches minimize a Bellman equation [2]. In the NeuroChess\ndomain, a close-to-optimal approximation of the Bellman equation is the constant function\nV(s) == O. This function violates the Bellman equation only at the end of games (Eq. (1?,\nwhich is rare if complete games are considered. To prevent this, we amplified the learning\nrate for final values by a factor of20, which was experimentally found to produce sufficiently\nnon-constant evaluation functions.\nSoftware architecture. Training is performed completely asynchronously on up to 20\nworkstations simultaneously. One of the workstations acts as a weight server, keeping track\nof the most recent weights and biases of the evaluation network. The other workstations\ncan dynamically establish links to the weight server and contribute to the process of weight\nrefinement. The main process also monitors the state of all other workstations and restarts\nprocesses when necessary. Training examples are stored in local ring buffers (1000 items\nper workstation).\n\n5\n\nResults\n\nIn this section we will present results obtained with the NeuroChess architecture. Prior to\nlearning an evaluation function, the model M (175 input, 165 hidden, and 175 output units)\nis trained using a database of 120,000 expert games. NeuroChess then learns an evaluation\n\n\x0c1074\n\nI. e2e3 b8c6\n2. dlf3 c6e5\n3. f3d5 d7d6\n4. flb5 c7c6\n5. b5a4 g8f6\n6. d5d4 c8f5\n7. f2f4 e5d7\n8. ele2d8a5\n9. a4b3 d7c5\n10. b I a3 c5b3\n11 . a2b3 e7e5\n12. f4e5 f6e4\n13. e5d6 e8c8\n14. b3b4 a5a6\n15. b4b5 a6a5\n\nSebastian Thrun\n\n16. b2b4 a5a4\n17. b5c6 a4c6\n18. gl f3 d8d6\n19. d4a7 f5g4\n20. c2c4 c8d7\n21. b4b5 c6c7\n22. d2d3 d6d3\n23. b5b6 c7c6\n24. e2d3 e4f2\n25. d3c3 g4f3\n26. g2f3 f2h 1\n27. clb2 c6f3\n28. a7a4 d7e7\n29. a3c2 hi f2\n30. b2a3 e7f6\n\n31 . a3f8 f2e4\n32. c3b2 h8f8\n33. a4d7 f3f5\n34. d7b7 f5e5\n35. b2cl f8e8\n36. b7d5 e5h2\n37. ala7 e8e6\n38. d5d8 f6g6\n39. b6b7 e6d6\n40. d8a5 d6c6\n41 . a5b4 h2b8\n42. a7a8 e4c3\n43. c2d4 c6f6\n44. b4e7 c3a2\n45. cldl a2c3\n\n46. d I c2 b8h2\n47. c2c3 f6b6\n48. e7e4 g6h6\n49. d4f5 h6g5\n50. e4e7 g5g4\n51. f5h6 g7h6\n52. e7d7 g4h5\n53. d7d I h5h4\n54. d I d4 h4h3\n55. d4b6 h2e5\n56. b6d4 e5e6\n57. c3d2 e6f5\n58. e3e4 f5 g5\n59. d4e3 g5e3\n60. d2e3 f7f5\n\n61 . e4f5 h3g4 65. a8e8 e6d7\n62. f5f6 h6h5\n66. e8e7 d7d8\n63. b7b8q g4f5 67. f4c7\n64. b8f4 f5e6\nfinal board\n\nFigure 3: NeuroChess against GNU-Chess. NeuroChess plays white. Parameters: Both\nplayers searched to depth 3, which could be extended by quiescence search to at most 11.\nThe evaluation network had no hidden units. Approximately 90% of the training boards\nwere sampled from expert play.\n\nnetwork V (175 input units, 0 to 80 hidden units, and one output units). To evaluate the level\nof play, NeuroChess plays against GNU-Chess in regular time intervals. Both players employ\nthe same search mechanism which is adopted from GNU-Chess. Thus far, experiments lasted\nfor 2 days to 2 weeks on I to 20 SUN Sparc Stations.\nA typical game is depicted in Fig. 3. This game has been chosen because it illustrates both\nthe strengths and the shortcomings of the NeuroChess approach. The opening of NeuroChess\nis rather weak. In the first three moves NeuroChess moves its queen to the center of the\nboard.\' NeuroChess then escapes an attack on its queen in move 4, gets an early pawn\nadvantage in move 12, attacks black\'s queen pertinaciously through moves 15 to 23, and\nsuccessfully exchanges a rook. In move 33, it captures a strategically important pawn, which,\nafter chasing black\'s king for a while and sacrificing a knight for no apparent reason, finally\nleads to a new queen (move 63). Four moves later black is mate. This game is prototypical.\nAs can be seen from this and various other games, NeuroChess has learned successfully to\nprotect its material, to trade material, and to protect its king. It has not learned, however, to\nopen a game in a coordinated way, and it also frequently fails to play short.endgames even\nif it has a material advantage (this is due to the short planning horizon). Most importantly, it\nstill plays incredibly poor openings, which are often responsible for a draw or a loss. Poor\nopenings do not surprise, however, as TD propagates values from the end of a game to the\nbeginning.\nTable I shows a performance comparison of NeuroChess versus GNU-Chess, with and\nwithout the explanation-based learning strategy. This table illustrates that NeuroChess wins\napproximately 13% of all games against GNU-Chess, if both use the same search engine. It\n\'This is because in the current version NeuroChess still heavily uses expert games for sampling.\nWhenever a grand-master moves its queen to the center of the board, the queen is usually safe, and there\nis indeed a positive correlation between having the queen in the center and winning in the database.\nNeuroChess falsely deduces that having the queen in the center is good. This effect disappears when\nthe level of self-play is increased, but this comes at the expense of drastically increased training time,\nsince self-play requires search.\n\n\x0cLearning to Play the Game of Chess\n\n# of games\n100\n200\n500\n1000\n1500\n2000\n2400\n\nGNU depth 2, NeuroChess depth 2\nBack-propagation\nEBNN\n1\n0\n6\n2\n35\n13\n73\n85\n130\n135\n190\n215\n239\n316\n\n1075\n\nGNU depth 4, NeuroChess depth 2\nBack-propagation\nEBNN\n0\n0\n0\n0\nI\n0\n2\n1\n3\n3\n3\n8\nII\n3\n\nTable 1: Performance ofNeuroChess vs. GNU-Chess during training. The numbers show the\ntotal number of games won against GNU-Chess using the same number of games for testing\nas for training. This table also shows the importance of the explanation-based learning\nstrategy in EBNN. Parameters: both learners used the original GNU-Chess features, the\nevaluation network had 80 hidden units and search was cut at depth 2, or 4, respectively (no\nquiescence extensions).\nalso illustrates the utility of explanation-based learning in chess.\n\n6 Discussion\nThis paper presents NeuroChess, an approach for learning to play chess from the final\noutcomes of games. NeuroChess integrates TD, inductive neural network learning and\na neural network version of explanation-based learning. The latter component analyzes\ngames using knowledge that was previously learned from expert play. Particular care has\nbeen taken in the design of an appropriate feature representation, sampling methods, and\nparameter settings. Thus far, NeuroChess has successfully managed to beat GNU-Chess in\nseveral hundreds of games. However, the level of play still compares poorly to GNU-Chess\nand human chess players.\nDespite the initial success, NeuroChess faces two fundamental problems which both might\nweB be in the way of excellent chess play. Firstly, training time is limited, and it is to\nbe expected that excellent chess skills develop only with excessive training time. This is\nparticularly the case if only the final outcomes are considered. Secondly, with each step of\nTO-learning NeuroChess loses information. This is partially because the features used for\ndescribing chess boards are incomplete, i.e., knowledge about the feature values alone does\nnot suffice to determine the actual board exactly. But, more importantly, neural networks have\nnot the discriminative power to assign arbitrary values to all possible feature combinations.\nIt is therefore unclear that a TD-like approach will ever, for example, develop good chess\nopenmgs.\nAnother problem of the present implementation is related to the trade-off between knowledge\nand search. It has been well recognized that the ul timate cost in chess is determi ned by the ti me\nit takes to generate a move. Chess programs can generally invest their time in search, or in the\nevaluation of chess boards (search-knowledge trade-off) [3] . Currently, NeuroChess does a\npoor job, because it spends most of its time computing board evaluations. Computing a large\nneural network function takes two orders of magnitude longer than evaluating an optimized\nlinear evaluation function (like that of GNU-Chess). VLSI neural network technology offers\na promising perspective to overcome this critical shortcoming of sequential neural network\nsimulations.\n\n\x0c1076\n\nSebastian Thrun\n\nAcknowledgment\nThe author gratefully acknowledges the guidance and advise by Hans Berliner, who provided\nthe features for representing chess boards, and without whom the current level of play would\nbe much worse. He also thanks Tom Mitchell for his suggestion on the learning methods,\nand Horst Aurisch for his help with GNU-Chess and the database.\n\nReferences\n[I] Thomas S. Anantharaman. A Statistical Study of Selective Min-Max Search in Computer Chess.\nPhD thesis, Carnegie Mellon University, School of Computer Science, Pittsburgh, PA, 1990.\nTechnical Report CMU-CS-90-173.\n[2] R. E. Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, 1957.\n[3] Hans J. Berliner, Gordon Goetsch, Murray S. Campbell, and Carl Ebeling. Measuring the\nperformance potential of chess programs. Artificial Intelligence, 43:7-20, 1990.\n[4] Justin A. Boyan. Generalization in reinforcement learning: Safely approximating the value\nfunction. In G. Tesauro, D. Touretzky, and T. Leen, editors, Advances in Neural Information\nProcessing Systems 7, San Mateo, CA, 1995. Morgan Kaufmann. (to appear).\n[5] Gerald Dejong and Raymond Mooney. Explanation-based learning: An alternative view. Machine Learning, 1(2): 145-176, 1986.\n[6] Michael Gherrity. A Game-Learning Machine. PhD thesis, University of California, San Diego,\n1993.\n[7] Tom M. Mitchell, Rich Keller, and Smadar Kedar-Cabelli. Explanation-based generalization: A\nunifying view. Machine Learning, 1(1 ):47-80, 1986.\n[8] Tom M. Mitchell and Sebastian Thrun. Explanation based learning: A comparison of symbolic\nand neural network approaches. In Paul E. Utgoff, editor, Proceedings of the Tenth International\nConference on Machine Learning, pages 197-204, San Mateo, CA, 1993. Morgan Kaufmann.\n[9] Tom M. Mitchell and Sebastian Thrun. Explanation-based neural network learning for robot\ncontrol. In S. J. Hanson, J. Cowan, and C. L. Giles, editors, Advances in Neural Information\nProcessing Systems 5, pages 287-294, San Mateo, CA, 1993. Morgan Kaufmann.\n[10] A. L. Samuel. Some studies in machine learning using the game of checkers. IBM Journal on\nresearch and development, 3:210-229, 1959.\n[11] Johannes Schafer. Erfolgsorientiertes Lemen mit Tiefensuche in Bauemendspielen. Technical\nreport, UniversiUit Karlsruhe, 1993. (in German).\n[12] Nikolaus Schraudolph, Pater Dayan, and Terrence J. Sejnowski. Using the TD(lambda) algorithm\nto learn an evaluation function for the game of go. In Advances in Neural Information Processing\nSystems 6, San Mateo, CA, 1994. Morgan Kaufmann.\n[13] Patrice Simard, Bernard Victorri, Yann LeCun, and John Denker. Tangent prop -a formalism for\nspecifying selected invariances in an adaptive network. In J. E. Moody, S. J. Hanson, and R. P.\nLippmann, editors, Advances in Neural Information Processing Systems 4, pages 895-903, San\nMateo, CA, 1992. Morgan Kaufmann.\n[14] Richard S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning,\n3,1988.\n[15] Prasad Tadepalli. Planning in games using approximately learned macros. In Proceedings of the\nSixth International Workshop on Machine Learning, pages 221-223, Ithaca, NY, 1989. Morgan\nKaufmann.\n[16] Gerald J. Tesauro. Practical issues in temporal difference learning. Machine Learning, 8, 1992.\n[17] Sebastian Thrun and Anton Schwartz. Issues in using function approximation for reinforcement learning. In M. Mozer, P. Smolensky, D. Touretzky, J. Elman, and A. Weigend, editors,\nProceedings of the 1993 Connectionist Models Summer School, Hillsdale, NJ, 1993. Erlbaum\nAssociates.\n\n\x0c'
p83176
sg14
S'Real-Time Control of a Tokamak Plasma\nUsing Neural Networks\n\nChris M Bishop\nNeural Computing Research Group\nDepartment of Computer Science\nAston University\nBirmingham, B4 7ET, U.K.\nc.m .bishop@aston .ac .uk\n\nPaul S Haynes, Mike E U Smith, Tom N Todd,\nDavid L Trotman and Colin G Windsor\nAEA Technology, Culham Laboratory,\nOxfordshire OX14 3DB\n(Euratom/UKAEA Fusion Association)\n\nAbstract\nThis paper presents results from the first use of neural networks\nfor the real-time feedback control of high temperature plasmas in\na tokamak fusion experiment. The tokamak is currently the principal experimental device for research into the magnetic confinement approach to controlled fusion. In the tokamak, hydrogen\nplasmas, at temperatures of up to 100 Million K, are confined\nby strong magnetic fields. Accurate control of the position and\nshape of the plasma boundary requires real-time feedback control\nof the magnetic field structure on a time-scale of a few tens of microseconds. Software simulations have demonstrated that a neural\nnetwork approach can give significantly better performance than\nthe linear technique currently used on most tokamak experiments.\nThe practical application of the neural network approach requires\nhigh-speed hardware, for which a fully parallel implementation of\nthe multilayer perceptron, using a hybrid of digital and analogue\ntechnology, has been developed.\n\n\x0c1008\n\n1\n\nC. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor\n\nINTRODUCTION\n\nFusion of the nuclei of hydrogen provides the energy source which powers the sun.\n\nIt also offers the possibility of a practically limitless terrestrial source of energy.\nHowever, the harnessing of this power has proved to be a highly challenging problem. One of the most promising approaches is based on magnetic confinement of a\nhigh temperature (10 7 - 108 Kelvin) plasma in a device called a tokamak (from the\nRussian for \'toroidal magnetic chamber\') as illustrated schematically in Figure 1.\nAt these temperatures the highly ionized plasma is an excellent electrical conductor, and can be confined and shaped by strong magnetic fields. Early tokamaks\nhad plasmas with circular cross-sections, for which feedback control of the plasma\nposition and shape is relatively straightforward. However, recent tokamaks, such as\nthe COMPASS experiment at Culham Laboratory, as well as most next-generation\ntokamaks, are designed to produce plasmas whose cross-sections are strongly noncircular. Figure 2 illustrates some of the plasma shapes which COMPASS is designed to explore. These novel cross-sections provide substantially improved energy\nconfinement properties and thereby significantly enhance the performance of the\ntokamak.\n\nz\n\nR\n\nFigure 1: Schematic cross-section of a tokamak experiment showing the toroidal vacuum vessel (outer D-shaped curve) and plasma\n(shown shaded). Also shown are the radial (R) and vertical (Z) coordinates. To a good approximation, the tokamak can be regarded\nas axisymmetric about the Z-axis, and so the plasma boundary can\nbe described by its cross-sectional shape at one particular toroidal\nlocation.\nUnlike circular cross-section plasmas, highly non-circular shapes are more difficult to\nproduce and to control accurately, since currents through several control coils must\nbe adjusted simultaneously. Furthermore, during a typical plasma pulse, the shape\nmust evolve, usually from some initial near-circular shape. Due to uncertainties\nin the current and pressure distributions within the plasma, the desired accuracy\nfor plasma control can only be achieved by making real-time measurements of the\nposition and shape of the boundary, and using error feedback to adjust the currents\nin the control coils.\nThe physics of the plasma equilibrium is determined by force balance between the\n\n\x0c1009\n\nReal-Time Control of Tokamak Plasma Using Neural Networks\n\ncircle\n\nellipse\n\nO-shape\n\nbean\n\nFigure 2: Cross-sections of the COMPASS vacuum vessel showing\nsome examples of potential plasma shapes. The solid curve is the\nboundary of the vacuum vessel, and the plasma is shown by the\nshaded regions.\n\nthermal pressure of the plasma and the pressure of the magnetic field, and is relatively well understood. Particular plasma configurations are described in terms\nof solutions of a non-linear partial differential equation called the Grad-Shafranov\n(GS) equation. Due to the non-linear nature of this equation, a general analytic\nsolution is not possible. However, the GS equation can be solved by iterative numerical methods, with boundary conditions determined by currents flowing in the\nexternal control coils which surround the vacuum vessel. On the tokamak itself it\nis changes in these currents which are used to alter the position and cross-sectional\nshape of the plasma. Numerical solution of the GS equation represents the standard technique for post-shot analysis of the plasma, and is also the method used\nto generate the training dataset for the neural network, as described in the next\nsection. However , this approach is computationally very intensive and is therefore\nunsuitable for feedback control purposes.\nFor real-time control it is necessary to have a fast (typically:::; 50J.lsec.) determination of the plasma boundary shape. This information can be extracted from a\nvariety of diagnostic systems , the most important being local magnetic measurements taken at a number of points around the perimeter of the vacuum vessel.\nMost tokamaks have several tens or hundreds of small pick up coils located at carefully optimized points around the torus for this purpose. We shall represent these\nmagnetic signals collectively as a vector m .\nFor a large class of equilibria, the plasma boundary can be reasonably well represented in terms of a simple parameterization, governed by an angle-like variable B,\ngiven by\n\nR(B)\nZ(B)\n\nRo + a cos(B + 8 sinB)\nZo + a/\\,sinB\n\nwhere we have defined the following parameters\n\n(1)\n\n\x0c1010\n\nRo\nZo\na\nK\n\n6\n\nC. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor\n\nradial distance of the plasma center from the major axis of the torus,\nvertical distance of the plasma center from the torus midplane,\nminor radius measured in the plane Z = Zo,\nelongation,\ntriangularity.\n\nWe denote these parameters collectively by Yk. The basic problem which has to be\naddressed, therefore, is to find a representation for the (non-linear) mapping from\nthe magnetic signals m to the values of the geometrical parameters Yk, which can\nbe implemented in suitable hardware for real-time control.\nThe conventional approach presently in use on many tokamaks involves approximating the mapping between the measured magnetic signals and the geometrical\nparameters by a single linear transformation. However, the intrinsic non-linearity\nof the mappings suggests that a representation in terms of feedforward neural networks should give significantly improved results (Lister and Schnurrenberger, 1991;\nBishop et a/., 1992; Lagin et at., 1993). Figure 3 shows a block diagram of the\ncontrol loop for the neural network approach to tokamak equilibrium control.\nNeural\n\nNetwork\n\nFigure 3: Block diagram of the control loop used for real-time\nfeedback control of plasma position and shape.\n\n2\n\nSOFTWARE SIMULATION RESULTS\n\nThe dataset for training and testing the network was generated by numerical solution of the GS equation using a free-boundary equilibrium code. The data base\ncurrently consists of over 2,000 equilibria spanning the wide range of plasma positions and shapes available in COMPASS. Each equilibrium configuration takes\nseveral minutes to generate on a fast workstation. The boundary of each configuration is then fitted using the form in equation 1, so that the equilibria are labelled\nwith the appropriate values of the shape parameters. Of the 120 magnetic signals\navailable on COMPASS which could be used to provide inputs to the network, a\n\n\x0c1011\n\nReal-Time Control o/Tokamak PLasma Using Neural Networks\n\nsubset of 16 has been chosen using sequential forward selection based on a linear\nrepresentation for the mapping (discussed below) .\nIt is important to note that the transformation from magnetic signals to flux surface\nparameters involves an exact linear invariance. This follows from the fact that, if all\nof the currents are scaled by a constant factor, then the magnetic fields will be scaled\nby this factor, and the geometry of the plasma boundary will be unchanged . It is\nimportant to take advantage of this prior knowledge and to build it into the network\nstructure, rather than force the network to learn it by example. We therefore\nnormalize the vector m of input signals to the network by dividing by a quantity\nproportional to the total plasma current. Note that this normalization has to be\nincorporated into the hardware implementation of the network, as will be discussed\nin Section 3.\n1.2\n\n4\n01\n\n2\n\n2\n\n01\n\nc\n\nc\n.5.\n\n0-\n\n~\n:E\n\n.5.\nCIS\n\n:E\n\n1iI\n\n~\n::J\n\n?\n\n1iI\nCD\n\n-2\n\ngo.8\n\n.5.\n0-\n\n?\n\nCIS\n\n:E\n\n1iI 0 .4\nCD\n\nc\n::J\n\nc\n\n::J\n\n-2\n\n-4\nDatabase\n\n?\nDatabase\n\n1.2\n\n4\n~\n\n~CD\n\nZ\n\n~\n:::I\n\nCD\n\nz\n\n.2\nDatabase\n\n2\n\n~O.8\n~\n\n?\n\nCD\n\nz\n\n~O.4\n\n-2\n\n:::I\n\nCD\n\nZ\n\n?\n\n-4\nDatabase\n\nDatabase\n\n.2\nDatabase\n\nFigure 4: Plots of the values from the test set versus the values\npredicted by the linear mapping for the 3 equilibrium parameters,\ntogether with the corresponding plots for a neural network with 4\nhidden units.\n\nThe results presented in this paper are based on a multilayer perceptron architecture\nhaving a single layer of hidden units with \'tanh\' activation functions , and linear\noutput units. Networks are trained by minimization of a sum-of-squares error using\na standard conjugate gradients optimization algorithm, and the number of hidden\n\n\x0cJ012\n\nC. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor\n\nunits is optimized by measuring performance with respect to an independent test\nset. Results from the neural network mapping are compared with those from the\noptimal linear mapping, that is the single linear transformation which minimizes\nthe same sum-of-squares error as is used in the neural network training algorithm,\nas this represents the method currently used on a number of present day tokamaks .\nInitial results were obtained on networks having 3 output units, corresponding to\nthe values of vertical position ZQ, major radius RQ, and elongation K; these being\nparameters which are of interest for real-time feedback control. The smallest normalized test set error of 11.7 is obtained from the network having 16 hidden units.\nBy comparison, the optimal linear mapping gave a normalized test set error of 18.3.\nThis represents a reduction in error of about 30% in going from the linear mapping\nto the neural network. Such an improvement, in the context of this application , is\nvery significant.\nFor the experiments on real-time feedback control described in Section 4 the currently available hardware only permitted networks having 4 hidden units, and so we\nconsider the results from this network in more detail. Figure 4 shows plots of the\nnetwork predictions for various parameters versus the corresponding values from\nthe test set portion of the database. Analogous plots for the optimal linear map\npredictions versus the database values are also shown. Comparison of the corresponding figures shows the improved predictive capability of the neural network,\neven for this sub-optimal network topology.\n\n3\n\nHARDWARE IMPLEMENTATION\n\nThe hardware implementation of the neural network must have a bandwidth of 2:\n20 kHz in order to cope with the fast timescales of the plasma evolution. It must\nalso have an output precision of at least (the the analogue equivalent of) 8 bits in\norder to ensure that the final accuracy which is attainable will not be limited by the\nhardware system. We have chosen to develop a fully parallel custom implementation\nof the multilayer perceptron, based on analogue signal paths with digitally stored\nsynaptic weights (Bishop et al., 1993). A VME-based modular construction has\nbeen chosen as this allows flexibility in changing the network architecture, ease of\nloading network weights, and simplicity of data acquisition. Three separate types\nof card have been developed as follows:\n? Combined 16-input buffer and signal normalizer.\nThis provides an analogue hardware implementation of the input normalization described earlier.\n? 16 x 4 matrix multiplier\nThe synaptic weights are produced using 12 bit frequency-compensated\nmultiplying DACs (digital to analogue converters) which can be configured\nto allow 4-quadrant multiplication of analogue signals by a digitally stored\nnumber.\n? 4-channel sigmoid module\nThere are many ways to produce a sigmoidal non-linearity, and we have\nopted for a solution using two transistors configured as along-tailed-pair,\n\n\x0cReal-Time Control of Tokamak Plasma Using Neural Networks\n\n1013\n\nto generate a \'tanh \' sigmoidal transfer characteristic. The principal drawback of such an approach is the strong temperature sensitivity due to the\nappearance of temperature in the denominator of the exponential transistor\ntransfer characteristic. An elegant solution to this problem has been found\nby exploiting a chip containing 5 transistors in close thermal contact. Two\nof the transistors form the long-tailed pair, one of the transistors is used\nas a heat source, and the remaining two transistors are used to measure\ntemperature. External circuitry provides active thermal feedback control,\nand stability to changes in ambient temperature over the range O?C to 50?C\nis found to be well within the acceptable range.\nThe complete network is constructed by mounting the appropriate combination\nof cards in a VME rack and configuring the network topology using front panel\ninterconnections. The system includes extensive diagnostics, allowing voltages at\nall key points within the network to be monitored as a function of time via a series\nof multiplexed output channels.\n\n4\n\nRESULTS FROM REAL-TIME FEEDBACK CONTROL\n\nFigure 5 shows the first results obtained from real-time control of the plasma in\nthe COMPASS tokamak using neural networks. The evolution of the plasma elongation, under the control of the neural network, is plotted as a function of time\nduring a plasma pulse. Here the desired elongation has been preprogrammed to\nfollow a series of steps as a function of time. The remaining 2 network outputs\n(radial position Ro and vertical position Zo) were digitized for post-shot diagnosis ,\nbut were not used for real-time control. The solid curve shows the value of elongation given by the corresponding network output, and the dashed curve shows the\npost-shot reconstruction of the elongation obtained from a simple \'filament\' code,\nwhich gives relatively rapid post-shot plasma shape reconstruction but with limited\naccuracy. The circles denote the elongation values given by the much more accurate\nreconstructions obtained from the full equilibrium code. The graph clearly shows\nthe network generating the required elongation signal in close agreement with the\nreconstructed values. The typical residual error is of order 0.07 on elongation values\nup to around 1.5. Part of this error is attributable to residual offset in the integrators used to extract magnetic field information from the pick-up coils, and this is\ncurrently being corrected through modifications to the integrator design. An additional contribution to the error arises from the restricted number of hidden units\navailable with the initial hardware configuration. While these results represent the\nfirst obtained using closed loop control, it is clear from earlier software modelling of\nlarger network architectures (such as 32- 16-4) that residual errors of order a few %\nshould be attainable. The implementation of such larger networks is being persued,\nfollowing the successes with the smaller system.\nAcknowledgements\nWe would like to thank Peter Cox, Jo Lister and Colin Roach for many useful\ndiscussions and technical contributions. This work was partially supported by the\nUK Department of Trade and Industry.\n\n\x0cC. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor\n\n1014\n\n1.8\nshot 9576\n\nc:\n\no\n\n~\n14\nC)\n?\nc:\n\no\n\nas\n1.0\n0.0\n\n0.1\n\n0.2\n\ntime (sec.)\nFigure 5: Plot of the plasma elongation K. as a function of time\nduring shot no. 9576 on the COMPASS tokamak, during which the\nelongation was being controlled in real-time by the neural network.\n\nReferences\n\nBishop C M, Cox P, Haynes P S, Roach C M, Smith M E U, Todd T N and Trotman\nD L, 1992. A neural network approach to tokamak equilibrium control. In Neural\nNetwork Applications, Ed. J G Taylor, Springer Verlag, 114-128.\nBishop C M, Haynes P S, Roach C M, Smith ME U, Todd T N, and Trotman D L.\n1993. Hardware implementation of a neural network for plasma position control in\nCOMPASS-D. In Proceedings of the 17th. Symposium on Fusion Technology, Rome,\nItaly. 2 997-1001.\nLagin L, Bell R, Davis S, Eck T, Jardin S, Kessel C, Mcenerney J, Okabayashi\nM, Popyack J and Sauthoff N. 1993. Application of neural networks for real-time\ncalculations of plasma equilibrium parameters for PBX-M, In Proceedings of the\n17th. Symposium on Fusion Technology, Rome, Italy. 21057-106l.\nLister J Band Schnurrenberger H. 1991. Fast non-linear extraction of plasma\nparameters using a neural network mapping. Nuclear Fusion. 31, 1291-1300.\n\n\x0cPulsestream Synapses with Non-Volatile\nAnalogue Amorphous-Silicon Memories.\n\nA.J. Holmes, A.F. Murray, S. Churcher and J. Hajto\nDepartment of Electrical Engineering\nUniversity of Edinburgh\nEdinburgh, EH9 3JL\nM. J. Rose\nDept. of Applied Physics and Electronics,\nDundee University\nDundee DD14HN\n\nAbstract\nA novel two-terminal device, consisting of a thin lOooA layer of p+\na-Si:H sandwiched between Vanadium and Chromium electrodes,\nexhibits a non-volatile, analogue memory action. This device stores\nsynaptic weights in an ANN chip, replacing the capacitor previously\nused for dynamic weight storage. Two different synapse designs are\ndiscussed and results are presented.\n\n1\n\nINTRODUCTION\n\nAnalogue hardware implementations of neural networks have hitherto been hampered by the lack of a straightforward (local) analogue memory capability. The\nideal storage mechanism would be compact, non-volatile, easily reprogrammable,\nand would not interfere with the normal silicon chip fabrication process.\nTechniques which have been used to date include resistors (these are not generally\nreprogrammable, and suffer from being large and difficult to fabricate with any accuracy), dynamic capacitive storage [4] (this is compact, reprogrammable and simple,\nbut implies an increase in system complexity, arising from off-chip refresh circuitry),\n\n\x0c764\n\nA. J. Holmes, A. F. Murray, S. Churcher, J. Hajto, M. J. Rose\n\nEEPROM ("floating gate") memory [5] (which is compact, reprogrammable, and\nnon-volatile, but is slow, and cannot be reprogrammed in situ), and local digital\nstorage (which is non-volatile, easily programmable and simple, but consumes area\nhorribly).\nAmorphous silicon has been used for synaptic weight storage [1, 2], but only as\neither a high-resistance fixed weight medium or a binary memory.\nIn this paper, we demonstrate that novel amorphous silicon memory devices can be\nincorporated into standard CMOS synapse circuits, to provide an analogue weight\nstorage mechanism which is compact, non-volatile, easily reprogrammable, and simple to implement.\n\n2\n\na-Si:H MEMORY DEVICES\n\nThe a-Si:H analogue memory device [3] comprises a lOooA thick layer of amorphous\nsilicon (p+ a-Si:H) sandwiched between Vanadium and Chromium electrodes.\nThe a-Si device takes the form of a two-terminal, programmable resistor. It is an\n"add-on" to a conventional CMOS process, and does not demand that the normal\nCMOS fabrication cycle be disrupted. The a-Si device sits on top of the completed\nchip circuitry, making contact with the CMOS arithmetic elements via holes cut in\nthe protective passivation layer, as shown in Figure 1.\n\nCMOS Passivation\nFigure 1: The construction of a-Si:H Devices on a CMOS chip\nAfter fabrication a number of electronic procedures must be performed in order to\nprogram the device to a given resistance state.\nProgramming, and Pre-Programming Procedures\n\nBefore the a-Si device is usable, the following steps must be carried out:\n? Forming: This is a once-only process, applied to the a-Si device in its\n"virgin" state, where it has a resistance of several MO. A series of 300ns\npulses, increasing in amplitude from 5v to 14v, is applied to the device\nelectrodes. This creates a vertical conducting channel or filament whose\napproximate resistance is 1KO. This filament can then be programmed to\na value in the range lKO to 1 MO . The details of the physical mechanisms\nare not yet fully established, but it is clear that conduction occurs through\na narrow (sub-micron) conducting channel.\n\n\x0cPulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories\n\n765\n\n? Write: To decrease the device\'s resistance, negative "Write", pulses are\napplied.\n? Erase: To increase the device\'s resistance, positive" Erase" , pulses are applied.\n? Usage: Pulses below O.5v do not change the device resistance. The resistance can therefore be utilised as a weight storage medium using a voltage\nof less than O.5v without causing reprogramming.\nProgramming pulses, which range between 2v and 5v, are typically 120ns in duration. Programming is therefore much faster than for other EEPROM (floating\ngate) devices used in the same context, which use a series of 100jls pulses to set the\nthreshold voltage [5].\nThe following sections describe synapse circuits using the a-Si:H devices. These\nsynapses use the reprogrammable a-Si:H resistor in the place of a storage capacitor\nor EEPROM cell. These new synapses were implemented on a chip referred to as\nASiTEST2, consisting of five main test blocks, each comprising of four synapses\nconnected to a single neuron.\n\n3\n\nThe EPSILON based synapse\n\nThe first synapse to be designed used the a-Si:H resistor as a direct replacement for\nthe storage capacitor used in the EPSILON [4] synapse.\n\n+Sv\n\nNeuron\n\n1\nI\n\nV\n\n..\n\nt.\n\n~:l:\n\n><!:\n\n~\n\nMirror Set\n\nE\n\n30\n\na-Si => Vw\n\nCircuitry\n\nOriginal\nStorage\nCapacitor\n\nO.5v\n\n<0-----------.,...\n__\n\n...\n\nEPSILON Synapse\n\nFigure 2: The EPSILON Synapse with a-Si:H weight storage\n\nIn the original EPSILON chip the weight voltage was stored as a voltage on a\ncapacitor. In this new synapse design, shown in Figure 2, the a-Si:H resistance is\nset such that the voltage drop produced by Iset is equivalent to the original weight\nvoltage, Vw, that was stored dynamically on the capacitor.\nA new, simpler, synapse, which can be operated from a single +5v supply, was also\nbe included on the ASiTEST2 chip.\n\n\x0cA. J. Holmes, A. F. Murray, S. Churcher, J. Hajto, M. J. Rose\n\n766\n\n4\n\nThe MkII synapse\n\nThe circuit is shown in Figure 3. The a-Si:H memory is used to store a current,\nIasi. This current is subtracted from a zero current, Isy...:z" to give a weight current\n, +/-Iw, which adds or subtracts charge from the activity capacitor, Cact, thus\nimplementing excitation or inhibition respectively.\nFor the circuit to function correctly we must limit the voltage on the activity capacitor to the range [1.5v,3.5v], to ensure that the transistors mirroring Isy_z and\nIasi remain in saturation. As Figure 3 shows, there are few reference signals and\nthe circuit operates from a single +5v power supply rail, in sharp contrast to many\nearlier analogue neural circuits, including our own.\n\n1v""\n\n+5vPWm\n\nII .\n\n~\n\n881\n\nVsel\n\n.r--\\.\n\n-.L\n\n....L\n\n*\n\nComparator\nPWout\n..rL\n\nCact\n\nVramp\n\n~\n\nOv\nE\n;.\nMirror Set\n\n"\'E~-----------:~~\n\nE\n\nSynapse\n\nPower Supplies\nV5_0=5.Ov\n\nReferences\nVrstv?2.5v\nIsy_z=5uA\n\n;.\n\nNeuron\n\nTail Currents\n\nIneu=4uA\n\nFigure 3: The MkII synapse\n\nOn first inspection the main drawback of this design would appear to be a reliance\non the accuracy with which the zero current Isy...:z, is mirrored across an entire chip.\nThe variation in this current means that two cells with the same synapse resistance\ncould produce widely differing values of Iw. However, during programming we\ndo not use the resistance of the a-Si:H device as a target value. We monitor the\nvoltage on Cact for a given PWin signal, increasing or decreasing the resistance\nof the a-Si:H device until the desired voltage level is achieved.\nExample: To set a weight to be the maximum positive value, we adjust the a-Si\nresistance until a PWin signal of 5us, the maximum input signal, gives a voltage of\n3.5v on the integration capacitor.\nWe are able to set the synapse weight using the whole integration range of [1.5v,3.5v]\nby only closing Vsel for the desired synapse during programming. In normal operating mode all four Vsel switches will be closed so that the integration charge is\nsummed over all four local capacitors.\n\n\x0cPulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories\n\n4.1\n\n767\n\nExample - Stability Test\n\nAs an example of the use of integration voltage as means of monitoring the resistance\nof a particular synapse we have included a stability test. This was carried out on\none of the test chips which contained the MkII synapse.\nThe four synapses on the test chip were programmed to give different levels of\nactivation. The chip was then powered up for 30mins each day during a 7-day\nperiod, and the activation levels for each synapse were measured three times.\n3.5\n\nStability Test - PWin = 3us\n\n,----~-_._--;--__..___r...:....,.-_._,.__-,.--.,..,.-__..__:___.,\n\ntestl\n\ntest2\n\ntest4\n\ntest3\n\ntestS\n\ntest7\n\ntest6\n\n3\n\n?\n,.\nI\n\nt:\'"\n-~\n\n1:1\n?\n\n\'t\n\n,\n\n?\n\n~\n\n?\n\n- ~ -:- - ~ -:- -\n\n25\n?\n\n2\n\n?\n\n~.\n\n?\n\n-:- - - . of - ~-:- -\n\n..\n\n.\n\nI\n\nI\n\n?\n\n?\n\n{,o-:- - .\n- ~-s4\n.\n\n,\n.\n.\n\'.\n.?\n.?\n.?\n.?\n- - ~ - - ~ - - --:- - - ~ - - -~- - - w. -- --r s2\n- - - .;. \'" - -: -011>- - :.. ~ - ~ -oGii - -:- - - - i-- ~ - ..; -sl\n?\n.\n.\n.\n.\n.\n?\n\n?\n\n~ - - ~ - - -~- - - ~ - ?\n\nI\n\n?\n\nI\nI\n\n,\nI\n\n?\n?\n\n?\n\n,\n\n?\n?\nI\n\nI\n?\n?\n\n10\n\n20\n\n30\n\n-.\nL---L- -- -~\n\n~ -s3\n\n,\n\n?\n\n,\n,\n\n?\n?\n\n?\n?\n\n?\n?\n\n?\n\n,\n\nI\n\n,\n\n?\n\n?\nI\n:\n\nI\n?\n:\n\n?\nI\n?\n\n?\n?\n?\n\n?\n?\n?\n\n40\n\n50\n\n60\n\n,\n\n70\n\n80\n\n?\n\n90\n\nMeasurement Index\n\nFigure 4: ASiTEST2- Stability Test\nAs figure 4 shows, the memories remain in the same resistance state (i.e retain their\nprogrammed weight value) over the whole 7-day period. Separate experiments on\nisolated devices indicate much longer hold times - of the order of months at least.\n\n5\n\nASiTEST3\n\nRecently we have received our latest, overtly neural, a-Si:H based test chip. This\ncontains an 8x8 array of the MkII synapses.\nThe circuit board for this device has been constructed and partially tested while\nthe ASiTEST3 chips are awaiting the deposition of the a-Si:H layers. We have been\nable to use an ASiTEST2 chip containing two of the MkII synapse test blocks i.e.\n8 synapses and 2 neurons to exercise much of the board\'s functionality.\nThe test board contains a simple state machine which has four different states:\n? State 0: Load Input Pulsewidths into SRAM from PC.\n? State 1: Apply Input Pulsewidth signals to chipl.\n? State 2: Use Vramp to generate threshold function for chipl. The resulting\nPulsewidth outputs are used as the inputs to chip2, as well as being stored\n\n\x0cA. J. Holmes, A. F. Murray, S. Churcher, J. Hajto, M. J. Rose\n\n768\n\nin SRAM .\n? State 3: Use Vramp to generate threshold function for chip2. Read resulting\nPulsewidth Outputs into SRAM .\n? State 0: Read Output Pulsewidths from SRAM into PC.\nThe results obtained during a typical test cycle are shown in Figure 5.\nIE-- Statel --;,,;,,*1E E - - State2\n\n-----;>~i~\n\nState3 ---;!>~I\n\n~r-IF~~~--r-------~---------l\n4v\n3v\n\nPWin_O\n\n2v\nIv\n\nOv~ . . . . . . . . .\n\n3.5v r;;;;;""-~,"",,,or;:::::;;:;::;;;;:;;;::;;t---~----t--------,\n\n~:~\n\n....... ~;a~;"""\'"\n\n2.Ov\n\n~\'Sig~;,id"""""\n... ~"Li~""""\n......\n\n. . . . . . . . . . . . . . . . . . . . . . . ,.\n\n....... .\n\n.... .....\n\n..........\n\n.. ..\n\nl.~\ne?_e\n\n_____ ...... ___\n\n?\n\n_____\n\n?\n\n____________ . . . . . . . .\n\n_____\n\n????\n\n____\n\n..........\n\n.\n\n5v\n4v\n3v\n\n2v\n\n:;; ~,...,................-..--t;?.~...:...~__--! ..........~~.n-.-~~~~~~......J\nIS.\n\n10.\n\nFigure 5: ASiTEST3 Board Scope Waveforms\nAs this figure shows different ramp signals, corresponding to different threshold\nfunctions, can be applied to chipl and chip2 neurons.\n10.0\n\nSingle Buffer PulscWidth Sweeps\n\n.----.,..----r------.----r----.------,\n\n9.0\n8.0\n\n!\n\n7.0 -\n\n~\n\n5\'O~~~~~~~-~~-+++~~~~N~~~\n\n~\n\ni6.o~\n\n~----\n\nJ::\n2D\n\nNeal-Syal\nN~~JIII\n\n1.0\n\nN~~ya2\n\no.oL---~--~- --~--?\n\no\n\n0.5\n\n1.0\n\n1.5\n\n2.0\n\n2.5\n\n3.0\n\nPulscwldlh Input [WI)\n\nFigure 6: ASiTEST3 Board - MkII Synapse Characteristic\nWhile the signals shown in Figure 5 appear noisy the multiplier characteristic that\nthe chip produces is still admirably linear, as shown in Figure 6. In this experiment\nall eight synapses on a test chip were programmed into different resistance states\nand PWin was swept from 0 to 3us.\n\n\x0cPulsestream Synapses with Non- Volatile Analogue Amorphous-Silicon Memories\n\n6\n\n769\n\nConclusions\n\nWe have demonstrated the use of novel a-Si:H analogue memory devices as a means\nof storing synaptic weights in a Pulsewidth ANN. We have also demonstrated the\noperation of an interface board which allows two 8x8 ANN chips, operating as a\ntwo layer network, to be controlled by a simple PC interface card.\nThis technology is most suitable for small networks in, for example, remote control and other embedded-system applications where cost and power considerations\nfavour a single all-inclusive ANN chip with non-volatile, but programmable weights.\nAnother possible application of this technology is in large networks constructed\nusing Thin Film Technology(TFT). If TFT\'s were used in place of the CMOS transistors then the area constraint imposed by crystalline silicon would be removed,\nallowing truly massively parallel networks to be integrated.\nIn summary - the a-Si:H analogue memory devices described in this paper provide a\nroute to an analogue, non-volatile and fast synaptic weight storage medium. At the\npresent time neither the programming nor storage mechanisms are fully understood\nmaking it difficult to compare this new device with more established technologies\nsuch as the ubiquitous Floating-Gate EEPROM technique. Current research is\nfocused on firstly, improving the yield on the a-Si:H device which is unacceptably\nlow at present, a demerit that we attribute to imperfections in the a-Si fabrication\nprocess and secondly, improving understanding of the device physics and hence the\nprogramming and storage mechanisms.\nAcknowledgements\nThis research has been jointly funded by BT, and EPSRC (formerly SERC), the\nEngineering and Physical Sciences Research Council.\n\nReferences\n[1] W. Hubbard et al.(1986) Electronic Neural Networks AlP Conference Proceedings - Snowbird 1986 :227-234\n[2] H.P. Graf (1986) VLSI Implementation of a NN memory with several hundreds\nof neurons AlP Conference Proceedings - Snowbird 1986 :182-187.\n[3] M.J. Rose et al (1989) Amorphous Silicon Analogue Memory Devices Journal\nof Non-Crystalline Solids 1(115):168-170\n[4] A.Hamilton et al. (1992) Integrated Pulse-Stream Neural Networks - Results,\nIssues and Pointers IEEE Transactions on N.N.s 3(3):385-393\n[5] M.Holler, S.Tam, H.Castro and R.Benson (1989) An Electrically Trainable ANN\nwith 10240 Floating Gate Synapses. Int Conf on N.N.s Proc :191-196\n[6] A.F.Murray and A.V.W.Smith.(1987) Asynchronous Arithmetic for VLSI Neural Systems. Electronics Letters 23(12):642-643\n[7] A.J. Holmes et al. (1993) Use of a-Si:H Memory Devices for Non-volatile Weight\nStorage in ANNs. Proc lCAS 15 :817-820\n\n\x0c\x0c'
p83177
sg16
S"Real-Time Control of a Tokamak Plasma\nUsing Neural Networks\n\nChris M Bishop\nNeural Computing Research Group\nDepartment of Computer Science\nAston University\nBirmingham, B4 7ET, U.K.\nc.m .bishop@aston .ac .uk\n\nPaul S Haynes, Mike E U Smith, Tom N Todd,\nDavid L Trotman and Colin G Windsor\nAEA Technology, Culham Laboratory,\nOxfordshire OX14 3DB\n(Euratom/UKAEA Fusion Association)\n\nAbstract\nThis paper presents results from the first use of neural networks\nfor the real-time feedback control of high temperature plasmas in\na tokamak fusion experiment. The tokamak is currently the principal experimental device for research into the magnetic confinement approach to controlled fusion. In the tokamak, hydrogen\nplasmas, at temperatures of up to 100 Million K, are confined\nby strong magnetic fields. Accurate control of the position and\nshape of the plasma boundary requires real-time feedback control\nof the magnetic field structure on a time-scale of a few tens of microseconds. Software simulations have demonstrated that a neural\nnetwork approach can give significantly better performance than\nthe linear technique currently used on most tokamak experiments.\nThe practical application of the neural network approach requires\nhigh-speed hardware, for which a fully parallel implementation of\nthe multilayer perceptron, using a hybrid of digital and analogue\ntechnology, has been developed.\n\n\x0c1008\n\n1\n\nC. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor\n\nINTRODUCTION\n\nFusion of the nuclei of hydrogen provides the energy source which powers the sun.\n\nIt also offers the possibility of a practically limitless terrestrial source of energy.\nHowever, the harnessing of this power has proved to be a highly challenging problem. One of the most promising approaches is based on magnetic confinement of a\nhigh temperature (10 7 - 108 Kelvin) plasma in a device called a tokamak (from the\nRussian for 'toroidal magnetic chamber') as illustrated schematically in Figure 1.\nAt these temperatures the highly ionized plasma is an excellent electrical conductor, and can be confined and shaped by strong magnetic fields. Early tokamaks\nhad plasmas with circular cross-sections, for which feedback control of the plasma\nposition and shape is relatively straightforward. However, recent tokamaks, such as\nthe COMPASS experiment at Culham Laboratory, as well as most next-generation\ntokamaks, are designed to produce plasmas whose cross-sections are strongly noncircular. Figure 2 illustrates some of the plasma shapes which COMPASS is designed to explore. These novel cross-sections provide substantially improved energy\nconfinement properties and thereby significantly enhance the performance of the\ntokamak.\n\nz\n\nR\n\nFigure 1: Schematic cross-section of a tokamak experiment showing the toroidal vacuum vessel (outer D-shaped curve) and plasma\n(shown shaded). Also shown are the radial (R) and vertical (Z) coordinates. To a good approximation, the tokamak can be regarded\nas axisymmetric about the Z-axis, and so the plasma boundary can\nbe described by its cross-sectional shape at one particular toroidal\nlocation.\nUnlike circular cross-section plasmas, highly non-circular shapes are more difficult to\nproduce and to control accurately, since currents through several control coils must\nbe adjusted simultaneously. Furthermore, during a typical plasma pulse, the shape\nmust evolve, usually from some initial near-circular shape. Due to uncertainties\nin the current and pressure distributions within the plasma, the desired accuracy\nfor plasma control can only be achieved by making real-time measurements of the\nposition and shape of the boundary, and using error feedback to adjust the currents\nin the control coils.\nThe physics of the plasma equilibrium is determined by force balance between the\n\n\x0c1009\n\nReal-Time Control of Tokamak Plasma Using Neural Networks\n\ncircle\n\nellipse\n\nO-shape\n\nbean\n\nFigure 2: Cross-sections of the COMPASS vacuum vessel showing\nsome examples of potential plasma shapes. The solid curve is the\nboundary of the vacuum vessel, and the plasma is shown by the\nshaded regions.\n\nthermal pressure of the plasma and the pressure of the magnetic field, and is relatively well understood. Particular plasma configurations are described in terms\nof solutions of a non-linear partial differential equation called the Grad-Shafranov\n(GS) equation. Due to the non-linear nature of this equation, a general analytic\nsolution is not possible. However, the GS equation can be solved by iterative numerical methods, with boundary conditions determined by currents flowing in the\nexternal control coils which surround the vacuum vessel. On the tokamak itself it\nis changes in these currents which are used to alter the position and cross-sectional\nshape of the plasma. Numerical solution of the GS equation represents the standard technique for post-shot analysis of the plasma, and is also the method used\nto generate the training dataset for the neural network, as described in the next\nsection. However , this approach is computationally very intensive and is therefore\nunsuitable for feedback control purposes.\nFor real-time control it is necessary to have a fast (typically:::; 50J.lsec.) determination of the plasma boundary shape. This information can be extracted from a\nvariety of diagnostic systems , the most important being local magnetic measurements taken at a number of points around the perimeter of the vacuum vessel.\nMost tokamaks have several tens or hundreds of small pick up coils located at carefully optimized points around the torus for this purpose. We shall represent these\nmagnetic signals collectively as a vector m .\nFor a large class of equilibria, the plasma boundary can be reasonably well represented in terms of a simple parameterization, governed by an angle-like variable B,\ngiven by\n\nR(B)\nZ(B)\n\nRo + a cos(B + 8 sinB)\nZo + a/\\,sinB\n\nwhere we have defined the following parameters\n\n(1)\n\n\x0c1010\n\nRo\nZo\na\nK\n\n6\n\nC. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor\n\nradial distance of the plasma center from the major axis of the torus,\nvertical distance of the plasma center from the torus midplane,\nminor radius measured in the plane Z = Zo,\nelongation,\ntriangularity.\n\nWe denote these parameters collectively by Yk. The basic problem which has to be\naddressed, therefore, is to find a representation for the (non-linear) mapping from\nthe magnetic signals m to the values of the geometrical parameters Yk, which can\nbe implemented in suitable hardware for real-time control.\nThe conventional approach presently in use on many tokamaks involves approximating the mapping between the measured magnetic signals and the geometrical\nparameters by a single linear transformation. However, the intrinsic non-linearity\nof the mappings suggests that a representation in terms of feedforward neural networks should give significantly improved results (Lister and Schnurrenberger, 1991;\nBishop et a/., 1992; Lagin et at., 1993). Figure 3 shows a block diagram of the\ncontrol loop for the neural network approach to tokamak equilibrium control.\nNeural\n\nNetwork\n\nFigure 3: Block diagram of the control loop used for real-time\nfeedback control of plasma position and shape.\n\n2\n\nSOFTWARE SIMULATION RESULTS\n\nThe dataset for training and testing the network was generated by numerical solution of the GS equation using a free-boundary equilibrium code. The data base\ncurrently consists of over 2,000 equilibria spanning the wide range of plasma positions and shapes available in COMPASS. Each equilibrium configuration takes\nseveral minutes to generate on a fast workstation. The boundary of each configuration is then fitted using the form in equation 1, so that the equilibria are labelled\nwith the appropriate values of the shape parameters. Of the 120 magnetic signals\navailable on COMPASS which could be used to provide inputs to the network, a\n\n\x0c1011\n\nReal-Time Control o/Tokamak PLasma Using Neural Networks\n\nsubset of 16 has been chosen using sequential forward selection based on a linear\nrepresentation for the mapping (discussed below) .\nIt is important to note that the transformation from magnetic signals to flux surface\nparameters involves an exact linear invariance. This follows from the fact that, if all\nof the currents are scaled by a constant factor, then the magnetic fields will be scaled\nby this factor, and the geometry of the plasma boundary will be unchanged . It is\nimportant to take advantage of this prior knowledge and to build it into the network\nstructure, rather than force the network to learn it by example. We therefore\nnormalize the vector m of input signals to the network by dividing by a quantity\nproportional to the total plasma current. Note that this normalization has to be\nincorporated into the hardware implementation of the network, as will be discussed\nin Section 3.\n1.2\n\n4\n01\n\n2\n\n2\n\n01\n\nc\n\nc\n.5.\n\n0-\n\n~\n:E\n\n.5.\nCIS\n\n:E\n\n1iI\n\n~\n::J\n\n?\n\n1iI\nCD\n\n-2\n\ngo.8\n\n.5.\n0-\n\n?\n\nCIS\n\n:E\n\n1iI 0 .4\nCD\n\nc\n::J\n\nc\n\n::J\n\n-2\n\n-4\nDatabase\n\n?\nDatabase\n\n1.2\n\n4\n~\n\n~CD\n\nZ\n\n~\n:::I\n\nCD\n\nz\n\n.2\nDatabase\n\n2\n\n~O.8\n~\n\n?\n\nCD\n\nz\n\n~O.4\n\n-2\n\n:::I\n\nCD\n\nZ\n\n?\n\n-4\nDatabase\n\nDatabase\n\n.2\nDatabase\n\nFigure 4: Plots of the values from the test set versus the values\npredicted by the linear mapping for the 3 equilibrium parameters,\ntogether with the corresponding plots for a neural network with 4\nhidden units.\n\nThe results presented in this paper are based on a multilayer perceptron architecture\nhaving a single layer of hidden units with 'tanh' activation functions , and linear\noutput units. Networks are trained by minimization of a sum-of-squares error using\na standard conjugate gradients optimization algorithm, and the number of hidden\n\n\x0cJ012\n\nC. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor\n\nunits is optimized by measuring performance with respect to an independent test\nset. Results from the neural network mapping are compared with those from the\noptimal linear mapping, that is the single linear transformation which minimizes\nthe same sum-of-squares error as is used in the neural network training algorithm,\nas this represents the method currently used on a number of present day tokamaks .\nInitial results were obtained on networks having 3 output units, corresponding to\nthe values of vertical position ZQ, major radius RQ, and elongation K; these being\nparameters which are of interest for real-time feedback control. The smallest normalized test set error of 11.7 is obtained from the network having 16 hidden units.\nBy comparison, the optimal linear mapping gave a normalized test set error of 18.3.\nThis represents a reduction in error of about 30% in going from the linear mapping\nto the neural network. Such an improvement, in the context of this application , is\nvery significant.\nFor the experiments on real-time feedback control described in Section 4 the currently available hardware only permitted networks having 4 hidden units, and so we\nconsider the results from this network in more detail. Figure 4 shows plots of the\nnetwork predictions for various parameters versus the corresponding values from\nthe test set portion of the database. Analogous plots for the optimal linear map\npredictions versus the database values are also shown. Comparison of the corresponding figures shows the improved predictive capability of the neural network,\neven for this sub-optimal network topology.\n\n3\n\nHARDWARE IMPLEMENTATION\n\nThe hardware implementation of the neural network must have a bandwidth of 2:\n20 kHz in order to cope with the fast timescales of the plasma evolution. It must\nalso have an output precision of at least (the the analogue equivalent of) 8 bits in\norder to ensure that the final accuracy which is attainable will not be limited by the\nhardware system. We have chosen to develop a fully parallel custom implementation\nof the multilayer perceptron, based on analogue signal paths with digitally stored\nsynaptic weights (Bishop et al., 1993). A VME-based modular construction has\nbeen chosen as this allows flexibility in changing the network architecture, ease of\nloading network weights, and simplicity of data acquisition. Three separate types\nof card have been developed as follows:\n? Combined 16-input buffer and signal normalizer.\nThis provides an analogue hardware implementation of the input normalization described earlier.\n? 16 x 4 matrix multiplier\nThe synaptic weights are produced using 12 bit frequency-compensated\nmultiplying DACs (digital to analogue converters) which can be configured\nto allow 4-quadrant multiplication of analogue signals by a digitally stored\nnumber.\n? 4-channel sigmoid module\nThere are many ways to produce a sigmoidal non-linearity, and we have\nopted for a solution using two transistors configured as along-tailed-pair,\n\n\x0cReal-Time Control of Tokamak Plasma Using Neural Networks\n\n1013\n\nto generate a 'tanh ' sigmoidal transfer characteristic. The principal drawback of such an approach is the strong temperature sensitivity due to the\nappearance of temperature in the denominator of the exponential transistor\ntransfer characteristic. An elegant solution to this problem has been found\nby exploiting a chip containing 5 transistors in close thermal contact. Two\nof the transistors form the long-tailed pair, one of the transistors is used\nas a heat source, and the remaining two transistors are used to measure\ntemperature. External circuitry provides active thermal feedback control,\nand stability to changes in ambient temperature over the range O?C to 50?C\nis found to be well within the acceptable range.\nThe complete network is constructed by mounting the appropriate combination\nof cards in a VME rack and configuring the network topology using front panel\ninterconnections. The system includes extensive diagnostics, allowing voltages at\nall key points within the network to be monitored as a function of time via a series\nof multiplexed output channels.\n\n4\n\nRESULTS FROM REAL-TIME FEEDBACK CONTROL\n\nFigure 5 shows the first results obtained from real-time control of the plasma in\nthe COMPASS tokamak using neural networks. The evolution of the plasma elongation, under the control of the neural network, is plotted as a function of time\nduring a plasma pulse. Here the desired elongation has been preprogrammed to\nfollow a series of steps as a function of time. The remaining 2 network outputs\n(radial position Ro and vertical position Zo) were digitized for post-shot diagnosis ,\nbut were not used for real-time control. The solid curve shows the value of elongation given by the corresponding network output, and the dashed curve shows the\npost-shot reconstruction of the elongation obtained from a simple 'filament' code,\nwhich gives relatively rapid post-shot plasma shape reconstruction but with limited\naccuracy. The circles denote the elongation values given by the much more accurate\nreconstructions obtained from the full equilibrium code. The graph clearly shows\nthe network generating the required elongation signal in close agreement with the\nreconstructed values. The typical residual error is of order 0.07 on elongation values\nup to around 1.5. Part of this error is attributable to residual offset in the integrators used to extract magnetic field information from the pick-up coils, and this is\ncurrently being corrected through modifications to the integrator design. An additional contribution to the error arises from the restricted number of hidden units\navailable with the initial hardware configuration. While these results represent the\nfirst obtained using closed loop control, it is clear from earlier software modelling of\nlarger network architectures (such as 32- 16-4) that residual errors of order a few %\nshould be attainable. The implementation of such larger networks is being persued,\nfollowing the successes with the smaller system.\nAcknowledgements\nWe would like to thank Peter Cox, Jo Lister and Colin Roach for many useful\ndiscussions and technical contributions. This work was partially supported by the\nUK Department of Trade and Industry.\n\n\x0cC. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor\n\n1014\n\n1.8\nshot 9576\n\nc:\n\no\n\n~\n14\nC)\n?\nc:\n\no\n\nas\n1.0\n0.0\n\n0.1\n\n0.2\n\ntime (sec.)\nFigure 5: Plot of the plasma elongation K. as a function of time\nduring shot no. 9576 on the COMPASS tokamak, during which the\nelongation was being controlled in real-time by the neural network.\n\nReferences\n\nBishop C M, Cox P, Haynes P S, Roach C M, Smith M E U, Todd T N and Trotman\nD L, 1992. A neural network approach to tokamak equilibrium control. In Neural\nNetwork Applications, Ed. J G Taylor, Springer Verlag, 114-128.\nBishop C M, Haynes P S, Roach C M, Smith ME U, Todd T N, and Trotman D L.\n1993. Hardware implementation of a neural network for plasma position control in\nCOMPASS-D. In Proceedings of the 17th. Symposium on Fusion Technology, Rome,\nItaly. 2 997-1001.\nLagin L, Bell R, Davis S, Eck T, Jardin S, Kessel C, Mcenerney J, Okabayashi\nM, Popyack J and Sauthoff N. 1993. Application of neural networks for real-time\ncalculations of plasma equilibrium parameters for PBX-M, In Proceedings of the\n17th. Symposium on Fusion Technology, Rome, Italy. 21057-106l.\nLister J Band Schnurrenberger H. 1991. Fast non-linear extraction of plasma\nparameters using a neural network mapping. Nuclear Fusion. 31, 1291-1300.\n\n\x0c"
p83178
sg135
S'ICEG Morphology Classification using an\nAnalogue VLSI Neural Network\n\nRichard Coggins, Marwan Jabri, Barry Flower and Stephen Pickard\nSystems Engineering and Design Automation Laboratory\nDepartment of Electrical Engineering J03,\nUniversity of Sydney, 2006, Australia.\nEmail: richardc@sedal.su.oz.au\n\nAbstract\nAn analogue VLSI neural network has been designed and tested\nto perform cardiac morphology classification tasks. Analogue techniques were chosen to meet the strict power and area requirements\nof an Implantable Cardioverter Defibrillator (ICD) system. The robustness of the neural network architecture reduces the impact of\nnoise, drift and offsets inherent in analogue approaches. The network is a 10:6:3 multi-layer percept ron with on chip digital weight\nstorage, a bucket brigade input to feed the Intracardiac Electrogram (ICEG) to the network and has a winner take all circuit\nat the output. The network was trained in loop and included a\ncommercial ICD in the signal processing path. The system has successfully distinguished arrhythmia for different patients with better\nthan 90% true positive and true negative detections for dangerous\nrhythms which cannot be detected by present ICDs. The chip was\nimplemented in 1.2um CMOS and consumes less than 200nW maximum average power in an area of 2.2 x 2.2mm2.\n\n1\n\nINTRODUCTION\n\nTo the present time, most ICDs have used timing information from ventricular\nleads only to classify rhythms which has meant some dangerous rhythms can not\nbe distinguished from safe ones, limiting the use of the device. Even two lead\n\n\x0c732\n\nRichard Coggins, Marwan Jabri, Barry Flower, Stephen Pickard\n\n4.00\n\nHO\n3.00\n\n2.00\n\nI.SO\n\n_ _ _:::::::!\n\nQ\n1.00\n\nO.SO\n\nFigure 1: The Morphology of ST and VT retrograde 1:1.\n\natrial/ventricular systems fail to distinguish some rhythms when timing information alone is used [Leong and Jabri, 1992]. A case in point is the separation of Sinus Tachycardia (ST) from Ventricular Tachycardia with 1:1 retrograde conduction.\nST is a safe arrhythmia which may occur during vigorous exercise and is characterised by a heart rate of approximately 120 beats/minute. VT retrograde 1:1 also\noccurs at the same low rate but can be a potentially fatal condition. False negative\ndetections can cause serious heart muscle injury while false positive detections deplete the batteries, cause patient suffering and may lead to costly transplantation\nof the device. Figure 1 shows however, the way in which the morphology changes\non the ventricular lead for these rhythms. Note, that the morphology change is\npredominantly in the "QRS complex" where the letters QRS are the conventional\nlabels for the different points in the conduction cycle during which the heart is\nactually pumping blood.\nFor a number of years, researchers have studied template matching schemes in order\nto try and detect such morphology changes. However, techniques such as correlation\nwaveform analysis [Lin et. al., 1988], though quite successful are too computationally intensive to meet power requirements. In this paper, we demonstrate that\nan analogue VLSI neural network can detect such morphology changes while still\nmeeting the strict power and area requirements of an implantable system. The\nadvantages of an analogue approach are born out when one considers that an energy efficient analogue to digital converter such as [Kusumoto et. al., 1993] uses\n1.5nJ per conversion implying 375nW power consumption for analogue to digital\nconversion of the ICEG alone. Hence, the integration of a bucket brigade device and\nanalogue neural network provides a very efficient way of interfacing to the analogue\ndomain. Further, since the network is trained in loop with the ICD in real time,\nthe effects of device offsets, noise, QRS detection jitter and signal distortion in the\nanalogue circuits are largely alleviated.\nThe next section discusses the chip circuit designs. Section 3 describes the method\n\n\x0cICEG Morphology Classification Using an Analogue VLSI Neural Network\n\n733\n\nAowAcId. . .\n\n1axl Syna.... AIRy\n\n"-\n\nColumn\nAoIcIr.-\n\nI\n\no.ta Reglsl...\n\nIClkcMmux\n\nI\n\nBu1I...\n\nI WTAI\n\n10 DOD DO\n\nFigure 2: Floor Plan and Photomicrograph of the chip\nused to train the network for the morphology classification task. Section 4 describes\nthe classifier performance on seven patients with arrhythmia which can not be\ndistinguished using the heart rate only. Section 5 summarises the results, remaining\nproblems and future directions for the work .\n\n2\n\nARCHITECTURE\n\nThe neural network chip consists of a 10:6:3 multilayer perceptron, an input bucket\nbrigade device (BBD) and a winner take all (WTA) circuit at the output. A floor\nplan and photomicrograph of the chip appears in figure 2. The BBD samples the\nincoming ICEG at a rate of 250Hz. For three class problems, the winner take all\ncircuit converts the winning class to a digital signal. For the two class problem\nconsidered in this paper , a simple thresholding function suffices. The following\nsubsections briefly describe the functional elements of the chip . The circuit diagrams\nfor the chip building blocks appear in figure 3.\n\n2.1\n\nBUCKET BRIGADE DEVICE\n\nOne stage of the bucket brigade circuit is shown in figure 3. The BBD uses a\ntwo phase clock to shift charge from cell to cell and is based on a design by\nLeong [Leong, 1992] . The BBD operates by transferring charge deficits from S\nto D in each of the cells. PHIl and PHI2 are two phase non-overlapping clocks.\nThe cell is buffered from the synapse array to maintain high charge transfer efficiency. A sample and hold facility is provided to store the input on the gates of the\nsynapses. The BBD clocks are generated off chip and are controlled by the QRS\ncomplex detector in the lCD.\n\n2.2\n\nSYNAPSE\n\nThis synapse has been used on a number of neural network chips previously.\ne.g . [Coggins et. al., 1994] . The synapse has five bits plus sign weight storage which\n\n\x0c734\n\nRichard Coggins, Marwan Jabri, Barry Flower, Stephen Pickard\n\nNEURON\n\n.-----------------------------------------------------------,,,\n,,\n~ !\nBUJIOIII\'\n\n00\n\nBUCKET BRIGADE ClLL\n\n"\n\nFigure 3: Neuron, Bucket Brigade and Synapse Circuit Diagrams.\nsets the bias to a differential pair which performs the multiplication. The bias references for the weights are derived from a weighted current source in the corner of\nthe chip. A four quadrant multiplication is achieved by the four switches at the top\nof the differential pair.\n\n2.3\n\nNEURON\n\nDue to the low power requirements, the bias currents of the synapse arrays are of\nthe order of hundreds of nano amps, hence the neurons must provide an effective\nresistance of many mega ohms to feed the next synapse layer while also providing\ngain control. Without special high resistance polysilicon, simple resistive neurons\nuse prohibitive area, However, for larger networks with fan-in much greater than\nten, an additional problem of common mode cancellation is encountered, That is,\nas the fan-in increases, a larger common mode range is required or a cancellation\nscheme using common mode feedback is needed.\nThe neuron of figure 3 implements such a cancellation scheme, The mirrors MO/M2\nand Ml/M3 divide the input current and facilitate the sum at the drain of M7.\nM7/M8 mirrors the sum so that it may be split into two equal currents by the\nmirrors formed by M4, M5 and M6 which are then subtracted from the input\ncurrents. Thus, the differential voltage vp - Vm is a function of the transistor\ntransconductances, the common mode input current and the feedback factor , The\ngain of the neuron can be controlled by varying the width to length ratio of the\nmirror transistors MO and Ml. The implementation in this case allows seven gain\ncombinations, using a three bit RAM cell to store the gain,\n\n\x0cICEG Morphology Classification Using an Analogue VLSI Neural Network\n\n735\n\nImplantable\nC.cio?erlor\n\nDefibrillalOr\n\nRunnngMUME\n\nNe .....1\nNelwa\'1<\nChip\n\nFigure 4: Block Diagram of the Training and Testing System.\nThe importance of a common mode cancellation scheme for large networks can\nbe seen when compared to the straight forward approach of resistive or switched\ncapacitor neurons. This may be illustrated by considering the energy usage of\nthe two approaches. Firstly, we need to define the required gain of the neuron\nas a function of its fan-in . If we assume that useful inputs to the network are\nmostly sparse, i.e. with a small fraction of non-zero values, then the gain is largely\nindependent of the fan-in, yet the common mode signal increases linearly with fanin. For the case of a neuron which does not cancel the common mode, the power\nsupply voltage must be increased to accommodate the common mode signal, thus\nleading to a quadratic increase in energy use with fan-in. A common mode cancelling\nneuron on the other hand , suffers only a linear increase in energy use with fan-in\nsince extra voltage range is not required and the increased energy use arises only\ndue to the linear increase in common mode current.\n\n3\n\nTRAINING SYSTEM\n\nThe system used to train and test the neural network is shown in figure 4. Control\nof training and testing takes place on the PC. The PC uses a PC-LAB card to\nprovide analogue and digital I/O . The PC plays the ICEG signal to the input of\nthe commercial ICD in real time. Note, that the PC is only required for initially\ntraining the network and in this case as a source of the heart signal. The commercial\nICD performs the function of QRS complex detection using analogue circuits. The\nQRS complex detection signal is then used to freeze the BBD clocks of the chip, so\nthat a classification can take place.\nWhen training, a number of examples of the arrhythmia to be classified are selected\nfrom a single patient data base recorded during an electrophysiological study and\npreviously classified by a cardiologist. Since most of the morphological information\nis in the QRS complex, only these segments of the data are repeatedly presented to\n\n\x0c736\n\nRichard Coggins. Marwan Jabri. Barry Flower. Stephen Pickard\n\nPatient\n1\n2\n3\n4\n5\n6\n7\n\n% Training Attempts Converged\nRun ~\nRun 1\n\nH=3\n80\n80\n0\n60\n100\n100\n80\n\nH= 6\n10\n100\n0\n10\n80\n40\n100\n\nH=3\n60\n0\n0\n40\n0\n60\n40\n\nH=6\n60\n10\n10\n40\n60\n60\n100\n\nAverage\nIterations\n62\n86\n101\n77\n44\n46\n17\n\nTable 1: Training Performance of the system on seven patients.\nthe network. The weights are adjusted according to the training algorithm running\non the PC using the analogue outputs of the network to reduce the output error .\nThe PC writes weights to the chip via the digital I/Os of the PC-LAB card and the\nserial weight bus of network. The software package implementing the training and\ntesting, called MUME [Jabri et. al ., 1992], provides a suite of training algorithms\nand control options. Online training was used due to its success in training small\nnetworks and because the presentation of the QRS complexes to the network was\nthe slowest part of the training procedure. The algorithm used for weight updates\nin this paper was summed weight node perturbation [Flower and Jabri, 1993].\nThe system was trained on seven different patients separately all of whom had\nVT with 1: 1 retrograde conduction. Note, that patient independent training has\nbeen tried but with mixed results [Tinker, 1992] . Table 1 summarises the training\nstatistics for the seven patients. For each patient and each architecture, five training\nruns were performed starting from a different random initial weight set. Each\nof the patients was trained with eight of each class of arrhythmia. The network\narchitecture used was 10:H:1, where H is the number of hidden layer neurons and\nthe unused neurons being disabled by setting their input weights to zero. Two sets\nof data were collected denoted Run 1 and Run 2. Run 1 corresponded to output\ntarget values of ?0.6V within margin 0.45V and Run 2 to output target values of\n?0.2V within margin 0.05V. A training attempt was considered to have converged\nwhen the training set was correctly classified within two hundred training iterations.\nOnce the morphologies to be distinguished have been learned for a given patient,\nthe remainder of the patient data base is played back in a continuous stream and\nthe outputs of the classifier at each QRS complex are logged and may be compared\nto the classifications of a cardiologist. The resulting generalisation performance is\ndiscussed in the next section.\n\n4\n\nMORPHOLOGY CLASSIFIER GENERALISATION\nPERFORMANCE\n\nTable 2 summarises the generalisation performance of the system on the seven\npatients for the training attempts which converged. Most of the patients show a\ncorrect classification rate better than 90% for at least one architecture on one of the\n\n\x0cICEG Morphology Classification Using an Analogue VLSI Neural Network\n\nPatient\n1\n2\n3\n4\n5\n6\n7\n\nNo. of\nComplexes\nST\nVT\n440\n61\n57\n94\n67\n146\n166\n65\n61\n96\n61\n99\n28\n80\n\n1\n2\n3\n4\n5\n6\n7\n\n440\n94\n67\n166\n61\n61\n28\n\n61\n57\n146\n65\n96\n99\n80\n\n737\n\n% Correct Classifications Run 1\nH = 6\nH - i3\nVT\nST\nST\nVT\n89?10 89?3\n58?0\n99?0\n99?1\n99?1\n100?0 99?1\n66?44 76?37\n99?1\n50?3\n82?1 75?13\n89?9\n94?6\n84?8\n97?1\n90?5\n99?1\n97?3\n98?5\n99?1\n99?1\n% Correct Classifications Run 2\n86?14 99?1\n88?2\n99?1\n94?6\n94?3\n84?2\n99?1\n76?18 59?2\n87?7 100?0\n88?2\n49?5\n84?1\n82?5\n92?6 90?10\n99?1\n99?1\n94?3\n99?0\n94?3\n92?3\n\nTable 2: Generalisation Performance of the system on seven patients.\nruns, whereas, a timing based classifier can not separate these arrhythmia at all.\nFor each convergent weight set the network classified the test set five times. Thus,\nthe "% Correct" columns denote the mean and standard deviation of the classifier\nperformance with respect to both training and testing variations. By duty cycling\nthe bias to the network and buffers, the chip dissipates less than 200n W power for\na nominal heart rate of 120 beats/minute during generalisation.\n\n5\n\nDISCUSSION\n\nReferring to table 1 we see that the patient 3 data was relatively difficult to train.\nHowever, for the one occasion when training converged generalisation performance\nwas quite acceptable. Inspection of this patients data showed that typically, the\nmorphologies of the two rhythms were very similar. The choice of output targets,\nmargins and architecture appear to be patient dependent and possibly interacting\nfactors. Although larger margins make training easier for some patients they appear\nto also introduce more variability in generalisation performance. This may be due\nto the non-linearity of the neuron circuit. Further experiments are required to\noptimise the architecture for a given patient and to clarify the effect of varying\ntargets, margins and neuron gain. Penalty terms could also be added to the error\nfunction to minimise the possibility of missed detections of the dangerous rhythm.\nThe relatively slow rate of the heart results in the best power consumption being\nobtained by duty cycling the bias currents to the synapses and the buffers. Hence,\nthe bias settling time of the weighted current source is the limiting factor for reducing power consumption further for this design. By modifying the connection of the\ncurrent source to the synapses using a bypassing technique to reduce transients in\n\n\x0cRiclulrd Coggins, Marwan Jabri, Barry Flower, Stephen Pickard\n\n738\n\nthe weighted currents, still lower power consumption could be achieved.\n\n6\n\nCONCLUSION\n\nThe successful classification of a difficult cardiac arrhythmia problem has been\ndemonstrated using. an analogue VLSI neural network approach. Furthermore, the\nchip developed has shown very low power consumption of less than 200n W, meeting the requirements of an implantable system. The chip has performed well, with\nover 90% classification performance for most patients studied and has proved to be\nrobust when the real world influence of analogue QRS detection jitter is introduced\nby a commercial implantable cardioverter defibrillator placed in the signal path to\nthe classifier.\nAcknowledgements\n\nThe authors acknowledge the funding for the work in this paper provided under\nAustralian Generic Technology Grant Agreement No. 16029 and thank Dr. Phillip\nLeong of the University of Sydney and Dr. Peter Nickolls of Telectronics Pacing\nSystems Ltd., Australia for their helpful suggestions and advice.\nReferences\n\n[Castro et. al., 1993] H.A. Castro, S.M. Tam, M.A. Holler, "Implementation and\nPerformance of an analogue Nonvolatile Neural Network," Analogue Integrated\nCircuits and Signal Processing, vol. 4(2), pp. 97-113, September 1993.\n[Lin et. al., 1988] D. Lin, L.A. Dicarlo, and J .M. Jenkins, "Identification of Ventricular Tachycardia using Intracavitary Electrograms: analysis of time and frequency domain patterns," Pacing (3 Clinical Electrophysiology, pp. 1592-1606,\nNovember 1988.\n[Leong, 1992] P.H.W. Leong, Arrhythmia Classification Using Low Power VLSI,\nPhD Thesis, University of Sydney, Appendix B, 1992.\n[ Kusumoto et. al., 1993] K. Kusumoto et. al., "A lObit 20Mhz 30mW Pipelined\nInterpolating ADC," ISSCC, Digest of Technical Papers, pp. 62-63, 1993.\n[Leong and Jabri, 1992] P.H.W. Leong and M. Jabri, "MATIC - An Intracardiac Tachycardia Classification System", Pacing (3 Clinical Electrophysiology,\nSeptember 1992.\n[Coggins et. al., 1994] R.J. Coggins and M.A. Jabri, "WATTLE: A Trainable Gain\nAnalogue VLSI Neural Network", NIPS6, Morgan Kauffmann Publishers, 1994.\n[Jabri et. al., 1992] M.A. Jabri, E.A. Tinker and L. Leerink, "MUME- A MultiNet-Multi-Architecture Neural Simulation Environment", Neural Network Simulation Environments, Kluwer Academic Publications, January, 1994.\n[Flower and Jabri, 1993] B. Flower and M. Jabri, "Summed Weight Neuron Perturbation: an O(N) improvement over Weight Perturbation," NIPS5, Morgan\nKauffmann Publishers, pp. 212-219, 1993.\n[Tinker, 1992] E.A. Tinker, "The SPASM Algorithm for Ventricular Lead Timing and Morphology Classification," SEDAL ICEG-RPT-016-92, Department of\nElectrical Engineering, University of Sydney, 1992.\n\n\x0c'
p83179
sg50
S'Plasticity-Mediated Competitive Learning\n\nTerrence J. Sejnowski\nterry@salk.edu\n\nNicol N. Schraudolph\nnici@salk.edu\n\nComputational Neurobiology Laboratory\nThe Salk Institute for Biological Studies\nSan Diego, CA 92186-5800\n\nand\nComputer Science & Engineering Department\nUniversity of California, San Diego\nLa Jolla, CA 92093-0114\n\nAbstract\nDifferentiation between the nodes of a competitive learning network is conventionally achieved through competition on the basis of neural activity. Simple inhibitory mechanisms are limited\nto sparse representations, while decorrelation and factorization\nschemes that support distributed representations are computationally unattractive. By letting neural plasticity mediate the competitive interaction instead, we obtain diffuse, nonadaptive alternatives for fully distributed representations. We use this technique\nto Simplify and improve our binary information gain optimization algorithm for feature extraction (Schraudolph and Sejnowski,\n1993); the same approach could be used to improve other learning\nalgorithms.\n\n1 INTRODUCTION\nUnsupervised neural networks frequently employ sets of nodes or subnetworks\nwith identical architecture and objective function. Some form of competitive interaction is then needed for these nodes to differentiate and efficiently complement\neach other in their task.\n\n\x0c476\n\nNicol Schraudolph, Terrence 1. Sejnowski\n\n1.00 -\n\n-\n\nj ................................. \'.\'\n\nf(y)\n\n?4r(y)\'....\n\n0.50 -\n\n........../ /....??1\n\n0.00 -\n\n\'.:!\' ...." ? ? , , ? ? . , ,. .. \' ..1???? ?????? ?? ?\n\n=...:::....::::...:j:....:........-.. -~\n\n=...\n\n-4.00\n\ny\n-2.00\n\n0.00\n\n2.00\n\n4.00\n\nFigure 1: Activity f and plasticity f\' of a logistic node as a function of its net input\ny. Vertical lines indicate those values of y whose pre-images in input space are\ndepicted in Figure 2.\n\nInhibition is the simplest competitive mechanism: the most active nodes suppress\nthe ability of their peers to learn, either directly or by depressing their activity.\nSince inhibition can be implemented by diffuse, nonadaptive mechanisms, it is an\nattractive solution from both neurobiological and computational points of view.\nHowever, inhibition can only form either localized (unary) or sparse distributed\nrepresentations, in which each output has only one state with significant information content.\nFor fully distributed representations, schemes to decorrelate (Barlow and Foldiak,\n1989; Leen, 1991) and even factorize (Schmidhuber, 1992; Bell and Sejnowski, 1995)\nnode activities do exist. Unfortunately these require specific, weighted lateral\nconnections whose adaptation is computationally expensive and may interfere\nwith feedforward learning. While they certainly have their place as competitive\nlearning algorithms, the capability of biological neurons to implement them seems\nquestionable.\nIn this paper, we suggest an alternative approach: we extend the advantages of\n\nsimple inhibition to distributed representations by decoupling the competition\nfrom the activation vector. In particular, we use neural plasticity - the derivative\nof a logistic activation function - as a medium for competition.\nPlasticity is low for both high and low activation values but high for intermediate\nones (Figure 1); distributed patterns of activity may therefore have localized plasticity. If competition is controlled by plasticity then, simple competitive mechanisms\nwill constrain us to localized plasticity but allow representations with distributed\nactivity.\nThe next section reintroduces the binary information gain optimization (BINGO)\nalgorithm for a single node; we then discuss how plasticity-mediated competition\nimproves upon the decorrelation mechanism used in our original extension to\nmultiple nodes. Finally, we establish a close relationship between the plasticity\nand the entropy of a logistiC node that provides an intuitive interpretation of\nplasticity-mediated competitive learning in this context.\n\n\x0cPlasticity-Med;ated Competitive Learning\n\n477\n\n2 BINARY INFORMATION GAIN OPTIMIZATION\nIn (Schraudolph and Sejnowski, 1993), we proposed an unsupervised learning rule\n\nthat uses logistic nodes to seek out binary features in its input. The output\nz\n\n= f(y),\n\nwhere f(y)\n\n1\n\n= 1 + e- Y\n\nand y\n\n= tV ? x\n\n(1)\n\nof each node is interpreted stochastically as the probability that a given feature is\npresent. We then search for informative directions in weight space by maximizing\nthe information gained about an unknown binary feature through observation of\nz. This binary infonnation gain is given by\n\nD.H(z)\n\n= H(Z) -\n\nH(z) ,\n\n(2)\n\nwhere H(z) is the entropy of a binary random variable with probability z, and z\nis a prediction of z based on prior knowledge. Gradient ascent in this objective\nresults in the learning rule\n\nD.w\n\n<X\n\nJ\'(y) . (y - fI) . x,\n\n(3)\n\nwhere fI is a prediction of y. In the simplest case, fI is an empirical average (y) of past\nactivity, computed either over batches of input data or by means of an exponential\ntrace; this amounts to a nonlinear version of the covariance rule (Sejnowski, 1977).\nUsing just the average as prediction introduces a strong preference for splitting the\ndata into two equal-sized clusters. While such a bias is appropriate in the initial\nphase of learning, it fails to take the nonlinear nature of f into account. In order\nto discount data in the saturated regions of the logistic function appropriately, we\nweigh the average by the node\'s plasticity J\'(y):\n\n(y . f\'(y))\n(f\'(y)) + C ,\n\nfI = --\'-\'---\'--\'-\'--\'-\'--\n\n(4)\n\nwhere c is a very small positive constant introduced to ensure numerical stability\nfor large values of y. Now the bias for splitting the data evenly is gradually relaxed\nas the network\'s weights grow and data begins to fall into saturated regions of f.\n\n3\n\nPLASTICITY-MEDIATED COMPETITION\n\nFor multiple nodes the original BINGO algorithm used a decorrelating predictor\nas the competitive mechanism:\n\ng = y + (Qg -\n\n2I)(y - (y)) ,\n\n(5)\n\nwhere Qg is the autocorrelation matrix of y, and I the identity matrix. Note that\nQg is computationally expensive to maintain; in connectionist implementations it\n\n\x0c478\n\nNicol Schraudolph, Terrence J. Sejnowski\n\nj\n\n!\ni\n\n.: f\n. ....~\'. ..i..\n\n,.\n\n.: . ?,"f. e: 1\',.\n\n..... ...\n\n" ... ~.\',, " . ..:.....\n\n, , :~X ~."\n\n..\n\n?\'IJ"~~ .~~~.\n.. . ~~\n\n.. .\n\n.j\n\nI . .\n\n::\n\n\': " !\n\nFigure 2: The "three cigars" problem. Each plot shows the pre-image of zero net\n. input, superimposed on a scatter plot of the data set, in input space. The two\nflanking lines delineate the "plastic region" where the logistic is not saturated,\nproviding an indication of weight vector size. Left, two-node BINGO network\nusing decorrelation (Equations 3 & 5) fails to separate the three data clusters. Right,\nsame network using plasticity-mediated competition (Equations 4 & 6) succeeds.\n\nis often approximated by lateral anti-Hebbian connections whose adaptation must\noccur on a faster time scale than that of the feedforward weights (Equation 3) for\nreasons of stability (Leen, 1991). In practice this means that learning is slowed\nsignificantly.\nIn addition, decorrelation can be inappropriate when nonlinear objectives are optimized - in our case, two prominent binary features may well be correlated.\nConsider the "three cigars" problem illustrated in Figure 2: the decorrelating predictor (left) forces the two nodes into a near-orthogonal arrangement, interfering\nwith their ability to detect the parallel gaps separating the data clusters.\nFor our purposes, decorrelation is thus too strong a constraint on the discriminants:\nall we require is that the discovered features be distinct. We achieve this by reverting\nto the simple predictor of Equation 4 while adding a global, plasticity-mediated\nexcitation l factor to the weight update:\n~Wi ex: f\'(Yi) . (Yi - 1li) . X ?\n\nL\n\nf\'(Yj)\n\n(6)\n\nj\n\nAs Figure 2 (right) illustrates, this arrangement solves the "three cigars" problem. In the high-dimensional environment of hand-written digit recognition, this\nalgorithm discovers a set of distributed binary features that preserve most of the\ninformation needed to classify the digits, even though the network was never given\nany class labels (Figure 3).\n1 The interaction is excitatory rather than inhibitory since a node\'s plasticity is inversely\ncorrelated with the magnitude of its net input.\n\n\x0cPlasticity-Mediated Competitive Learning\n\n.... ........\n.\n..\n.....\n.......\n....\n. ..\n.\n....\n?\n???????\n........ .. ................\n........\n-.....\n..\n.......\n.\n..........\n??????\n....\n?????????\n..............\n.\n..\n??????????\n\n................... . ..\n...................\n,\n\n...?\n\n479\n\n"\n\n..,\n\n?\n\n............ ............. ......\n.......\n?????\n???????? ....\n" ,\n\n? I\n\n,\n\n~\n\n\'\n\n.\n\n"\n\n...\n\n,\n\n???\n\n..?.............. ,\n?...\n.-\n\na ....\n\n"\n\nI.\n\nI\n\n......\n\n?\n\n.....\n.....\n....\n.?..????\n??\n\n? I ............\n\n.. ..\n\n"\n\n"\n\nt\n\n\'"\n\n~\n\n_\n\n......\n.....\n......\n..... ....\n......\n..?????.....\n..\n..\n???\n...?........\n??\n........\n.\n????\n\n..\n, ?????\na .......\n\'\n\n? ......\n\n?\n?????\n???\n\'\n\n?\n\n\'\n\n\'\n\n\'"\n\n,\n\n...... I\n\n??? ,\n...........\n\nl\n\n. . . . . . to. . . . .\n\n.. ... a ..\n\nFigure 3: Weights found by a four-node network running the improved BINGO\nalgorithm (Equations 4 & 6) on a set of 1200 handwritten digits due to (Guyon et aI.,\n1989). Although the network is unsupervised, its four-bit output conveys most of\nthe information necessary to classify the digits.\n\n4 PLASTICITY AND BINARY ENTROPY\nIt is possible to establish a relationship between the plasticity /\' of a logistiC node\nand its entropy that provides an intuitive account of plasticity-mediated competition as applied to BINGO. Consider the binary entropy\n\nH(z)\n\n= - z logz -\n\n(1 - z) log(l - z)\n\n(7)\n\nA well-known quadratic approximation is\n\n= 8e- 1 z (1 -\n\nH(z)\n\nz) ~ H(z)\n\n(8)\n\nNow observe that the plasticity of a logistic node\n\n!\'(Y)=:Y l+le _ y =, .. =z(l-z)\n\n(9)\n\nis in fact proportional to H(z) - that is, a logistic node\'s plasticity is in effect\na convenient quadratic approximation to its binary output entropy. The overall\nentropy in a layer of such nodes equals the sum of individual entropies less their\nredundancy:\n(10)\nH(z) =\nH(zj) - R(Z)\n\nL\nj\n\nThe plasticity-mediated excitation factor in Equation 6\n\n(11)\nj\n\nj\n\nis thus proportional to an approximate upper bound on the entropy of the layer,\nwhich in turn indicates how much more information remains to be gained by\nlearning from a particular input. In the context of BINGO, plasticity-mediated\n\n\x0c480\n\nNicol SchraudoLph. Terrence J. Sejnowski\n\ncompetition thus scales weight changes according to a measure of the network\'s\nignorance: the less it is able to identify a given input in terms of its set of binary\nfeatures, the more it tries to learn doing so.\n\n5 CONCLUSION\nBy using the derivative of a logistic activation function as a medium for competitive\ninteraction, we were able to obtain differentiated, fully distributed representations\nwithout resorting to computationally expensive decorrelation schemes. We have\ndemonstrated this plasticity-mediated competition approach on the BINGO feature\nextraction algorithm, which is significantly improved by it. A close relationship\nbetween the plasticity of a logistic node and its binary output entropy provides an\nintuitive interpretation of this unusual form of competition.\nOur general approach of using a nonmonotonic function of activity - rather than\nactivity itself - to control competitive interactions may prove valuable in other\nlearning schemes, in particular those that seek distributed rather than local representations.\nAcknowledgements\n\nWe thank Rich Zemel and Paul Viola for stimulating discussions, and the McDonnell-Pew Center for Cognitive Neuroscience in San Diego for financial support.\nReferences\n\nBarlow, H. B. and Foldiak, P. (1989). Adaptation and decorrelation in the cortex. In\nDurbin, R. M., Miall, c., and Mitchison, G. J., editors, The Computing Neuron,\nchapter 4, pages 54-72. Addison-Wesley, Wokingham.\nBell, A. J. and Sejnowski, T. J. (1995). A non-linear information maximisation\nalgorithm that performs blind separation. In Advances in Neural Information\nProcessing Systems, volume 7, Denver 1994.\nGuyon,!., Poujaud, 1., Personnaz, L., Dreyfus, G., Denker, J., and Le Cun, Y. (1989).\nComparing different neural network architectures for classifying handwritten\ndigits. In Proceedings of the International Joint Conference on Neural Networks,\nvolume II, pages 127-132. IEEE.\nLeen, T. K. (1991). Dynamics of learning in linear feature-discovery networks.\n\nNetwork, 2:85-105.\nSchmidhuber, J. (1992). Learning factorial codes by predictability minimization.\nNeural Computation, 4(6):863-879.\nSchraudolph, N. N. and Sejnowski, T. J. (1993). Unsupervised discrimination of\nclustered data via optimization of binary information gain. In Hanson, S. J.,\nCowan, J. D., and Giles, C. L., editors, Advances in Neural Information Processing Systems, volume 5, pages 499-506, Denver 1992. Morgan Kaufmann, San\nMateo.\nSejnowski, T. J. (1977). Storing covariance with nonlinearly interacting neurons.\nJournal of Mathematical Biology, 4:303-321.\n\n\x0c'
p83180
sg138
S'U sing a neural net to instantiate a\ndeformable model\nChristopher K. I. Williams; Michael D. Revowand Geoffrey E. Hinton\nDepartment of Computer Science, University of Toronto\nToronto, Ontario, Canada M5S lA4\n\nAbstract\nDeformable models are an attractive approach to recognizing nonrigid objects which have considerable within class variability. However, there are severe search problems associated with fitting the\nmodels to data. We show that by using neural networks to provide\nbetter starting points, the search time can be significantly reduced.\nThe method is demonstrated on a character recognition task.\nIn previous work we have developed an approach to handwritten character recognition based on the use of deformable models (Hinton, Williams and Revow, 1992a;\nRevow, Williams and Hinton, 1993). We have obtained good performance with this\nmethod, but a major problem is that the search procedure for fitting each model to\nan image is very computationally intensive, because there is no efficient algorithm\n(like dynamic programming) for this task. In this paper we demonstrate that it is\npossible to "compile down" some of the knowledge gained while fitting models to\ndata to obtain better starting points that significantly reduce the search time.\n\n1\n\nDEFORMABLE MODELS FOR DIGIT RECOGNITION\n\nThe basic idea in using deformable models for digit recognition is that each digit has\na model, and a test image is classified by finding the model which is most likely to\nhave generated it. The quality of the match between model and test image depends\non the deformation of the model, the amount of ink that is attributed to noise and\nthe distance of the remaining ink from the deformed model.\n?Current address: Department of Computer Science and Applied Mathematics, Aston\nUniversity, Birmingham B4 7ET, UK.\n\n\x0c966\n\nChristopher K. T. Williams, Michael D. Revow, Geoffrey E. Hinton\n\nMore formally, the two important terms in assessing the fit are the prior probability distribution for the instantiation parameters of a model (which penalizes very\ndistorted models), and the imaging model that characterizes the probability distribution over possible images given the instantiated model l . Let I be an image, M\nbe a model and z be its instantiation parameters. Then the evidence for model M\nis given by\nP(IIM)\n\n=\n\nJ\n\nP(zIM)P(IIM, z)dz\n\n(1)\n\nThe first term in the integrand is the prior on the instantiation parameters and the\nsecond is the imaging model i.e., the likelihood of the data given the instantiated\nmodel. P(MII) is directly proportional to P(IIM), as we assume a uniform prior\non each digit.\nEquation 1 is formally correct, but if z has more than a few dimensions the evaluation of this integral is very computationally intensive. However, it is often possible\nto make an approximation based on the assumption that the integrand is strongly\npeaked around a (global) maximum value z*. In this case, the evidence can be approximated by the highest peak of the integrand times a volume factor ~(zII, M),\nwhich measures the sharpness of the peak 2 .\nP(IIM) ~ P(z*IM)P(Ilz*, M)~(zII, M)\n\n(2)\n\nBy Taylor expanding around z* to second order it can be shown that the volume\nfactor depends on the determinant of the Hessian of 10gP(z, 11M) . Taking logs\nof equation 2, defining EdeJ as the negative log of P(z*IM), and EJit as the corresponding term for the imaging model, then the aim of the search is to find the\nminimum of E tot = EdeJ + EJit . Of course the total energy will have many local\nminima; for the character recognition task we aim to find the global minimum by\nusing a continuation method (see section 1.2).\n1.1\n\nSPLINES, AFFINE TRANSFORMS AND IMAGING MODELS\n\nThis section presents a brief overview of our work on using deformable models for\ndigit recognition. For a fuller treatment, see Revow, Williams and Hinton (1993) .\nEach digit is modelled by a cubic B-spline whose shape is determined by the positions of the control points in the object-based frame. The models have eight control\npoints, except for the one model which has three, and the seven model which has\nfive. To generate an ideal example of a digit the control points are positioned at\ntheir "home" locations. Deformed characters are produced by perturbing the control points away from their home locations. The home locations and covariance\nmatrix for each model were adapted in order to improve the performance.\nThe deformation energy only penalizes shape deformations. Affine transformations,\ni.e., translation, rotation, dilation, elongation, and shear, do not change the underlying shape of an object so we want the deformation energy to be invariant under\nthem . We achieve this by giving each model its own "object-based frame" and\ncomputing the deformation energy relative to this frame.\nlThis framework has been used by many authors, e.g. Grenander et al (1991) .\n2The Gaussian approximation has been popularized in the neural net community by\nMacKay (1992) .\n\n\x0cUsing a Neural Net to Instantiate a Deformable Model\n\n967\n\nThe data we used consists of binary-pixel images of segmented handwritten digits.\nThe general flavour of a imaging model for this problem is that there should be a\nhigh probability of inked pixels close to the spline, and lower probabilities further\naway. This can be achieved by spacing out a number of Gaussian "ink generators"\nuniformly along the contour; we have found that it is also useful to have a uniform\nbackground noise process over the area of the image that is able to account for\npixels that occur far away from the generators. The ink generators and background\nprocess define a mixture model. Using the assumption that each data point is\ngenerated independently given the instantiated model, P(Ilz*, M) factors into the\nproduct of the probability density of each black pixel under the mixture model.\n\n1.2\n\nRECOGNIZING ISOLATED DIGITS\n\nFor each model, the aim of the search is to find the instantiation parameters that\nminimize E tot . The search starts with zero deformations and an initial guess for\nthe affine parameters which scales the model so as to lie over the data with zero\nskew and rotation. A small number of generators with the same large variance are\nplaced along the spline, forming a broad, smooth ridge of high ink-probability along\nthe spline. We use a search procedure similar to the (iterative) Expectation Maximization (EM) method of fitting an unconstrained mixture of Gaussians, except\nthat (i) the Gaussians are constrained to lie on the spline (ii) there is a deformation energy term and (iii) the affine transformation must be recalculated on each\niteration. During the search the number of generators is gradually increased while\ntheir variance decreases according to predetermined "annealing" schedule3 .\nAfter fitting all the models to a particular image, we wish to evaluate which of the\nmodels best "explains" the data. The natural measure is the sum of Ejit, Edej\nand the volume factor. However, we have found that performance is improved by\nincluding four additional terms which are easily obtained from the final fits of the\nmodel to the image. These are (i) a measure which penalizes matches in which\nthere are beads far from any inked pixels (the "beads in white space" problem),\nand (ii) the rotation, shear and elongation of the affine transform. It is hard to\ndecide in a principled way on the correct weightings for all of these terms in the\nevaluation function. We estimated the weightings from the data by training a\nsimple postprocessing neural network. These inputs are connected directly to the\nten output units. The output units compete using the "softmax" function which\nguarantees that they form a probability distribution, summing to one.\n\n2\n\nPREDICTING THE INSTANTIATION PARAMETERS\n\nThe search procedure described above is very time consuming. However, given many\nexamples of images and the corresponding instantiation parameters obtained by the\nslow method, it is possible to train a neural network to predict the instantiation\nparameters of novel images. These predictions provide better starting points, so the\nsearch time can be reduced.\n3The schedule starts with 8 beads increasing to 60 beads in six steps, with the variance\ndecreasing from 0.04 to 0.0006 (measured in the object frame). The scale is set in the\nobject-based frame so that each model is 1 unit high.\n\n\x0c968\n\n2.1\n\nChristopher K. I. Williams, Michael D. Revow, Geoffrey E. Hinton\n\nPREVIOUS WORK\n\nPrevious work on hypothesizing instantiation parameters can be placed into two\nbroad classes, correspondence based search and parameter space search. In correspondence based search, the idea is to extract features from the image and identify\ncorresponding features in the model. Using sufficient correspondences the instantiation parameters of the model can be determined. The problem is that simple, easily\ndetectable image features have many possible matches, and more complex features\nrequire more computation and are more difficult to detect. Grimson (1990) shows\nhow to search the space of possible correspondences using an interpretation tree.\nAn alternative approach, which is used in Hough transform techniques, is to directly work in parameter space. The Hough transform was originally designed for\nthe detection of straight lines in images, and has been extended to cover a number\nof geometric shapes, notably conic sections. Ballard (1981) further extended the\napproach to arbitrary shapes with the Generalized Hough Transform . The parameter space for each model is divided into cells ("binned"), and then for each image\nfeature a vote is added to each parameter space bin that could have produced that\nfeature. After collecting votes from all image features we then search for peaks in\nthe parameter space accumulator array, and attempt to verify pose. The Hough\ntransform can be viewed as a crude way of approximating the logarithm of the\nposterior distribution P(zII, M) (e.g. Hunt et al , 1988).\nHowever, these two techniques have only been used on problems involving rigid\nmodels, and are not readily applicable to the digit recognition problem. For the\nHough space method, binning and vote collection is impractical in the high dimensional parameter space, and for the correspondence based approach there is a\nlack of easily identified and highly discriminative features. The strengths of these\ntwo techniques, namely their ability to deal with arbitrary scalings, rotations and\ntranslations of the data, and their tolerance of extraneous features, are not really\nrequired for a task where the input data is fairly well segmented and normalized.\nOur approach is to use a neural network to predict the instantiation parameters for\neach model, given an input image. Zemel and Hinton (1991) used a similar method\nwith simple 2-d objects, and more recently, Beymer et al (1993) have constructed\na network which maps from a face image to a 2-d parameter space spanning head\nrotations and a smile/no-smile dimension. However, their method does not directly\nmap from images to instantiation parameters; they use a computer vision correspondence algorithm to determine the displacement field of pixels in a novel image\nrelative to a reference image, and then use this field as the input to the network.\nThis step limits the use of the approach to images that are sufficiently similar so\nthat the correspondence algorithm functions well.\n\n2.2\n\nINSTANTIATING DIGIT MODELS USING NEURAL\nNETWORKS\nThe network which is used to predict the model instantiation parameters is shown\nin figure 1. The (unthinned) binary images are normalized to give 16 x 16 8-bit\ngreyscale images which are fed into the neural network. The network uses a standard\nthree-layer architecture; each hidden unit computes a weighted sum of its inputs,\nand then feeds this value through a sigmoidal nonlinearity u(x) = 1/(1 + e- X ). The\n\n\x0cUsing a Neural Net to Instantiate a Deformable Model\n\ncps for 0 model\n\ncps for I model\n\n969\n\ncps for 9 model\n\no\n\nFigure 1: The prediction network architecture. "cps" stands for control points.\noutput values are a weighted linear combination of the hidden unit activities plus\noutput biases. The targets are the locations of the control points in the normalized\nimage, found from fitting models as described in section 1.2.\nThe network was trained with backpropagation to minimize the squared error, using\n900 training images and 200 validation images of each digit drawn from the br\nset of the CEDAR CDROM 1 database of Cities, States, ZIP Codes, Digits, and\nAlphabetic Characters4 . Two test sets were used; one was obtained from data in the\nbr dataset, and the other was the (official) bs test set. After some experimentation\nwe chose a network with twenty hidden units, which means that the net has over\n8,000 weights . With such a large number of weights it is important to regularize the\nsolution obtained by the network by using a complexity penalty; we used a weight\nand optimized A on a validation set. Targets were only set for the\npenalty AL: j\ncorrect digit at the output layer; nothing was backpropagated from the other output\nunits. The net took 440 epochs to train using the default conjugate gradient search\nmethod in the Xerion neural network simulator 5 . It would be possible to construct\nten separate networks to carry out the same task as the net described above, but\nthis would intensify the danger of overfitting, which is reduced by giving the network\na common pool of hidden units which it can use as it decides appropriate.\n\nwJ\n\nFor comparison with the prediction net described above, a trivial network which\njust consisted of output biases was trained; this network simply learns the average\nvalue of the control point locations. On a validation set the squared error of the\nprediction net was over three times smaller than the trivial net. Although this is\nencouraging, the acid test is to compare the performance of elastic models settled\nfrom the predicted positions using a shortened annealing schedule; if the predictions\nare good, then only a short amount of settling will be required.\n4Made available by the Unites States Postal Service Office of Advanced Technology.\n5Xerion was designed and implemented by Drew van Camp, Tony Plate and Geoffrey\nHinton at the University of Toronto.\n\n\x0c970\n\nChristopher K. I. Williams, Michael D. Revow, Geoffrey E. Hinton\n\nFigure 2: A comparision of the initial instantiations due to the prediction net (top row)\nand the trivial net (bottom row) on an image of a 2. Notice that for the two model the\nprediction net is much closer to the data. The other digit models mayor may not be greatly\naffected by the input data; for example, the predictions from both nets seem essentially\nthe same for the zero, but for the seven the prediction net puts the model nearer to the\ndata.\nThe feedforward net predicts the position of the control points in the normalized\nimage. By inverting the normalization process, the positions of the control points\nin the un-normalized image are determined. The model deformation and affine\ntransformation corresponding to these image control point locations can then be\ndetermined by running a part of one iteration of the search procedure. Experiments\nwere then conducted with a number of shortened annealing schedules; for each one,\ndata obtained from settling on a part of the training data was used to train the\npostprocessing net. The performance was then evaluated on the br test set.\nThe full annealing schedule has six stages. The shortened annealing schedules are:\n1. No settling at all\n2. Two iterations at the final variance of 0.0006\n3. One iteration at 0.0025 and two at 0.0006\n4. The full annealing schedule (for comparison)\nThe results on the br test set are shown in table 1. The general trends are that the\nperformance obtained using the prediction net is consistently better than the trivial\nnet, and that longer annealing schedules lead to better performance. A comparison\nof schedules 3 and 4 in table 1 indicates that the performance of the prediction\nnet/schedule 3 combination is similar to (or slightly better than) that obtained\nwith the full annealing schedule, and is more than a factor of two faster. The\nresults with the full schedule are almost identical to the results obtained with the\ndefault "box" initialization described in section 1.2. Figure 2 compares the outputs\nof the prediction and trivial nets on a particular example. Judging from the weight\n\n\x0cUsing a Neural Net to Instantiate a Deformable Model\n\nSchedule number\n\nTrivial net\n\nPrediction net\n\n1\n2\n3\n4\n\n427\n329\n160\n40\n\n200\n58\n32\n36\n\n971\n\nAverage time required\nto settle one model (s)\n0.12\n0.25\n0.49\n1.11\n\nTable 1: Errors on the internal test set of 2000 examples for different annealing schedules.\nThe timing trials were carried out on a R-4400 machine.\n\nvectors and activity patterns of the hidden units, it does not seem that some of the\nunits are specialized for a particular digit class.\nA run on the bs test set using schedule 3 gave an error rate of 4.76 % (129 errors),\nwhich is very similar to the 125 errors obtained using the full annealing schedule\nand the box initialization. A comparison of the errors made on the two runs shows\nthat only 67 out of the 129 errors were common to the two sets. This suggests that\nit would be very sensible to reject cases where the two methods do not agree.\n\n3\n\nDISCUSSION\n\nThe prediction net used above can be viewed as an interpolation scheme in the\ncontrol point position space of each digit z(I) = Zo + 2:i ai(I)zi, where z(I) is\nthe predicted position in the control point space, Zo is the contribution due to the\nbiases, ai is the activity of hidden unit i and Zi is its location in the control point\nposition space (learned from the data) . If there are more hidden units than output\ndimensions, then for any particular image there are an infinite number of ways to\nmake this equation hold exactly. However, the network will tend to find solutions\nso that the ai(I)\'s will vary smoothly as the image is perturbed.\nThe nets described above output just one set of instantiation parameters for a\ngiven model. However, it may be preferable to be able to represent a number of\nguesses about model instantiation parameters; one way of doing this is to train a\nnetwork that has multiple sets of output parameters, as in the "mixture of experts"\narchitecture of Jacobs et aI (1991). The outputs can be interpreted as a mixture\ndistribution in the control point position space, conditioned on the input image.\nAnother approach to providing more information about the posterior distribution\nis described in (Hinton, Williams and Revow, 1992b), where P(zlI) is approximated\nusing a fixed set of basis functions whose weighting depends on the input image I.\nThe strategies descriped above directly predict the instantiation parameters in parameter space. It is also possible to use neural networks to hypothesize correspondences, i.e. to predict an inked pixel\'s position on the spline given a local window\nof context in the image. With sufficient matches it is then possible to compute\nthe instantiation parameters of the model. We have conducted some preliminary\nexperiments with this method (described in Williams, 1994), which indicate that\ngood performance can be achieved for the correspondence prediction task.\n\n\x0c972\n\nChristopher K. I. Williams, Michael D. Revow, Geoffrey E. Hinton\n\nWe have shown that the we can obtain significant speedup using the prediction net.\nThe schemes outlined above which allow multimodal predictions in instantiation\nparameter space may improve performance and deserve further investigation. We\nare also interested in improving the performance of the prediction net, for example\nby outputting a confidence measure which could be used to adjust the length of\nthe elastic models\' search appropriately. We believe that using machine learning\ntechniques like neural networks to help reduce the amount of search required to fit\ncomplex models to data may be useful for many other problems.\nAcknowledgements\nThis research was funded by Apple and by the Ontario Information Technology Research\nCentre. We thank Allan Jepson, Richard Durbin, Rich Zemel, Peter Dayan, Rob Tibshirani\nand Yann Le Cun for helpful discussions. Geoffrey Hinton is the Noranda Fellow of the\nCanadian Institute for Advanced Research.\n\nReferences\nBallard, D. H. (1981). Generalizing the Hough transfrom to detect arbitrary shapes.\nPattern Recognition, 13(2):111-122.\nBeymer, D., Shashua, A., and Poggio, T . (1993). Example Based Image Analysis and\nSynthesis. AI Memo 1431, AI Laboratory, MIT.\nGrenander, U., Chow, Y., and Keenan, D. M. (1991). Hands: A pattern theoretic study of\nbiological shapes. Springer-Verlag.\nGrimson, W. E. 1. (1990) . Object recognition by computer. MIT Press, Cambridge, MA.\nHinton, G. E., Williams, C. K. 1., and Revow, M. D. (1992a). Adaptive elastic models\nfor hand-printed character recognition. In Moody, J. E., Hanson, S. J., and Lippmann, R. P., editors, Advances in Neural Information Processing Systems 4. Morgan\nKauffmann.\nHinton, G. E., Williams, C. K. 1., and Revow, M. D. (1992b). Combinining two methods\nof recognizing hand-printed digits. In Aleksander, 1. and Taylor, J., editors, Artificial\nNeural Networks 2. Elsevier Science Publishers.\nHunt, D. J., Nolte, L. W., and Ruedger, W . H. (1988) . Performance of the Hough Transform and its Relationship to Statistical Signal Detection Theory. Computer Vision,\nGraphics and Image Processing, 43:221- 238.\nJacobs, R. A., Jordan, M. 1., Nowlan, S. J., and Hinton, G. E. (1991). Adaptive mixtures\nof local experts. Neural Computation, 3(1).\nMacKay, D. J. C. (1992). Bayesian Interpolation. Neural Computation, 4(3):415-447.\nRevow, M. D., Williams, C. K. 1., and Hinton, G. E. (1993) . Using mixtures of deformable\nmodels to capture variations in hand printed digits. In Srihari, S., editor, Proceedings\nof the Third International Workshop on Frontiers in Handwriting Recognition, pages\n142-152, Buffalo, New York, USA.\nWilliams, C. K. 1. (1994) . Combining deformable models and neural networks for handprinted digit recognition. PhD thesis, Dept. of Computer Science, University of\nToronto.\nZemel, R . S. and Hinton, G. E. (1991) . Discovering viewpoint-invariant relationships that\ncharacterize objects. In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, Advances In Neural Information Processing Systems 3, pages 299-305. Morgan\nKaufmann Publishers.\n\n\x0c'
p83181
sg140
S'Neural Network Ensembles, Cross\nValidation, and Active Learning\n\nAnders Krogh"\nNordita\nBlegdamsvej 17\n2100 Copenhagen, Denmark\n\nJesper Vedelsby\nElectronics Institute, Building 349\nTechnical University of Denmark\n2800 Lyngby, Denmark\n\nAbstract\nLearning of continuous valued functions using neural network ensembles (committees) can give improved accuracy, reliable estimation of the generalization error, and active learning. The ambiguity\nis defined as the variation of the output of ensemble members averaged over unlabeled data, so it quantifies the disagreement among\nthe networks. It is discussed how to use the ambiguity in combination with cross-validation to give a reliable estimate of the ensemble\ngeneralization error, and how this type of ensemble cross-validation\ncan sometimes improve performance. It is shown how to estimate\nthe optimal weights of the ensemble members using unlabeled data.\nBy a generalization of query by committee, it is finally shown how\nthe ambiguity can be used to select new training data to be labeled\nin an active learning scheme.\n\n1\n\nINTRODUCTION\n\nIt is well known that a combination of many different predictors can improve predictions. In the neural networks community "ensembles" of neural networks has been\ninvestigated by several authors, see for instance [1, 2, 3]. Most often the networks\nin the ensemble are trained individually and then their predictions are combined.\nThis combination is usually done by majority (in classification) or by simple averaging (in regression), but one can also use a weighted combination of the networks .\n.. Author to whom correspondence should be addressed. Email: kroghlnordita. elk\n\n\x0c232\n\nAnders Krogh, Jesper Vedelsby\n\nAt the workshop after the last NIPS conference (December, 1993) an entire session\nwas devoted to ensembles of neural networks ( "Putting it all together", chaired by\nMichael Perrone) . Many interesting papers were given, and it showed that this area\nis getting a lot of attention .\nA combination of the output of several networks (or other predictors) is only useful\nif they disagree on some inputs. Clearly, there is no more information to be gained\nfrom a million identical networks than there is from just one of them (see also\n[2]). By quantifying the disagreement in the ensemble it turns out to be possible\nto state this insight rigorously for an ensemble used for approximation of realvalued functions (regression). The simple and beautiful expression that relates the\ndisagreement (called the ensemble ambiguity) and the generalization error is the\nbasis for this paper, so we will derive it with no further delay.\n\n2\n\nTHE BIAS-VARIANCE TRADEOFF\n\nAssume the task is to learn a function J from RN to R for which you have a sample\nof p examples, (xiJ , yiJ), where yiJ = J(xiJ) and J.t = 1, . . . ,p. These examples\nare assumed to be drawn randomly from the distribution p(x) . Anything in the\nfollowing is easy to generalize to several output variables.\nThe ensemble consists of N networks and the output of network a on input x is\ncalled va (x). A weighted ensemble average is denoted by a bar , like\n\nV(x) =\n\nL Wa Va(x).\n\n(1)\n\na\n\nThis is the final output of the ensemble. We think of the weight Wa as our belief in\nnetwork a and therefore constrain the weights to be positive and sum to one. The\nconstraint on the sum is crucial for some of the following results.\nThe ambiguity on input x of a single member of the ensemble is defined as aa (x)\n(V a(x) - V(x))2 . The ensemble ambiguity on input x is\n\na(x)\n\n= Lwaaa(x) = LWa(va(x) a\n\nV(x))2 .\n\n=\n\n(2)\n\na\n\nIt is simply the variance of the weighted ensemble around the weighed mean, and\nit measures the disagreement among the networks on input x. The quadratic error\nof network a and of the ensemble are\n\n(J(x) - V a(x))2\n(J(x) - V(X))2\n\n(3)\n(4)\n\nrespectively. Adding and subtracting J( x) in (2) yields\n\na(x)\n\n=L\n\nWafa(X) - e(x)\n\n(5)\n\na\n\n(after a little algebra using that the weights sum to one) . Calling the weighted\naverage of the individual errors ?( x) = La Wa fa (x) this becomes\n\ne(x)\n\n= ?(x) -\n\na(x).\n\n(6)\n\n\x0cNeural Network Ensembles, Cross Validation, and Active Learning\n\n233\n\nAll these formulas can be averaged over the input distribution . Averages over the\ninput distribution will be denoted by capital letter, so\n\nJ dxp(xVl! (x)\nJ dxp(x)aa(x)\nJ dxp(x)e(x).\n\nE\n\n(7)\n(8)\n(9)\n\nThe first two of these are the generalization error and the ambiguity respectively\nfor network n , and E is the generalization error for the ensemble. From (6) we then\nfind for the ensemble generalization error\n(10)\nThe first term on the right is the weighted average of the generalization errors of\nthe individual networks (E = La waEa), and the second is the weighted average\nof the ambiguities (A = La WaAa), which we refer to as the ensemble ambiguity.\nThe beauty of this equation is that it separates the generalization error into a term\nthat depends on the generalization errors of the individual networks and another\nterm that contain all correlations between the networks . Furthermore, the correlation term A can be estimated entirely from unlabeled data, i. e., no knowledge is\nrequired of the real function to be approximated. The term "unlabeled example" is\nborrowed from classification problems, and in this context it means an input x for\nwhich the value of the target function f( x) is unknown.\nEquation (10) expresses the tradeoff between bias and variance in the ensemble ,\nbut in a different way than the the common bias-variance relation [4] in which the\naverages are over possible training sets instead of ensemble averages. If the ensemble\nis strongly biased the ambiguity will be small , because the networks implement very\nsimilar functions and thus agree on inputs even outside the training set. Therefore\nthe generalization error will be essentially equal to the weighted average of the\ngeneralization errors of the individual networks. If, on the other hand , there is a\nlarge variance , the ambiguity is high and in this case the generalization error will\nbe smaller than the average generalization error . See also [5].\nFrom this equation one can immediately see that the generalization error of the\nensemble is always smaller than the (weighted) average of the ensemble errors,\nE < E. In particular for uniform weights:\n\nE\n\n~ ~ \'fEcx\n\n(11)\n\nwhich has been noted by several authors , see e.g. [3] .\n\n3\n\nTHE CROSS-VALIDATION ENSEMBLE\n\nFrom (10) it is obvious that increasing the ambiguity (while not increasing individual\ngeneralization errors) will improve the overall generalization. We want the networks\nto disagree! How can we increase the ambiguity of the ensemble? One way is to\nuse different types of approximators like a mixture of neural networks of different\ntopologies or a mixture of completely different types of approximators. Another\n\n\x0c234\n\nAnders Krogh, Jesper Vedelsby\n\n.\n\n:~\n\n1. -\n\nt\n\n-\n\n,\',\n\n.. ,\n\nE o...... -\' \'.- .. \' ........ ....,.\n\n.\'\n\n..... , ...\n\nv \'. --:\n\n,\n\n.~.--c??\n\n__ .. -.tI"\n\n.\n\n. -- - -\\\\\n\n\'1\n\n-\n\n.~\n\n~.\n\n, . _ ? ." ?\n\n.. - .....\n\n_._ ..... .\'-._._.1\n\n,\n\n-\n\n>\n\n-\n\n-1.k!\n~\n\n-4\n\n.t.\n\nf.\n\n1\\.1\n\n:\\,\'. - ?-.l\n\n:--,____\n..\n\n~~\n.\n\n~.\n\n,\n\n,\'\n\n-2\n\n.~\n\nIf\n\no\n\n2\n\n\\.\n~\n:\n?\n\n\' 0\'\n\n~:\n\n4\n\nx\n\nFigure 1: An ensemble of five networks were trained to approximate the square\nwave target function f(x). The final ensemble output (solid smooth curve) and\nthe outputs of the individual networks (dotted curves) are shown. Also the square\nroot of the ambiguity is shown (dash-dot line) _ For training 200 random examples\nwere used, but each network had a cross-validation set of size 40, so they were each\ntrained on 160 examples.\n\nobvious way is to train the networks on different training sets. Furthermore, to be\nable to estimate the first term in (10) it would be desirable to have some kind of\ncross-validation. This suggests the following strategy.\nChose a number K :::; p. For each network in the ensemble hold out K examples for\ntesting, where the N test sets should have minimal overlap, i. e., the N training sets\nshould be as different as possible. If, for instance, K :::; piN it is possible to choose\nthe K test sets with no overlap. This enables us to estimate the generalization error\nE(X of the individual members of the ensemble, and at the same time make sure\nthat the ambiguity increases . When holding out examples the generalization errors\nfor the individual members of the ensemble, E(X, will increase, but the conjecture\nis that for a good choice of the size of the ensemble (N) and the test set size\n(K), the ambiguity will increase more and thus one will get a decrease in overall\ngeneralization error.\nThis conjecture has been tested experimentally on a simple square wave function\nof one variable shown in Figure 1. Five identical feed-forward networks with one\nhidden layer of 20 units were trained independently by back-propagation using 200\nrandom examples. For each network a cross-validation set of K examples was held\nout for testing as described above. The "true" generalization and the ambiguity were\nestimated from a set of 1000 random inputs. The weights were uniform, w(X\n1/5\n(non-uniform weights are addressed later).\n\n=\n\nIn Figure 2 average results over 12 independent runs are shown for some values of\n\n\x0cNeural Network Ensembles, Cross Validation, and Active Learning\n\nFigure 2: The solid line shows the generalization error for uniform weights as\na function of K, where K is the size\nof the cross-validation sets. The dotted\nline is the error estimated from equation (10) . The dashed line is for the\noptimal weights estimated by the use of\nthe generalization errors for the individual networks estimated from the crossvalidation sets as described in the text.\nThe bottom solid line is the generalization error one would obtain if the individual generalization errors were known\nexactly (the best possible weights).\n\n0.08\n\n235\n\n,-----r----,--~---r-----,\n\no\n\nt=\nw\n0.06\n\nc\n\no\n~\n\n.!::!\n\nco...\n\n~ 0.04\n\nQ)\n\n(!)\n\n0 .02 \'---_---1_ _---\'-_ _--\'-_ _-----\'\no\n20\n40\n60\n80\nSize of CV set\n\nK (top solid line) . First, one should note that the generalization error is the same\nfor a cross-validation set of size 40 as for size 0, although not lower, so it supports\nthe conjecture in a weaker form. However, we have done many experiments, and\ndepending on the experimental setup the curve can take on almost any form, sometimes the error is larger at zero than at 40 or vice versa. In the experiments shown,\nonly ensembles with at least four converging networks out of five were used . If all\nthe ensembles were kept, the error would have been significantly higher at ]{ = a\nthan for K > a because in about half of the runs none of the networks in the ensemble converged - something that seldom happened when a cross-validation set\nwas used. Thus it is still unclear under which circumstances one can expect a drop\nin generalization error when using cross-validation in this fashion.\n\nThe dotted line in Figure 2 is the error estimated from equation (10) using the\ncross-validation sets for each of the networks to estimate Ea, and one notices a\ngood agreement.\n\n4\n\nOPTIMAL WEIGHTS\n\nThe weights Wa can be estimated as described in e.g. [3]. We suggest instead\nto use unlabeled data and estimate them in such a way that they minimize the\ngeneralization error given in (10) .\nThere is no analytical solution for the weights , but something can be said about\nthe minimum point of the generalization error. Calculating the derivative of E as\ngiven in (10) subject to the constraints on the weights and setting it equal to zero\nshows that\nEa - Aa\nE or Wa = O.\n(12)\n\n=\n\n(The calculation is not shown because of space limitations, but it is easy to do.)\nThat is, Ea - Aa has to be the same for all the networks. Notice that Aa depends\non the weights through the ensemble average of the outputs. It shows that the\noptimal weights have to be chosen such that each network contributes exactly waE\n\n\x0c236\n\nAnders Krogh, Jesper Vedelsby\n\nto the generalization error. Note, however, that a member of the ensemble can have\nsuch a poor generalization or be so correlated with the rest of the ensemble that it\nis optimal to set its weight to zero.\nThe weights can be "learned" from unlabeled examples, e.g. by gradient descent\nminimization of the estimate of the generalization error (10). A more efficient\napproach to finding the optimal weights is to turn it into a quadratic optimization\nproblem. That problem is non-trivial only because of the constraints on the weights\n(L:a Wa = 1 and Wa 2:: 0). Define the correlation matrix,\nC af3\n\n=\n\nf\n\ndxp(x)V a (x)V f3 (x) .\n\n(13)\n\nThen, using that the weights sum to one, equation (10) can be rewritten as\nE\n\n=\n\nL\na\n\nwa Ea\n\n+ L w a C af3 w f3 - L\naf3\n\nwaCaa .\n\n(14)\n\na\n\nHaving estimates of E a and C af3 the optimal weights can be found by linear programming or other optimization techniques. Just like the ambiguity, the correlation\nmatrix can be estimated from unlabeled data to any accuracy needed (provided that\nthe input distribution p is known).\nIn Figure 2 the results from an experiment with weight optimization are shown.\nThe dashed curve shows the generalization error when the weights are optimized as\ndescribed above using the estimates of Ea from the cross-validation (on K exampies). The lowest solid curve is for the idealized case, when it is assumed that the\nerrors Ea are known exactly, so it shows the lowest possible error. The performance\nimprovement is quite convincing when the cross-validation estimates are used.\nIt is important to notice that any estimate of the generalization error of the individual networks can be used in equation (14). If one is certain that the individual\nnetworks do not overfit, one might even use the training errors as estimates for\nEa (see [3]). It is also possible to use some kind of regularization in (14), if the\ncross-validation sets are small.\n\n5\n\nACTIVE LEARNING\n\nIn some neural network applications it is very time consuming and/or expensive\nto acquire training data, e.g., if a complicated measurement is required to find the\nvalue of the target function for a certain input. Therefore it is desirable to only use\nexamples with maximal information about the function. Methods where the learner\npoints out good examples are often called active learning.\nWe propose a query-based active learning scheme that applies to ensembles of networks with continuous-valued output. It is essentially a generalization of query by\ncommittee [6, 7] that was developed for classification problems. Our basic assumption is that those patterns in the input space yielding the largest error are those\npoints we would benefit the most from including in the training set.\nSince the generalization error is always non-negative, we see from (6) that the\nweighted average of the individual network errors is always larger than or equal to\nthe ensemble ambiguity,\nf(X) 2:: a(x),\n(15)\n\n\x0cNeural Network Ensembles. Cross Validation. and Active Learning\n\n237\n\n2.5 r"\':":\'T---r--"T""--.-----r---,\n\n.\n\n.\n\n.\n\n:\n\n0.5\n\no\n\n10\n\n20\n\n30\n\nTraining set size\n\n40\n\n50\n\no\n\n10\n\n20\n\n30\n\n40\n\n50\n\nTraining set size\n\nFigure 3: In both plots the full line shows the average generalization for active\nlearning, and the dashed line for passive learning as a function of the number of\ntraining examples. The dots in the left plot show the results of the individual\nexperiments contributing to the mean for the active learning. The dots in right plot\nshow the same for passive learning.\n\nwhich tells us that the ambiguity is a lower bound for the weighted average of the\nsquared error. An input pattern that yields a large ambiguity will always have a\nlarge average error. On the other hand, a low ambiguity does not necessarily imply\na low error. If the individual networks are trained to a low training error on the\nsame set of examples then both the error and the ambiguity are low on the training\npoints. This ensures that a pattern yielding a large ambiguity cannot be in the close\nneighborhood of a training example. The ambiguity will to some extent follow the\nfluctuations in the error. Since the ambiguity is calculated from unlabeled examples\nthe input-space can be scanned for these areas to any detail. These ideas are well\nillustrated in Figure 1, where the correlation between error and ambiguity is quite\nstrong, although not perfect.\nThe results of an experiment with the active learning scheme is shown in Figure 3.\nAn ensemble of 5 networks was trained to approximate the square-wave function\nshown in Figure 1, but in this experiments the function was restricted to the interval\nfrom - 2 to 2. The curves show the final generalization error of the ensemble in a\npassive (dashed line) and an active learning test (solid line). For each training set\nsize 2x40 independent tests were made, all starting with the same initial training\nset of a single example. Examples were generated and added one at a time. In the\npassive test examples were generated at random, and in the active one each example\nwas selected as the input that gave the largest ambiguity out of 800 random ones.\nFigure 3 also shows the distribution of the individual results of the active and\npassive learning tests. Not only do we obtain significantly better generalization by\nactive learning, there is also less scatter in the results. It seems to be easier for the\nensemble to learn from the actively generated set.\n\n\x0c238\n\n6\n\nAnders Krogh. Jesper Vedelsby\n\nCONCLUSION\n\nThe central idea in this paper was to show that there is a lot to be gained from\nusing unlabeled data when training in ensembles. Although we dealt with neural\nnetworks, all the theory holds for any other type of method used as the individual\nmembers of the ensemble.\nIt was shown that apart from getting the individual members of the ensemble to\ngeneralize well, it is important for generalization that the individuals disagrees as\nmuch as possible, and we discussed one method to make even identical networks\ndisagree. This was done by training the individuals on different training sets by\nholding out some examples for each individual during training. This had the added\nadvantage that these examples could be used for testing, and thereby one could\nobtain good estimates of the generalization error.\nIt was discussed how to find the optimal weights for the individuals of the ensemble.\nFor our simple test problem the weights found improved the performance of the\nensemble significantly.\n\nFinally a method for active learning was described, which was based on the method\nof query by committee developed for classification problems. The idea is that if the\nensemble disagrees strongly on an input, it would be good to find the label for that\ninput and include it in the training set for the ensemble. It was shown how active\nlearning improves the learning curve a lot for a simple test problem.\nAcknowledgements\n\nWe would like to thank Peter Salamon for numerous discussions and for his implementation of linear programming for optimization of the weights. We also thank\nLars Kai Hansen for many discussions and great insights, and David Wolpert for\nvaluable comments.\n\nReferences\n[1] L.K. Hansen and P Salamon. Neural network ensembles. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 12(10):993- 1001, Oct. 1990.\n[2] D.H Wolpert. Stacked generalization. Neural Networks, 5(2):241-59, 1992.\n[3] Michael P. Perrone and Leon N Cooper. When networks disagree: Ensemble method\nfor neural networks. In R. J. Mammone, editor, Neural Networks for Speech and Image\nprocessing. Chapman-Hall, 1993.\n[4] S. Geman , E . Bienenstock, and R Doursat. Neural networks and the bias/variance\ndilemma. Neural Computation, 4(1):1-58, Jan. 1992.\n[5] Ronny Meir. Bias, variance and the combination of estimators; the case of linear least\nsquares. Preprint (In Neuroprose), Technion, Heifa, Israel, 1994.\n[6] H.S. Seung, M. Opper, and H. Sompolinsky. Query by committee. In Proceedings of\nthe Fifth Workshop on Computational Learning Theory, pages 287-294, San Mateo,\nCA, 1992. Morgan Kaufmann.\n[7] Y. Freund, H.S. Seung, E. Shamir, and N. Tishby. Information, prediction, and query\nby committee. In Advances in Neural Information Processing Systems, volume 5, San\nMateo, California, 1993. Morgan Kaufmann.\n\n\x0c'
p83182
sg354
S'Bayesian Query Construction for Neural\nNetwork Models\nGerhard Paass\nJorg Kindermann\nGerman National Research Center for Computer Science (GMD)\nD-53757 Sankt Augustin, Germany\npaass@gmd.de\nkindermann@gmd.de\n\nAbstract\nIf data collection is costly, there is much to be gained by actively selecting particularly informative data points in a sequential way. In\na Bayesian decision-theoretic framework we develop a query selection criterion which explicitly takes into account the intended use\nof the model predictions. By Markov Chain Monte Carlo methods\nthe necessary quantities can be approximated to a desired precision. As the number of data points grows, the model complexity\nis modified by a Bayesian model selection strategy. The properties of two versions of the criterion ate demonstrated in numerical\nexperiments.\n\n1\n\nINTRODUCTION\n\nIn this paper we consider the situation where data collection is costly, as when\nfor example, real measurements or technical experiments have to be performed. In\nthis situation the approach of query learning (\'active data selection\', \'sequential\nexperimental design\', etc.) has a potential benefit. Depending on the previously\nseen examples, a new input value (\'query\') is selected in a systematic way and\nthe corresponding output is obtained. The motivation for query learning is that\nrandom examples often contain redundant information, and the concentration on\nnon-redundant examples must necessarily improve generalization performance.\nWe use a Bayesian decision-theoretic framework to derive a criterion for query construction. The criterion reflects the intended use of the predictions by an appropriate\n\n\x0c444\n\nGerhard Paass. Jorg Kindermann\n\nloss function. We limit our analysis to the selection of the next data point, given a\nset of data already sampled. The proposed procedure derives the expected loss for\ncandidate inputs and selects a query with minimal expected loss.\nThere are several published surveys of query construction methods [Ford et al. 89,\nPlutowski White 93, Sollich 94]. Most current approaches, e.g. [Cohn 94], rely\non the information matrix of parameters. Then however, all parameters receive\nequal attention regardless of their influence on the intended use of the model\n[Pronzato Walter 92]. In addition, the estimates are valid only asymptotically. Bayesian approaches have been advocated by [Berger 80], and applied to neural networks\n[MacKay 92]. In [Sollich Saad 95] their relation to maximum information gain is\ndiscussed. In this paper we show that by using Markov Chain Monte Carlo methods it is possible to determine all quantities necessary for the selection of a query.\nThis approach is valid in small sample situations, and the procedure\'s precision can\nbe increased with additional computational effort. With the square loss function,\nthe criterion is reduced to a variant of the familiar integrated mean square error\n[Plutowski White 93].\n\nIn the next section we develop the query selection criterion from a decision-theoretic\npoint of view. In the third section we show how the criterion can be calculated using\nMarkov Chain Monte Carlo methods and we discuss a strategy for model selection.\nIn the last section, the results of two experiments with MLPs are described.\n2\n\nA DECISION-THEORETIC FRAMEWORK\n\nAssume we have an input vector x and a scalar output y distributed as y "" p(y I x, w)\nwhere w is a vector of parameters. The conditional expected value is a deterministic\nfunction !(x, w) := E(y I x, w) where y = !(x, w)+? and ? is a zero mean error term.\nSuppose we have iteratively collected observations D(n) := ((Xl, iii), .. . , (Xn, Yn)).\nWe get the Bayesian posterior p(w I D(n)) = p(D(n) Iw) p(w)/ J p(D(n) Iw) p(w) dw\nand the predictive distribution p(y I x, D(n)) = p(y I x, w)p(w I D(n)) dw if p(w) is\nthe prior distribution.\n\nJ\n\nWe consider the situation where, based on some data x, we have to perform an\naction a whose result depends on the unknown output y. Some decisions may have\nmore severe effects than others. The loss function L(y, a) E [0,00) measures the\nloss if y is the true value and we have taken the action a E A. In this paper we\nconsider real-valued actions, e.g. setting the temperature a in a chemical process.\nWe have to select an a E A only knowing the input x. According to the Bayes\nPrinciple [Berger 80, p.14] we should follow a decision rule d : x --t a such that\nthe average risk J R(w, d) p(w I D(n)) dw is minimal, where the risk is defined as\nR(w, d) := J L(y, d(x)) p(y I x, w) p(x) dydx. Here p(x) is the distribution of future\ninputs, which is assumed to be known.\nFor the square loss function L(y, a) = (y - a)2, the conditional expectation\nd(x) := E(y I x, D(n)) is the optimal decision rule. In a control problem the loss\nmay be larger at specific critical points. This can be addressed with a weighted square loss function L(y, a) := h(y)(y - a)2, where h(y) 2: a [Berger 80,\np.1U]. The expected loss for an action is J(y - a)2h(y) p(y I x, D(n)) dy. Replacing the predictive density p(y I x, D(n)) with the weighted predictive density\n\n\x0cBayesian Query Construction for Neural Network Models\n\n445\n\np(y I x, Den) := h(y) p(y I x, Den)/G(x), where G(x) := I h(y) p(y I x, Den) dy,\nwe get the optimal decision rule d(x) := I yp(y I x, Den) dy and the average loss\nG(x) I(y - E(y I x, D(n))2 p(y I x, Den) dy for a given input x. With these modifications, all later derIvations for the square loss function may be applied to the\nweighted square loss.\nThe aim of query sampling is the selection of a new observation x in such a way\nthat the average risk will be maximally reduced. Together with its still unknown\ny-value, x defines a new observation (x, y) and new data Den) U (x, y). To determine\nthis risk for some given x we have to perform the following conceptual steps for a\ncandidate query x:\n1. Future Data: Construct the possible sets of \'future\' observations Den) U\n\n(x, y), where y ""\' p(y I x, Den).\n2. Future posterior: Determine a \'future\' posterior distribution of parameters\np(w I Den) U (x, y? that depends on y in the same way as though it had\nactually been observed.\n3. Future Loss: Assuming d~,x(x) is the optimal decision rule for given values\nof x, y, and x, compute the resulting loss as\n\n1\';,x(x):=\n\nJ\n\nL(y,d;,x(x?p(ylx,w)p(wIDen)U(x,y?dydw\n\n(1)\n\n4. Averaging: Integrate this quantity over the future trial inputs x distributed\nas p(x) and the different possible future outputs y, yielding\n\n1\';:= Ir;,x(x)p(x)p(ylx,Den)dxdy.\nThis procedure is repeated until an x with minimal average risk is found. Since local\noptima are typical, a global optimization method is required. Subsequently we then\ntry to determine whether the current model is still adequate or whether we have to\nincrease its complexity (e.g. by adding more hidden units).\n\n3\n\nCOMPUTATIONAL PROCEDURE\n\nLet us assume that the real data Den) was generated according to a regression model\ny = !(x, w)+{ with i.i.d. Gaussian noise {""\' N(O, (T2(w?. For example !(x, w) may\nbe a multilayer perceptron or a radial basis function network. Since the error terms\nare independent, the posterior density is p( w I Den) ex: p( w) rr~=l P(Yi I Xi, w) even\nin the case of query sampling [Ford et al. 89].\nAs the analytic derivation of the posterior is infeasible except in trivial cases, we\nhave to use approximations. One approach is to employ a normal approximation\n[MacKay 92], but this is unreliable if the number of observations is small compared to the number of parameters. We use Markov Chain Monte Carlo procedures\n[PaaB 91, Neal 93] to generate a sample WeB) := {WI, .. .WB} of parameters distributed according to p( w I Den). If the number of sampling steps approaches infinity,\nthe distribution of the simulated Wb approximates the posterior arbitrarily well.\nTo take into account the range of future y-values, we create a set of them by simulation. For each Wb E WeB) a number of y ""\' p(y I x, Wb) is generated. Let\n\n\x0c446\n\ny(x.R)\n\nGerhard Paass. JiJrg Kindermann\n\n{YI, ... , YR} be the resulting set. Instead of performing a new Markov\nMonte Carlo run to generate a new sample according to p(w I DCn) U (x, y)), we\n:=\n\nuse the old set WCB) of parameters and reweight them (importance sampling).\nIn this way we may approximate integrals of some function g( w) with respect to\np(w I DCn) U (x, y)) [Kalos Whitlock 86, p.92]:\n\n- -))d\nj 9 (w ) P(W IDCn) U( X,\nY\nW\n\n__\n\n--\n\nL~-lg(Wb)P(ylx,Wb)\nB\n\nLb=l p(Y I x, Wb)\n\n(2)\n\nThe approximation error approaches zero as the size of WCB) increases.\n\n3.1\n\nAPPROXIMATION OF FUTURE LOSS\n\nConsider the future loss f;,x(x) given new observation (x, y) and trial input Xt. In\nthe case of the square loss function, (1) can be transformed to\n\nf~,.t(Xt)\n\n=\n\nj[!(Xt,w)-E(yIXt,Dcn)U(X,y)Wp(wIDcn)U(x,y))dw (3)\n\n+ j ?T2(w) p(w I DCn) U (x, y)) dw\nwhere ?T2(w) := Var(y I x, w) is independent of x. Assume a set XT = {Xl, ... , XT}\nis given, which is representative of trial inputs for the distribution p(x). Define\nS(x, y) := L~=i p(Y I x, Wb) for y E YCx,R) . Then from equations (2) and (3) we get\nE(ylxt,DCn)U(x,y)):= 1/S(x,Y)L~=1!(Xt,Wb)P(Ylx,Wb) and\n1\n\nB\n\nS(x -) L?T 2(Wb)P(Ylx,Wb)\n,y b=l\n1\n\n+ S(x\n\n(4)\n\nB\n\n-) I)!(Xt, Wb) - E(y I Xt, DCn) U (x, y))]2 p(Y I x, Wb)\n\n,y\n\nb=l\n\nThe final value of f; is obtained by averaging over the different y E YCx,R) and\ndifferent trial inputs Xt E XT. To reduce the variance, the trial inputs Xt should\nbe selected by importance sampling (2) to concentrate them on regions with high\ncurrent loss (see (5) below). To facilitate the search for an x with minimal f; we\nreduce the extent of random fluctuations of the y values. Let (Vi, ... , VR) be a\nvector of random numbers Vr -- N(O,1), and let jr be randomly selected from\n{1, ... , B}. Then for each x the possible observations Yr E YCx,R) are defined as\nYr := !(x, wir) + V r?T2(wir). In this way the difference between neighboring inputs\nis not affected by noise, and search procedures can exploit gradients.\n\n3.2\n\nCURRENT LOSS\n\nAs a proxy for the future loss, we may use the current loss at\n\nx,\n\nrcurr(x) = p(x) j L(y, d*(x)) p(y I x, DCn)) dy\n\n(5)\n\n\x0cBayesian Query Construction for Neural Network Models\n\n447\n\nwhere p(x) weights the inputs according to their relevance. For the square loss\nfunction the average loss at x is the conditional variance Var(y I x, DCn?. We get\n\n=\n\nTcurr(X)\n\np(x) jU(x,w)-E(YIX,DCn?)2p(wIDcn?dw\n\n(6)\n\n+ p(x) j 0"2(w) p(w I D(n? dw\nIf E(y I x,DCn?\nfr~~=lf(x,wb) and the sample WCB):= {Wl, ... ,WB} is\nrepresentative of p(w I DCn? we can approximate the current loss with\n\nTcurr(X)\n\n~\n\np( x) ~\n\n13 L..tU(x, Wb) -\n\n2\n\nE(y I x, DCn?) +\nA\n\np( x) ~\n\n13 L..t 0"\n\nb=l\n\n2\n\n(Wb)\n\n(7)\n\nb=l\n\nIf the input distribution p( x) is uniform, the second term is independent of x.\n3.3\n\nCOMPLEXITY REGULARIZATION\n\nNeural network models can represent arbitrary mappings between finite-dimensional\nspaces if the number of hidden units is sufficiently large [Hornik Stinchcombe 89].\nAs the number of observations grows, more and more hidden units are necessary to catch the details of the mapping. Therefore we use a sequential procedure to increase the capacity of our networks during query learning. White and\nWooldridge call this approach the "method of sieves" and provide some asymptotic results on its consistency [White Wooldridge 91]. Gelfand and Dey compare Bayesian approaches for model selection and prove that, in the case of nested models Ml and M2, model choice by the ratio of popular Bayes factors\np(DCn) I Mi) := J p(DCn) I W, Mi ) p(w I Mi) dw will always choose the full model\nregardless of the data as n --t 00 [Gelfand Dey 94]. They show that the pseudoBayes factor, a Bayesian variant of crossvalidation, is not affected by this paradox\n\nA(Ml\' M2) :=\n\nn\n\nn\n\n;=1\n\nj=1\n\nII p(y; I x;, DCn,j), Mt}j II p(Y; Ix;, DCn,j), M2)\n\n(8)\n\nHere DCn ,;) := D(n) \\ (x;, y;). As the difference between p(w I DCn? and p( wi D(n,j?\nis usually small, we use the full posterior as the importance function (2) and get\n\np(Y;\n\nI x;, DCn,j),Mi) =\n\nj p(Y; IXj,w,Mi)p(wIDCn,j),Mi)dw\n\n\'" B/(t,l/P(Y;li;,W"M,))\n4\n\n(9)\n\nNUMERICAL DEMONSTRATION\n\nIn a first experiment we tested the approach for a small a 1-2-1 MLP target function with Gaussian noise N(0,0.05 2 ). We assumed the square loss function and a\nuniform input distribution p(x) over [-5,5]. Using the "true" architecture for the\napproximating model we started with a single randomly generated observation. We\n\n\x0c448\n\nGerhard Paass, JiJrg Kindermann\n\n~\n\n=~!?~\n\n--- ~tuo:io_\n\n~\n\n.. .\' .\n\n1\'01\n\n..\n\non\n\n~\n\nI - \'~ \' =~ I\n\n:;\n\n"\n\n. ..\n\na:\n0\n\n::::.:::::.::::\\....\nd\n\n:;\n\n....\n\n\\~.\n\n\'\\ ------ -- - - - - - - -----\n\n\\., 1\\l\n\n. . ......_. _-_._...........__................... _. ._......._..\n\n~\n\n\\!\n\n~\n\n,\n\n\\\n\n:.,.\n\\, \'\n\n"\n\n\\!\n\n\'"\n0\n\n..\n\n-2\n\n10\n\n15\n20\nNo.d_\n\n25\n\n30\n\nFigure 1: Future loss exploration: predicted posterior mean, future loss and current\nloss for 12 observations (left), and root mean square error of prediction (right) .\nestimated the future loss by (4) for 100 different inputs and selected the input with\nsmallest future loss as the next query. B = 50 parameter vectors were generated requiring 200,000 Metropolis steps. Simultaneously we approximated the current loss\ncriterion by (7). The left side of figure 1 shows the typical relation of both measures.\nIn most situations the future loss is low in the same regions where the current loss\n(posterior standard deviation of mean prediction) is high. The queries are concentrated in areas of high variation and the estimated posterior mean approximates\nthe target function quite well.\nIn the right part of figure 1 the RMSE of prediction averaged over 12 independent\nexperiments is shown. After a few observations the RMSE drops sharply. In our\nexample there is no marked difference between the prediction errors resulting from\nthe future loss and the current loss criterion (also averaged over 12 experiments).\nConsidering the substantial computing effort this favors the current loss criterion.\nThe dots indicate the RMSE for randomly generated data (averaged over 8 experiments) using the same Bayesian prediction procedure. Because only few data points\nwere located in the critical region of high variation the RMSE is much larger.\nIn the second experiment, a 2-3-1 MLP defined the target function I(x, wo) , to which\nGaussian noise of standard deviation 0.05 was added. I( x, wo) is shown in the left\npart of figure 2. We used five MLPs with 2-6 hidden units as candidate models\nMl, .. . , M5 and generated B = 45 samples WeB) of the posterior pew I D(n)\' M.),\nwhere D(n) is the current data. We started with 30,000 Metropolis steps for small\nvalues of n and increased this to 90,000 Metropolis steps for larger values of n.\nFor a network with 6 hidden units and n = 50 observations, 10,000 Metropolis\nsteps took about 30 seconds on a Sparc10 workstation. Next, we used equation (9)\nto compare the different models, and then used the optimal model to calculate the\ncurrent loss (7) on a regular grid of 41 x 41 = 1681 query points x. Here we assumed\nthe square loss function and a uniform input distribution p(x) over [-5,5] x [-5,5].\nWe selected the query point with maximal current loss and determined the final\nquery point with a hillclimbing algorithm. In this way we were rather sure to get\nclose to the true global optimum.\nThe main result of the experiment is summarized in the right part of figure 2. It\n\n\x0cBayesian Query Construct.ion for Neural Network Models\n\n449\n\n?\no\n\n".\n\n.m\n\neXDlorati~n\nrandom a\n\n:2"\': \\\n<:>\n\n\\\n\n~\\?{l? .\n.,\n."\n\no .. o .. o ............. __ (). ...\n\n\\\n\n. . .......... 0 ... .. ........ --\n\n..\n\n~.\n\n20\n\n40\n\n0\n\n60\n\n80\n\n100\n\nNo. of Observations\n\nFigure 2: Current loss exploration: MLP target function and root mean square error.\nshows - averaged over 3 experiments - the root mean square error between the true\nmean value and the posterior mean E(y I x) on the grid of 1681 inputs in relation to\nthe sample size. Three phases of the exploration can be distinguished (see figure 3).\nIn the beginning a search is performed with many queries on the border of the\ninput area. After about 20 observations the algorithm knows enough detail about\nthe true function to concentrate on the relevant parts of the input space. This leads\nto a marked reduction ofthe mean square error. After 40 observations the systematic\npart of the true function has been captured nearly perfectly. In the last phase of\nthe experiment the algorithm merely reduces the uncertainty caused by the random\nnoise. In contrast , the data generated randomly does not have sufficient information\non the details of f(x , w), and therefore the error only gradually decreases. Because\nof space constraints we cannot report experiments with radial basis functions which\nled to similar results.\nAcknowledgements\nThis work is part of the joint project \'REFLEX\' of the German Fed. Department\nof Science and Technology (BMFT), grant number 01 IN 111Aj4. We would like to\nthank Alexander Linden, Mark Ring, and Frank Weber for many fruitful discussions.\n\nReferences\n[Berger 80] Berger, J. (1980): Statistical Decision Theory, Foundations, Concepts, and\nMethods. Springer Verlag, New York.\n[Cohn 94] Cohn, D. (1994): Neural Network Exploration Using Optimal Experimental\nDesign. In J. Cowan et al. (eds.): NIPS 5. Morgan Kaufmann, San Mateo.\n[Ford et al. 89] Ford, I. , Titterington, D.M., Kitsos, C.P. (1989): Recent Advances in Nonlinear Design. Technometrics, 31, p.49-60.\n[Gelfand Dey 94] Gelfand, A.E., Dey, D.K. (1994): Bayesian Model Choice: Asymptotics\nand Exact Calculations. J. Royal Statistical Society B, 56, pp.501-514.\n\n\x0c450\n\nGerhard Paass, Jorg Kindermann\n\nFigure 3: Squareroot of current loss (upper row) and absolute deviation from true\nfunction (lower row) for 10,25, and 40 observations (which are indicated by dots) .\n[Hornik Stinchcombe 89] Hornik, K., Stinchcombe, M. (1989): Multilayer Feedforward\nNetworks are Universal Approximators. Neural Networks 2, p.359-366.\n[Kalos Whitlock 86] Kalos, M.H., Whitlock, P.A. (1986): Monte Carlo Methods, Wiley,\nNew York.\n[MacKay 92] MacKay, D. (1992): Information-Based Objective Functions for Active Data\nSelection. Neural Computation 4, p .590-604.\n[Neal 93] Neal, R.M. (1993): Probabilistic Inference using Markov Chain Monte Carlo\nMethods. Tech. Report CRG-TR-93-1, Dep. of Computer Science, Univ. of Toronto.\n[PaaB 91] PaaB, G. (1991): Second Order Probabilities for Uncertain and Conflicting Evidence. In: P.P. Bonissone et al. (eds.) Uncertainty in Artificial Intelligence 6. Elsevier,\nAmsterdam, pp. 447-456.\n[Plutowski White 93] Plutowski, M., White, H. (1993): Selecting Concise Training Sets\nfrom Clean Data. IEEE Tr. on Neural Networks, 4, p.305-318.\n[Pronzato Walter 92] Pronzato, L., Walter, E. (1992): Nonsequential Bayesian Experimental Design for Response Optimization. In V. Fedorov, W.G. Miiller, I.N. Vuchkov\n(eds.): Model Oriented Data-Analysis. Physica Verlag, Heidelberg, p. 89-102.\n[Sollich 94] Sollich, P. (1994): Query Construction, Entropy and Generalization in Neural\nNetwork Models. To appear in Physical Review E.\n[Sollich Saad 95] Sollich, P., Saad, D. (1995): Learning from Queries for Maximum Information Gain in Unlearnable Problems. This volume.\n[White Wooldridge 91] White, H., Wooldridge, J. (1991): Some Results for Sieve Estimation with Dependent Observations. In W. Barnett et al. (eds.) : Nonparametric and\nSemiparametric Methods in Econometrics and Statistics, New York, Cambridge Univ.\nPress.\n\n\x0c'
p83183
sg306
S'Stable Linear Approximations to\nDynamic Programming for Stochastic\nControl Problems with Local Transitions\n\nBenjamin Van Roy and John N. Tsitsiklis\nLaboratory for Information and Decision Systems\nMassachusetts Institute of Technology\nCambridge, MA 02139\ne-mail: bvr@mit.edu, jnt@mit.edu\n\nAbstract\nWe consider the solution to large stochastic control problems by\nmeans of methods that rely on compact representations and a variant of the value iteration algorithm to compute approximate costto-go functions. While such methods are known to be unstable in\ngeneral, we identify a new class of problems for which convergence,\nas well as graceful error bounds, are guaranteed. This class involves linear parameterizations of the cost-to- go function together\nwith an assumption that the dynamic programming operator is a\ncontraction with respect to the Euclidean norm when applied to\nfunctions in the parameterized class. We provide a special case\nwhere this assumption is satisfied, which relies on the locality of\ntransitions in a state space. Other cases will be discussed in a full\nlength version of this paper.\n\n1\n\nINTRODUCTION\n\nNeural networks are well established in the domains of pattern recognition and\nfunction approximation, where their properties and training algorithms have been\nwell studied. Recently, however, there have been some successful applications of\nneural networks in a totally different context - that of sequential decision making\nunder uncertainty (stochastic control).\nStochastic control problems have been studied extensively in the operations research\nand control theory literature for a long time, using the methodology of dynamic\nprogramming [Bertsekas, 1995]. In dynamic programming, the most important\nobject is the cost-to-go (or value) junction, which evaluates the expected future\n\n\x0c1046\n\nB. V. ROY, 1. N. TSITSIKLIS\n\ncost to be incurred, as a function of the current state of a system. Such functions\ncan be used to guide control decisions.\nDynamic programming provides a variety of methods for computing cost-to- go\nfunctions. Unfortunately, dynamic programming is computationally intractable in\nthe context of many stochastic control problems that arise in practice. This is\nbecause a cost-to-go value is computed and stored for each state, and due to the\ncurse of dimensionality, the number of states grows exponentially with the number\nof variables involved.\nDue to the limited applicability of dynamic programming, practitioners often rely\non ad hoc heuristic strategies when dealing with stochastic control problems. Several recent success stories - most notably, the celebrated Backgammon player of\nTesauro (1992) - suggest that neural networks can help in overcoming this limitation. In these applications, neural networks are used as compact representations\nthat approximate cost- to-go functions using far fewer parameters than states. This\napproach offers the possibility of a systematic and practical methodology for addressing complex stochastic control problems.\nDespite the success of neural networks in dynamic programming, the algorithms\nused to tune parameters are poorly understood. Even when used to tune the parameters of linear approximators, algorithms employed in practice can be unstable\n[Boyan and Moore, 1995; Gordon, 1995; Tsitsiklis and Van Roy, 1994].\nSome recent research has focused on establishing classes of algorithms and compact\nrepresentation that guarantee stability and graceful error bounds. Tsitsiklis and Van\nRoy (1994) prove results involving algorithms that employ feature extraction and interpolative architectures. Gordon (1995) proves similar results concerning a closely\nrelated class of compact representations called averagers. However, there remains\na huge gap between these simple approximation schemes that guarantee reasonable\nbehavior and the complex neural network architectures employed in practice.\nIn this paper, we motivate an algorithm for tuning the parameters of linear compact representations, prove its convergence when used in conjunction with a class\nof approximation architectures, and establish error bounds. Such architectures are\nnot captured by previous results. However, the results in this paper rely on additional assumptions. In particular, we restrict attention to Markov decision problems\nfor which the dynamic programming operator is a contraction with respect to the\nEuclidean norm when applied to functions in the parameterized class. Though\nthis assumption on the combination of compact representation and Markov decision problem appears restrictive, it is actually satisfied by several cases of practical\ninterest. In this paper, we discuss one special case which employs affine approximations over a state space, and relies on the locality of transitions. Other cases will\nbe discussed in a full length version of this paper.\n\n2\n\nMARKOV DECISION PROBLEMS\n\nWe consider infinite horizon, discounted Markov decision problems defined on a\nfinite state space S = {I, .. . , n} [Bertsekas, 1995]. For every state i E S, there is\na finite set U(i) of possible control actions, and for each pair i,j E S of states and\ncontrol action u E U (i) there is a probability Pij (u) of a transition from state i to\nstate j given that action u is applied. Furthermore, for every state i and control\naction u E U (i), there is a random variable Ciu which represents the one-stage cost\nif action u is applied at state i.\nLet\n\nf3\n\nE [0,1) be a discount factor. Since the state spaces we consider in this paper\n\n\x0cStable Linear Approximations Programming for Stochastic Control Problems\n\n1047\n\nare finite, we choose to think of cost-to-go functions mapping states to cost- to-go\nvalues in terms of cost-to-go vectors whose components are the cost-to-go values\nof various states. The optimal cost-to-go vector V* E !R n is the unique solution to\nBellman\'s equation:\n\nVi*= min. (E[CiU]+.BLPij(U)Vj*),\nuEU(t)\n\nViES.\n\n(1)\n\njES\n\nIf the optimal cost-to-go vector is known, optimal decisions can be made at any\nstate i as follows:\n\nu*=arg min. (E[CiU]+.BLPij(U)l--j*),\nuEU(t)\n\nViES.\n\njES\n\nThere are several algorithms for computing V* but we only discuss the value iteration algorithm which forms the basis of the approximation algorithm to be considered later on. We start with some notation. We define the dynamic programming\noperator as the mapping T : !R n r-t !R n with components Ti : !R n r-t !R defined by\n\nTi(V) = min. (E[CiU]+.BLPij(U)Vj ),\nuEU(t)\n\nViES.\n\n(2)\n\njES\n\nIt is well known and easy to prove that T is a maximum norm contraction. In\nparticular ,\n\nIIT(V) - T(V\')lloo :s;\n\n.BIIV -\n\nV\'lIoo,\n\nThe value iteration algorithm is described by\nV(t + 1) = T(V(t)),\nwhere V (0) is an arbitrary vector in !R n used to initialize the algorithm. It is easy\nto see that the sequence {V(t)} converges to V*, since T is a contraction.\n\n3\n\nAPPROXIMATIONS TO DYNAMIC PROGRAMMING\n\nClassical dynamic programming algorithms such as value iteration require that we\nmaintain and update a vector V of dimension n. This is essentially impossible when\nn is extremely large, as is the norm in practical applications. We set out to overcome\nthis limitation by using compact representations to approximate cost-to-go vectors.\nIn this section, we develop a formal framework for compact representations, describe\nan algorithm for tuning the parameters of linear compact representations, and prove\na theorem concerning the convergence properties of this algorithm.\n\n3.1\n\nCOMPACT REPRESENTATIONS\n\nA compact representation (or approximation architecture) can be thought of as a\nscheme for recording a high-dimensional cost-to-go vector V E !R n using a lowerdimensional parameter vector wE !Rm (m ?n). Such a scheme can be described by\na mapping V : !Rm r-t !R n which to any given parameter vector w E !R m associates\na cost-to-go vector V(w). In particular, each component Vi (w) of the mapping is\nthe ith component of a cost-to-go vector represented by the parameter vector w.\nNote that, although we may wish to represent an arbitrary vector V E !R n , such a\nscheme allows for exact representation only of those vectors V which happen to lie\nin the range of V.\nIn this paper, we are concerned exclusively with linear compact representations of\nthe form V(w) = Mw, where M E !Rnxm is a fixed matrix representing our choice\nof approximation architecture. In particular, we have Vi(w) = Miw, where Mi (a\nrow vector) is the ith row of the matrix M.\n\n\x0c1048\n\n3.2\n\nB. V. ROY, J. N. TSITSIKLIS\n\nA STOCHASTIC APPROXIMATION SCHEME\n\nOnce an appropriate compact representation is chosen, the next step is to generate\na parameter vector w such that V{w) approximates V*. One possible objective is\nto minimize squared error of the form IIMw - V*II~. If we were given a fixed set\nof N samples {( iI, ~:), (i2\' Vi;), ... , (i N, ~:)} of an optimal cost-to-go vector V*, it\nseems natural to choose a parameter vector w that minimizE\'s E7=1 (Mij w - ~;)2.\nOn the other hand, if we can actively sample as many data pairs as we want, one\nat a time, we might consider an iterative algorithm which generates a sequence of\nparameter vectors {w(t)} that converges to the desired parameter vector. One such\nalgorithm works as follows: choose an initial guess w(O), then for each t E {O, 1, ... }\nsample a state i{t) from a uniform distribution over the state space and apply the\niteration\n(3)\nwhere {a(t)} is a sequence of diminishing step sizes and the superscript T denotes\na transpose. Such an approximation scheme conforms to the spirit of traditional\nfunction approximation - the algorithm is the common stochastic gradient descent\nmethod. However, as discussed in the introduction, we do not have access to such\nsamples of the optimal cost-to-go vector. We therefore need more sophisticated\nmethods for tuning parameters.\nOne possibility involves the use of an algorithm similar to that of Equation 3,\nreplacing samples of ~(t) with TiCt) (V(t)). This might be justified by the fact that\nT(V) can be viewed as an improved approximation to V*, relative to V. The\nmodified algorithm takes on the form\n(4)\n\nIntuitively, at each time t this algorithm treats T(Mw(t)) as a "target" and takes\na steepest descent step as if the goal were to find a w that would minimize IIMwT(Mw(t))II~. Such an algorithm is closely related to the TD(O) algorithm of Sutton\n(1988). Unfortunately, as pointed out in Tsitsiklis and Van Roy (1994), such a\nscheme can produce a diverging sequence {w(t)} of weight vectors even when there\nexists a parameter vector w* that makes the approximation error V* - Mw* zero at\nevery state. However, as we will show in the remainder of this paper, under certain\nassumptions, such an algorithm converges.\n\n3.3\n\nMAIN CONVERGENCE RESULT\n\nOur first assumption concerning the step size sequence {a(t)} is standard to stochastic approximation and is required for the upcoming theorem.\nAssumption 1 Each step size a(t) is chosen prior to the generation of i(t), and\nthe sequence satisfies E~o a(t) = 00 and E~o a 2 (t) < 00.\nOur second assumption requires that T : lR n t-+ lR n be a contraction with respect\nto the Euclidean norm, at least when it operates on value functions that can be\nrepresented in the form Mw, for some w. This assumption is not always satisfied,\nbut it appears to hold in some situations of interest, one of which is to be discussed\nin Section 4.\n\n{3\' E [0, 1) such that\n::; {3\'IIMw - Mw\'112,\n\nAssumption 2 There exists some\n\nIIT(Mw) - T(Mw\')112\n\nVw,w\' E lRm.\n\n\x0cStable Linear Approximations to Programming for Stochastic Control Problems\n\n1049\n\nThe following theorem characterizes the stability and error bounds associated with\nthe algorithm when the Markov decision problem satisfies the necessary criteria.\nTheorem 1 Let Assumptions 1 and 2 hold, and assume that M has full column\n\nrank. Let I1 = M(MT M)-l MT denote the projection matrix onto the subspace\nX = {Mwlw E ~m}. Then,\n(a) With probability 1, the sequence w(t) converges to w*, the unique vector that\nsolves:\nMw* = I1T(Mw*).\n(b) Let V* be the optimal cost-to-go vector. The following error bound holds:\nIIMw* - V*1I2\n\n3.4\n\n~ (1 ;!~ynllI1V* - V*lloo.\n\nOVERVIEW OF PROOF\n\nDue to space limitations, we only provide an overview of the proof of Theorem 1.\nLet s : ~m\n\nf-7 ~m\n\nbe defined by\n\ns(w)\n\n=E\n\n[( Miw - Ti(Mw(t)))MT] ,\n\nwhere the expectation is taken over i uniformly distributed among {I, .. . , n}.\nHence,\nE[w(t + l)lw(t), a(t)] = w(t) - a(t)s(w(t)),\nwhere the expectation is taken over i(t). We can rewrite s as\n\ns(w) =\n\n~(MTMW -\n\nMTT(MW)) ,\n\nand it can be thought of as a vector field over ~m. If the sequence {w(t)} converges\nto some w, then s (w) must be zero, and we have\n\nMTMw\nMw\n\nMTT(Mw)\nI1T(Mw).\n\n=\n\nNote that\n\nIII1T(Mw) -\n\nI1T(Mw\')lb\n\n~\n\n{j\'IIMw - Mw\'112,\n\nVw,w\' E\n\n~m,\n\ndue to Assumption 2 and the fact that projection is a nonexpansion of the Euclidean\nnorm. It follows that I1Te) has a unique fixed point w* E ~m, and this point\nuniquely satisfies\nMw* = I1T(Mw*).\nWe can further establish the desired error bound:\n\nIIMw* -\n\nV*112 <\n\nIIMw* - I1T(I1V*) 112 + III1T(I1V*) - I1V*112 + III1V*\n< {j\'IIMw* - V*112 + IIT(I1V*) - V*112 + III1V* - V*1I2\n< t3\'IIMw* - V*112 + (1 + mv\'nII I1 V* - V*lloo,\n\n- V*112\n\nand it follows that\n\nConsider the potential function U(w)\n(\\1U(w))T s(w) 2 ,U(w), for some,\n\n=\n>\n\n~llw\n\n-\n\nw*II~.\n\nWe will establish that\n0, and we are therefore dealing with a\n\n\x0cB. V. ROY, J. N. TSITSIKLIS\n\n1050\n\n"pseudogradient algorithm" whose convergence follows from standard results on\nstochastic approximation [Polyak and Tsypkin, 1972J. This is done as follows:\n\n(\\7U(w)f s(w)\n\n~ (w -\n\nw*) T MT (Mw - T(Mw))\n\n~ (w -\n\nw*) T MT(Mw - IIT(Mw) - (J - II)T(MW))\n\n~(MW-Mw*)T(MW-IIT(MW)),\n\n=\n\nwhere the last equality follows because MTrr = MT. Using the contraction assumption on T and the nonexpansion property of projection mappings, we have\n\nIlIIT(Mw) - Mw*112\n::;\n\nIIIIT(Mw) - rrT(Mw*)112\n,6\'IIMw - Mw*1I2\'\n\nand applying the Cauchy-Schwartz inequality, we obtain\n\n(\\7U(W))T s(w)\n\n1\nn\n\n> -(IIMw - Mw*ll~ -IIMw - Mw*1121IMw* - IIT(Mw)112)\n> !:.(l - ,6\')IIMw - Mw*II~?\nn\n\nSince M has full column rank, it follows that (\\7U(W))T s(w) ~ 1\'U(w), for some\nfixed l\' > 0, and the proof is complete.\n\n4\n\nEXAMPLE: LOCAL TRANSITIONS ON GRIDS\n\nTheorem 1 leads us to the next question: are there some interesting cases for which\nAssumption 2 is satisfied? We describe a particular example here that relies on\nproperties of Markov decision problems that naturally arise in some practical situations.\nWhen we encounter real Markov decision problems we often interpret the states\nin some meaningful way, associating more information with a state than an index\nvalue. For example, in the context of a queuing network, where each state is one\npossible queue configuration, we might think of the state as a vector in which each\ncomponent records the current length of a particular queue in the network. Hence,\nif there are d queues and each queue can hold up to k customers, our state space is\n(Le., the set of vectors with integer components each in the range\na finite grid\n{O, ... ,k-l}).\n\nzt\n\nConsider a state space where each state i E {I, ... , n} is associated to a point\nxi E\n(n = k d ), as in the queuing example. We might expect that individual\ntransitions between states in such a state space are local. That is, if we are at\na state xi the next visited state x j is probably close to xi in terms of Euclidean\ndistance. For instance, we would not expect the configuration of a queuing network\nto change drastically in a second. This is because one customer is served at a time\nso a queue that is full can not suddenly become empty.\n\nzt\n\nzt\n\ngrows exponentially\nNote that the number of states in a state space of the form\nwith d. Consequently, classical dynamic programming algorithms such as value\niteration quickly become impractical. To efficiently generate an approximation to\nthe cost-to-go vector, we might consider tuning the parameters w E Rd and a E R\nof an affine approximation ~(w, a) = w T xi + a using the algorithm presented in\nthe previous section. It is possible to show that, under the following assumption\n\n\x0cStable Linear Approximations to Programming for Stochastic Control Problems\n\n1051\n\nconcerning the state space topology and locality of transitions, Assumption 2 holds\nwith f3\' = .;f32\nthe algorithm.\n\n+ k~3\' and\n\nthus Theorem 1 characterizes convergence properties of\n\nAssumption 3 The Markov decision problem has state space S = {1, ... , k d }, and\neach state i is uniquely associated with a vector xi E\nwith k ~ 6(1 - (32)-1 + 3.\nA ny pair xi, x j E\nof consecutively visited states either are identical or have\nexactly one unequal component, which differs by one.\n\nzt\n\nzt\n\nWhile this assumption may seem restrictive, it is only one example. There are many\nmore candidate examples, involving other approximation architectures and particular classes of Markov decision problems, which are currently under investigation.\n\n5\n\nCONCLUSIONS\n\nWe have proven a new theorem that establishes convergence properties of an algorithm for generating linear approximations to cost-to-go functions for dynamic\nprogramming. This theorem applies whenever the dynamic programming operator\nfor a Markov decision problem is a contraction with respect to the Euclidean norm\nwhen applied to vectors in the parameterized class. In this paper, we have described\none example in which such a condition holds. More examples of practical interest\nwill be discussed in a forthcoming full length version of this paper.\nAcknowledgments\n\nThis research was supported by the NSF under grant ECS 9216531, by EPRI under\ncontract 8030-10, and by the ARO.\nReferences\n\nBertsekas, D. P. (1995) Dynamic Programming and Optimal Control. Athena Scientific, Belmont, MA.\nBoyan, J. A. & Moore, A. W. (1995) Generalization in Reinforcement Learning:\nSafely Approximating the Value Function. In J. D. Cowan, G. Tesauro, and D.\nTouretzky, editors, Advances in Neural Information Processing Systems 7. Morgan\nKaufmann.\nGordon, G. J. (1995) Stable Function Approximation in Dynamic Programming.\nTechnical Report: CMU-CS-95-103, Carnegie Mellon University.\nPolyak, B. T. & Tsypkin, Y. Z., (1972) Pseudogradient Adaptation and Training\nAlgorithms. A vtomatika i Telemekhanika, 3:45-68.\nSutton, R. S. (1988) Learning to Predict by the Method of Temporal Differences.\nMachine Learning, 3:9-44.\nTesauro, G. (1992) Practical Issues in Temporal Difference Learning.\nLearning, 8:257-277.\n\nMachine\n\nTsitsiklis, J. & Van Roy, B. (1994) Feature-Based Methods for Large Scale Dynamic\nProgramming. Technical Report: LIDS-P-2277, Laboratory for Information and\nDecision Systems, Massachusetts Institute of Technology. Also to appear in Machine\nLearning.\n\n\x0c'
p83184
sg87
S'Context-Dependent Classes in a Hybrid\nRecurrent Network-HMM Speech\nRecognition System\n\nDan Kershaw\nTony Robinson\nMike Hochberg ?\nCambridge University Engineering Department,\nTrumpington Street, Cambridge CB2 1PZ, England.\nTel: [+44]1223332800, Fax: [+44]1223332662.\nEmail: djk.ajr@eng.cam.ac.uk\n\nAbstract\nA method for incorporating context-dependent phone classes in\na connectionist-HMM hybrid speech recognition system is introduced . A modular approach is adopted, where single-layer networks\ndiscriminate between different context classes given the phone class\nand the acoustic data. The context networks are combined with a\ncontext-independent (CI) network to generate context-dependent\n(CD) phone probability estimates. Experiments show an average\nreduction in word error rate of 16% and 13% from the CI system\non ARPA 5,000 word and SQALE 20,000 word tasks respectively.\nDue to improved modelling, the decoding speed of the CD system\nis more than twice as fast as the CI system.\n\nINTRODUCTION\nThe ABBOT hybrid connectionist-HMM system performed competitively with many\nconventional hidden Markov model (HMM) systems in the 1994 ARPA evaluations\nof speech recognition systems (Hochberg, Cook, Renals, Robinson & Schechtman\n1995). This hybrid framework is attractive because it is compact, having far fewer\nparameters than conventional HMM systems, whilst also providing the discriminative powers of a connectionist architecture.\nIt is well established that particular phones vary acoustically when they occur in\ndifferent phonetic contexts. For example a vowel may become nasalized when following a nasal sound. The short-term contextual influence of co-articulation is\n?Mike Hochberg is now at Nuance Communications, 333 Ravenswood Avenue, Building\n110, Menlo Park, CA 94025, USA. Tel: [+1] 415 6148260.\n\n\x0cContext-dependent Classes in a Speech Recognition System\n\n751\n\nhandled in HMMs by creating a model for all sufficiently differing phonetic contexts with enough acoustic evidence. This modelling of phones in their particular\nphonetic contexts produces sharper probability density functions . This approach\nvastly improves HMM recognition accuracy over equivalent context-independent\nsystems (Lee 1989). Although the recurrent neural network (RNN) model acoustic\ncontext internally (within the state vector) , it does not model phonetic context.\nThis paper presents an approach to improving the ABBOT system through phonetic\ncontext-dependent modelling.\nIn Cohen, Franco, Morgan , Rumelhart & Abrash (1992) separate sets of contextdependent output layers are used to model context effects in different states ofHMM\nphone models. A set of networks discriminate between phones in 8 different broadclass left and right contexts. Training time is reduced by initialising from a CI multilayer perceptron (MLP) and only changing the hidden-to-output weights during\ncontext-dependent training. This system performs well on the DARPA Resource\nManagement Task. The work presented in Zhoa, Schwartz , Sroka & Makhoul (1995)\nfollowed along similar work to Cohen et al. (1992) . A context-dependent mixture\nof experts (ME) system (Jordan & Jacobs 1994) based on the structure of the\ncontext-independent ME was built. For each state, the whole training data was\ndivided into 46 parts according to its left or right context. Then, a separate ME\nmodel was built for each context.\nAnother approach to phonetic context-dependent modelling with MLPs was proposed by Bourlard & Morgan (1993) . It was based on factoring the conditional\nprobability of a phone-in-context given the data in terms of the phone given the\ndata , and its context given the data and the phone. The approach taken in this\npaper is a mixture of the above work. However, this work augments a recurrent network (rather than an MLP) and concentrates on building a more compact system,\nwhich is more suited to our requirements. As a result, the context training scheme is\nfast and is implemented on a workstation (rather than a parallel processing machine\nas is used for training the RNN) .\n\nOVERVIEW OF THE ABBOT HYBRID SYSTEM\nThe basic framework of the ABBOT system is similar to the one described in Bourlard\n& Morgan (1994) except that a recurrent network is used as the acoustic model\nfor the within the HMM framework. A more detailed description of the recurrent\nnetwork for phone probability estimation is given in Robinson (1994). At each 16ms\ntime frame , the acoustic vector u(t) is mapped to an output vector y(t), which\nrepresents an estimate of the posterior probability of each of the phone classes\n\nYi(t) ~ Pr(qi(t)lui H ) ,\n(1)\nwhere qi(t) is phone class i at time t , and ul = {u(l) , .. . , u(t)} is the input from\ntime 1 to t . Left (past) acoustic context is modelled internally by a 256 dimensional\nstate vector x(t) , which can be envisaged as "storing" the information that has\nbeen presented at the input. Right (future) acoustic context is given by delaying\nthe posterior probability estimation until four frames of input have been seen by the\nnetwork . The network is trained using a modified version of error back-propagation\nthrough time (Robinson 1994) .\nDecoding with the hybrid connectionist-HMM approach is equivalent to conventional HMM decoding, with the difference being that the RNN models the state\nobservations. Like typical HMM systems, the recognition process is expressed as\nfinding the maximum a posteriori state sequence for the utterance . The decoding\ncriterion specified above requires the computation of the likelihood of the acoustic\n\n\x0c752\n\nD. KERSHAW, T. ROBINSON, M. HOCHBERG\n\ndata given a phone (state) sequence,\n\n( (t)1 .(t)) = Pr(qi(t)lu(t))p(u(t))\nq,\nPr(qi)\'\n\n(2)\n\np u\n\nwhere p(u(t)) is the same for all phones, and hence drops out of the decoding\nprocess. Hence, the network outputs are mapped to scaled likelihoods by,\n\nYi(t)\np(U(t)lqi(t)) ::: Pr(qd \'\n\n(3)\n\nwhere the priors Pr(qi) are estimated from the training data. Decoding uses the\nNOWAY decoder (Renals & Hochberg 1995) to compute the utterance model that is\nmost likely to have generated the observed speech signal.\n\nCONTEXT-DEPENDENT PROBABILITY ESTIMATION\nThe approach taken by this work is to augment the CI RNN, in a similar vein\nto Bourlard & Morgan (1993). The context-dependent likelihood, p(UtIC t , Qd,\ncan be factored as,\np\n\n(u\n\nt\n\nIC Q) = Pr(Ct!Ut, Qt)p(Ut/Qt)\nt, t\nPr(Ct!Qd\'\n\n(4)\n\nwhere C is a set of context classes and Q is a set of context-independent phones or\nmonophones. Substituting for the context independent probability density function,\np(U t IQt), using (2), this becomes\np\n\n(u\n\nt\n\nIC Q) = Pr(C t IUt, Qd Pr(Qt!U t ) (U)\nt, t\nPr(CtIQt) Pr(Qt)\nPt?\n\n(5)\n\nThe term p(U t} is constant for all frames, so this drops out of the decoding process\nand is ignored for all further purposes. This format is extremely appealing since\nPr(C t IQt) and Pr(Qt) are estimated from the training data and the CI RNN estimates Pr(QtIUt). All that is then needed is an estimate of Pr(CtIU t , Qt). The\napproach taken in this paper uses a set of context experts or modules for each monophone class to augment the existing CI RNN.\n\nTRAINING ON THE STATE VECTOR\nAn estimate of Pr(Ct!U t , Qt) can be obtained by training a recurrent network to\ndiscriminate between contexts Cj(t) for phone class qi(t), such that\n\n(6)\nwhere Yjli (t) is an estimate of the posterior probability of context class j given\nphone class i. However, training recurrent neural networks in this format would\nbe expensive and difficult. For a recurrent format, the network must contain no\ndiscontinuities in the frame-by-frame acoustic input vectors. This implies all recurrent networks for all the phone classes i must be "shown" all the data. Instead, the\nassumption is made that since the state vector x = f(u), then\n\nx(t\n\n+ 4)\n\nis a good representation for\n\nuiH .\n\nHence, a single-layer perceptron is trained on the state vectors corresponding to\neach monophone, qi, to classify the different phonetic context classes. Finally,\n\n\x0cContext-dependent Classes in a Speech Recognition System\n\n753\n\nthe likelihood estimates for the phonetic context class j for phone class i used\nin decoding are given by,\nPr(qi(t)lui+4) Pr(cj (t)lx(t\n\n+ 4) , qi(t))\n\nPr(cj(t)lqi(t)) Pr(qi(t))\nYi (t)Yjli (t)\nPr( Cj Iqi) Pr( qd .\n\n(7)\n\nEmbedded training is used to estimate the parameters of the CD networks and\nthe training data is aligned using a Viterbi segmentation. Each context network\nis trained on a non-overlapping subset of the state vectors generated from all the\nViterbi aligned training data. The context networks were trained using the RProp\ntraining procedure (Robinson 1994).\n\n=>\n=> ~\n=> -i\noo\n\n:I\n\nI\n\n~\n\ni:I\n\nC.\nCD\n\n:I\n\n"\'tJ\n\nCD\n\n~.\n\no\n""I\n\n"\'tJ\n\n01ime 0\nO~elayO\n\n\'---------fO ~\n\n=>\n\n0 11 - - - - \'\n\na\nC"\nD)\n\ng:\n\n\'~\nyj1i(t)\n~\n\n_______ oJ\'\n\n,,\n\n,\',.-.._,1\n\nFigure 1: The Phonetic Context-Dependent RNN Modular System.\nThe frame-by-frame phonetic context posterior probabilities are required as input to\nthe NOWAY decoder, i.e. all the outputs from the context modules on the right hand\nside of Figure 1. These posterior probabilities are calculated from the numerator\nof (7). The CI RNN stage operates in its normal fashion, generating frame-by-frame\nmonophone posterior probabilities. At the same time the CD modules take the state\nvector generated by the RNN as input, in order to classify into a context class. The\n\n\x0c754\n\nD. KERSHAW, T. ROBINSON, M. HOCHBERG\n\nRNN posterior probability outputs are multiplied by the module outputs to form\ncontext-dependent posterior probability estimates.\n\nRELATIONSHIP WITH MIXTURE OF EXPERTS\nThis architecture has similarities with mixture of experts (Jordan & Jacobs 1994).\nDuring training, rather than making a "soft" split of the data as in the mixture of\nexperts case, the Viterbi segmentation selects one expert at every exemplar. This\nmeans only one expert is responsible for each example in the data. This assumes that\nthe Viterbi segmentation is a good approximation to tjle segmentation/selection\nprocess. Hence, each expert is trained on a small subset of the training data,\navoiding the computationally expensive requirement for each expert to "see" all\nthe data. During decoding, the RNN is treated as a gating network, smoothing the\npredictions of the experts, in an analogous manner to a standard mixture of experts\ngating network . For further description of the system see Kershaw, Hochberg &\nRobinson (1995) .\n\nCLUSTERING CONTEXT CLASSES\nOne of the problems faced by having a context-dependent system is to decide which\ncontext classes are to be included in the CD system. A method for overcoming\nthis problem is a decision-tree based approach to cluster the context classes. This\nguarantees a full coverage of all phones in any context with the context classes\nbeing chosen using the acoustic evidence available. The tree clustering framework\nalso allows for the building of a small number of context-dependent phones, keeping\nthe new context-dependent connectionist system architecture compact. The tree\nbuilding algorithm was based on Young, Odell & Woodland (1994), and further\ndetails can be found in Kershaw et al. (1995). Once the trees were built, they were\nused to relabel the training data and the pronunciation lexicon.\n\nEVALUATION OF THE CONTEXT SYSTEM\nThe context-independent networks were trained on the ARPA Wall Street Journal S184 Corpus. The phonetic context-dependent classes were clustered on the\nacoustic data according to the decision tree algorithm. Running the data through a\nrecurrent network in a feed-forward fashion to obtain three million frames with 256\ndimensional state vectors took approximately 8 hours on an HP735 workstation.\nTraining all the context-dependent networks on all the training data takes between\n4- 6 hours (in total) on an HP735 workstation. The context-dependent modules\nwere cross-validated on a development set at the word level.\nResults for two context-dependent systems, compared with the context-independent\nbaseline are shown in Table 1, where the 1993 spoke 5 test is used for cross-validation\nand development purposes.\nThe context-dependent systems were also applied to larger tasks such as the recent\n1995 SQALE (a European multi-language speech recognition evaluation) 20,000\nword development and evaluation sets. The American English context-dependent\nsystem (CD527) was extended to include a set of modules trained backwards in\ntime (which were log-merged with the forward context), to augment a four way logmerged context-independent system (Hochberg, Cook, Renals & Robinson 1994).\n\n\x0c755\n\nContext-dependent Classes in a Speech Recognition System\n\nTable 1: Comparison Of The CI System With The CD205 And CD527 Systems,\nFor 5000 Word , Bigram Language Model Tasks.\n1993\nTest Sets\nSpoke 5\nSpoke 6\nEval.\n\nCI System\nWER\n16.0\n14.6\n15.7\n\nCD205 System\nWER I Red!!. WER\n14.0\n12.7\n12.2\n16.3\n14.3\n8.4\n\nCD527 System\nWER I Red!!. WER\n13.6\n14.9\n11.7\n19.8\n13.7\n12.6\n\nTable 2: Comparison Of The Merged CI Systems With The CD527US And\nCD465UK Systems, For 20 ,000 Word Tasks. All Tests Use A Trigram Language\nModel. The CD527US And CD465UK Evaluation Results Have Been Officially\nAdjudicated .\n1995 Test Sets\n\nUS English dev _test\nUS English evLtest\nUK English dev _test\nUK English evLtest\n\nCI System\nWER\n12.8\n14.5\n15.6\n16.4\n\nCD System\nWER\n11.3\n12.9 T\n12.7\n13.8 T\n\nRed!!.\nWER\n12.2\n9.8\n18.9\n15.7\n\nTable 3: Comparison Of Average Utterance Decode Speed Of The CI Systems With\nThe CD527US And CD465UK Systems On An HP735, For 20,000 Word Tasks. All\nTests Use A Trigram Language Model, And The Same Pruning Levels.\nTests\nAmerican English\nBritish English\n\nCI\nUtterance Av .\nDecode Speed (s)\n67\n131\n\nCD\nUtterance Av .\nDecode Speed (s)\n31\n48\n\nSpeedup\n2.16\n2.73\n\nTable 4: The Number Of Parameters Used For The CI Systems As Compared With\nThe CD527US And CD465UK Systems.\nSystem\nAmerican English\nBritish English\n\n# CI\nParameters\n341,000\n331 ,000\n\n#CD\nParameters\n612,000\n570,000\n\n\'fo Increase In\nParameters\n79.0\n72.2\n\nA similar system was built for British English (CD465). Table 2 shows the improvement gained by using context models. The daggers indicate the official entries for\nthe 1995 SQALE evaluation. These figures represent the lowest reported word error\nrate for both the US and UK English tasks.\nAs a result of improved phonetic modelling and class discrimination the search\nspace was reduced. This meant that decoding speed was over twice as fast as the\ncontext-dependent system, Table 3, even though there were roughly ten times as\nmany context-dependent phones compared to the monophones.\nThe increase in the number of parameters due to the introduction of the context\nmodels for the SQALE evaluation system are shown in Table 4. Although this\nseems a large increase in the number of system parameters, it is still an order of\nmagnitude less than any equivalent HMM system built for this task.\n\n\x0c756\n\nD. KERSHAW, T. ROBINSON, M. HOCHBERG\n\nCONCLUSIONS\nThis paper has discussed a successful way of integrating phonetic context-dependent\nclasses into the current ABBOT hybrid system. The architecture followed a modular\napproach which could be used to augment any current RNN-HMM hybrid system.\nFast training of the context-dependent modules was achieved. Training on all of the\nSI84 corpus took between 4 and 6 hours. Utterance decoding was performed using\nthe standard NOWAY decoder. The word error was significantly reduced, whilst the\ndecoding speed of the context system was over twice as fast as the baseline system\n(for 20,000 word tasks).\n\nReferences\nBourlard, H. & Morgan, N. (1993), \'Continuous Speech Recognition by Connectionist Statistical Methods\', IEEE Transactions on Neural Networks 4(6), 893- 909.\nBourlard, H. & Morgan, N. (1994), Connectionist Speech Recognition: A Hybrid\nApproach, Kluwer Acedemic Publishers.\nCohen, M., Franco, H., Morgan, N., Rumelhart, D. & Abrash, V. (1992), ContextDependent Multiple Distribution Phonetic Modeling with MLPs, in \'NIPS 5\'.\nHochberg, M., Cook, G., Renals, S. & Robinson, A. (1994), Connectionist Model\nCombination for Large Vocabulary Speech Recognition, in \'Neural Networks\nfor Signal Processing\', Vol. IV, pp. 269-278.\nHochberg, M., Cook, G., Renals, S., Robinson, A. & Schechtman, R. (1995), The\n1994 ABBOT Hybrid Connectionist-HMM Large-Vocabulary Recognition System, in \'Spoken Language Systems Technology Workshop\', ARPA, pp. 170-6.\nJordan, M. & Jacobs, R. (1994), \'Hierarchical Mixtures of Experts and the EM\nAlgorithm\', Neural Computation 6, 181-214.\nKershaw, D., Hochberg, M. & Robinson, A. (1995), Incorporating ContextDependent Classes in a Hybrid Recurrent Network-HMM Speech Recognition\nSystem, F-INFENG TR217, Cambridge University Engineering Department.\nLee, K.-F. (1989), Automatic Speech Recognition; The Development of the SPHINX\nSystem, Kluwer Acedemic Publishers.\nRenals, S. & Hochberg, M. (1995), Efficient Search Using Posterior Phone Probability Estimates, in \'ICASSP\', Vol. 1, pp. 596-9.\nRobinson, A. (1994), \'An Application of Recurrent Nets to Phone Probability Estimation.\', IEEE Transactions on Neural Networks 5(2),298-305.\nYoung, S., Odell, J. & Woodland, P. (1994), \'Tree-Based State Tying for High Accuracy Acoustic Modelling\', Spoken Language Systems Technology Workshop.\nZhoa, Y., Schwartz, R., Sroka, J . & Makhoul, J. (1995), Hierarchical Mixtures of\nExperts Methodology Applied to Continuous Speech Recognition, in \'NIPS 7\'.\n\n\x0c'
p83185
sg245
S'VLSI Model of Primate Visual Smooth Pursuit\n\nRalph Etienne-Cummings\n\nJan Van der Spiegel\n\nDepartment of Electrical Engineering,\nSouthern Illinois University, Carbondale,\nIL 62901\n\nMoore School of Electrical Engineering,\nUniversity of Pennsylvania, Philadelphia,\nPA 19104\n\nPaul Mueller\nCorticon, Incorporated,\n3624 Market Str, Philadelphia,\nPA 19104\n\nAbstract\nA one dimensional model of primate smooth pursuit mechanism has\nbeen implemented in 2 11m CMOS VLSI. The model consolidates\nRobinson\'s negative feedback model with Wyatt and Pola\'s positive\nfeedback scheme, to produce a smooth pursuit system which zero\'s the\nvelocity of a target on the retina. Furthermore, the system uses the\ncurrent eye motion as a predictor for future target motion. Analysis,\nstability and biological correspondence of the system are discussed. For\nimplementation at the focal plane, a local correlation based visual\nmotion detection technique is used. Velocity measurements, ranging\nover 4 orders of magnitude with < 15% variation, provides the input to\nthe smooth pursuit system. The system performed successful velocity\ntracking for high contrast scenes. Circuit design and performance of the\ncomplete smooth pursuit system is presented.\n\n1 INTRODUCTION\nThe smooth pursuit mechanism of primate visual systems is vital for stabilizing a region\nof the visual field on the retina. The ability to stabilize the image of the world on the\nretina has profound architectural and computational consequences on the retina and visual\ncortex, such as reducing the required size, computational speed and communication\nhardware and bandwidth of the visual system (Bandera, 1990; Eckert and Buchsbaum,\n1993). To obtain similar benefits in active machine vision, primate smooth pursuit can\nbe a powerful model for gaze control. The mechanism for smooth pursuit in primates\nwas initially believed to be composed of a simple negative feedback system which\nattempts to zero the motion of targets on the fovea, figure I (a) (Robinson, 1965).\nHowever, this scheme does not account for many psychophysical properties of smooth\n\n\x0c707\n\nVLSI Model of Primate Visual Smooth Pursuit\n\npursuit, which led Wyatt and Pola (1979) to proposed figure l(b), where the eye\nmovement signal is added to the target motion in a positive feed back loop. This\nmechanism results from their observation that eye motion or apparent target motion\nincreases the magnitude of pursuit motion even when retinal motion is zero or constant.\nTheir scheme also exhibited predictive qualities, as reported by Steinbach (1976). The\nsmooth pursuit model presented in this paper attempts the consolidate the two models\ninto a single system which explains the findings of both approaches.\nTarget\nMoticn\n\nEye\nMotion\n\nRetinal\nMotion\n\ne~\n\nlee\n\nG\n\nee = e t G+l\n~;\n\n>\n\nI\nG ~ co G r\n\nTarget\nMotion\n\nEye\nMotion\n\ne~~\n\n>\n\n=0\n(b)\n\n(a)\n\nFigure I: System Diagrams of Primate Smooth Pursuit Mechanism.\n(a) Negative feedback model by Robinson (1965). (b) Positive\nfeedback model by Wyatt and Pola (1979).\nThe velocity based smooth pursuit implemented here attempts to zero the relative velocity\nof the retina and target. The measured retinal velocity, is zeroed by using positive\nfeedback to accumulate relative velocity error between the target and the retina, where the\naccumulated value is the current eye velocity. Hence, this model uses the Robinson\napproach to match target motion, and the Wyatt and Pola positive feed back loop to\nachieve matching and to predict the future velocity of the target. Figure 2 shows the\nsystem diagram of the velocity based smooth pursuit system. This system is analyzed\nand the stability criterion is derived. Possible computational blocks for the elements in\nfigure I (b) are also discussed. Furthermore, since this entire scheme is implemented on a\nsingle 2 /lm CMOS chip, the method for motion detection, the complete tracking circuits\nand the measured results are presented.\nRetinal\nMotion\n\nEye\nMotion\n\ner\n\nFigure 2: System Diagram of VLSI Smooth Pursuit Mechanism.\nis target velocity in space, Bt is projected target velocity, Be is the eye\nvelocity and Br is the measured retinal velocity.\n\n2 VELOCITY BASED SMOOTH PURSUIT\nAlthough figure I (b) does not indicate how retinal motion is used in smooth pursuit, it\nprovides the only measurement of the projected target motion. The very process of\ncalculating retinal motion realizes negative feed back between the eye movement and the\ntarget motion, since retinal motion is the difference between project target and eye\nmotion. If Robinson\'s model is followed, then the eye movement is simply the\namplified version of the retinal motion. If the target disappears from the retina, the eye\nmotion would be zero. However, Steinbach showed that eye movement does not cea~\nwhen the target fades off and on, indicating that memory is used to predict target motion.\nWyatt and Palo showed a direct additive influence of eye movement on pursuit. However,\nthe computational blocks G\' and a of their model are left unfilled.\n\n\x0cR. ETIENNE-CUMMINGS, J. VAN DER SPIEGEL, P. MUELLER\n\n708\n\nIn figure 2, the gain G models the internal gain of the motion detection system , and the\ninternal representation of retinal velocity is then Vr. Under zero-slip tracking, the retinal\nvelocity is zero. This is obtained by using positive feed back to correct the velocity error\nand eye,\nThe delay element represents a memory of the last eye\nbetween target,\nvelocity while the current retinal motion is measured. If the target disappears, the eye\nmotion continues with the last value, as recorded by Steinbach, thus anticipating the\nposition of the target in space. The memory also stores the current eye velocity during\nperfect pursuit. The internal representation of eye velocity, Ve , is subsequently amplified\nby H and used to drive the eye muscles. The impulse response of the system is given in\nequations (I). Hence, the relationship between eye velocity and target velocity is recursive\nand given by equations (2). To prove the stability of this system, the retinal velocity can\nbe expressed in terms of the target motion as given in equations (3a). The ideal condition\nfor accurate performance is for GH = 1. However, in practice, gains of different amplifiers\n\ner,\n\n()\n\nz-)\n\n=GH--_-)\n\n-.f..(z)\n\n1- Z\n\n(}r\n\nee.\n\n()\n\n(a); ~(I1)\n(}r\n\n=GH[-8(11) + u(n)]\n\n(I)\n\n(b)\nn-)\n\n(}e(n)\n\n= (},(n) -\n\n(}r(n)\n\n=GH[-8(n) + u(n)] * (}r(n) = GHL(},.(k)\n\n(2)\n\nk=O\n() r ( 11)\n\n() r (n)\n\n= (),( n ) (1\n11\n\n~\n\n00\n\n)\n\n11\n\n- GH)\n0\n\nif 11 -\n\n=> () r( 1l )\n\nI\n\nGH < 1\n\n= 0 if\n\nGH\n\n= 1 =>\n\n() in)\n\n= (),( 11 )\n\n=> 0 < GH < 2 for stability\n\n(\n\na)\n\n(3)\n\n( b)\n\nare rarely perfectly matched. Equations (3b) shows that stability is assured for O<GH< 2.\nFigure 3 shows a plot of eye motion versus updates for various choices of GH. At each\nupdate, the retinal motion is computed. Figure 3(a) shows the eye\'s motion at the on-set\nof smooth pursuit. For GH = 1, the eye movement tracks the target\'s motion exactly,\nand lags slightly only when the target accelerates. On the other hand, if GH? I, the\neye\'s motion always lags the target\'s. If GH -> 2, the system becomes increasing\nunstable, but converges for GH < 2. The three cases presented correspond to the smooth\npursuit system being critically, over and under damped, respectively.\n\n3 HARDWARE IMPLEMENTATION\nUsing the smooth pursuit mechanism described, a single chip one dimensional tracking\nsystem has been implemented. The chip has a multi-layered computational architecture,\nsimilar to the primate\'s visual system. Phototransduction, logarithmic compression,\nedge detection, motion detection and smooth pursuit control has been integrated at the\nfocal-plane. The computational layers can be partitioned into three blocks, where each\nblock is based on a segment of biological oculomotor systems.\n\n3.1\n\nIMAGING AND PREPROCESSING\n\nThe first three layers of the system mimics the photoreceptors, horizontal cells arx:l\nbipolar cells of biological retinas. Similar to previous implementations of silicon\nretinas, the chip uses parasitic bipolar transistors as the photoreceptors. The dynamic\nrange of photoreceptor current is compressed with a logarithmic response in low light arx:l\nsquare root response in bright light. The range compress circuit represents 5-6 orders of\nmagnitude of light intensity with 3 orders of magnitude of output current dynamic range.\nSubsequently, a passive resistive network is used to realize a discrete implementation of a\nLaplacian edge detector. Similar to the rods and cones system in primate retinas, the\nresponse time, hence the maximum detectable target speed, is ambient intensity dependent\n(160 (12.5) Ils in 2.5 (250) IlW/cm2). However, this does prevent the system from\nhandling fast targets even in dim ambient lighting.\n\n\x0cVLSI Model of Primate Visual Smooth Pursuit\n\n~\n\ng\n\nu\n>\n\n709\n\n20\n\n20\n\n15\n\n15\n\n10\n\n10\n\n5\n\n~\n\n5\n\n0\n\n]\n\n-5\n\n>" -5\n\n- 10\n\n?\n\n? 10\n\nTarget\n\n- -Eye: GH=I 99\n- E ye GH=IOO\n__ . Eye: GH=O_IO\n\n-15\n\n0\n\n? 15\n-20\n\n-20\n100\n\n50\n\n0\n\n150\n\n500\n\n600\n\nUpdates\n\n(a)\n\n700\n800\nUpdates\n\n900\n\n1000\n\n(b)\n\nFigure 3: (a) The On-Set of Smooth Pursuit for Various GH Values.\n(b) Steady-State Smooth Pursuit.\n\n3.2\n\nMOTION MEASUREMENT\n\nThis computational layer measures retinal motion. The motion detection technique\nimplemented here differs from those believed to exist in areas V 1 and MT of the primate\nvisual cortex. Alternatively, it resembles the fly\'s and rabbit\'s retinal motion detection\nsystem (Reichardt, 1961; Barlow and Levick, 1965; Delbruck, 1993). This is not\ncoincidental, since efficient motion detection at the focal plane must be performed in a\nsmall areas and using simple computational elements in both systems.\nThe motion detection scheme is a combination of local correlation for direction\ndetermination, and pixel transfer time measurement for speed. In this framework, motion\nis defined as the disappearance of an object, represented as the zero-crossings of its edges,\nat a pixel , followed by its re-appearance at a neighboring pixel. The (dis)appearance of\nthe zero-crossing is determined using the (negative) positive temporal derivative at the\npixel. Hence, motion is detected by AND gating the positive derivative of the zerocrossing of the edge at one pixel with the negative derivative at a neighboring pixel. The\ndirection of motion is given by the neighboring pixel from which the edge disappeared.\nProvided that motion has been detected at a pixel, the transfer time of the edge over the\npixel\'s finite geometry is inversely proportional to its speed.\nEquation (4) gives the mathematical representation of the motion detection process for an\nobject moving in +x direction. In the equation. f,(l.\'k ,y.t) is the temporal response of\npixel k as the zero crossing of an edge of an object passes over its 2a aperture. Equation\n(4) gives the direction of motion, while equation (5) gives the speed. The schematic of\n\nmotion _ x = [\n\nf f,( l: k, y, t) > 0] [ f f t(l.\' k + J, y, t) < 0] =0\n\nmotion+x=[~f,(l.\'k-J,y,t)<O][~f/l.\'k , y,t?O]\n\n= 8[t\nMotion.\' t m =\n\nSpeed + x\n\n=\n\nt\n\n-\n\n(b)\n\n(4)\n\n2a(k-n)-a\nv\n]8[x - 2ak]\nx\n\n2a(k -n) -a\nvx\n\nJ\n- t\n\n( a)\n\nvx\n\n2a\n\nDisappear .\' t d\n\n2a(k -n) +a\n\n= --~--?\nvx\n\n(5)\n\nd\nm\nthe VLSI circuit of the motion detection model is shown in figure 4(a). Figure 4(b)\nshows reciprocal of the measured motion pulse-width for 1 D motion. The on-chip speed,\net, is the projected target speed. The measured pulse-widths span 3-4 orders magnitude,\n\n\x0c710\n\nR. ETIENNE-CUMMINGS, J. VAN DER SPIEGEL, P. MUELLER\n\nOne-Over Pulse-Width vs On-Chip Speed\n\n?\n\nO.R\n\n~\n\n0.4\n\n"\n\n~ -0.0 +--------::II~-----__+\nM\n~ -0.4\n\n---e-- \\IPW_Lefi\n\n-0 .8\n\n- - . - - IIPW_ Rlght\n\n- 1.2 +-\'----\'--\'\'-+--\'--\'--\'--t---\'--\'\'--\'-+-\'--\'--\'-t-\'--\'--\'-t---\'--\'--\'-+\n-40\n00\n4.0\n8.0\n12.0\n-12.0\n-R.O\nOn-Chip Speed rcml~J\n\nRight\n\nLeft\n\n(b)\n\n(a)\n\nFigure 4: (a) Schematic of the Motion Detection Circuit.\nMeasured Output of the Motion Detection Circuit.\n\n(b)\n\ndepending on the ambient lighting, and show less than 15% variation between chips,\npixels, and directions (Etienne-Cummings, 1993).\n\n3.3\n\nTHE SMOOTH PURSUIT CONTROL SYSTEM\n\nThe one dimensional smooth pursuit system is implemented using a 9 x I array of\nmotion detectors. Figure 5 shows the organization of the smooth pursuit chip. In this\nsystem, only diverging motion is computed to reduce the size of each pixel. The outputs\nof the motion detectors are grouped into one global motion signal per direction. This\ngrouping is performed with a simple, but delayed, OR, which prevents pulses from\nneighboring motion cells from overlapping. The motion pulse trains for each direction\nare XOR gated, which allows a single integrator to be used for both directions, thus\nlimiting mis-match_ The final value of the integrator is inversely proportional to the\ntarget\'s speed. The OR gates conserve the direction of motion. The reciprocal of the\nintegrator voltage is next computed using the linear mode operation of a MOS transistor\n(Etienne-Cummings, 1993). The unipolar integrated pulse allows a single inversion\ncircuit to be used for both directions of motion, again limiting mis-match. The output of\nthe "one-over" circuit is amplified, and the polarity of the measured speed is restored.\nThis analog voltage is proportional to retinal speed.\nThe measured retinal speed is subsequently ailed to the stored velocity. Figure 6 shows\nthe schematic for the retinal velocity accumulation (positive feedback) and storage (analog\nWave Forms\n\nMotion Pulse Integration\nand "One-Over"\nV = GIRetinal Velocityl\n\nPolarity\nRestoration\n\nRetinal Velocity\nAccumulation\nand Sample/Hold\n\nFigure 5: Architecture of the VLSI Smooth Pursuit System. Sketches\nof the wave forms for a fast leftward followed by a slow rightward\nretinal motion are shown.\n\n\x0c711\n\nVLSI Model of Primate Visual Smooth Pursuit\n\nmemory). The output of the XOR gate in figure 5 is used by the sample-and-hold circuit\nto control sampling switches S I and S2. During accumulation, the old stored velocity\nvalue, which is the current eye velocity, is isolated from the summed value. At the\nfalling edge of the XOR output, the stored value on C2 is replaced by the new value on\nCl. This stored value is amplified using an off chip motor driver circuit, and used to\nmove the chip. The gain of the motor driver can be finely controlled for optimal\noperation.\n\nMotor\n\nRetinal\nVelocity\n\nSystem\n\nAccumulatiun\n\nTarget\nVelocity\n\nTwo Phase Sample/Hold\n\nFigure 6: Schematic Retinal Velocity Error Accumulation, Storage and\nMotor Driver Systems.\nFigure 7(a) shows a plot of one-over the measured integrated voltage as a function of on\nchip target speed. Due to noise in the integrator circuit, the dynamic range of the motion\ndetection system is reduced to 2 orders of magnitude. However, the matching between left\nand right motion is unaffected by the integrator. The MaS "one-over" circuit, used to\ncompute the analog reciprocal of the integrated voltage, exhibits only 0.06% deviation\nfrom a fitted line (Etienne-Cummings, 1993b). Figure 7(b) shows the measured\nincrements in stored target velocity as a function of retinal (on-chip) speed. This is a test\nof all the circuit components of the tracking system. Linearity between retinal velocity\nincrements and target velocity is observed, however matching between opposite motion\nhas degraded. This is caused by the polarity restoration circuit since it is the only\nlocation where different circuits are used for opposite motion. On average, positive\nincrements are a factor of 1.2 times larger than negative increments. The error bars shows\nthe variation in velocity increments for different motion cells and different Chips. The\ndeviation is less than 15 %. The analog memory has a leakage of 10 mV/min and an\nasymmetric swing of 2 to -1 V, caused by the buffers. The dynamic range of the\ncomplete smooth pursuit system is measured to be 1.5 orders magnitude. The maximum\nspeed of the system is adjustable by varying the integrator charging time. The maximum\nspeed is ambient intensity dependent and ranges from 93 cmls to 7 cm/s on-chip speed in\nVelocity Error Increment vs On-Chip Speed\n\nIntegrated Pulse vs On-Chip Speed\n1.4\n24\n\n~\n\n16\n\n~\n\n8\n\n~\n\n0\n\nil\n?\noS\n\n.\'\n._\n\n1.2\n\n~\n\nl\'! 1.0\n\n"e~\nu\n\n-t--------",/II!...------+\n\n-8\n\n.s\n\nO.R\n\ng 0 .6\n\nLLl\n\n.::;.\n\ng 04\n\n:: -16\n-e--lnlPuI~_l..xft\n\n-24\n\n_ _? _\n\nJntPlllo;e_Rl~hl\n\n-32 -t-\'---\'---\'-\'--+-\'--~~-t--\'"-\'-~_t_--"--\'\'---\'---"-t\n10.0\n-100\n-5.0\n0.0\n5.0\nOn-Chip Speed lemlsl\n\n(a)\n\nOJ\n\n>\n\n- - - - . Nc~_ Jn c rt~nl\n\n02\n\n__ ? _ _Po,,_Incremclll\n\n0.0\n0\n\n4\n6\nOn-Chip Speed lem/s)\n\n(b)\n\nFigure 7. (a) Measured integrated motion pulse voltage. (b) Measured\noutput for the complete smooth pursuit system.\n\n10\n\n\x0cR. ETIENNE-CUMMINGS, J. VAN DER SPIEGEL, P. MUELLER\n\n712\n\nbright (250 JlW/cm 2) and dim (2.5 JlW/cm 2) lighting, respectively. However, for any\nmaximum speed chosen, the minimum speed is a factor of 0.03 slower. The minimum\nspeed is limited by the discharge time of the temporal differentiators in the motion\ndetection circuit to 0.004 cmls on chip. The contrast sensitivity of this system proved to\nbe the stumbling block, and it can not track objects in normal indoor lighting. However,\nall circuits components tested successfully when a light source is used as the target.\nAdditional measured data can be found in (Etienne-Cummings, 1995). Further work will\nimprove the contrast sensitivity, combat noise and also consider two dimensional\nimplementations with target acquisition (saccades) capabilities.\n\n4\n\nCONCLUSION\n\nA model for biological and silicon smooth pursuit has been presented. It combines the\nnegative feed back and positive feedback models of Robinson and Wyatt and Pola. The\nsmooth pursuit system is stable if the gain product of the retinal velocity detection\nsystem and the eye movement system is less than 2. VLSI implementation of this\nsystem has been performed and tested. The performance of the system suggests that wide\nrange (92.9 - 0.004 cmls retinal speed) target tracking is possible with a single chip focal\nplane system. To improve this chip\'s performance, care must be taken to limit noise,\nimprove matching and increase contrast sensitivity. Future design should also include a\nsaccadic component to re-capture escaped targets, similar to biological systems.\n\nReferences\nC. Bandera, "Foveal Machine Vision Systems", Ph.D. Thesis, SUNY Buffalo, New\nYork, ]990\nH. Barlow and W. Levick, \'The Mechanism for Directional Selective Units in Rabbit\' s\nRetina", Journal of Physiology, Vol. 178, pp. 477-504, ]965\nT. Delbruck, "Silicon Retina with Correlation-Based, Velocity-Tuned Pixels ", IEEE\nTransactions on Neural Networks, Vol. 4:3, pp. 529-41, 1993\n\nM. Eckert and G. Buchsbaum, "Effect of Tracking Strategies on the Velocity Structure of\nTwo-Dimensional Image Sequences", J. Opt. Soc. Am., Vol. AIO:7, pp. 1582-85, 1993\nR. Etienne-Cummings et at., "A New Temporal Domain Optical Flow Measurement\nTechnique for Focal Plane VLSI Implementation", Proceedings of CAMP 93, M.\nBayoumi, L. Davis and K. Valavanis (Eds.), pp. 24]-25] , 1993\nR. Etienne-Cummings, R. Hathaway and J. Van der Spiegel, "An Accurate and Simple\nCMOS \'One-Over\' Circuit", Electronic Letters, Vol. 29-18, pp. ]618-]620, 1993b\nR. Etienne-Cummings et aI., "Real-Time Visual Target Tracking: Two Implementations\nof Velocity Based Smooth Pursuit", Visual Information Processing IV, SPIE Vol. 2488,\nOrlando, 17-18 April 1995\n\nW. Reichardt, "Autocorrelation, A Principle for the Evaluation of Sensory Information by\nthe Central Nervous System", Sensory Communication, Wiley, New York, 1961\nD. Robinson, "The Mechanism of Human Smooth Pursuit Eye Movement", Journal of\nPhysiology ( London) Vol. 180, pp. 569-591 , 1965\nM. Steinbach, "Pursuing the Perceptual Rather than the Retinal Stimuli", Vision\nResearch, Vol. 16, pp. 1371-1376,1976\nH. Wyatt and J. Pola, "The Role of Perceived Motion in Smooth Pursuit Eye\nMovements", Vision Research, Vol. 19, pp. 613-618, 1979\n\n\x0c'
p83186
sg46
S'Gradient and Hamiltonian Dynamics\nApplied to Learning in Neural Networks\nJames W. Howse\n\nChaouki T. Abdallah\n\nGregory L. Heileman\n\nDepartment of Electrical and Computer Engineering\nUniversity of New Mexico\nAlbuquerque, NM 87131\n\nAbstract\nThe process of machine learning can be considered in two stages: model\nselection and parameter estimation. In this paper a technique is presented\nfor constructing dynamical systems with desired qualitative properties. The\napproach is based on the fact that an n-dimensional nonlinear dynamical\nsystem can be decomposed into one gradient and (n - 1) Hamiltonian systems. Thus, the model selection stage consists of choosing the gradient and\nHamiltonian portions appropriately so that a certain behavior is obtainable.\nTo estimate the parameters, a stably convergent learning rule is presented.\nThis algorithm has been proven to converge to the desired system trajectory\nfor all initial conditions and system inputs. This technique can be used to\ndesign neural network models which are guaranteed to solve the trajectory\nlearning problem.\n\n1\n\nIntroduction\n\nA fundamental problem in mathematical systems theory is the identification of dynamical systems. System identification is a dynamic analogue of the functional approximation problem. A set of input-output pairs {u(t), y(t)} is given over some time\ninterval t E [7i, 1j]. The problem is to find a model which for the given input sequence\nreturns an approximation of the given output sequence. Broadly speaking, solving an\nidentification problem involves two steps. The first is choosing a class of identification models which are capable of emulating the behavior of the actual system. The\nsecond is selecting a method to determine which member of this class of models best\nemulates the actual system. In this paper we present a class of nonlinear models and\na learning algorithm for these models which are guaranteed to learn the trajectories\nof an example system. Algorithms to learn given trajectories of a continuous time\nsystem have been proposed in [6], [8], and [7] to name only a few. To our knowledge,\nno one has ever proven that the error between the learned and desired trajectories\nvanishes for any of these algorithms. In our trajectory learning system this error is\nguaranteed to vanish. Our models extend the work in [1] by showing that Cohen\'s\nsystems are one instance of the class of models generated by decomposing the dynamics into a component normal to some surface and a set of components tangent to the\nsame surface. Conceptually this formalism can be used to design dynamical systems\nwith a variety of desired qualitative properties. Furthermore, we propose a provably\nconvergent learning algorithm which allows the parameters of Cohen\'s models to be\nlearned from examples rather than being programmed in advance. The algorithm is\n\n\x0c275\n\nGradient and Hamiltonian Dynamics Applied to Learning in Neural Networks\n\nconvergent in the sense that the error between the model trajectories and the desired trajectories is guaranteed to vanish. This learning procedure is related to one\ndiscussed in [5] for use in linear system identification.\n\n2\n\nConstructing the Model\n\nFirst some terminology will be defined. For a system of n first order ordinary differential equations, the phase space of the system is the n-dimensional space of all state\ncomponents. A solution trajectory is a curve in phase space described by the differential equations for one specific starting point. At every point on a trajectory there\nexists a tangent vector. The space of all such tangent vectors for all possible solution\ntrajectories constitutes the vector field for this system of differential equations.\nThe trajectory learning models in this paper are systems of first order ordinary differential equations. The form of these equations will be obtained by considering the\nsystem dynamics as motion relative to some surface. At each point in the state space\nan arbitrary system trajectory will be decomposed into a component normal to this\nsurface and a set of components tangent to this surface. This approach was suggested\nto us by the results in [4], where it is shown that an arbitrary n-dimensional vector\nfield can be decomposed locally into the sum of one gradient vector field and (n - 1)\nHamiltonian vector fields. The concept of a potential function will be used to define these surfaces. A potential function V(:z:) is any scalar valued function of the\nsystem states :z: = [Xl, X2, ??? , Xn.] t which is at least twice continuously differentiable\n(Le. V(:z:) E or : r ~ 2). The operation [.]t denotes the transpose of the vector. If\nthere are n components in the system state, the function V{:z:), when plotted with\nrespect all of the state components, defines a surface in an (n + 1)-dimensional space.\nThere are two curves passing through every point on this potential surface which are\nof interest in this discussion, they are illustrated in Figure 1(a). The dashed curve is\n(z - zo)t \\7 ... v (z)l ...o = 0\n\n(a)\n\n(b)\n\nV(z) = K-\n\nFigure 1: (a) The potential function V(z) = X~ (Xl _1)2 +x~ plotted versus its two dependent variables Xl and X2. The dashed curve is called a level surface and is given\nby V(z) = 0.5. The solid curve follows the path of steepest descent through Zo.\n(b) The partitioning of a 3-dimensional vector field at the point Zo into a 1dimensional portion which is normal to the surface V(z) = K- and a 2-dimensional\nportion which is tangent to V(z) = K-. The vector -\\7 ... V(z) 1"\'0 is the normal vector to the surface V(z) = K- at the point Zo. The plane (z - zo)t \\7 ... V (z) 1"\'0 = 0\ncontains all of the vectors which are tangent to V(z) = K- at Zo. Two linearly\nindependent vectors are needed to form a basis for this tangent space, the pair\nQ2(z) \\7 ... V (z)l ... o and Q3(Z) \\7 ... V (z)l ... o that are shown are just one possibility.\nreferred to as a level surface, it is a surface along which V(:z:) = K for some constant\nK. Note that in general this level surface is an n-dimensional object. The solid curve\n\n\x0c276\n\nJ. W. HOWSE, C. T. ABDALLAH, G. L. HEILEMAN\n\nmoves downhill along V (X) following the path of steepest descent through the point\nXo. The vector which is tangent to this curve at Xo is normal to the level surface\nat Xo. The system dynamics will be designed as motion relative to the level surfaces\nof V(x). The results in [4] require n different local potential functions to achieve\narbitrary dynamics. However, the results in [1] suggest that a considerable number\nof dynamical systems can be achieved using only a single global potential function.\nA system which is capable of traversing any downhill path along a given potential\nsurface V(x), can be constructed by decomposing each element of the vector field\ninto a vector normal to the level surface of V(x) which passes through each point\nand a set of vectors tangent to the level surface of V(x) which passes through the\nsame point. So the potential function V(x) is used to partition the n-dimensional\nphase space into two subspaces. The first contains a vector field normal to some\nlevel surface V(x) = }( for }( E IR, while the second subspace holds a vector field\ntangent to V(x) = IC. The subspace containing all possible normal vectors to the\nn-dimensional level surface at a given point, has dimension one. This is equivalent\nto the statement that every point on a smooth surface has a unique normal vector.\nSimilarly, the subspace containing all possible tangent vectors to the level surface at\na given point has dimension (n - 1). An example of this partition in the case of a\n3-dimensional system is shown in Figure 1(b). Since the space of all tangent vectors\nat each point on a level surface is (n - I)-dimensional, (n - 1) linearly independent\nvectors are required to form a basis for this space.\nMathematically, there is a straightforward way to construct dynamical systems which\neither move downhill along V(x) or remain at a constant height on V(x). In this\npaper, dynamical systems which always move downhill along some potential surface\nare called gradient-like systems. These systems are defined by differential equations\nof the form\nx = -P(x) VII:V(x),\n(1)\nwhere P(x) is a matrix function which is symmetric (Le. pt = P) and positive\n:z~]f. These systems\ndefinite at every point x, and where VIII V(x) =\nare similar to the gradient flows discussed in [2]. The trajectories of the system\nformed by Equation (1) always move downhill along the potential surface defined by\nV(x). This can be shown by taking the time derivative of V(x) which is V(x) =\n-[VII: V (x)]t P(x) [VII: V(x)] :5 O. Because P(x) is positive definite, V(x) can only be\nzero where V II: V (x) = 0, elsewhere V(x) is negative. This means that the trajectories\nof Equation (1) always move toward a level surface of V(x) formed by "slicing" V(x)\nat a lower height, as pointed out in [2]. It is also easy to design systems which remain\nat a constant height on V(x). Such systems will be denoted Hamiltonian-like systems.\nThey are specified by the equation\nx = Q(x) VII: V(x),\n(2)\nwhere Q(x) is a matrix function which is skew-symmetric (Le. Qt = -Q) at every\npoint x. These systems are similar to the Hamiltonian systems defined in [2]. The\nelements of the vector field defined by Equation (2) are always tangent to some level\nsurface of V (x). Hence the trajectories ofthis system remain at a constant height on\nthe potential surface given by V(x). Again this is indicated by the time derivative\nof V(x), which in this case is V(x) = [VII: V(x)]f Q(x)[VII: V(x)] = o. This indicates\nthat the trajectories of Equation (2) always remain on the level surface on which the\nsystem starts. So a model which can follow an arbitrary downhill path along the\npotential surface V(x) can be designed by combining the dynamics of Equations (1)\nand (2) . The dynamics in the subspace normal to the level surfaces of V(x) can be\n\n[g;: , g;: ,... ,\n\n\x0cGradient and Hamiltonian Dynamics Applied to Learning in Neural Networks\n\n277\n\ndefined using one equation of the form in Equation (1). Similarly the dynamics in the\nsubspace tangent to the level surfaces of Vex) can be defined using (n - 1) equations\nof the form in Equation (2). Hence the total dynamics for the model are\nn\n\nz= -P(x)VIDV(x) + LQi(X)VIDV(x).\n\n(3)\n\ni=2\n\nFor this model the number and location of equilibria is determined by the function\nVex), while the manner in which the equilibria are approached is determined by the\nmatrices P(x) and Qi(x).\nIf the potential function Vex) is bounded below (i.e. Vex) > Bl V x E IRn , where\nBl is a constant), eventually increasing (i.e. limlllDlI-+oo Vex) ~ 00) , and has only\na finite number of isolated local maxima and minima (i.e. in some neighborhood\nof every point where V III V (x) = 0 there are no other points where the gradient\nvanishes), then the system in Equation (3) satisfies the conditions of Theorem 10\nin [1]. Therefore the system will converge to one of the points where V ID Vex) = 0,\ncalled the critical points of Vex), for all initial conditions. Note that this system\nis capable of all downhill trajectories along the potential surface only if the (n - 1)\nvectors Qi(X) V ID Vex) V i = 2, ... , n are linearly independent at every point x. It\nis shown in [1] that the potential function\n\nV(z) = C (\n\n1:., (-y) d-y +\n\nt, [~\n\n(XI - I:.,(xd)\'\n\n+~\n\nJ:\'\n\n1:., h )II:.: (-y)]\' d-y\n\n1\n\n(4)\n\nsatisfies these three criteria. In this equation ?.i(Xt} Vi = 1, ... , n are interpolation\npolynomials, C is a real positive constant, Xi Vi = 1, ... , n are real constants chosen\nso that the integrals are positive valued, and ?.Hxt} ==\n\nf:-.\n\n3\n\nThe Learning Rule\n\nIn Equation (3) the number and location of equilibria can be controlled using the\npotential function Vex), while the manner in which the equilibria are approached can\nbe controlled with the matrices P(x) and Qi(X). If it is assumed that the locations\nof the equilibria are known, then a potential function which has local minima and\nmaxima at these points can be constructed using Equation (4). The problem of\ntrajectory learning is thereby reduced to the problem of parameterizing the matrices\nP(x) and Qi(x) and finding the parameter values which cause this model to best\nemulate the actual system. If the elements P(x) and Qi(x) are correctly chosen,\nthen a learning rule can be designed which makes the model dynamics converge to\nthat of the actual system. Assume that the dynamics given by Equation (3) are a\nparameterized model of the actual dynamics. Using this model and samples of the\nactual system states, an estimator for states of the actual system can be designed. The\nbehavior of the model is altered by changing its parameters, so a parameter estimator\nmust also be constructed. The following theorem provides a form for both the state\nand parameter estimators which guarantees convergence to a set of parameters for\nwhich the error between the estimated and target trajectories vanishes.\nTheorem 3.1. Given the model system\nk\n\nZ = LAili(x) +Bg(u)\n\n(5)\n\ni=l\n\nwhere Ai E IRnxn and BE IRnxm are unknown, and li(\') and g(.) are known smooth\nfunctions such that the system has bounded solutions for bounded inputs u(t). Choose\n\n\x0cJ. W. HOWSE, C. T. ABDALLAH, G. L. HEILEMAN\n\n278\n\na state estimator of the form\nk\n\n~ = \'R. B (x - x) +\n\nL Ai fi(x) + iJ g(u)\n\n(6)\n\ni=1\n\nwhere\'R. B is an (n x n) matrix of real constants whose eigenvalues must all be in the\nleft half plane, and Ai and iJ are the estimates of the actual parameters. Choose\nparameter estimators of the form\n~\nt\nAi = -\'R.p (x - x) [fi(x)] V i = 1, ... , k\n(7)\n= -\'R.p (x - x) [g(u)]t\n\nB\n\nwhere \'R. p is an (n x n) matrix of real constants which is symmetric and positive\ndefinite, and (x - x) [.]t denotes an outer product. For these choices of state and\nparameter estimators limt~oo(x(t) -x(t? = 0 for all initial conditions. Furthermore,\nthis remains true if any of the elements of Ai or iJ are set to 0, or if any of these\nmatrices are restricted to being symmetric or skew-symmetric.\nThe proof of this theorem appears in [3]. Note that convergence of the parameter\nestimates to the actual parameter values is not guaranteed by this theorem. The\nmodel dynamics in Equation (3) can be cast in the form of Equation (5) by choosing\neach element of P(x) and Qi(X) to have the form\nI-I\n\nn\n\nn\n\nI-I\n\n= LL~rBjkt?k(Xj)\n\nand\nQrB = LLArBjk ek(Xj),\n(8)\nj=1 k=O\nj=1 k=O\nwhere {t?o(Xj), t?1 (Xj), ... ,t?I-1 (Xj)} and {eo(Xj), el (Xj), ... ,el-l (Xj)} are a set of 1\northogonal polynomials which depend on the state Xj\' There is a set of such polynomials for every state Xj, j = 1,2, ... , n. The constants ~rBjk and ArBjk determine\nthe contribution of the kth polynomial which depends on the jth state to the value\nof Prs and Qrs respectively. In this case the dynamics in Equation (3) become\nPrB\n\n:i:\n\n=\n\nt. ~ {\n\nS;. [11.(x;) V. V (z)j\n\n+\n\nt,\n\nA;;. [e;.(x;)\n\nv. V(z)j } + T g(u(t))\n\n(9)\n\nwhere 8 jk is the (n x n) matrix of all values ~rsjk which have the same value of j and\nk. Likewise A ijk is the (n x n) matrix of all values Arsjk, having the same value of\nj and k, which are associated with the ith matrix Qi(X). This system has m inputs,\nwhich may explicitly depend on time, that are represented by the m-element vector\nfunction u(t). The m-element vector function g(.) is a smooth, possibly nonlinear,\ntransformation of the input function. The matrix Y is an (n x m) parameter matrix\nwhich determines how much of input S E {I, ... , m} effects state r E {I, ... , n}.\nAppropriate state and parameter estimators can be designed based on Equations (6)\nand (7) respectively.\n\n4\n\nSimulation Results\n\nNow an example is presented in which the parameters of the model in Equation (9)\nare trained, using the learning rule in Equations (6) and (7), on one input signal and\nthen are tested on a different input signal. The actual system has three equilibrium\npoints, two stable points located at (1,3) and (3,5), and a saddle point located at\n(2 - ~,4 + ~). In this example the dynamics of both the actual system and the\nmodel are given by\n\n(~1) =\nZ2\n\nZ~\n\nZ~\n\nO\n\n(1\'1 + 1\'2\n+:3\n2)\n0 1\'4 + 1\'5 Z1 + 1\'6 Z2\n\n(:~)\n+ (0 - {1\'7 + 1\'8 Z1 + 1\'9 Z2}) (:~ ) + (1\'10) u(t)\n8Y\n\'P7 + \'P8 ZI + 1\'9 Z2\n8Y\n0\n\n8Z2\n\n0\n\n8Z2\n\n(10)\n\n\x0cGradient and Hamiltonian Dynamics Applied to Learning in Neural Networks\n\n279\n\nwhere V(x) is defined in Equation (4) and u(t) is a time varying input. For the actual\nsystem the parameter values were \'PI = \'P4 = -4, \'P2 = \'Ps = -2, \'P3 = \'P6 = -1,\n\'P7 = 1, \'Ps = 3, \'P9 = 5, and \'PIO = 1. In the model the 10 elements \'Pi are\ntreated as the unknown parameters which must be learned. Note that the first matrix\nfunction is positive definite if the parameters \'PI-\'P6 are all negative valued. The\nsecond matrix function is skew-symmetric for all values of \'P7-\'P9. The two input\nsignals used for training and testing were Ul = 10000 (sin! 1000t + sin ~ 1000t) and\nU2 = 5000 sin 1000 t. The phase space responses of the actual system to the inputs UI\nand U2 are shown by the solid curves in Figures 3(b) and 3(a) respectively. Notice that\nboth of these inputs produce a periodic attractor in the phase space of Equation (10).\nIn order to evaluate the effectiveness of the learning algorithm the Euclidean distance\nbetween the actual and learned state and parameter values was computed and plotted\nversus time. The results are shown in Figure 2. Figure 2(a) shows these statistics when\n{1I~zll, II~\'PII}\n\n{1I~zll, II~\'PII}\n\n17.5\n15\n15\n12.5\n12.5\n10\n\n7.5\n\ni\n\n----\n\n,., ~--.----... ... .......\n\n- --\n\n2.5\n\n150\n200\n250\n300 t\n50\n100\n150\n200\n250\n300 t\n(a)\n(b)\nFigure 2: (a) The state and parameter errors for training using input signal Ut. The solid\ncurve is the Euclidean distance between the state estimates and the actual states\nas a function of time. The dashed curve shows the distance between the estimated\nand actual parameter values versus time.\n(b) The state and parameter errors for training using input signal U2.\n50\n\n100\n\ntraining with input UI, while Figure 2(b) shows the same statistics for input U2. The\nsolid curves are the Euclidean distance between the learned and actual system states,\nand the dashed curves are the distance between the learned and actual parameter\nvalues. These statistics have two noteworthy features. First, the error between the\nlearned and desired states quickly converges to very small values, regardless of how\nwell the actual parameters are learned. This result was guaranteed by Theorem 3.1.\nSecond, the final error between the learned and desired parameters is much lower when\nthe system is trained with input UI. Intuitively this is because input Ul excites more\nfrequency modes of the system than input U2. Recall that in a nonlinear system the\nfrequency modes excited by a given input do not depend solely on the input because\nthe system can generate frequencies not present in the input. The quality of the\nlearned parameters can be qualitatively judged by comparing the phase plots using\nthe learned and actual parameters for each input, as shown in Figure 3. In Figure 3(a)\nthe system was trained using input Ul and tested with input U2, while in Figure 3(b)\nthe situation was reversed. The solid curves are the system response using the actual\nparameter values, and the dashed curves are the response for the learned parameters.\nThe Euclidean distance between the target and test trajectories in Figure 3(a) is in\nthe range (0,0.64) with a mean distance of 0.21 and a standard deviation of 0.14. The\ndistance between the the target and test trajectories in Figure 3(b) is in the range\n(0,4.53) with a mean distance of 0.98 and a standard deviation of 1.35. Qualitatively,\nboth sets of learned parameters give an accurate response for non-training inputs.\n\n\x0c280\n\n1. W. HOWSE, C. T. ABDALLAH, G. L. HEILEMAN\n\n5\nI\n\no\n\n{i\n\n-------r-- -- ----- --- -- I\n\n-5\n\n-10\n\n-15\n\n-l\n\n-1\n\n1\n\n-2\n\n-1\n\n4\n\nXl\n\n(a)\n(b)\nFigure 3: (a) A phase plot of the system response when trained with input UI and tested\nwith input U2. The solid line is the response to the test input using the actual\nparameters. The dotted line is the system response using the learned parameters.\n(b) A phase plot of the system response when trained with input U2 and tested\nwith input UI.\n\nNote that even when the error between the learned and actual parameters is large,\nthe periodic attractor resulting from the learned parameters appears to have the same\n"shape" as that for the actual parameters.\n\n5\n\nConclusion\n\nWe have presented a conceptual framework for designing dynamical systems with\nspecific qualitative properties by decomposing the dynamics into a component normal\nto some surface and a set of components tangent to the same surface. We have\npresented a specific instance of this class of systems which converges to one of a finite\nnumber of equilibrium points. By parameterizing these systems, the manner in which\nthese equilibrium points are approached can be fitted to an arbitrary data set. We\npresent a learning algorithm to estimate these parameters which is guaranteed to\nconverge to a set of parameter values for which the error between the learned and\ndesired trajectories vanishes.\n\nAcknowledgments\nThis research was supported by a grant from Boeing Computer Services under Contract\nW-300445. The authors would like to thank Vangelis Coutsias, Tom Caudell, and Bill\nHome for stimulating discussions and insightful suggestions.\n\nReferences\n[1] M.A. Cohen. The construction of arbitrary stable dynamics in nonlinear neural networks.\nNeural Networks, 5(1):83-103, 1992.\n[2] M.W. Hirsch and S. Smale. Differential equations, dynamical systems, and linear algebra,\nvolume 60 of Pure and Applied Mathematics. Academic Press, Inc., San Diego, CA, 1974.\n[3] J.W. Howse, C.T. Abdallah, and G.L. Heileman. A gradient-hamiltonian decomposition\nfor designing and learning dynamical systems. Submitted to Neural Computation, 1995.\n[4] R.V. Mendes and J .T. Duarte. Decomposition of vector fields and mixed dynamics.\nJournal of Mathematical Physics, 22(7):1420-1422, 1981.\n[5] K.S. Narendra and A.M. Annaswamy. Stable adaptitJe systems. Prentice-Hall, Inc., Englewood Cliffs, NJ, 1989.\n[6] B.A. Pearlmutter. Learning state space trajectories in recurrent neural networks. Neural\nComputation, 1(2):263-269, 1989.\n[7] D. Saad. Training recurrent neural networks via trajectory modification. Complex Systems, 6(2) :213-236, 1992.\n[8] M.-A. Sato. A real time learning algorithm for recurrent analog neural networks. Biological Cybernetics, 62(2):237-241, 1990.\n\n\x0c'
p83187
sg20
S'Neuron-MOS Temporal Winner Search\nHardware for Fully-Parallel Data\nProcessing\n\nTadashi SHIBATA, Tsutomu NAKAI, Tatsuo MORIMOTO\nRyu KAIHARA, Takeo YAMASHITA, and Tadahiro OHMI\nDepartment of Electronic Engineering\nTohoku University\nAza-Aoba, Aramaki, Aobaku, Sendai 980-77 JAPAN\n\nAbstract\nA unique architecture of winner search hardware has been developed using a novel neuron-like high functionality device called\nNeuron MOS transistor (or vMOS in short) [1,2] as a key circuit\nelement. The circuits developed in this work can find the location\nof the maximum (or minimum) signal among a number of input\ndata on the continuous-time basis, thus enabling real-time winner\ntracking as well as fully-parallel sorting of multiple input data. We\nhave developed two circuit schemes. One is an ensemble of selfloop-selecting v MOS ring oscillators finding the winner as an oscillating node. The other is an ensemble of vMOS variable threshold\ninverters receiving a common ramp-voltage for competitive excitation where data sorting is conducted through consecutive winner\nsearch actions. Test circuits were fabricated by a double-polysilicon\nCMOS process and their operation has been experimentally verified.\n\n1\n\nINTRODUCTION\n\nSearch for the largest (or the smallest) among a number of input data, Le., the\nwinner-take-all (WTA) action, is an essential part of intelligent data processing\nsuch as data retrieval in associative memories [3], vector quantization circuits [4],\nKohonen\'s self-organizing maps [5] etc. In addition to the maximum or minimum\nsearch, data sorting also plays an essential role in a number of signal processing\nsuch as median filtering in image processing, evolutionary algorithms in optimizing\nproblems [6] and so forth . Usually such data processing is carried out by software\nrunning on general purpose computers, but the computation time increases explo-\n\n\x0c686\n\nT. SHIBATA, T. NAKAI, T. MORIMOTO, R. KAIHARA, T. YAMASHITA, T. OHMI\n\nsively with the increase in the volume of data. In order to build electronic systems\nhaving a real-time-response capability, the direct implementation of fully parallel\nalgorithms on the integrated circuits hardware is critically demanded.\nA variety of WTA [4, 7, 8) circuits have been implemented so far based on analog\ncurrent-mode circuit technologies. A number of cells, each composed of a current\nsource, competitively share the total current specified by a global current sink and\nthe winner is identified through the current concentration toward the cell via tacit\npositive feedback mechanisms. The circuit implementations using MOSFET\'s operating in the subthreshold regime [4, 7) are ideal for large scale integration due to\nits ultra low power nature. Although they are inherently slow at circuit levels, the\nperformance at a system level is far superior to digital counterparts owing to the\nflexible computing algorithms of analog. In order to achieve a high speed operation, MOSFET\'s biased at strong inversion is also utilized in Ref. [8). However,\ncost must be traded off for increased power.\nWhat we are presenting in this paper is a unique WTA architecture implemented\nby vMOS technology [1,2]. In vMOS circuits the summation of multiples of voltage\nsignals is conducted on the vMOS floating gate (or better be called "temporary floating gate" when used in a clocked scheme [9]) via charge sharing among capacitors,\nand the result of the summation controls the transistor action. The voltage-mode\nsummation capability of vMOS has been uniquely utilized to produce the WTA\naction. No DC current flows for the sum operation itself in contrast to the Kirchhoff sum. In vMOS transistors, however, DC current flows in a CMOS inverter\nconfiguration when the floating gate is biased in the transition region. Therefore\nthe power consumption is larger than in the subthreshold circuitries. However, the\nvMOS WTA\'s presented in this article will give an opportunity of high speed operation at much less power consumption than current-mode circuitries operating in the\nstrong inversion mode. In the following we present two kinds of winner search hardware featuring very fast operation. The winner can be tracked in a continuous-time\nregime with a detection delay time of about lOOpsec, while the sorting of multiple\ndata is conducted in a fixed frame of time of about 100nsec.\n\n2\n\nNEURON-MOS CONTINUOUS-TIME WTA\n\nFig. 1(a) shows a schematic circuit diagram of a vMOS continuous-time WTA\nfor four input signals. Each signal is fed to an input-stage vMOS inverter-A: a\n\n,ole \'Lc\n0:71\' .\nV,.,-VA4\n\nVA\'-V ...\n\nV ?? 1\n\nVs\n\nl\n\n~\n\no~\n:\nVa ~..\nV.\nV ?? 1\n: v. (c)\n\n(b)\n\n\'ole\nVAI-VA4\n\n::~fw\n\n(a)\n\no~: v.\n\nVoc1\n\nl\n\n~??\n\n(d)\n\nFigure 1: (a) Circuit diagram of vMOS continuous-time WTA circuit. (b)lV(d)\nResponse of VAl V A4 as & function of the floating-gate potential of vMOS inverterIV\n\nA.\n\n\x0cNeuron-MOS Temporal Winner Search Hardware for Fully-parallel Data Processing\n\n687\n\nCMOS inverter in which the common gate is made floating and its potential ,pFA\nis determined via capacitance coupling with three input terminals. VI (\'" \'V4) and\nVR are equally coupled to the floating gate and a small capacitance pulls down the\nfloating gate to ground. The vMOS inverter-B is designed to turn on when the\nnumber of l\'s in its inputs (VAl\'" VA4) is more than 1. When a feedback loop is\nformed as shown in the figure, it becomes a ring oscillator composed of odd-numbers\nof inverter stages.\n\n=\n\n=\n\nWhen Vi \'" V4\n0, the circuit is stable with VR\n1 because inverter-A\'s do not\nturn on. This is because the small grounded capacitor pulls down the floating gate\npotential ,pFA a little smaller than its inverting threshold (VDD/2) (see Fig. l(b)).\nIf non-zero signals are given to input terminals, more-than-one inverter-A\'s turn on\n(see Fig. l(c)) and the inverter-B also turns on, thus initiating the transition of VR\nfrom VDD to O. According to the decrease in VR, some of the inverter-A\'s turn off\nbut the inverter-B (number 1 detector) still stays at on-state until the last inverterA turns off. When the last inverter-A, the one receiving the largest voltage input,\nturns off, the inverter-B also turns off and VR begins to increase. As a result, ring\noscillation occurs only in the loop including the largest-input inverter-A(Fig. l(d)).\nIn this manner, the winner is identified as an oscillating node. The inverter-B can\nbe altered to a number "2" detector or a number "3" detector etc. by just reducing\nthe input voltage to the largest coupling capacitor. Then it is possible for top two\nor top three to be winners.\n\no\n\n4()\n\n.0\n\nlOD\n\n120\n\n10\n\n140\n\n~~\n\n31)\n\n20\n\n40\n\n50\n\n:~\nVAi ???f]\n? ? .: . fl?. . .., . ...-.\n-\n\n. . . ..\n\n\',\n\n4fl;...\n\n~\n\n!\n\n70\n\n60\n\n2~\n\n. . . . . .. . ... .. . .. . . ...\n;\n\no~, .. .\n\nI\n\n"\n\n?\n\nI.\n\n.\n\n. . 1.\n\n\'\n\n..\n..\n\nw\n\nCJ\n\n<\n....\n-\'\n\n"\n\no\n\n>\n\n....\n\n2"~\n4\n\n"\'\n\n... ...,. VA. :\n\nJ.\n\nf ~..J.!~.........\no t..\ni ~-\'-\'-\'\n! ,~.....i~...J\no\n10\n20\n30\n40\n50\n\n(a)\n\n.\n\n-~\n\no\n\nTIME [JIaec]\n\n~\n\n~\n\nTIME [nsec)\n\nI\n\nl\n\nI\n\n...\n\nl\n\n60\n\nI\n\nl\n\nI\n\n..\n\nj\n70\n\n(b)\n\nFigure 2: (a) Measured wave forms of four-input WTA as depicted in Fig. 1(80)\n(bread board experoment) . (b) Simulation results for non-oscillating WTA explained in Fig. 3.\nFig. 2(80) demonstrates the measured wave forms of a bread-board test circuit\ncomposed of discrete components for verifying the circuit idea. It is clearly seen that\nring oscillation occurs only at the temporal winner. However, the ring oscillation\nincreases the power dissipation, and therefore, non-oscillating circuitry would be\npreferred. An example of simulation results for such a non-oscillating circuit is\ndemonstrated in Fig. 2(b).\nFig.\n\n3(80) gives the circuit diagram of a non-oscillating version of the vMOS\n\n\x0c688\n\nT. SHIBATA. T. NAKAI. T. MORIMOTO. R. KAIHARA. T. YAMASHITA. T. OHMI\n\n\'1\n~: I~I I\n\nvMOS Inv.,.. r-A\nVt\nVa\n\n~\n\nVa\nV.\n\nvMOS Inv_r-B\n\n~T\nV..\n\naD\n? No,,-olCillalinl mod,\n\no Olcillatl,. mod,\n\n1 aD\n\n;0,2\n\n0\n\n!\n\n()\n\n.;=-\n\n?\n\nf ??\n,,0.1i-\n\n00\n0\n\n0\n\n0\n\n?\n\n0\n\n.R.O 1111\n\n1[>.1>:\n\n0\n\nR\n\nCOXT~\n\nVA\n\na\n\n?RI\'0)? ?\n\n2000\n\n10\n\n??0\n\n4000\n\nCUT/c...\n\n(b)\n(c)\nFigure 3: (a) Circuit diagram of non-oscillating-mode WTA. HSPICE simulation\nresults: (b) combinations of R and CEXT for non-oscillating mode; (c) winner\ndetection delay as a function of capacitance load.\n\n(a)\n\ncontinuous-time WTA. In order to suppress the oscillation, the loop gain is reduced\nby removing the two-stage CMOS inverters in front of the inverter-B and RC delay\nelement is inserted in the feedback loop. The small grounded capacitors were removed in inverter-A\'s. The waveforms demonstrated in Fig. 2(b) are the HSPICE\nsimulation results with R = 0 and CEXT = 20Cgote(Cgote: input capacitance of\nelemental CMOS inverter=5.16f.F) . The circuit was simulated assuming a typical\ndouble-poly 0.5-pm CMOS process. Fig. 3(b) indicates the combinations of Rand\nC EXT yielding the non-oscillating mode of operation obtained by HSPICE simulation. It is important to note that if CEXT ~ 15Cgote , non-oscillating mode appears\nwith R = O. This me8JlS the output resistance of the inverter-B plays the role of\nR. When the number of inverter-A\'s is increased, the increased capacitance load\nserves as CEXT. Therefore, WTA having more than 19 input signals C8Jl operate in\nthe non-oscillating mode. Fig. 3(c) represents the detection delay as a function of\nCEXT. It is known that the increase in CEXT, therefore the increase in the number\nof input signals to the WTA, does not significantly increase the detection delay and\nthat the delay is only in the r8Jlge of 100 to 200psec.\nA photomicrograph of a test circuit of the non-oscillating mode WTA fabricated\nby Tohoku-University st8Jldard double-polysilicon CMOS process on 3-pm design\nrules, and the measurement results are shown in Fig. 4(80) and (b), respectively.\n~\n\nI-\n\nv\n\n/\n:--- ""\n\n1\n\nV [\\( Y?. ~ V\nI"-\n\nINPU T OAl ~\n~~\n\no\n\n(a)\n\nI--\n\n~\n\nr-"---\'V\n\nOUTP TO ~TA\n\n(b)\n\n~\n\n~\n\nTIM E\n\nV\nv.\n\nV\n\n"-\n\nI\'-- /\n\n"""\n\n....\n\nVA\'\n\n[2511uc/dlv)\n\nFigure 4: (a) Photomicrograph of a test circuit for 4-input continuous-time WTA.\nChip size is 800pmx500pm including all peripherals (3-pm rules). The core circuit\nof Fig. 3(80) occupies approximately 0.12 mm2 ? (b) Measured wave forms.\n\n\x0cNeuron-MOS Temporal Winner Search Hardware for Fully-parallel Data Processing\n\n3\n\n689\n\nNEURON-MOS DATA SORTING CIRCUITRY\n\nThe elemental idea of this circuit was first proposed at ISSCC \'93 [3] as an application of the vMOS WTA circuit. In the present work, a clocked-vMOS technique [9]\nwas introduced to enhance the accuracy and reliability of vMOS circuit operation\nand test circuits were fabricated and their operation have been verified.\nFig. 5(80) shows the circuit diagram of a test circuit for sorting three analog data VA,\nVB, and Vc , and a photomicrograph of a fabricated test circuit designed on 3-pm\nrules is shown in Fig. 5(b). Each input stage is a vMOS inverter: a CMOS inverter\nin which the common gate is made floating and its potential fjJ F is determined\nby two input voltages via equa.lly-weighted capacitance coupling, namely fjJF =\n(VA + VRAMP)/2. The reset signal forces the floating node be grounded, thus\ncancelling the charge on the vMOS floating gate each time before sorting. This is\nquite essential in achieving long-term reliability of vMOS operation. In the second\nstage are flip-flop memory cells to store sorting results. The third stage is a circuit\nwhich counts the number of 1\'s at its three input terminals and outputs the result in\nbinary code. The concept of the vMOS A/D converter design [10] has been utilized\nin the circuit.\n\n(a)\n\n(b)\n\n...............\n\n(j) vMOS\n\n@ Data latch\n\n~\n\n@ Counter\n\n.\n\nInverter\n\nFigure 5: (a) Circuit diagram of vMOS data-soring circuit. (b) Photomicrograph\nof a test circuit fabricated by Tohoku Univ. Standard double-polysillicon CMOS\nprocess (3-pm rules). Chip size is 1250pmxBOOpm including a.ll peripherals.\nThe sorting circuit is activated by ramping up VRAMP from OV to VDD. Then the\nvMOS inverter receiving the largest input turns on first and the output data of the\ncounter at this moment (0,0) is latched in the respective memory cells. The counter\noutput changes to (0,1) after gate delays in the counter and this code is latched\nwhen the vMOS inverter receiving the second largest turns on. Then the counter\ncounts up to (1,0). In this manner, the all input data are numbered according to\nthe order of their magnitudes after a ramp voltage scan is completed.\nThe measurement results are demonstrated in Fig. 6(80) in comparison with the\nHSPICE simulation results. Simulation was carried out on the same architecture\ncircuit designed on O.5-pm design rules and operated under 3V power supply. For\nthree analog input voltages: VA = 5V, VB = 4V, and Vc = 2V, (0,0), (0,1),\n\n\x0cT. SHIBATA, T. NAKAI, T. MORIMOTO, R. KAIHARA, T. YAMASHITA, T. OHMI\n\n690\n\n40\n\nMEASUREMENT\n\n~30\n\nS\n\n3-INPUT\n\nSORnNG CIRCUIT\n\n20\n\nj 1:\n\n~\n\n4\n8\n6\n2\nSortIng Accuracy (bit ]\n\nr\n\n(b)\n\nIi\n\n_100\n\nL\n\n~80\n-eo\n\nr\n\n15-INPUT\nSORTING CIRCUIT\n\nS40\nc:\n~\n\n20\n0\n\n2\n\n6\n\n4\n\n8\n\nSortIng Accuracy (bit ]\n\nr \'\n\n10~/div\n\n20nsec/civ\n\n(a)\n\n(c)\n\nFigure 6: (a) Wave forms of the test circuit shown in Fig. 5(a) measured without\nbuffer circuitry (left) and simulation results of a circuit designed with 0.5-pm rules\n(right). (b) Minimum scan time vs. sorting accuracy for a three-input sorter. (c)\nMinimum scan time vs. sorting accuracy for a 15-input sorter.\n\nand (1,0) are latched, respectively, after the ramp voltage scan, thus accomplishing\ncorrect sorting. Slow operation of the test circuit is due to the loading effect caused\nby the direct probing of the node voltage without output buffer circuitries. The\nsimulation with a 0.5-pm-design-rule circuit indicates the sorting is accomplished\nwithin the scan time of 4Onsec.\nIn Fig. 6(b), the minimum scan time obtained by simulation is plotted as a function of the bit accuracy in sorting analog data. N -bit accuracy means the minimum\nvoltage difference required for winner discrimination is VDD/2 2 ? If the ramp rate\nis too fast, the vMOS inverter receiving the next largest data turns on before the\ncorrect counting results become available, leading to an erroneous operation. The\nscan time/accuracy relation in Fig. 6(b) is primarily determined by the response\ndelay in the counter. It should be noted that the number of inverter stages in the\ncounter (vMOS A/D converter) is always three indifferent to the number of output\nbits, namely, the delay would not increase significantly by the increase in the number of input data. In order to investigate this, a 15-input counter was designed and\nthe delay time was evaluated by HSPICE simulation. It was 312 psec in comparison\nwith 110 psec of the 3-input counter of Fig. 5(a). The scan time/accuracy relation\nfor the 15-input sorting circuit is shown in Fig. 6( c), indicating the sorting of 15\ninput data can be accomplished in 100 nsec with 8-bit accuracy.\n\n\x0cNeuron-MOS Temporal Winner Search Hardware for Fully-parallel Data Processing\n\n4\n\n691\n\nCONCLUSIONS\n\nA novel neuron-like functional device liMOS has been successfully utilized in constructing intelligent electronic circuits which can carry out search for the temporal\nwinner. As a result, it has become possible to perform data sorting as well as\nwinner search in an instance, both requiring very time-consuming sequential data\nprocessing on a digital computer. The hardware algorithms presented here are typical examples of the liMOS binary-multivalue-analog merged computation scheme,\nwhich would play an important role in the future flexible data processing.\nAcknowledgements\nThis work was partially supported by Grant-in-Aid for Scientific Research\n(06402038) from the Ministry of Education, Science, Sports, and Culture, Japan. A\npart of this work was carried out in the Super Clean Room of Laboratory for Electronic Intelligent Systems, Research Institute of Electrical communication, Tohoku\nUniversity.\nReferences\n[1] T. Shibata and T . Ohmi, "A functional MOS transistor featuring gate-level\nweighted sum and threshold operations," IEEE Trans. Electron Devices, Vol. 39,\nNo.6, pp.1444-1455 (1992).\n[2] T. Shibata, K. Kotani, T. Yamashita, H. Ishii, H. Kosaka, and T. Ohmi, "Implementing interlligence on silicon using neuron-like functional MOS transistors," in\nAdvances in Neural Information Processing Systems 6 (San Francisco, CA: Morgan\nKaufmann 1994) pp. 919-926.\n[3] T. Yamashita, T. Shibata, and T. Ohmi, "Neuron MOS winner-take-all circuit\nand its application to associative memory," in ISSCC Dig. Tech. Papers, Feb. 1993,\nFA 15.2, pp. 236-237.\n[4] G. Gauwenberghs and V. Pedroni, " A charge-based CMOS parallel analog vector\nquantizer," in Advances in Neural Information Processing Systems 7 (Cambridge,\nMA: The MIT Press 1995) pp. 779-786.\n[5] T. Kohonen, Self-Organization and Associative Memory, 2nd ed. (New York:\nSpringer-Verlag 1988).\n[6] M. Kawamata, M. Abe, and T. Higuchi, "Evolutionary digital filters," in Proc.\nInt. Workshop on Intelligent Signal Processing and Communication Systems, seoul,\nOct., 1994, pp. 263-268.\n[7] J. Lazzaro, S. Ryckebusch, M. A. Mahowald, and C. A. Mead, "Winner-TakeAll networks of O(N) complexity," in Advances in Neural Information Processing\nSystems 1 (San Mateo, CA: Morgan Kaufmann 1989) pp. 703-711.\n[8] J . Choi and B. J. Sheu, "A high-precision VLSI winner-take-all circuit for selforganizing neural networks," IEEE J. Solid State Circuits, Vol. 28, No.5, pp.576584 (1993).\n[9] K. Kotani, T. Shibata, M. Imai, and T. Ohmi, "Clocked-Neuron-MOS logic\ncircuits employing auto-threshold-adjustment," in ISSCC Dig. Technical Papers,\nFeb. 1995, FA 19.5, pp. 320-321.\n[10] T. Shibata and T. Ohmi, "Neuron MOS binary-logic integrated circuits: Part\nII, Simplifying techniques of circuit configuration and their practical applications,"\nIEEE Trans. Electron Devices, Vol. 40, No.5, 974-979 (1993).\n\n\x0c'
p83188
sg18
S'Dynamics of Attention as Near\nSaddle-Node Bifurcation Behavior\n\nHiroyuki Nakahara"\n\nKenji Doya\n\nGeneral Systems Studies\nU ni versi ty of Tokyo\n3-8-1 Komaba, Meguro\nTokyo 153, Japan\nnakahara@vermeer.c.u-tokyo.ac.jp\n\nATR Human Information Processing\nResearch Laboratories\n2-2 Hikaridai, Seika, Soraku\nKyoto 619-02, Japan\ndoya@hip.atr.co.jp\n\nAbstract\nIn consideration of attention as a means for goal-directed behavior in non-stationary environments, we argue that the dynamics of\nattention should satisfy two opposing demands: long-term maintenance and quick transition. These two characteristics are contradictory within the linear domain. We propose the near saddlenode bifurcation behavior of a sigmoidal unit with self-connection\nas a candidate of dynamical mechanism that satisfies both of these\ndemands. We further show in simulations of the \'bug-eat-food\'\ntasks that the near saddle-node bifurcation behavior of recurrent\nnetworks can emerge as a functional property for survival in nonstationary environments.\n\n1\n\nINTRODUCTION\n\nMost studies of attention have focused on the selection process of incoming sensory\ncues (Posner et al., 1980; Koch et al., 1985; Desimone et al., 1995). Emphasis was\nplaced on the phenomena of causing different percepts for the same sensory stimuli.\nHowever, the selection of sensory input itself is not the final goal of attention. We\nconsider attention as a means for goal-directed behavior and survival of the animal.\nIn this view, dynamical properties of attention are crucial. While attention has\nto be maintained long enough to enable robust response to sensory input, it also\nhas to be shifted quickly to a novel cue that is potentially important. Long-term\nmaintenance and quick transition are critical requirements for attention dynamics.\n?currently at Dept. of Cognitive Science and Institute for Neural Computation,\nU. C. San Diego, La Jolla CA 92093-0515. hnakahar@cogsci.ucsd.edu\n\n\x0c39\n\nDynamics of Attention as Near Saddle-node Bifurcation Behavior\n\nWe investigate a possible neural mechanism that enables those dynamical characteristics of attention.\nFirst, we analyze the dynamics of a network of sigmoidal units with self-connections.\nWe show that both long-term maintenance and quick transition can be achieved\nwhen the system parameters are near a "saddle-node bifurcation" point . Then, we\ntest if such a dynamical mechanism can actually be helpful for an autonomously\nbehaving agent in simulations of a \'bug-eat-food\' task. The result indicates that\nnear saddle-node bifurcation behavior can emerge in the course of evolution for\nsurvival in non-stationary environments.\n\n2\n\nNEAR SADDLE-NODE BIFURCATION BEHAVIOR\n\nWhen a pulse-like input is given to a linear system, the rising and falling phases\nof the response have the same time constants. This means that long-term maintenance and quick transition cannot be simultaneously achieved by linear dynamics.\nTherefore, it is essential to consider a nonlinear dynamical mechanism to achieve\nthese two demands.\n\n2.1\n\nDYNAMICS OF A SELF-RECURRENT UNIT\n\nFirst, we consider the dynamics of a single sigmoidal unit with the self-connection\nweight a and the bias b.\n\ny(t\n\n+ 1)\n\nF(ay(t)\n\n+ b) ,\n\n(1)\n\nF(x)\n\n1\n1 + exp( -x)\'\n\n(2)\n\nThe parameters (a, b) determine the qualitative behavior of the system such as the\nnumber of fixed points and their stabilities. As we change the parameters , the\nqualitative behavior of the system may suddenly change. This is referred to as\n"bifurcation" (Guckenheimer, et al., 1983). A typical example is a "saddle-node\nbifurcation" in which a pair of fixed points, one stable and one unstable, emerges.\nIn our system, this occurs when the state transition curve y(t + 1) = F(ay(t) + b) is\ntangent to y(t + 1) = y(t). Let y* be this point of tangency. We have the following\ncondi tion for saddle-node bifurcation.\n\nF(ay*\ndF(ay + b)\ndy\n\n+ b)\n\nI\n\nb =\n\n(3)\n\n1\n\n( 4)\n\ny=y.\n\nThese equations can be solved, by noting F\'(x)\na\n\ny*\n\n= F(x)(l- F(x)), as\n\n1\n\n(5)\n\ny* (1 - y*)\n1\n\nF-1(y*) - ay* = F-l(y*) - - I - y*\n\n(6)\n\nBy changing the fixed point value y* between a and 1, we can plot a curve in the\nparameter space (a, b) on which saddle-node bifurcation occurs, as shown in Figure\n1 (left). A pair of a saddle point and a stable fixed point emerges or disappears\nwhen the parameters pass across the cusp like curve (cases 2 and 4) . The system\nhas only one stable fixed point when the parameters are outside the cusp (case 1)\nand three fixed points inside the cusp (case 3).\n\n\x0cH.NAKAHARA,K.DOYA\n\n40\n\ny ( t+ l )\n\nb\n\nBifurcat ion Diagr am\n\ny(t +1 J\n\nCASE 1\n\n~\n\n06,\n\n0 61\n\n",\n\n041\n\nO~/\n\n.,\n\nH\n\n-\'0\n\n"\n\n0\n\n../\n\n,,\'\n\n"i\n\n"\n\n""0~\n1O:".,,,,0\';;;-0;;-;\n8 lY lt )\n\nY\'~tL\n? " CA\nSE3\n\nY\',t." CASE?\n\no. a:\n\n0 B\n\n(I\n\n,,/\n\n\'\n\n\'t1x.d p t ., "\' 3\n6\n\'\n\n,\n\nOJ\'\no\n\n- lO\n\n8:tixed pts,\'\n\n0\'1/\n\n0.20. 4 0.60 8 ly(tJ\n\n.\n\n- 15\n\nCASE 2\n\n"\n\nC. 8f::.x.d Pt ?. , ~\' 1\n\n,/\n\n0 6\n\n,\'\n\n0"\n0 2:\n\n\'0 20 40 60 8 1 y( t )\n\n.,,\'.\n\n"\n\n.\n\nf lxed p ts " . 2\n\'\n\'\n\n0 2(\'\n\n4~\n\n60 8: 1 y (tJ\n\nFigure 1: Bifurcation Diagram of a Self-Recurrent Unit . Left : the curve in the\nparameter space (a , b) on which saddle-node bifurcation is seen. Right : state transition diagrams for four different cases.\ny ( t ? l)\n\nY\n(t:l)ll.ll11\n1d\n=~9\n\no\'~=Lil\nl. 1111 b =- 7. 9\n\no.\n\n0.6\n\nO.\n\nI\n\n0. 4\n\n0 .2\na\n\n"\n\n0.20. 4 0 . 60. 81\n\nI\nI\n\nO.\nO.\n\nI\n\n... "\n\nb\n\ny et)\n\nI\n\no\n\no . 20 . 4 0\n\nyet)\n60. 8 1\n\nyet)\n\ny et )\n\nL\n\no.~\n\ndo. &\n\no.\n\n0.21\n\no.\no.\n\no\n\n"\n\n,I\n\n5\n\n10 15 2 (f i me (t)\n\n0 . 41\n\no\n\n5\n\n1 0 l S 2a:\'i rne l t )\n\nFigure 2: Temporal Responses of Self-Recurrent Units. Left : near saddle-node\nbifurcation. Right : far from bifurcation.\nAn interesting behavior can be seen when the parameters are just outside the cusp,\nas shown in Figure 2 (left) . The system has only one fixed point near Y = 0, but\nonce the unit is activated (y ~ 1) , it stays "on" for many time steps and then goes\nback to the fixed point quickly. Such a mechanism may be useful in satisfying the\nrequirements of attention dynamics: long-term maintenance and quick transition.\n\n2.2\n\nNETWORK OF SELF-RECURRENT UNITS\n\nNext, we consider the dynamics of a network of the above self-recurrent units.\n\nYi(t\n\n+ 1) = F[aYi(t) + b + L\n\nCijYj(t)\n\n+ diUi(t)],\n\n(7)\n\nj,jti\n\nwhere a is the self connection weight , b is the bias, Cij is the cross connection weight,\nand di is the input connection weight , and Ui(t) is the external input. The effect of\nlateral and external inputs is equivalent to the change in the bias, which slides the\nsigmoid curve horizontally without changing the slope.\nFor example, one parameter set of the bifurcation at y* = 0.9 is a = 11.11 and\nb ~ -7.80. Let b = -7.90 so that the unit has a near saddle-node bifurcation\nbehavior when there is no lateral or external inputs. For a fixed a = 11.11, as we\nincrease b, the qualitative behavior of the system appears as case 3 in Figure 1, and\n\n\x0c41\n\nDynamics of Attention as Near Saddle-node Bifurcation Behavior\nSensory\nInputs\n,\n,\n\'olr lood\n\nActions\n\nNetwork Structure\n\n"\n\n- \'--,~ ,- \\, :/~ - - - .\n\nr-.~ "==:-:"~-\n\n~~ .......\n\n\'on:\'oocl\n\n~~\n\n-\n\n.....\n\nCreature\n\n. . ...\n\n...\n\nCreature\n\nInpol. IJrI .111\'2\n\nFigure 3: A Creature\'s Sensory Inputs(Left), Motor System(Center) and Network\nArchitecture(Right)\nthen, it changes again at b:::::: -3.31, where the fixed point at Y = 0.1, or another\nbifurcation point , appears as case 4 in Figure L Therefore , ifthe input sum is large\nenough, i.e . L j ,j;Ci CijYj + diuj > -3.31- (-7.90) :::::: 4.59, the lower fixed point\nat Y = 0.1 disappears and the state jumps up to the upper fixed point near Y = 1,\nquickly turning the unit "on". If the lateral connections are set properly, this can\nin turn suppress the activation of other units. Once the external input goes away,\nas we see in Figure 2 (left), the state stays "on" for a long time until it returns to\nthe fixed point near Y = O.\n\n3\n\nEVOLUTION OF NEAR BIFURCATION DYNAMICS\n\nIn the above section, we have theoretically shown the potential usefulness of near\nsaddle-node bifurcation behavior for satisfying demands for attention dynamics. We\nfurther hypothesize that such behavior is indeed useful in animal behaviors and can\nbe found in the course of learning and evolution of the neural system.\nTo test our hypothesis, we simulated a \'bug-eat-food \' task . Our purpose in t.his\nsimulation was to see whether the attention dynamics discussed in the previous\nsection would help obtain better performance in a non-stationary environment. Vve\nused evolutionary programming (Fogel et aI, 1990) to optimize the performance of\nrecurrent networks and feedforward networks.\n\n3.1\n\nTHE BUG AND THE WORLD\n\nIn our simulation, a simple creature traveled around a non-stationary environment.\nIn the world, there were a certain number of food items. Each item was fixed at a\ncertain place in the world but appeared or disappeared in a stochastic fashion, as\ndetermined by a two-state Markov system. In order to survive, A creature looked\nfor food by traveling the world . The amount of food a creature found in a certain\ntime period was the measure of its performance.\nA creature had five sensory inputs, each of which detected food in the sector of 45\ndegrees (Figure 3, right). Its output level was given by L J\' .l..,\nwhere Tj ,"vas the\nrJ\ndistance to the j-th food item within the sector. Note that the format of the input\ncontained information about distance and also that the creature could only receive\nthe amount of the input but could not distinguish each food from others.\nFor the sake of simplicity, we assumed that the creature lived in a grid-like world .\nOn each time step, it took one of three motor commands: L: turn left (45 degrees),\n\n\x0cH. NAKAHARA, K. DOYA\n\n42\n\nDensity of Food\nMarkov Transition Matrix\nof each food\nRandom Walk\nNearest Visible\nFeedForward\nRecurrent\nNearest Visible/Invisible\n\n0.05\n.5 .5 .8 .8\n.5 .5 .2 .2\n7.0\n6.9\n42.7 18.6\n58.6 37.3\n65.7 43.6\n97.7 97.1\n\n0.10\n.5 .5\n.8 .8\n.2 .2\n.5 .5\n13.8\n13.9\n65.3\n32.4\n84.8\n60.0\n94.0\n66.1\n129.1 128.8\n\nTable 1: Performances of the Recurrent Network and Other Strategies.\n\nC: step forward, and R: turn right (Figure 3, center). Simulations were run with\ndifferent Markov transition matrices of food appearance and with different food\ndensities. A creature got the food when it reached the food, whether it was visible\nor invisible. When a creature ate a food item, a new food item was placed randomly.\nThe size of the world was 10x10 and both ends were connected as a torus.\nA creature was composed of two layers: visual layer and motor layer (Figure 3,\nleft). There were five units 1 in visual layer, one for each sensory input, and their\ndynamics were given by Equation (7). The self-connection a, the bias b and the\ninput weight di were the same for all units. There were three units in motor layer ,\neach coding one of three motor commands, and their state was given by\n\nek\n\n+ L: fkiYi(t),\n\nexp(xk(t))\n\nL:/ exp(x/(t)) \'\n\n(8)\n\n(9)\n\nwhere ek was the bias and fki was the feedforward connection weight. 2 One of the\nthree motor commands (L,C,R) was chosen stochastically with the probability Pk\n(k=L,C,R). The activation pattern in visual layer was shifted when the creature\nmade a turn, which should give proper mapping between the sensory input and the\nworking memory.\n\n3.2\n\nEVOLUTIONARY PROGRAMMING\n\nEach recurrent network was characterized by the parameters (a,b,Cij,di,ek,lkd,\nsome of which were symmetrically shared, e.g. C12 = C21. For comparison, we\nalso tested feedforward networks where recurrent connections were removed, i.e.\na\nCij\nO.\n\n=\n\n=\n\nA population of 60 creatures was tested on each generation. The initial population\nwas generated with random parameters. Each of the top twenty scoring creatures\nproduced three offspring; one identical copy of the parameters of the parent\'s and\ntwo copies of these parameters with a Gaussian fluctuation. In this paper, we report\nthe result after 60 generations.\n\n3.3\n\nPERFORMANCE\n\n1 We denote each unit in visual layer by Ul, U2, U3, U4, Us from the left to the right for\nthe later convenience\n2In this simulation reported here, we set ek = O.\n\n\x0cDynamics of Attention as Near Saddle-node Bifurcation Behavior\n\n-,\n\n-, ,\n-,\n\n-, ,\n-,\n\n-7 ,\n\n_7\n\n,\n\n- 10\n- 12 . 5\n\n,\n\n43\n\n......: ....\n"\n\n-L25\n\na\n\nb\n\n"Transition matrix\n\n= ( :~\n\n.5 )\n\n.5\n\nbTransition matrix = (\n\n:~\n\n:~ )\n\nFigure 4: The Convergence of the Parameter of (a , b) by Evolutionary Programming\nPlotted in the Bifurcation Diagram. The food density is 0.10 in both examples\nabove .\nTable 1 shows the average of food found after 60 generations. As a reference of\nperformance level, we also measured the performances of three other simple algorithms: 1) random walk : one of the three motor commands is taken randomly with\nequal probability. 2) nearest visible: move toward the nearest food visible at the\ntime within the creature\'s field of view of (U2, U3, U4). 3) nearest visible/invisible:\nmove toward the nearest food within the view of (U2, U3, U4) no matter if it is visible\nor not, which gives an upper bound of performance.\nThe performance of recurrent network is better than that of feedforward network\nand \'nearest visible\'. This suggests that the ability of recurrent network to remember the past is advantageous.\nThe performance of feedforward network is better than that of \'nearest visible \'.\nOne reason is that feedforward network could cover a broader area to receive inputs\nthan \'nearest visible\' . In addition, two factors, the average time in which a creature\nreaches the food and the average time in which the food disappears, may influence\nthe performance of feedforward network and \'nearest visible\'. Feedforward network\ncould optimize its output to adapt two factors with its broader view in evolution\nwhile \'nearest visible\' did not have such adaptability.\nIt should be noted that both of \'nearest visible/invisible \' and \'nearest visible\' explicitly assumed the higher-order sensory processing: distinguishing each food item from\nthe others and measuring the distance between each food and its body. Since its performance is so different regardless of its higher-order sensory processing, it implies\nthe importance of remembering the past. We can regard recurrent network as compromising two characteristics, remembering the past as \'nearest visible/invisible\'\ndid and optimizing the sensitivity as feedforward network did , although recurrent\nnetwork did not have a perfect memory as \'nearest visible/invisible\' .\n\n3.4\n\nCONVERGENCE TO NEAR-BIFURCATION REGIME\n\nWe plotted the histogram of the performance in each generation and the history of\nthe performance of a top-scoring creature over generations. Though they are not\nshown here, the performance was almost optimal after 60 generations.\nFigure 4 shows that two examples of a graph in which we plotted the parameter\n\n\x0cH. NAKAHARA, K. DOYA\n\n44\n\nset (a , b) of top twenty scoring creatures in the 60th generation in the bifurcation\ndiagram. In the left graph, we can see the parameter set has converged to a regime\nthat gives a near saddle-node bifurcation behavior. On the other hand, in the right\ngraph, the parameter set has converged into the inside of cusp. It is interesting\nto note that the area inside of the cusp gives bistable dynamics. Hence, if the\ninput is higher than a repelling point, it goes up and if the input is lower , it goes\ndown . The reason of the convergence to that area is because of the difference of\nthe world setting, that is, a Markov transition matrix. Since food would disappear\nmore quickly and stay invisible longer in the setting of the right graph, it should\nbe beneficial for a creature to remember the direction of higher inputs longer . In\nmost of cases reported in Table 1, we obtained the convergence into our predicted\nregime and/or the inside of the cusp.\n\n4\n\nDISCUSSION\n\nNear saddle-node bifurcation behavior can have the long-term maintenance and\nquick transition, which characterize attention dynamics. A recurrent network\nhas better performance than memoryless systems for tasks in our simulated nonstationary environment. Clearly, near saddle-node bifurcation behavior helped a\ncreature\'s survival and in fact, creatures actually evolved to our expected parameter regime . However, we also obtained the convergence into another unexpected\nregime which gives bistable dynamics . How the bistable dynamics are used remains\nto be investigated.\nAcknowledgments\nH.N . is grateful to Ed Hutchins for his generous support, to John Batali and David\nFogel for their advice on the implementation of evolutionary programming and to\nDavid Rogers for his comments on the manuscript of this paper.\nReferences\nR. Desimone, E. K. Miller , L. Chelazzi, & A. Lueschow. (1995) Multiple Memory\nSystems in the Visual Cortex. In M. Gazzaniga (ed .) , The Cognitive Neurosciences,\n475-486. MIT Press.\nD. B. Fogel, L. J. Fogel, & V. W . Porto. (1990) Evolving Neural Networks. Biological cybernetics 63:487-493.\nJ. Guckenheimer & P. Homes. (1983) Nonlinear Oscillations, Dynamical Systems,\nand Bifurcation of Vector Fields\n\nC. Koch & S. Ullman . (1985) Shifts in selective visual attention:towards the underlying neural circuitry. Human Neurobiology 4:219-227 .\nM. Posner , C .. R .R. Snyder, & B. J. Davidson. (1980) Attention and the detection\nof signals. Journal of Experimental Psychology: General 109:160-174\n\n\x0c'
p83189
sg221
S'Improved Gaussian Mixture Density\nEstimates Using Bayesian Penalty Terms\nand Network Averaging\n\nDirk Ormoneit\nInstitut fur Informatik (H2)\nTechnische Universitat Munchen\n80290 Munchen, Germany\normoneit@inJormatik.tu-muenchen.de\n\nVolker Tresp\nSiemens AG\nCentral Research\n81730 Munchen, Germany\nVolker. Tresp@zJe.siemens.de\n\nAbstract\n\nWe compare two regularization methods which can be used to improve the generalization capabilities of Gaussian mixture density\nestimates. The first method uses a Bayesian prior on the parameter space. We derive EM (Expectation Maximization) update rules\nwhich maximize the a posterior parameter probability. In the second approach we apply ensemble averaging to density estimation.\nThis includes Breiman\'s "bagging" , which recently has been found\nto produce impressive results for classification networks.\n\n1\n\nIntroduction\n\nGaussian mixture models have recently attracted wide attention in the neural network community. Important examples of their application include the training of\nradial basis function classifiers, learning from patterns with missing features, and\nactive learning. The appeal of Gaussian mixtures is based to a high degree on the\napplicability of the EM (Expectation Maximization) learning algorithm, which may\nbe implemented as a fast neural network learning rule ([Now91], [Orm93]). Severe\nproblems arise, however, due to singularities and local maxima in the log-likelihood\nfunction. Particularly in high-dimensional spaces these problems frequently cause\nthe computed density estimates to possess only relatively limited generalization capabilities in terms of predicting the densities of new data points. As shown in this\npaper, considerably better generalization can be achieved using regularization.\n\n\x0c543\n\nImproved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms\n\nWe will compare two regularization methods. The first one uses a Bayesian prior\non the parameters. By using conjugate priors we can derive EM learning rules\nfor finding the MAP (maximum a posteriori probability) parameter estimate. The\nsecond approach consists of averaging the outputs of ensembles of Gaussian mixture\ndensity estimators trained on identical or resampled data sets. The latter is a form\nof "bagging" which was introduced by Breiman ([Bre94]) and which has recently\nbeen found to produce impressive results for classification networks. By using the\nregularized density estimators in a Bayes classifier ([THA93], [HT94], [KL95]) , we\ndemonstrate that both methods lead to density estimates which are superior to the\nunregularized Gaussian mixture estimate.\n\n2\n\nGaussian Mixtures and the EM Algorithm\n\nConsider the lroblem of estimating the probability density of a continuous random\nvector x E \'R based on a set x* = {x k 11 S k S m} of iid. realizations of x. As a density model we choose the class of Gaussian mixtures p(xle) = L:7=1 Kip(xli, pi, E i ),\nwhere the restrictions Ki ~ 0 and L:7=1 Kj = 1 apply. e denotes the parameter\nvector (Ki\' Iti, E i )i=1. The p(xli, Pi, E i ) are multivariate normal densities:\np( xli , Pi , Ei) = (271")- 41Ei 1- 1 / 2 exp [-1/2(x - Pi)tEi 1 (x - Iti)] .\nThe Gaussian mixture model is well suited to approximate a wide class of continuous\nprobability densities. Based on the model and given the data x*, we may formulate\nthe log-likelihood as\n\nlee)\n\n= log [rr mk=l p(xkle)] = ",m\nlog "\'~ Kip(xkli, Pi, Ei) .\n.L...".k=1\n.L...".J=l\n\ne\n\nMaximum likelihood parameter estimates may efficiently be computed with the\nEM (Expectation Maximization) algorithm ([DLR77]) . It consists of the iterative\napplication of the following two steps:\n1. In the E-step, based on the current parameter estimates, the posterior\nprobability that unit i is responsible for the generation of pattern xk is\n\nestimated as\n(1)\n\n2. In the M-step, we obtain new parameter estimates (denoted by the prime):\n,\n\nK ?\nJ\n\n= -m1 L mk=1 h?k\nJ\n\n~m\n\n,\n\n(2)\n\n=\n\nPi\n\nwk-l\n\n~m\n\nhki X k\nhi\n\nwl=l\n\n~.\' _ L:~1 hf(x k - pD(x k - pDt\nL.J J\n\n-\n\nm\n\nI\n\n(3)\n\ni\n\n(4)\n\nL:l=l hi\nNote that K~ is a scalar , whereas p~ denotes a d-dimensional vector and E/\nis a d x d matrix.\nIt is well known that training neural networks as predictors using the maximum\nlikelihood parameter estimate leads to overfitting. The problem of overfitting is\neven more severe in density estimation due to singularities in the log-likelihood\nfunction. Obviously, the model likelihood becomes infinite in a trivial way if we\nconcentrate all the probability mass on one or several samples of the training set.\n\n\x0c544\n\nD. ORMONEIT, V. TRESP\n\nIn a Gaussian mixture this is just the case if the center of a unit coincides with\none of the data points and E approaches the zero matrix. Figure 1 compares the\ntrue and the estimated probability density in a toy problem. As may be seen,\nthe contraction of the Gaussians results in (possibly infinitely) high peaks in the\nGaussian mixture density estimate. A simple way to achieve numerical stability\nis to artificially enforce a lower bound on the diagonal elements of E. This is a\nvery rude way of regularization, however, and usually results in low generalization\ncapabilities. The problem becomes even more severe in high-dimensional spaces.\nTo yield reasonable approximations, we will apply two methods of regularization,\nwhich will be discussed in the following two sections.\n\nFigure 1: True density (left) and unregularized density estimation (right).\n\n3\n\nBayesian Regularization\n\nIn this section we propose a Bayesian prior distribution on the Gaussian mixture\nparameters, which leads to a numerically stable version of the EM algorithm. We\nfirst select a family of prior distributions on the parameters which is conjugate*.\nSelecting a conjugate prior has a number of advantages. In particular, we obtain\nanalytic solutions for the posterior density and the predictive density. In our case,\nthe posterior density is a complex mixture of densities t . It is possible, however, to\nderive EM-update rules to obtain the MAP parameter estimates.\nA conjugate prior of a single multivariate normal density is a product of a normal\ndensity N(JLilft,1]-lE i ) and a Wishart density Wi(E;lla,,8) ([Bun94]). A proper\nconjugate prior for the the mixture weightings \'" = ("\'1, ... , "\'n) is a Dirichlet density\nD("\'hV. Consequently, the prior of the overall Gaussian mixture is the product\nD(",lr)\nN(JLilil, 71- 1Ei)Wi(E;1I a , ,8). Our goal is to find the MAP parameter\nestimate, that is parameters which assume the maximum of the log-posterior\n\nil7=1\n\nIp(S)\n\n2:=~=1 log 2:=;=1 "\'iP(X k Ii, JLi, Ei ) + log D("\'lr)\n\n+ 2:=;=1 [logN(JLilft, 71- 1Ei) + log Wi(E;lla, ,8)].\nAs in the unregularized case, we may use the EM-algorithm to find a local maximum\n? A family F of probability distributions on 0 is said to be conjugate if, for every 1r E F,\nthe posterior 1r(0Ix) also belongs to F ([Rob94]).\ntThe posterior distribution can be written as a sum of nm simple terms.\ntThose densities are defined as follows (b and c are normalizing constants):\n\nbII n\n\nD(1I:17)\n\n.=1\n\n~.=l\n\n(21r)-i 11,-IE;I-l/2 exp [-~(Il\' -\n\nN(Il.lp,1,-IE.)\nW i(Ei l la,,8)\n\n11:7,-1, with 11:, ~ 0 and ",n\n\n=\n\ncIEillo-Cd+l)/2 exp [-tr(,8Ei 1 )]\n\n11:.\n\n=1\n\nMt Ei 1 (1l\' - M]\n?\n\n\x0c545\n\nImproved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms\n\nof Ip(8). The E-step is identical to (1). The M-step becomes\n,\nL.."k-l hki + ri - 1\n(5)\n,L.."k=l hki x k + \'1J1.\n"\'i\nJ1.i\nhi\nm + L.."i=l ri - n\nL..,,1=1 i + 11\n\n=\n\n"m\n\nE~ =\n\n"m\n\n"n\n\n2:;-1 hf(x k -\n\nA\n\n= "m\n\n+ 11(J1.i 2:~1 h~ + 20: - d\n\nJ1.D(xk - J1.D t\n\nI\n\njJ.)(J1.i - jJ.)t\n\n(6)\n\n+ 2f3\n\n(7)\n\nAs typical for conjugate priors, prior knowledge corresponds to a set of artificial\ntraining data which is also reflected in the EM-update equations. In our experiments, we focus on a prior on the variances which is implemented by f3 =F 0, where\no denotes the d x d zero matrix. All other parameters we set to "neutral" values:\n\nri=l\'v\'i : l::;i::;n,\n\n0:= (d+I)/2,\n\n11=0,\n\nf3=iJl d\n\nld is the d x d unity matrix. The choice of 0: introdu~es a bias which favors large\nvariances?. The effect of various values of the scalar f3 on the density estimate is\nillustrated in figure 2. Note that if iJ is chosen too small, overfitting still occurs. If\nit is chosen to large , on the other hand, the model is too constraint to recognize the\nunderlying structure.\n\nFigure 2: Regularized density estimates (left:\n\niJ =\n\n0.05, right: \'iJ = 0.1).\n\nTypically, the optimal value for iJ is not known a priori. The simplest procedure\nconsists of using that iJ which leads to the best performance on a validation set,\nanalogous to the determination of the optimal weight decay parameter in neural\nnetwork training. Alternatively, iJ might be determined according to appropriate\nBayesian methods ([Mac9I]). Either way, only few additional computations are\nrequired for this method if compared with standard EM.\n\n4\n\nAveraging Gaussian Mixtures\n\nIn this section we discuss the averaging of several Gaussian mixtures to yield improved probability density estimation. The averaging over neural network ensembles\nhas been applied previously to regression and classification tasks ([PC93]) .\nThere are several different variants on the simple averaging idea. First, one may\ntrain all networks on the complete set of training data. The only source of disagreement between the individual predictions consists in different local solutions\nfound by the likelihood maximization procedure due to different starting points.\nDisagreement is essential to yield an improvement by averaging, however, so that\nthis proceeding only seems advantageous in cases where the relation between training data and weights is extremely non-deterministic in the sense that in training,\n?If A is distributed according to Wi(AIO\', (3), then E[A- 1 ] = (0\' - (d + 1)/2)-1 {3. In our\ncase A is B;-I, so that E[Bi] -+ 00 ? {3 for 0\' -+ (d + 1)/2.\n\n\x0c546\n\nD. ORMONEIT, V. TRESP\n\ndifferent solutions are found from different random starting points. A straightforward way to increase the disagreement is to train each network on a resampled\nversion of the original data set. If we resample the data without replacement, the\nsize of each training set is reduced, in our experiments to 70% of the original. The\naveraging of neural network predictions based on resampling with replacement has\nrecently been proposed under the notation "bagging" by Breiman ([Bre94]), who\nhas achieved dramatic.ally improved results in several classification tasks. He also\nnotes, however, that an actual improvement of the prediction can only result if the\nestimation procedure is relatively unstable. As discussed, this is particularly the\ncase for Gaussian mixture training. We therefore expect bagging to be well suited\nfor our task.\n\n5\n\nExperiments and Results\n\nTo assess the practical advantage resulting from regularization, we used the density\nestimates to construct classifiers and compared the resulting prediction accuracies\nusing a toy problem and a real-world problem. The reason is that the generalization error of density estimates in terms of the likelihood based on the test data\nis rather unintuitive whereas performance on a classification problem provides a\ngood impression of the degree of improvement. Assume we have a set of N labeled\ndata z* = {(xk, lk)lk = 1, ... , N}, where lk E Y = {I, ... , C} denotes the class label\nof each input xk . A classifier of new inputs x is yielded by choosing the class I\nwith the maximum posterior class-probability p(llx). The posterior probabilities\nmay be derived from the class-conditional data likelihood p(xll) via Bayes theorem:\np(llx) = p(xll)p(l)/p(x) ex p(xll)p(l) . The resulting partitions ofthe input space are\noptimal for the true p(llx). A viable way to approximate the posterior p(llx) is to\nestimate p(xll) and p(l) from the sample data.\n5.1\n\nToy Problem\n\nIn the toy classification problem the task is to discriminate the two classes of circulatory arranged data shown in figure 3. We generated 200 data points for each class\nand subdivided them into two sets of 100 data points. The first was used for training, the second to test the generalization performance. As a network architecture\nwe chose a Gaussian mixture with 20 units. Table 1 summarizes the results, beginning with the unregularized Gaussian mixture which is followed by the averaging\nand the Bayesian penalty approaches. The three rows for averaging correspond to\nthe results yielded without applying resampling (local max.), with resampling with-\n\nFigure 3: Toy Classification Task.\n\n\x0c547\n\nImproved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms\n\nout replacement (70% subsets), and with resampling with replacement (bagging).\nThe performances on training and test set are measured in terms of the model loglikelihood. Larger values indicate a better performance. We report separate results\nfor dass A and B, since the densities of both were estimated separately. The final\ncolumn shows the prediction accuracy in terms of the percentage of correctly classified data in the test set. We report the average results from 20 experiments. The\nnumbers in brackets denote the standard deviations u of the results. Multiplying u\nwith T19;95%/v\'20 = 0.4680 yields 95% confidence intervals. The best result in each\ncategory is underlined.\nAlgorithm\n\nLog- Likelihood\n\nunreg.\nAveraging:\nlocal max.\n70% subset\nbagging\nPenalty:\n[3 = 0.01\n[3 = 0.02\n[3 = 0.05\n[3 = 0.1\n\nAccuracy\n\nA\n-120.8 (13.3)\n\n-120.4 (10.8)\n\nTest\nA\nB\n-224.9 (32.6) -241.9 (34.1)\n\n-115.6 (6.0)\n-106.8 (5.8)\n-83.8 (4.9)\n\n-112.6 (6.6)\n-105.1 (6.7)\n-83.1 (7.1)\n\n-200.9 (13.9)\n-188.8 (9.5)\n-194.2 (7.3)\n\n-209.1 (16.3)\n-196.4 (11.3)\n-200.1 (11.3)\n\n81.8% (3.1)\n83.2% (2.9)\n82.6% (3.4)\n\n-149.3\n-156.0\n-173.9\n-183.0\n\n-146.5 (5.9)\n-153.0 (4.8)\n-167.0 (15.8)\n-181.9 (21.1)\n\n-186.2\n-177.1\n-182.0\n-184.6\n\n-182.9 (11.6)\n-174.9 (7.0)\n-173.9 (14.3)\n-182.5 (21.1)\n\n83.1%\n84.4%\n81.5%\n78.5%\n\nTraining\n\n(18.5)\n(16.5)\n(24.3)\n(21.9)\n\nB\n\n(13.9)\n(11.8)\n(20.1)\n(21.0)\n\nI\n80.6\'70 (2.8)\n\n(2.9)\n(6.3)\n(5.9)\n(5.1)\n\nTable 1: Performances in the toy classification problem .\nAs expected, all regularization methods outperform the maximum likelihood approach in terms of correct classification. The performance of the Bayesian regularization is hereby very sensitive to the appropriate choice of the regularization\nparameter (3. Optimality of (3 with respect to the density prediction and oytimality\nwith respect to prediction accuracy on the test set roughly coincide (for (3 = 0.02).\nA veraging is inferior to the Bayesian approach if an optimal {3 is chosen.\n5.2\n\nBUPA Liver Disorder Classification\n\nAs a second task we applied our methods to a real-world decision problem from\nthe medical environment. The problem is to detect liver disorders which might\narise from excessive alcohol consumption. Available information consists of five\nblood tests as well as a measure of the patients\' daily alcohol consumption. We\nsubdivided the 345 available samples into a training set of 200 and a test set of 145\nsamples. Due to the relatively few data we did not try to determine the optimal\nregularization parameter using a validation process and will report results on the\ntest set for different parameter values.\nAlgorithm\nunregularized\nBayesian penalty ({3 = 0.05)\nBayesian penalty ?(3 = 0.10)\nBayesian penal ty (3 = 0.20\naveraging local maxima\naveraging (70 % subset)\naveraging (bagging)\n\nAccuracy\n64.8 %\n65.5 %\n66.9 %\n61.4 %\n65 .5 0\n72.4 %\n71.0 %\n\nTable 2: Performances in the liver disorder classification problem.\n\n\x0c548\n\nD. ORMONEIT. V. TRESP\n\nThe results of our experiments are shown in table 2. Again, both regularization\nmethods led to an improvement in prediction accuracy. In contrast to the toy problem, the averaged predictor was superior to the Bayesian approach here. Note that\nthe resampling led to an improvement of more than five percent points compared\nto unresampled averaging.\n\n6\n\nConclusion\n\nWe proposed a Bayesian and an averaging approach to regularize Gaussian mixture\ndensity estimates. In comparison with the maximum likelihood solution both approaches led to considerably improved results as demonstrated using a toy problem\nand a real-world classification task. Interestingly, none of the methods outperformed\nthe other in both tasks. This might be explained with the fact that Gaussian mixture density estimates are particularly unstable in high-dimensional spaces with\nrelatively few data. The benefit of averaging might thus be greater in this case.\nA veraging proved to be particularly effective if applied in connection with resampIing of the training data, which agrees with results in regression and classification\ntasks. If compared to Bayesian regularization, averaging is computationally expensive. On the other hand, Baysian approaches typically require the determination of\nhyper parameters (in our case 13), which is not the case for averaging approaches.\n\nReferences\n[Bre94]\n\nL. Breiman. Bagging predictors. Technical report , UC Berkeley, 1994.\n\n[Bun94]\n\nW . Buntine. Operations for learning with graphical models. Journal of Artificial\nIntelligence Research, 2:159-225, 1994.\n\n[DLR77] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from\nincomplete data via the EM algorithm. J. Royal Statistical Society B, 1977.\n[HT94]\n\nT. Hastie and R. Tibshirani. Discriminant analysis by gaussian mixtures. Technical report , AT&T Bell Labs and University of Toronto, 1994.\n\n[KL95]\n\nN. Kambhatla and T. K. Leen. Classifying with gaussian mixtures and clusters.\nIn Advances in Neural Information Processing Systems 7. Morgan Kaufman,\n1995.\n\n[Mac91]\n\nD. MacKay. Bayesian Modelling and Neural Networks. PhD thesis, California\nInstitute of Technology, Pasadena, 1991.\n\n[Now91] S. J. Nowlan. Soft Competitive Adaption: Neural Network Learning Algorithms\nbased on Fitting Statistical Mixtures. PhD thesis, School of Computer Science,\nCarnegie Mellon University, Pittsburgh, 1991.\n[Orm93] D. Ormoneit. Estimation of probability densities using neural networks. Master\'s\nthesis, Technische Universitiit Munchen, 1993.\n[PC93]\n\nM. P. Perrone and L. N. Cooper. When networks disagree: Ensemble methods for\nhybrid Neural networks. In Neural Networks for Speech and Image Processing.\nChapman Hall, 1993.\n\n[Rob94]\n\nC. P. Robert. The Bayesian Choice. Springer-Verlag, 1994.\n\n[THA93] V. Tresp, J. Hollatz, and S. Ahmad. Network structuring and training using\nrule-based knowledge. In Advances in Neural Information Processing Systems 5.\nMorgan Kaufman, 1993.\n\n\x0c'
p83190
sg535
S'Quadratic-Type Lyapunov Functions for\nCompetitive Neural Networks with\nDifferent Time-Scales\nAnke Meyer-Base\nInstitute of Technical Informatics\nTechnical University of Darmstadt\nDarmstadt, Germany 64283\n\nAbstract\nThe dynamics of complex neural networks modelling the selforganization process in cortical maps must include the aspects of\nlong and short-term memory. The behaviour of the network is such\ncharacterized by an equation of neural activity as a fast phenomenon and an equation of synaptic modification as a slow part of the\nneural system. We present a quadratic-type Lyapunov function for\nthe flow of a competitive neural system with fast and slow dynamic\nvariables. We also show the consequences of the stability analysis\non the neural net parameters.\n\n1\n\nINTRODUCTION\n\nThis paper investigates a special class of laterally inhibited neural networks. In\nparticular, we have examined the dynamics of a restricted class of laterally inhibited\nneural networks from a rigorous analytic standpoint.\nThe network models for retinotopic and somatotopic cortical maps are usually composed of several layers of neurons from sensory receptors to cortical units, with\nfeedforward excitations between the layers and lateral (or recurrent) connection\nwithin the layer. Standard techniques include (1) Hebbian rule and its variations\nfor modifying synaptic efficacies, (2) lateral inhibition for establishing topographical\norganization of the cortex, and (3) adiabatic approximation in decoupling the dynamics of relaxation (which is on the fast time scale) and the dynamics of learning\n(which is on the slow time scale) of the network . However, in most cases, only computer simulation results were obtained and therefore provided limited mathematical\nunderstanding of the self-organizating neural response fields.\nThe networks under study model the dynamics of both the neural activity levels,\n\n\x0cA. MEYER-BASE\n\n338\n\nthe short-term memory (STM), and the dynamics of synaptic modifications, the\nlong-term memory (LTM). The actual network models under consideration may be\nconsidered extensions of Grossberg\'s shunting network [Gr076] or Amari\'s model\nfor primitive neuronal competition [Ama82]. These earlier networks are considered\npools of mutually inhibitory neurons with fixed synaptic connections. Our results\nextended these earlier studies to systems where the synapses can be modified by\nexternal stimuli. The dynamics of competitive systems may be extremely complex,\nexhibiting convergence to point attractors and periodic attractors. For networks\nwhich model only the dynamic of the neural activity levels Cohen and Grossberg\n[CG83] found a Lyapunov function as a necessary condition for the convergence\nbehavior to point attractors.\nIn this paper we apply the results of the theory of Lyapunov functions for singularly\nperturbed systems on large-scale neural networks, which have two types of state\nvariables (LTM and STM) describing the slow and the fast dynamics of the system.\nSo we can find a Lyapunov function for the neural system with different time-scales\nand give a design concept of storing desired pattern as stable equilibrium points.\n\n2\n\nTHE CLASS OF NEURAL NETWORKS WITH\nDIFFERENT TIME-SCALES\n\nThis section defines the network of differential equations characterizing laterally\ninhibited neural networks. We consider a laterally inhibited network with a deterministic signal Hebbian learning law [Heb49] and is similar to the spatiotemporal\nsystem of Amari [Ama83] .\nThe general neural network equations describe the temporal evolution of the STM\n(activity modification) and LTM states (synaptic modification). For the jth neuron\nof aN-neuron network these equations are:\nN\n\nXj\n\n= -ajxj + L\n\nD i j!(Xi )\n\n+ BjSj\n\n(1)\n\ni=l\n\n(2)\n\nwhere Xj is the current activity level, aj is the time constant of the neuron , Bj is\nthe contribution of the external stimulus term, !(Xi) is the neuron\'s output , D ij is\nthe .lateral inhibition term and Yi is the external stimulus. The dynamic variable\nSj represents the synaptic modification state and lyl21 is defined as lyl2 = yTy.\nWe will assume that the input stimuli are normalized vectors of unit magnitude\nlyl2 = 1. These systems will be subject to our analysis considerations regarding the\nstability of their equilibrium points.\n\n3\n\nASYMPTOTIC STABILITY OF NEURAL\nNETWORKS WITH DIFFERENT TIME-SCALES\n\nWe show in this section that it is possible to determine the asymptotic stability of\nthis class of neural networks interpreting them as nonlinear singularly perturbed\nsystems. While singular perturbation theory, a traditional tool of fluid dynamics\nand nonlinear mechanics, embraces a wide variety of dynamic phenomena possesing\nslow and fast modes, we show that singular perturbations are present in many\n\n\x0c339\n\nQuadratic-type Lyapunov Functions for Competitive Neural Networks\n\nneurodynamical problems. In this sense we apply in this paper the results of this\nvaluable analysis tool on the dynamics of laterally inhibited networks.\nIn [SK84] is shown that a quadratic-type Lyapunov function for a singularly perturbed system is obtained as a weighted sum of quadratic-type Lyapunov functions\nof two lower order systems: the so-called reduced and the boundary-layer systems.\nAssuming that each of the two systems is asymptotically stable and has a Lyapunov\nfunction, conditions are derived to guarantee that, for a sufficiently small perturbation parameter, asymptotic stability of the singularly perturbed system can be\nestablished by means of a Lyapunov function which is composed as a weighted sum\nof the Lyapunov functions of the reduced and boundary-layer systems.\nAdopting the notations from [SK84] we will consider the singularly perturbed system 2\n\nx = f(x, y)\n\nx E Bx C R n\n\n(3)\n(4)\n\nWe assume that, in Bx and By, the origin (x = y = 0) is the unique equilibrium point\nand (3) and (4) has a unique solution. A reduced system is defined by setting c = in (3)\nand (4) to obtain\n\n?\n\nx = f(x,y)\n\n(5)\n\nO=g(x,y,O)\n\n(6)\n\nAssuming that in Bx and By, (6) has a unique root y = h(x), the reduced system is\nrewritten as\n\nx = f(x, h(x)) = fr(x)\n\n(7)\n\nA boundary-layer system is defined as\n\nay\naT\n\n(8)\n\n= g(X,y(T),O)\n\nwhere T = tic is a stretching time scale. In (8) the vector x E R n is treated as a fixed\nunknown parameter that takes values in Bx. The aim is to establish the stability properties\nof the singularly perturbed system (3) and (4), for small c, from those of the reduced system\n(7) and the boundary-layer system (8). The Lyapunov functions for system 7 and 8 are of\nquadratic-type. In [SK84] it is shown that under mild assumptions, for sufficiently small\nc, any weighted sum of the Lyapunov functions of the reduced and boundary-layer system\nis a quadratic-type Lyapunov function for the singularly perturbed system (3) and (4).\nThe necessary assumptions are stated now [SK84]:\n1. The reduced system (7) has a Lyapunov function V : R n\n\n-+\n\nR+ such that for all\n\nxE Bx\n(9)\nwhere t/I(x) is a scalar-valued function of x that vanishes at x = 0 and is different\nfrom zero for all other x E Bx. This condition guarantees that x = 0 is an\nasymptotically stable equilibrium point of the reduced system (7).\n2The symbol Bx indicates a closed sphere centered at x = OJ By is defined in the same\nway.\n\n\x0cA. MEYER-BASE\n\n340\n\n2. The boundary-layer system (8) has a Lyapunov function W(x, y) : R n x R m\nR+ such that for all x E Bx and y E By\n\n(\'\\7yW(X,y)fg(X,y , O)::;-0:2??(y-h(x))\n\n0:2>0\n\n->\n\n(10)\n\nwhere ?>(y - h(x)) is a scalar-valued function (y - h(x)) E R m that vanishes\nat y = h(x) and is different from zero for all other x E Bx and y E By. This\ncondition guarantees that y = h(x) is an asymptotically stable equilibrium point\nof the boundary-layer system (8).\n\n3. The following three inequalities hold "Ix E Bx and Vy E By:\n\na.)\n(\'\\7 ,..W(x, y)ff(x, y) ::; C1?>2(y - h(x)) + C21/J(X)?>(Y - h(x))\n\n(11)\n\nb.)\n(\'\\7,.. V(x)f[f(x, y) - f(x, h(x))] ::; /311/J(X)?>(y - h(x))\n\n(12)\n\nc.)\n\n<\n\n(\'\\7yW(x,y)f[g(x,y,()-g(x,y,O)]\n\n(K1?>2(y - h(x))\n\n+\n\n(K21/J(X)?>(Y - h(x)) (13)\n\nThe constants C1, C2, /31 , K1 and K2 are nonnegative. The inequalities above determine the\npermissible interaction between the slow and fast variables. They are basically smoothness\nrequirements of f and g.\n\nAfter these introductory remarks the stability criterion is now stated:\nTheorem: Suppose that conditions 1-3 hold; let d be a positive number such that\n0< d < 1, and let c*(d) be the positive number given by\n\n(14)\n\nwhere Ih = f{2 + G2, \'Y = f{l + Gl , then for all c < c*(d), the origin (x\nis an asymptotically stable equilibrium point of (3) and (.0 and\nv(x, y) = (1 - d)V(x)\n\n+ dW(x, y)\n\n= y = 0)\n(15)\n\nis a Lyapunov function of (3) and (4).\n\nt\n\nIf we put c =\nas a global neural time constant in equation (1) then we have\nto determine two Lyapunov functions: one for the boundary-layer system and the\nother for the reduced-order system.\nIn [CG83] is mentioned a global Lyapunov function for a competitive neural network\nwith only an activation dynamics.\n\n(16)\n\nunder the constraints: mij = mji, ai(xi)\n\n2: 0,\n\nfj(xj)\n\n2: O.\n\nThis Lyapunov-function can be t?aken as one for the boundary-layer system (STMequation) , if the LTM contribution Si is considered as a fixed unknown parameter:\n\n\x0cQuadratic-type Lyapunov Functions for Competitive Neural Networks\nN\n\nW(x, S) = L\nj=l\n\nr\n\n10\n\ni\n\n(Xi\n\nN\n\naj((j)!;((j)d(j-L BjSj\n\n0\n\n10\n\nj=l\n\n341\n\n1 N\nf;((j)d(j-2 L Dij!i(Xj)!k(Xk)\nj=l\n\n0\n\n(17)\n\nFor the reduced-order system (LTM- equation) we can take as a Lyapunov-function:\nN\n\nV(S)\n\n= ~STS = L S;\n\n(18)\n\ni=l\n\nThe Lyapunov-function for the coupled STM and LTM dynamics is the sum of the\ntwo Lyapunov-function:\n\nvex, S)\n\n4\n\n= (1 -\n\nd)V(S)\n\n+ dW(x, S)\n\n(19)\n\nDESIGN OF STABLE COMPETITIVE NEURAL\nNETWORKS\n\nCompetitive neural networks with learning rules have moving equilibria during the\nlearning process. The concept of asymptotic stability derived from matrix perturbation theory can capture this phenomenon.\nWe design in this section a competitive neural network that is able to store a desired\npattern as a stable equilibrium.\nThe theoretical implications are illustrated in an example of a two neuron network .\nExample: Let N = 2, ai = A, B j = B, Dii = a > 0, Dij = -(3\nnonlinearity be a linear function f(xj) = Xj in equations (1) and (2).\n\n<\n\n0 and the\n\nWe get for the boundary-layer system:\nN\n\nXj\n\n= -Axj + L\n\nDijf(xd + BSj\n\n(20)\n\ni=l\n\nand for the reduced-order system:\n\n.\n\nB\nlA-a\n\nC\nA-a\n\nS? = S ? [ - - -1] - - J\n\n(21)\n\nThen we get for the Lyapunov-functions:\n(22)\nand\n(23)\n\n\x0cA. MEYER-BASE\n\n342\n\n-0.2\n.\n\n\\\n\\\n\n-0.4\n[JJ\n\nOJ\n.IJ\n\nlIS\n\n.IJ\n[JJ\n\n-0.6\n\n/\n\n~\nU)\n\n\\J\n\n-0.8\n\n-1\n\n-1.2\n\n~\n\no\n\n__\n\n~\n\n__\n\n~\n\n1\n\n2\n\n____ __ __- L__\n3\n4\n5\ntime in msec\n~\n\n~\n\n~~\n\n6\n\n__\n\n~\n\n__- L__\n\n7\n\n~~~\n\n8\n\n9\n\n10\n\nFigure 1: Time histories of the neural network with the origin as an equilibrium\npoint: STM states.\nFor the nonnegative constants we get: al = 1 - A~a\' a2\nwith B < 0 , and C2 = i3l = i32 = 1 and I<l = I<2 = O.\n\n= (A -\n\na)2,\n\nCl\n\n= \'Y = - B,\n\nWe get some interesting implications from the above results as: A-a> B , A-a> 0\nand B < o.\nThe above impications can be interpreted as follows: To achieve a stable equilibrium\npoint (0,0) we should have a negative contribution of the external stimulus term\nand the sum of the excitatory and inhibitory contribution of the neurons should\nbe less than the time constant of a neuron. An evolution of the trajectories of the\nSTM and LTM states for a two neuron system is shown in figure 1 and 2. The\nSTM states exhibit first an oscillation from the expected equilibrium point, while\nthe LTM states reach monotonically the equilibrium point. We can see from the\npictures that the equilibrium point (0,0) is reached after 5 msec by the STM- and\nLTM-states.\n\n= 55+ll.of\n.\n.d(1-d)\nFrom the above formula we can see that f*(d) has a maximum at d = d* = 0.5.\nChoosing B = -5, A\n\n5\n\n= 1 and a = 0.5 we obtain for\n\nf*(d) : f*(d)\n\nCONCLUSIONS\n\nWe presented in this paper a quadratic-type Lyapunov function for analyzing the\nstability of equilibrium points of competitive neural networks with fast and slow\ndynamics. This global stability analysis method is interpreting neural networks\nas nonlinear singularly perturbed systems. The equilibrium point is constrained\nto a neighborhood of (0,0). This technique supposes a monotonically increasing\nnon-linearity and a symmetric lateral inhibition matrix. The learning rule is a\ndeterministic Hebbian. This method gives an upper bound on the perturbation\n\n\x0c343\n\nQuadratic-type Lyapunov Functions for Competitive Neural Networks\n0.6\n\n~--~--~----~--~--~----~--\'---~--~r---.\n\n0.5\n\n0.4\nIII\n\n<II\n\n.j.J\n\nIII\n\n.j.J\n\nIII\n\n0.3\n\n~\n~\n0.2\n\n0.1\n\no L-__\n1\no\n\n~~~~~~~\n\n2\n\n3\n\n____~__~__~__- L_ _~\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\ntime in msec\n\nFigure 2: Time histories of the neural network with the origin as an equilibrium\npoint: LTM states.\nparameter and such an estimation of a maximal positive neural time-constant. The\npractical implication ofthe theoretical problem is the design of a competitive neural\nnetwork that is able to store a desired pattern as a stable equilibrium.\n\nReferences\n[Ama82] S. Amari. Competitive and cooperative aspects in dynamics of neural excitation and self-organization. Competition and cooperation in neural networks, 20:1-28, 7 1982.\n[Ama83] S. Amari. Field theory of self-organizing neural nets. IEEE Transactions\non systems, machines and communication, SMC-13:741-748, 7 1983.\nA. M. Cohen und S. Grossberg. Absolute Stability of Global Pattern Formation and Parallel Memory Storage by Competitive Neural Networks.\nIEEE Transactions on Systems, Man and Cybernetics, SMC-13:815-826,\n9 1983.\n[Gro76] S. Grossberg. Adaptive Pattern Classification and Universal Recording.\nBiological Cybernetics, 23:121-134, 1 1976.\n\n[CG83]\n\n[Heb49] D. O. Hebb. The Organization of Behavior. J. Wiley Verlag, 1949.\n[SK84] Ali Saberi und Hassan Khalil. Quadratic-Type Lyapunov Functions for\nSingularly Perturbed Systems. IEEE Transactions on A utomatic Control,\npp. 542-550, June 1984.\n\n\x0c'
p83191
sg223
S'Is Learning The n-th Thing Any Easier Than\nLearning The First?\n\nSebastian Thrun I\nComputer Science Department\nCarnegie Mellon University\nPittsburgh, PA 15213-3891\nWorld Wide Web: http://www.cs.cmu.edul\'\'\'thrun\n\nAbstract\nThis paper investigates learning in a lifelong context. Lifelong learning\naddresses situations in which a learner faces a whole stream of learning tasks. Such scenarios provide the opportunity to transfer knowledge\nacross multiple learning tasks, in order to generalize more accurately from\nless training data. In this paper, several different approaches to lifelong\nlearning are described, and applied in an object recognition domain. It\nis shown that across the board, lifelong learning approaches generalize\nconsistently more accurately from less training data, by their ability to\ntransfer knowledge across learning tasks.\n\n1 Introduction\nSupervised learning is concerned with approximating an unknown function based on examples. Virtually all current approaches to supervised learning assume that one is given a set\nof input-output examples, denoted by X, which characterize an unknown function, denoted\nby f. The target function f is drawn from a class of functions, F, and the learner is given a\nspace of hypotheses, denoted by H, and an order (preference/prior) with which it considers\nthem during learning. For example, H might be the space of functions represented by an\nartificial neural network with different weight vectors.\nWhile this formulation establishes a rigid framework for research in machine learning, it\ndismisses important aspects that are essential for human learning. Psychological studies\nhave shown that humans often employ more than just the training data for generalization.\nThey are often able to generalize correctly even from a single training example [2, 10]. One\nof the key aspects of the learning problem faced by humans, which differs from the vast\nmajority of problems studied in the field of neural network learning, is the fact that humans\nencounter a whole stream of learning problems over their entire lifetime. When faced with\na new thing to learn, humans can usually exploit an enormous amount of training data and\nI also\n\naffiliated with: Institut fur Informatik III, Universitat Bonn, Romerstr. 164, Germany\n\n\x0cIs Learning the n-th Thing Any Easier Than Learning the First?\n\n641\n\nexperiences that stem from other, related learning tasks. For example, when learning to drive\na car, years of learning experience with basic motor skills, typical traffic patterns, logical\nreasoning, language and much more precede and influence this learning task. The transfer of\nknowledge across learning tasks seems to play an essential role for generalizing accurately,\nparticularly when training data is scarce.\nA framework for the study of the transfer of knowledge is the lifelong learning framework.\nIn this framework, it is assumed that a learner faces a whole collection of learning problems\nover its entire lifetime. Such a scenario opens the opportunity for synergy. When facing its\nn-th learning task, a learner can re-use knowledge gathered in its previous n - 1 learning\ntasks to boost the generalization accuracy.\nIn this paper we will be interested in the most simple version of the lifelong learning problem,\nin which the learner faces a family of concept learning tasks. More specifically, the functions\nto be learned over the lifetime of the learner, denoted by 11 , 12 , 13, .. . E F , are all of the type\nI : I --+ {O, I} and sampled from F. Each function I E {II , h ,13, . . .} is an indicator\nfunction that defines a particular concept: a pattern x E I is member of this concept if\nand only if I(x) = 1. When learning the n-th indicator function, In , the training set X\ncontains examples of the type (x , In(x)) (which may be distorted by noise). In addition to\nthe training set, the learner is also given n - 1 sets of examples of other concept functions,\ndenoted by Xk (k\n1, .. . , n - I). Each Xk contains training examples that characterize\nIk. Since this additional data is desired to support learning In, Xk is called a support set\nfor the training set X .\n\n=\n\nAn example of the above is the recognition of faces [5, 7]. When learning to recognize the\nn-th person, say IBob, the learner is given a set of positive and negative example of face\nimages of this person. In lifelong learning, it may also exploit training information stemming\nfrom other persons, such as I E {/Rieh, IMike , IDave , ... }. The support sets usually cannot be\nused directly as training patterns when learning a new concept, since they describe different\nconcepts (hence have different class labels). However, certain features (like the shape of the\neyes) are more important than others (like the facial expression, or the location of the face\nwithin the image). Once the invariances of the domain are learned, they can be transferred\nto new learning tasks (new people) and hence improve generalization.\nTo illustrate the potential importance of related learning tasks in lifelong learning, this\npaper does not present just one particular approach to the transfer of knowledge. Instead,\nit describes several, all of which extend conventional memory-based or neural network\nalgorithms. These approaches are compared with more traditional learning algorithms, i.e.,\nthose that do not transfer knowledge. The goal of this research is to demonstrate that,\nindependent of a particular learning approach, more complex functions can be learned from\nless training data iflearning is embedded into a lifelong context.\n\n2 Memory-Based Learning Approaches\nMemory-based algorithms memorize all training examples explicitly and interpolate them\nat query-time. We will first sketch two simple, well-known approaches to memory-based\nlearning, then propose extensions that take the support sets into account.\n\n2.1\n\nNearest Neighbor and Shepard\'s Method\n\nProbably the most widely used memory-based learning algorithm is\n\nJ{ -nearest\n\nneighbor\n\n(KNN) [15]. Suppose x is a query pattern, for which we would like to know the output y .\n\nKNN searches the set of training examples X for those J{ examples (Xi, Yi ) E X whose\ninput patterns Xi are nearest to X (according to some distance metric, e.g., the Euclidian\ndistance). It then returns the mean output value 2:= Yi of these nearest neighbors.\n\nk\n\nAnother commonly used method, which is due to Shepard [13], averages the output values\n\n\x0cs. THRUN\n\n642\n\nof all training examples but weights each example according to the inverse distance to the\nquery\n\n:~~~t x.\n\n(\n\nL\n\n(x"y.)EX\n\n)\n\nIlx -\n\n~: II + E ?\n\n(\n\nL\n\n(x. ,y.)EX\n\nIlx -\n\nI)\n\nXi\n\n-I\n\n(1)\n\nII + E\n\nHere E > 0 is a small constant that prevents division by zero. Plain memory-based learning\nuses exclusively the training set X for learning. There is no obvious way to incorporate the\nsupport sets, since they carry the wrong class labels.\n2.2\n\nLearning A New Representation\n\nThe first modification of memory-based learning proposed in this paper employs the support\nsets to learn a new representation of the data. More specifically, the support sets are employed\nto learn a function, denoted by 9 : I --+ I\', which maps input patterns in I to a new space,\nI\' . This new space I\' forms the input space for a memory-based algorithm.\nObviously, the key property of a good data representations is that multiple examples of a\nsingle concept should have a similar representation, whereas the representation of an example\nand a counterexample of a concept should be more different. This property can directly be\ntransformed into an energy function for g:\n\n~ (X,y~EXk (X"y~EXk Ilg(x)-g(x\')11\n\nn-I\n\nE:=\n\n(\n\n(X"y~EXk Ilg( x )-g(x\')11\n\n)\n\n(2)\n\nAdjusting 9 to minimize E forces the distance between pairs of examples of the same\nconcept to be small, and the distance between an example and a counterexample of a concept\nto be large. In our implementation, 9 is realized by a neural network and trained using the\nBack-Propagation algorithm [12].\nNotice that the new representation, g, is obtained through the support sets. Assuming that\nthe learned representation is appropriate for new learning tasks, standard memory-based\nlearning can be applied using this new representation when learning the n-th concept.\n2.3\n\nLearning A Distance Function\n\nAn alternative way for exploiting support sets to improve memory-based learning is to learn\na distance function [3, 9]. This approach learns a function d : I x I --+ [0, I] which accepts\ntwo input patterns, say x and x\' , and outputs whether x and x\' are members of the same\nconcept, regardless what the concept is. Training examples for d are\n\n(( x , x\'),I)\n((x, x\'), 0)\n\nify=y\'=l\n\nif(y=IAy\'=O)or(y=OAy\'=I).\nThey are derived from pairs of examples (x , y) , (x\', y\') E Xk taken from a single support\nset X k (k = 1, . .. , n - I). In our implementation, d is an artificial neural network trained\nwith Back-Propagation. Notice that the training examples for d lack information concerning\nthe concept for which they were originally derived. Hence, all support sets can be used to\ntrain d. After training, d can be interpreted as the probability that two patterns x, x\' E I are\nexamples of the same concept.\nOnce trained, d can be used as a generalized distance function for a memory-based approach.\nSuppose one is given a training set X and a query point x E I. Then, for each positive\nexample (x\' , y\' = I) EX , d( x , x\') can be interpreted as the probability that x is a member\nof the target concept. Votes from multiple positive examples (XI, I) , (X2\' I), ... E X are\ncombined using Bayes\' rule, yielding\n\nProb(fn(x)=I)\n\n.-\n\n1-\n\n(I\n\n+\n\nII\n(x\' ,y\'=I)EXk\n\nI:(~(::~,))-I\n\n(3)\n\n\x0cIs Learning the n-th Thing Any Easier Than Learning the First?\n\n643\n\nNotice that d is not a distance metric. It generalizes the notion of a distance metric, because\nthe triangle inequality needs not hold, and because an example of the target concept x\' can\nprovide evidence that x is not a member of that concept (if d(x, x\') < 0.5).\n\n3 Neural Network Approaches\nTo make our comparison more complete, we will now briefly describe approaches that rely\nexclusively on artificial neural networks for learning In.\n\n3.1\n\nBack-Propagation\n\nStandard Back-Propagation can be used to learn the indicator function In, using X as training\nset. This approach does not employ the support sets, hence is unable to transfer knowledge\nacross learning tasks.\n\n3.2 Learning With Hints\nLearning with hints [1, 4, 6, 16] constructs a neural network with n output units, one for\neach function Ik (k = 1,2, .. . , n). This network is then trained to simultaneously minimize\nthe error on both the support sets {Xk} and the training set X. By doing so, the internal\nrepresentation of this network is not only determined by X but also shaped through the\nsupport sets {X k }. If similar internal representations are required for al1 functions Ik\n(k\n1,2, .. . , n), the support sets provide additional training examples for the internal\nrepresentation.\n\n=\n\n3.3 Explanation-Based Neural Network Learning\nThe last method described here uses the explanation-based neural network learning algorithm (EBNN), which was original1y proposed in the context of reinforcement learning\n[8, 17]. EBNN trains an artificial neural network, denoted by h : I ----+ [0, 1], just like\nBack-Propagation. However, in addition to the target values given by the training set X,\nEBNN estimates the slopes (tangents) of the target function In for each example in X. More\nspecifically, training examples in EBNN are of the sort (x, In (x), \\7 xln(x)), which are fit\nusing the Tangent-Prop algorithm [14]. The input x and target value In(x) are taken from\nthe trai ning set X. The third term, the slope \\7 xln ( X ), is estimated using the learned distance\nfunction d described above. Suppose (x\', y\'\n1) E X is a (positive) training example.\nThen, the function d x \' : I ----+ [0, 1] with d x \' (z) := d(z , x\') maps a single input pattern to\n[0, 1], and is an approximation to In. Since d( z, x\') is represented by a neural network and\nneural networks are differentiable, the gradient 8dx \' (z) /8z is an estimate of the slope of In\nat z. Setting z := x yields the desired estimate of \\7 xln (x) . As stated above, both the target\nvalue In (x) and the slope vector \\7 xIn (x) are fit using the Tangent-Prop algorithm for each\ntraining example x EX .\n\n=\n\nThe slope \\7 xln provides additional information about the target function In. Since d is\nlearned using the support sets, EBNN approach transfers knowledge from the support sets\nto the new learning task. EBNN relies on the assumption that d is accurate enough to yield\nhelpful sensitivity information. However, since EBNN fits both training patterns (values)\nand slopes, misleading slopes can be overridden by training examples. See [17] for a more\ndetailed description of EBNN and further references.\n\n4 Experimental Results\nAll approaches were tested using a database of color camera images of different objects\n(see Fig. 3.3). Each of the object in the database has a distinct color or size. The n-th\n\n\x0c644\nl1\n\nS. THRUN\n\n....\n\nI\'t\'\n\n\'I\n\n\'I\n\n?\n\n:.\n\n<\n\n,\n\n~~~\n\n>\n-~""":::.~\n\n--....\n\n\'"\n\n.\n~\n\n~\n\n-,:~~,}\n\nI\n\n1:1 ,I\n\n......\n\n\'\n\n~..\n\n.\n\n,\n?\n\n~\n\n?.~ <.~\n~\n\n<.-\n\n~~-\n\n...\n\n:t\n-_,1-\n\n"\n~\n\n~,-l/> ;\' ;\'j III\n\'1 ~\' \'\'\',ll\n\nt!\n\n~[~\n\n,,-\n\n,\n\n~_\n\n~\n\n~ ~_l_~\nII\n\n-\n\n-...\n__\n\nE~\n\n_e?m;,\n\n\'~~\n;1 ~\n\nt\n\n~,~,AA(\n\n.d!t~)ltI!{iH-""\n\n,\n\n,\n\n,\n~\n\nc-\n\n_.\n\nML~._. . ,\n\n:R;1-;\n\nI\n\n\'\'\'!!!i!~,\n\n=\'\n\n;~~~\n\n"\n\n,\n\n""111\':\'i, It r\nf4~\n\n,\n\nFigure 1: The support sets were compiled out of a hundred\nimages of a bottle, a\nhat, a hammer, a coke\ncan, and a book. The\nn-th learning tasks\ninvolves distinguishing the shoe from the\nsunglasses. Images\nwere subsampled to\na 100x 100 pixel matrix (each pixel has a\ncolor, saturation, and\na brightness value),\nshown on the right\nside.\n\nlearning task was the recognition of one of these objects, namely the shoe. The previous\nn - 1 learning tasks correspond to the recognition of five other objects, namely the bottle,\nthe hat, the hammer, the coke can, and the book. To ensure that the latter images could\nnot be used simply as additional training data for In, the only counterexamples of the shoe\nwas the seventh object, the sunglasses. Hence, the training set for In contained images of\nthe shoe and the sunglasses, and the support sets contained images of the other five objects.\nThe object recognition domain is a good testbed for the transfer of knowledge in lifelong\nlearning. This is because finding a good approximation to In involves recognizing the target\nobject invariant of rotation, translation, scaling in size, change of lighting and so on. Since\nthese invariances are common to all object recognition tasks, images showing other objects\ncan provide additional information and boost the generalization accuracy.\nTransfer of knowledge is most important when training data is scarce. Hence, in an initial\nexperiment we tested all methods using a single image of the shoe and the sunglasses only.\nThose methods that are able to transfer knowledge were also provided 100 images of each\nof the other five objects. The results are intriguing. The generalization accuracies\n\nKNN\n\nShepard\n\n60.4%\n?8.3%\n\n60.4%\n?8.3%\n\nrepro g+Shep.\n74.4%\n?18.5%\n\ndistanced\n75.2%\n?18.9%\n\nBack-Prop\n\nhints\n\nEBNN\n\n59.7%\n?9.0%\n\n62.1%\n?10.2%\n\n74.8%\n?11.1%\n\nillustrate that all approaches that transfer knowledge (printed in bold font) generalize significantly better than those that do not. With the exception of the hint learning technique,\nthe approaches can be grouped into two categories: Those which generalize approximately\n60% of the testing set correctly, and those which achieve approximately 75% generalization accuracy. The former group contains the standard supervised learning algorithms, and\nthe latter contains the "new" algorithms proposed here, which are capable of transferring\nknOWledge. The differences within each group are statistically not significant, while the\ndifferences between them are (at the 95% level). Notice that random guessing classifies 50%\nof the testing examples correctly.\nThese results suggest that the generalization accuracy merely depends on the particular\nchoice of the learning algorithm (memory-based vs. neural networks). Instead, the main\nfactor determining the generalization accuracy is the fact whether or not knowledge is\ntransferred from past learning tasks.\n\n\x0cIs Learning the n-th Thing Any Easier Than Learning the First?\n95%\n\n645\n\n95%\n\ndistance function d\n85%\n~\n\n~\n-\n\n80%\n\nhepard \'s method with representation g\n\n15%\n70%\n\n, ,,\'~.\n\n.</\'~\n70%\n\nShepard\'s method\n\n65%\n\n65%\n\n60%\n\n60% ;;./\n\n55%\n\n55%\n\n50%~2--~~----~10~~1~2~1~4--1~6--1~.--~20\ntraining example.\n\n/f\n\nif\n\n.\n\nBack-Propagauon\n\n~%~2--~~----~1~O~1~2--1~4--1~6--~1B--~20\ntraining exampletl\n\nFigure 2: Generalization accuracy as a function of training examples, measured on an\nindependent test set and averaged over 100 experiments. 95%-confidence bars are also\ndisplayed.\nWhat happens as more training data arrives? Fig. 2 shows generalization curves with\nincreasing numbers of training examples for some of these methods. As the number of\ntraining examples increases, prior knowledge becomes less important. After presenting 20\ntraining examples, the results\n\nKNN\n81.0%\n?3.4%\n\nShepard\n70.5%\n?4.9%\n\nrepro g+Shep.\n81.7%\n?2.7%\n\ndistance d\n87.3%\n?O_9%\n\nBack-Prop\n88.4%\n?2.5%\n\nhints\nn_avail.\n\nEBNN\n\n90.8%\n?2.7%\n\nillustrate that some of the standard methods (especially Back-Propagation) generalize about\nas accurately as those methods that exploit support sets. Here the differences in the underlying\nlearning mechanisms becomes more dominant. However, when comparing lifelong learning\nmethods with their corresponding standard approaches, the latter ones are stiIl inferior: BackPropagation (88.4%) is outperformed by EBNN (90.8%), and Shepard\'s method (70.5%)\ngeneralizes less accurately when the representation is learned (81.7%) or when the distance\nfunction is learned (87.3%). All these differences are significant at the 95% confidence level.\n\n5\n\nDiscussion\n\nThe experimental results reported in this paper provide evidence that learning becomes easier\nwhen embedded in a lifelong learning context. By transferring knowledge across related\nlearning tasks, a learner can become "more experienced" and generalize better. To test\nthis conjecture in a more systematic way, a variety of learning approaches were evaluated\nand compared with methods that are unable to transfer knowledge. It is consistently found\nthat lifelong learning algorithms generalize significantly more accurately, particularly when\ntraining data is scarce.\nNotice that these results are well in tune with other results obtained by the author. One of\nthe approaches here, EBNN, has extensively been studied in the context of robot perception\n[11], reinforcement learning for robot control, and chess [17]. In all these domains, it has\nconsistently been found to generalize better from less training data by transferring knowledge\nfrom previous learning tasks. The results are also consistent with observations made about\nhuman learning [2, 10], namely that previously learned knowledge plays an important role\nin generalization, particularly when training data is scarce. [18] extends these techniques to\nsituations where most support sets are not related.w\nHowever, lifelong learning rests on the assumption that more than a single task is to be\nlearned, and that learning tasks are appropriately related. Lifelong learning algorithms\nare particularly well-suited in domains where the costs of collecting training data is the\ndominating factor in learning, since these costs can be amortized over several learning tasks.\nSuch domains include, for example, autonomous service robots which are to learn and\nimprove over their entire lifetime. They include personal software assistants which have\n\n\x0c646\n\nS. THRUN\n\nto perform various tasks for various users. Pattern recognition, speech recognition, time\nseries prediction, and database mining might be other, potential application domains for the\ntechniques presented here.\n\nReferences\n[1] Y. S. Abu-Mostafa. Learning from hints in neural networks. Journal of Complexity, 6: 192-198,\n1990.\n[2] W-K. Ahn and W F. Brewer. Psychological studies of explanation-based learning. In\nG. Dejong, editor, Investigating Explanation-Based Learning . Kluwer Academic Publishers,\nBostonlDordrechtILondon, 1993.\n[3] c. A. Atkeson. Using locally weighted regression for robot learning. In Proceedings of the 1991\n1EEE International Conference on Robotics and Automation, pages 958-962, Sacramento, CA,\nApril 1991.\n[4] J. Baxter. Learning internal representations. In Proceedings of the Conference on Computation\nLearning Theory, 1995.\n[5] D. Beymer and T. Poggio. Face recognition from one model view. In Proceedings of the\nInternational Conference on Computer Vision, 1995.\n[6] R. Caruana. MuItitask learning: A knowledge-based of source of inductive bias. In P. E. Utgoff,\neditor, Proceedings of the Tenth International Conference on Machine Learning, pages 41-48,\nSan Mateo, CA, 1993. Morgan Kaufmann.\n[7] M. Lando and S. Edelman. Generalizing from a single view in face recognition. Technical Report\nCS-TR 95-02, Department of Applied Mathematics and Computer Science, The Weizmann\nInstitute of Science, Rehovot 76100, Israel, January 1995.\n[8] T. M. Mitchell and S. Thrun. Explanation-based neural network learning for robot control. In\nS. J. Hanson, J. Cowan, and C. L. Giles, editors, Advances in Neural Information Processing\nSystems 5, pages 287-294, San Mateo, CA, 1993. Morgan Kaufmann.\n[9] A. W Moore, D. 1. Hill, and M. P. Johnson. An Empirical Investigation of Brute Force to choose\nFeatures, Smoothers and Function Approximators. In S. Hanson, S. Judd, and T. Petsche, editors,\nComputational Learning Theory and Natural Learning Systems, Volume 3. MIT Press, 1992.\n[10] Y. Moses, S. Ullman, and S. Edelman. Generalization across changes in illumination and viewing\nposition in upright and inverted faces. Technical Report CS-TR 93-14, Department of Applied\nMathematics and Computer Science, The Weizmann Institute of Science, Rehovot 76100, Israel,\n1993.\n[11] J. O\'Sullivan, T. M. Mitchell, and S. Thrun. Explanation-based neural network learning from\nmobile robot perception. In K. Ikeuchi and M. Veloso, editors, Symbolic Visual Learning. Oxford\nUniversity Press, 1995.\n[12] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations by error\npropagation. In D. E. Rumelhart and 1. L. McClelland, editors, Parallel Distributed Processing.\nVol. I + II. MIT Press, 1986.\n[13] D. Shepard. A two-dimensional interpolation function for irregularly spaced data. In 23rd\nNational Conference ACM, pages 517-523, 1968.\n[14] P. Simard, B. Victorri, Y. LeCun, and J. Denker. Tangent prop - a formalism for specifying\nselected invariances in an adaptive network. In 1. E. Moody, S. J. Hanson, and R. P. Lippmann,\neditors, Advances in Neural Information Processing Systems 4, pages 895-903, San Mateo, CA,\n1992. Morgan Kaufmann.\n[15] c. Stanfill and D. Waltz. Towards memory-based reasoning. Communications of the ACM,\n29(12): 1213-1228, December 1986.\n[16] S. C. Suddarth and A. Holden. Symbolic neural systems and the use of hints for developing\ncomplex systems. International Journal of Machine Studies, 35, 1991.\n[17] S. Thrun. Explanation-Based Neural Network Learning: A Lifelong Learning Approach. Kluwer\nAcademic Publishers, Boston, MA, 1996. to appear.\n[18] S. Thrun and J. O\'Sullivan. Clustering learning tasks and the selective cross-task transfer\nof knowledge. Technical Report CMU-CS-95-209, Carnegie Mellon University, School of\nComputer Science, Pittsburgh, PA 15213, November 1995.\n\n\x0c'
p83192
sg350
S'A Dynamical Model of Context Dependencies for the\nVestibulo-Ocular Reflex\nTerrence J. Sejnowskit\n\nOlivier J.M.D. Coenen*\n\nComputational Neurobiology Laboratory\nHoward Hughes Medical Institute\nThe Salk Institute for Biological Studies\n10010 North Torrey Pines Road\nLa Jolla, CA 92037, U.S.A.\nDepartments oftBiology and *tPhysics\nUniversity of California, San Diego\nLa Jolla, CA 92093, U.S.A\n\n{olivier,terry}@salk.edu\n\nAbstract\nThe vestibulo-ocular reflex (VOR) stabilizes images on the retina during rapid\nhead motions. The gain of the VOR (the ratio of eye to head rotation velocity)\nis typically around -1 when the eyes are focused on a distant target. However, to\nstabilize images accurately, the VOR gain must vary with context (eye position,\neye vergence and head translation). We first describe a kinematic model of the\nVOR which relies solely on sensory information available from the semicircular\ncanals (head rotation), the otoliths (head translation), and neural correlates of eye\nposition and vergence angle. We then propose a dynamical model and compare it\nto the eye velocity responses measured in monkeys. The dynamical model reproduces the observed amplitude and time course of the modulation of the VOR and\nsuggests one way to combine the required neural signals within the cerebellum and\nthe brain stem. It also makes predictions for the responses of neurons to multiple\ninputs (head rotation and translation, eye position, etc.) in the oculomotor system.\n\n1 Introduction\nThe VOR stabilizes images on the retina during rapid head motions: Rotations and translations of\nthe head in three dimensions must be compensated by appropriate rotations of the eye. Because the\nhead\'s rotation axis is not the same as the eye\'s rotation axis, the calculations for proper image stabilization of an object must take into account diverse variables such as object distance from each eye,\n\n\x0cO. J. M. D. COENEN, T. J. SEJNOWSKI\n\n90\n\ngaze direction, and head translation (Viire et al., 1986). The stabilization is achieved by integrating\ninfonnation from different sources: head rotations from the semicircular canals of the inner ear, head\ntranslations from the otolith organs, eye positions, viewing distance, as well as other context infonnation, such as posture (head tilts) or activity (walking, running) (Snyder and King, 1992; Shelhamer\net al.,1992; Grossman et al., 1989). In this paper we concentrate on the context modulation of the\nVOR which can be described by the kinematics of the reflex, i.e. eye position, eye vergence and\nhead translation.\n\n2\n\nThe Vestibulo-Ocular Reflex: Kinematic Model\nDefinition of Vectors\n\nTarget Object\n\nCoordinate System\n\nGaze Vector\n\nGaze Angle\nInterocular\nDistance\nEye position\nVector\n\nRotation Axis\n\nSemicircular\nCanals and\nOtoliths\nHead\nTop View\n\n?\n\n~_--+_\n\nOrigin of coordinate\nsyste,,-, (arbitrary)\n\nFigure 1: Diagram showing the definition of the vectors used in the equation of the kinematic model of the\nvestibulo-ocular reflex.\n\nThe ideal VOR response is a compensatory eye movement which keeps the image fixed on the retina\nfor any head rotations and translations. We therefore derived an equation for the eye rotation velocity\nby requiring that a target remains stationary on the retina. The velocity of the resulting compensatory\neye rotation can be written as (see fig. 1):\n\nw= -Oe + 1:1\n\nx [Dej x\n\nOe - To;]\n\n(1)\n\nwhere Oe is the head rotation velocity sensed by the semicircular canals, TOj is the head translation\nvelocity sensed by the otoliths, Dej == (e - OJ), eis a constant vector specifying the location of an\neye in the head, OJ is the position of either the left or right otolith, fJ and Igl are the unit vector and\namplitude of the gaze vector: fJ gives the eye position (orientation of the eye relative to the head),\nand Igl gives the distance from the eye to the object, and the symbol x indicates the cross-product\nbetween two vectors. wand Oe are rotation vectors which describe the instantaneous angUlar velocity\nof the eye and head, respectively. A rotation vector lies along the instantaneous axis of rotation;\nits magnitude indicates the speed of rotation around the axis, and its direction is given by the righthand screw rule. A motion of the head combining rotation (0) and translation (T) is sensed as the\ncombination of a rotation velocity Oe measured by the semicircular canals and a translation velocity\nTo sensed by the otoliths. The rotation vectors are equal (0 = Oe), and the translation velocity vector\nas measured by the otoliths is given by: TOj = OOj x 0 + T, where OOj == (a - OJ), and a is the\nposition vector of the axis of rotation.\n\n\x0c91\n\nA Dynarnical Model of Context Dependencies for the Vestibula-Ocular Reflex\n\nThe special case where the gaze is horizontal and the rotation vector is vertical (horizontal head rotation) has been studied extensively in the literature. We used this special case in the sirnulations.\nIn that case rnay be sirnplify by writing its equation with dot products. Since 9 and\nare then\nperpendicular (9 . fie = 0). the first term of the following expression in brackets is zero:\n\nw\n\nslc\n\n(2)\n\nThe sernicircular canals decornpose and report acceleration and velocity of head rotation fi by its\ncornponents along the three canals on each side of the head fie : horizontal. anterior and posterior.\nThe two otolith organs on each side report the dynamical inertial forces generated during linear rnotion (translation) in two perpendicular plane. one vertical and the other horizontal relative to the head.\nHere we assurne that a translation velocity signal (To) derived frorn or reported by the otolith afferents is available. The otoliths encode as well the head orientation relative to the gravity vector force.\nbut was not included in this study.\nTo cornplete the correspondence between the equation and a neural correlate. we need to determine\nThe eye position 9 is assurned to be given by the output of the\na physiological source for 9 and\nvelocity-to-position transformation or so-called "neural integrator" which provides eye position information and which is necessary for the activation of the rnotoneuron to sustain the eye in a fixed\nposition. The integrator for horizontal eye position appears to be located in the nucleus prepositus\nhypoglossi in the pons. and the vertical integrator in the rnidbrain interstitial nucleus of Cajal. (Crawford. Cadera and Vilis. 1991; Cannon and Robinson. 1987). We assurne that the eye position is given\nas the coordinates of the unit vector 9 along the ~ and 1; of fig. 1. The eye position depends on the\neye velocity according to\n= 9 x w. For the special case w(t) = w(t)z. i.e. for horizontal head\nrotation. the eye position coordinates are given by:\n\nI!I.\n\n\'*\n\n91 (t) =\n\n91 (0) + f~ iJ2( r )w( r) dr\n\n92(t) =\n\n92(0) - f~ 91(r)w(r)dr\n\n(3)\n\nThis is a set of two negatively coupled integrators. The "neural integrator" therefore does not integrate the eye velocity directly but a product of eye position and eye velocity. The distance frorn eye\nto target\ncan be written using the gaze angles in the horizontal plane of the head:\n\nI!I\n\n1\n\n(4)\n\n1\n\n(5)\n\nRight eye:\n\n19RT\n\nLeft eye:\n\n19LT\n\nwhere ?()R - () L) is the vergence angle. and I is the interocular distance; the angles are rneasured frorn\na straight ahead gaze. and take on negative values when the eyes are turned towards the right. Within\nthe oculornotor systern. the vergence angle and speed are encoded by the rnesencephalic reticular\nformation neurons (Judge and Curnrning. 1986; Mays. 1984). The nucleus reticularis tegrnenti pontis\nwith reciprocal connections to the flocculus. oculornotor vermis. paravermis of the cerebellurn also\ncontains neurons which activity varies linearly with vergence angle (Gamlin and Clarke. 1995).\nWe conclude that it is possible to perform the cornputations needed to obtain an ideal VOR with signals known to be available physiologically.\n\n\x0cO. J. M. D. COENEN, T. J. SEJNOWSKI\n\n92\nDynamical Model Overview\n\nNod_\nPftpoIItao\n\nIIyposIoooI\n\nFigure 2: Anatomical connections considered in the dynamical model. Only the left side is shown, the right\nside is identical and connected to the left side only for the calculation of vergence angle. The nucleus prepositus\nhypoglossi and the nucleus reticularis tegmenti pontis are meant to be representative of a class of nuclei in the\nbrain stem carrying eye position or vergence signal. All connections are known to exist except the connection\nbetween the prepositus nucleus to the reticularis nucleus which has not been verified. Details of the cerebellum\nare in fig. 3 and of the vestibular nucleus in fig. 4.\n\n3 Dynamical Model\nSnyder & King (1992) studied the effect of viewing distance and location of the axis of rotation on\nthe VOR in monkeys; their main results are reproduced in fig. 5. In an attempt to reproduce their\ndata and to understand how the signals that we have described in section 2 may be combined in time,\nwe constructed a dynamical model based on the kinematic model. Its basic anatomical structure is\nshown in fig. 2. Details of the model are shown in fig. 3, and fig . 4 where all constants are written\nusing a millisecond time scale. The results are presented in fig. 5. The dynamical variables represent\nthe change of average firing rate from resting level of activity. The firing rate of the afferents has a\ntonic component proportional to the velocity and a phasic component proportional to the acceleration\nof movement. Physiologically, the afferents have a wide range of phasic and tonic amplitudes. This\nis reflected by a wide selection of parameters in the numerators in the boxes of fig. 3 and fig. 4. The\nLaplace transform of the integration operator in equation (3) of the eye position coordinates is ~.\nFollowing Robinson (1981), we modeled the neural integrator with a gain and a time constant of\n20 seconds. We therefore replaced the pure integrator ~ with 20~~~~1 in the calculations of eye\nposition. The term 1 in fig. 3 is calculated by using equations (4) and (5), and by using the integrator\n9\n\n20~o:!~1 on the eye velocity motor command to find the angles (h and (JR.\n\nThe dynamical model is based on the assumption that the cerebellum is required for context modulation, and that because of its architecture, the cerebellum is more likely to implement complex functions of multiple signals than other relevant nuclei. The major contributions of vergence and eye\nposition modulation on the VOR are therefore mediated by the cerebellum. Smaller and more transient contributions from eye position are assumed to be mediated through the vestibular nucleus as\nshown in fig. 4. The motivation for combining eye position as in fig . 4 are, first, the evidence for eye\nresponse oscillations; second, the theoretical consideration that linear movement information (To) is\nuseless without eye position information for proper VOR.\nThe parameters in the dynamical model were adjusted by hand after observing the behavior of the different components of the model and noting how these combine to produce the oscillations observed\n\n\x0c93\n\nA Dynamical Model of Context Dependencies for the Vestibulo-Ocular Reflex\nVestibular\nSemicirtular\n\nCerebellum\n\nc..l\n\nO-\n\n- - - - t 401+1 r-----?--f--..j\n\nx\n\n300+1\n\nOIolith\n0Igan\n\nVHlibabr\nNuc1tul\n\nFigure 3: Contribution of the cerebellum to the dynamical model. Filtered velocity inputs from the canals and\notoliths are combined with eye position according to equation (2). These calculations could be performed either\noutside the cerebellum in one or multiple brain stem nuclei (as shown) or possibly inside the cerebellum. The\nonly output is to the vestibular nucleus. The Laplace notation is used in each boxes to represent a leaky integrator\nwith a time constant. input derivative and input gain. The term oe are the coordinates of the vector oe shown\nin fig. 1. The x indicates a multiplication. The term! multiplies each inputs individually. The open arrows\nindicate inhibitory (negative) connections.\nCere... lIum\n\nVHlibalu\n\nSemicimtlu\n\nc.w\n\nO--\'----t~l---+--?----t~~\n\nX\nFigure 4: Contribution of the vestibular nucleus to the dynamical model. Three pathways in the vestibular nucleus process the canal and otolith inputs to drive the eye. The first pathway is modulated by the output of the\ncerebellum through a FIN (Flocculus Target Neuron). The second and third pathways report transient information from the inputs which are combined with eye position in a manner identical to fig. 3. The location of these\ncalculations is hypothetical.\n\nin the data. Even though the number of parameters in the model is not small. it was not possible to\nfit any single response in fig. 5 without affecting most of the other eye responses. This puts severe\nlimits on the set of parameters allowed in the model.\nThe dynamical model suggests that the oscillations present in the data reflect: 1) important acceleration components in the neural signals. both rotational and linear, 2) different time delays between the\ncanal and otolith signal processing. and 3) antagonistic or synergistic action of the canal and otolith\nsignals with different axes of rotation, as described by the two terms in the bracket of equation (2).\n\n4 Discussion\nBy fitting the dynamical model to the data, we tested the hypothesis that the VOR has a response\nclose to ideal taking into account the time constraints imposed by the sensory inputs and the neural\nnetworks performing the computations. The vector computations that we used in the model may not\n\n\x0c94\n\nO. J. M. D. COENEN, T. J. SEJNOWSKI\nDynamical Model Responses vs Experimental Data\n80\n\n80\nLOMtIOftof\n.... 01 rotMIon\n\n-,a.-om\n\n.-\n\nT..........~\n\n60\n\n40\n20\n\n~\nw\n\n-20\n\n-20\n\n-400~----~5~0------~\n10\n=0\n~\nTime (m.)\n\n-40oL-----~\n5~\n0 ----~1~\n0~\n0-?\n\nTime (m.)\n\nFigure 5: Comparison between the dynamical model and monkey data. The dotted lines show the effect of\nviewing distance and location of the axis of rotation on the VOR as recorded by Snyder & King (1992) from\nmonkeys in the dark. The average eye velocity response (of left and right eye) to a sudden change in head velocity is shown for different target distances (left) and rotational axes (right). On the left, the location of the axis\nof rotation was in the midsagittal plane 12.5 cm behind the eyes (-12.5 cm), and the target distance was varied\nbetween 220 cm and 9 cm. On the right, the target di stance was kept constant at 9 cm in front of the eye, and the\nlocation of the axis of rotation was varied from 14 cm behind t04cm in front of the eyes (-14cm to 4cm) in the\nmidsagittal plane. The solid lines show the model responses. The model replicates many characteristics of the\ndata. On the left the model captures the eye velocity fluctuations between 20-50 ms, followed by a decrease and\nan increase which are both modulated with target distance (50-80 ms). The later phase of the response (80-100\nms) is almost exact for 220 cm, and one peak is seen at the appropriate location for the other distances. On the\nright the closest fits were obtained for the 4 cm and 0 cm locations. The mean values are in good agreement and\nthe waveforms are close, but could be shifted in time for the other locations of the axis of rotations. Finally, the\nlatest peak (..... lOOms) in the data appears in the model for -14 cm and 9 cm location.\n\nbe the representation used in the oculomotor system. Mathematically, the vector representation is\nonly one way to describe the computations involved. Other representations exist such as the quaternion representation which has been studied in the context of the saccadic system (Tweed and Vilis,\n1987; see also Handzel and Flash, 1996 for a very general representation). Detailed comparisons\nbetween the model and recordings from neurons will be require to settle this issue.\nDirect comparison between Purkinje cell recordings (L.H. Snyder & W.M. King, unpublished data)\nand predictions of the model could be used to determine more precisely the different inputs to some\nPurkinje cells. The model can therefore be an important tool to gain insights difficult to obtain directly with experiments.\nThe question of how the central nervous system learns the transformations that we described still\nremains. The cerebellum may be one site of learning for these transformations, and its output may\nmodulate the VOR in real time depending on the context. This view is compatible with the results\nof Angelaki and Hess (1995) which indicate that the cerebellum is required to correctly perform an\notolith transformation. It is also consistent with adaptation results in the VOR. To test this hypothesis,\nwe have been working on a model of the cerebellum which learns to anticipate sensory inputs and\nfeedbacks, and use these signals to modulate the VOR. The learning in the cerebellum and vestibular\nnuclei is mediated by the climbing fibers which report a reinforcement signal of the prediction error\n(Coenen and Sejnowski. in preparation).\n\n\x0cA Dynamical Model of Context Dependencies for the Vestibulo-Ocular Reflex\n\n95\n\n5 Conclusion\nMost research on the VOR has assumed forward gaze focussed at infinity. The kinematics of offcenter gaze and fixation at finite distance necessitates nonlinear corrections that require the integration of a variety of sensory inputs. The dynamical model studied here is a working hypothesis for\nhow these corrections could be computed and is generally consistent with what is known about the\ncerebellum and brain stem nuclei. We are, however, far from knowing the mechanisms underlying\nthese computations, or how they are learned through experience.\n\n6 Acknowledgments\nThe first author was supported by a McDonnell-Pew Graduate Fellowship during this research. We\nwould like to thank Paul Viola for helpful discussions.\nReferences\nAngelaki, D. E. and Hess, B. J. (1995). Inertial representation of angular motion in the vestibular system of rhesus monkeyus. II. Otolith-controlled transformation that depends on an intact cerebellar nodulus. Journal\nof Neurophysiology, 73(5): 1729-1751.\nCannon, S. C. and Robinson, D. A. (1987). Loss of the neural integrator of the oculomotor system from brain\nstem lesions in monkey. Journal of Neurophysiology, 57(5):1383-1409.\nCrawford, J. D., Cadera, W., and Vilis, T. (1991). Generation of torsional and vertical eye position signals by\nthe interstitial nucleus of Cajal. Science, 252:1551-1553.\nGamlin, P. D. R. and Clarke, R. J. (1995). Single-unit activity in the primate nucleus reticularis tegmenti pontis\nrelated to vergence and ocular accomodation. Journal of Neurophysiology, 73(5):2115-2119.\nGrossman, G. E., Leigh, R. J., Bruce, E. N., Huebner, W. P.,and Lanska, D.J. (1989). Performanceofthe human\nvestibu1oocu1ar reflex during locomotion. Journal of Neurophysiology, 62(1 ):264-272.\nHandzel, A. A. and Flash, T. (1996). The geometry of eye rotations and listing\'s law. In Touretzky, D., Mozer,\nM., and Hasselmo, M., editors, Advances in Neural Information Processing Systems 8, Cambridge, MA.\nMIT Press.\nJudge, S. J. and Cumming, B. G. (1986). Neurons in the monkey midbrain with activity related to vergence eye\nmovement and accomodation. Journal of Neurophysiology, 55:915-930.\nMays, L. E. (1984). Neural control of vergence eye movements: Convergence and divergence neurons in midbrain. Journal of Neurophysiology, 51:1091-1108.\nRobinson, D. A. (1981). The use of control systems analysis in the neurophysiology of eye movements. Ann.\nRev. Neurosci., 4:463-503.\nShelhamer, M., Robinson, D. A., and Tan, H. S. (1992). Context-specific adaptation of the gain of the vestibuloocular reflex in humans. Journal of Vestibular Research, 2:89-96.\nSnyder, L. H. and King, W. M. (1992). Effect of viewing distance and location ofthe axis of head rotation on the\nmonkey\'s vestibuloocular reflex I. eye movement response. Journal of Neurophysiology, 67(4):861-874.\nTweed, D. and Vilis, T. (1987). Implications of rotational kinematics for the oculomotor system in three dimensions. Journal of Neurophysiology, 58(4):832-849.\nViire, E., Tweed, D., Milner, K., and Vilis, T. (1986). A reexamination of the gain ofthe vestibuloocular reflex.\nJournal of Neurophysiology, 56(2):439-450.\n\n\x0c'
p83193
sg216
S"A model of transparent motion and\nnon-transparent motion aftereffects\n\nAlexander Grunewald*\nMax-Planck Institut fur biologische Kybernetik\nSpemannstrafie 38\nD-72076 Tubingen, Germany\n\nAbstract\nA model of human motion perception is presented. The model\ncontains two stages of direction selective units. The first stage contains broadly tuned units, while the second stage contains units\nthat are narrowly tuned. The model accounts for the motion aftereffect through adapting units at the first stage and inhibitory\ninteractions at the second stage. The model explains how two populations of dots moving in slightly different directions are perceived\nas a single population moving in the direction of the vector sum,\nand how two populations moving in strongly different directions are\nperceived as transparent motion. The model also explains why the\nmotion aftereffect in both cases appears as non-transparent motion.\n\n1\n\nINTRODUCTION\n\nTransparent motion can be studied using displays which contain two populations of\nmoving dots. The dots within each population have the same direction of motion,\nbut directions can differ between the two populations. When the two directions are\nvery similar, subjects report seeing dots moving in the average direction (Williams &\nSekuler, 1984). However, when the difference between the two directions gets large,\nsubjects perceive two overlapping sheets of moving dots. This percept is called\ntransparent motion. The occurrence of transparent motion cannot be explained by\ndirection averaging, since that would result in a single direction of perceived motion.\nRather than just being a quirk of the human visual system, transparent motion is\nan important issue in motion processing. For example, when a robot is moving its\n? Present address: Caltech, Mail Code 216-76, Pasadena, CA 91125.\n\n\x0c838\n\nA. GRUNEWALD\n\nmotion leads to a velocity field. The ability to detect transparent motion within\nthat velocity field enables the robot to detect other moving objects at the same time\nthat the velocity field can be used to estimate the heading direction of the robot.\nWithout the ability to code mUltiple directions of motion at the same location,\ni.e. without the provision for transparent motion, this capacity is not available.\nTraditional algorithms have failed to properly process transparent motion, mainly\nbecause they assigned a unique velocity signal to each location, instead of allowing\nthe possibility for multiple motion signals at a single location. Consequently, the\nstudy of transparent motion has recently enjoyed widespread interest.\n\nSTIMULUS\n\nPERCEPT\nTest\n\nFigure 1: Two populations of dots moving in different directions during an adaptation phase are perceived as transparent motion. Subsequent viewing of randomly\nmoving dots during a test phase leads to an illusory percept of unidirectional motion,\nthe motion aftereffect (MAE). Stimulus and percept in both phases are shown.\nAfter prolonged exposure to an adaptation display containing dots moving in one direction, randomly moving dots in a test display appear to be moving in the opposite\ndirection (Hiris & Blake, 1992; Wohlgemuth, 1911). This illusory percept of motion\nis called the motion aftereffect (MAE). Traditionally this is explained by assuming\nthat pairs of oppositely tuned direction selective units together code the presence\nof motion. When both are equally active, no motion is seen. Visual motion leads\nto stronger activation of one unit, and thus an imbalance in the activity of the two\nunits. Consequently, motion is perceived. Activation of that unit causes it to fatigue, which means its response weakens. After motion offset, the previously active\nunit sends out a reduced signal compared to its partner due to adaptation. Thus\nadaptation generates an imbalance between the two units, and therefore illusory\nmotion, the MAE, is perceived. This is the ratio model (Sutherland, 1961).\nRecent psychophysical results show that after prolonged exposure to transparent\nmotion, observers perceive a MAE of a single direction of motion, pointing in the\nvector average of the adaptation directions (Mather, 1980; Verstraten, Fredericksen, & van de Grind, 1994). Thus adaptation to transparent motion leads to a\nnon-transparent MAE. This is illustrated in Figure 1. This result cannot be accounted for by the ratio model, since the non-transparent MAE does not point in\nthe direction opposite to either of the adaptation directions. Instead, this result\nsuggests that direction selective units of all directions interact and thus contribute\nto the MAE. This explanation is called the distribution-shift model (Mather, 1980).\nHowever, thus far it has only been vaguely defined, and no demonstration has been\ngiven that shows how this mechanism might work.\n\n\x0cA Model of Transparent Motion and Non-transparent Motion Aftereffects\n\n839\n\nThis study develops a model of human motion perception based on elements from\nboth the ratio and the distribution-shift models for the MAE. The model is also\napplicable to the situation where two directions of motion are present. When the\ndirections differ slightly, only a single direction is perceived. When the directions\ndiffer a lot, transparent motion is perceived. Both cases lead to a unitary MAE.\n\n2\n\nOUTLINE OF THE MODEL\n\nThe model consists of two stages. Both stages contain units that are direction\nselective. The architecture of the model is shown in Figure 2.\n~----~--~,~r---~---'---\\\n\nStage 2\n\nCD080CD\n\n-,\n\n86)\n\n+----+~--+~--~----~--~--~--~~\n\nFigure 2: The model contains two stages of direction selective units. Units at stage\n1 excite units of like direction selectivity at stage 2, and inhibit units of opposite\ndirections. At stage 2 recurrent inhibition sharpens directional motion responses.\nThe grey level indicates the strength of interaction between units. Strong influence\nis indicated by black arrows, weak influence is indicated by light grey arrows.\nUnits in stage 1 are broadly tuned motion detectors. In the present study the precise\nmechanism of motion detection is not central, and hence it has not been modeled. It\nis assumed that the bandwidth of motion detectors at this stage is about 30 degrees\n(Raymond, 1993; Williams, Tweten, & Sekuler, 1991). In the absence of any visual\nmotion, all units are active at a baseline level; this is equivalent to neuronal noise.\nWhenever motion of a particular direction is present in the input, the activity of\nthe corresponding unit (Vi) is activated maximally (Vi = 9), and units of similar\ndirection selectivity are weakly activated (Vi = 3). The activities of all other units\ndecrease to zero. Associated with each unit i at stage 1 is a weight Wi that denotes\nthe adaptational state of unit i to fire a unit at stage 2. During prolonged exposure\nto motion these weights adapt, and their strength decreases. The equation governing\nthe strength of the weights is given below:\ndWi\n\n- dt = R(1- w?)\n~\n\nV?W\n?\n~~,\n\nwhere R = 0.5 denotes the rate of recovery to the baseline weight. When Wi = 1\nthe corresponding unit is not adapted. The further Wi is reduced from 1, the more\n\n\x0c840\n\nA. GRUNEWALD\n\nthe corresponding unit is adapted. The products ViWi are transmitted to stage 2.\nEach unit of stage 1 excites units coding similar directions at stage 2, and inhibits\nunits coding opposite directions of motion. The excitatory and inhibitory effects\nbetween units at stages 1 and 2 are caused by kernels, shown in Figure 3.\nFeedback kernels\n\nFeedforward kernels\n1\n\n-\n\n0.8\n\n-\n\n0.6\n\n0.6\n\n-\n\n0.4\n\n0.4 r-\n\n-\n\n0.2\n\n0.2 f--\n\n-\n\n1\n\nexcitatory\n0.8\n\n-------\n\nI\n\n1excitatory\ninhibitory\n\n---~---\n\ninhibitory\n\n0\n\n--- -- --180\n\n-----0\n\n180\n\n0\n\n-\n\n------+\n\n--------\n\no\n\n180\n\n-180\n\nFigure 3: Kernels used in the model. Left: excitatory and inhibitory kernels between\nstages 1 and 2; right: excitatory and inhibitory feedback kernels within stage 2.\nActivities at stage 2 are highly tuned for the direction of motion. The broad activation of motion signals at stage 1 is directionally sharpened at stage 2 through\nthe interactions between recurrent excitation and inhibition. Each unit in stage 2\nexcites itself, and interacts with other units at stage 2 through recurrent inhibition.\nThis inhibition is maximal for close directions, and falls off as the directions become more dissimilar. The kernels mediating excitatory and inhibitory interactions\nwithin stage 2 are shown in Figure 3. Through these inhibitory interactions the\ndirectional tuning of units at stage 2 is sharpened; through the excitatory feedback\nit is ensured that one unit will be maximally active. Activities of units at stage 2\nare given by Mi = max4 (mi' 0), where the behavior of mi is governed by:\n\nF/ and Fi- denote the result of convolving the products of the activities at stage\n1 and the corresponding adaptation level, VjWj , with excitatory and inhibitory\nfeedforward kernels respectively. Similarly, Bt and Bj denote the convolution of\nthe activities M j at stage 2 with the feedback kernels.\n\n3\n\nSIMULATIONS OF PSYCHOPHYSICAL RESULTS\n\nIn the simulations there were 24 units at each stage. The model was simulated\ndynamically by integrating the differential equations using a fourth order RungeKutta method with stepsize H = 0.01 time units. The spacing of units in direction\nspace was 15 degrees at both stages. Spatial interactions were not modeled. In\nthe simulations shown, a motion stimulus is present until t = 3. Then the motion\nstimulus ceases. Activity at stage 2 after t = 3 corresponds to a MAE.\n\n\x0cA Model of Transparent Motion and Non-transparent Motion Aftereffects\n\n3.1\n\n841\n\nUNIDIRECTIONAL MOTION\n\nWhen adapting to a single direction of motion, the model correctly generates a\nmotion signal for that particular direction of motion. After offset of the motion\ninput, the unit coding the opposite direction of motion is activated, as in the MAE.\nA simulation of this is shown in Figure 4.\nStage 1\n\nStage 2\n\nact\n\nact\n\n360\n\n360\n\nFigure 4: Simulation of single motion input and resulting MAE. Motion input is\npresented until t = 3.\nDuring adaptation the motion stimulus excites the corresponding units at stage 1,\nwhich in turn activate units at stage 2. Due to recurrent inhibition only one unit\nat stage 2 remains active (Grossberg, 1973), and thus a very sharp motion signal\nis registered at stage 2. During adaptation the weights associated with the units\nthat receive a motion input decrease. After motion offset, all units receive the same\nbaseline input. Since the weights of the previously active units are decreased, the\ncorresponding cells at stage 2 receive less feedforward excitation. At the same time,\nthe previously active units receive strong feedforward inhibition, since they receive\ninhibition from units tuned to very different directions of motion and whose weights\ndid not decay during adaptation. Similarly, the units coding the opposite direction\nof motion as those previously active receive more excitation and less inhibition.\nThrough recurrent inhibition the unit at stage 2 coding the opposite direction to that\nwhich was active during adaptation is activated after motion offset: this activity\ncorresponds to the MAE. Thus the MAE is primarily an effect of disinhibition.\n\n3.2\n\nTRANSPARENT MOTION: SIMILAR DIRECTIONS\n\nTwo populations of dots moving in different, but very similar, directions lead to\nbimodal activation at stage 1. Since the feedforward excitatory kernel is broadly\ntuned, and since the directions of motion are similar, the ensuing distribution of\nactivities at stage 2 is unimodal, peaking halfway between the two directions of\nmotion. This corresponds to the vector average of the directions of motion of the\ntwo populations of dots. A simulation of this is shown in Figure 5.\nDuring adaptation the units at stage 1 corresponding to the input adapt. As before\nthis means that after motion offset the previously active units receive less excitatory\ninput and more inhibitory input. As during adaptation this signal is unimodal. Also,\nthe unit at stage 2 coding the opposite direction to that of the stimulus receives\n\n\x0c842\n\nA. GRUNEWALD\n\nStage 2\n\nStage 1\n\nact\n\n60 120\n\n60 120 180\ndirection 240\n\n180\ndirection 240\n\nFigure 5: Simulation of two close directions of motion. Stage 2 of the network model\nregisters unitary motion and a unitary MAE.\nless inhibition and more excitation. Through the recurrent activities within stage\n2, that unit gets maximally activated. A unimodal MAE results.\n\n3.3\n\nTRANSPARENT MOTION: DIFFERENT DIRECTIONS\n\nWhen the directions of the two populations of dots in a transparent motion display\nare sufficiently distinct, the distribution of activities at stage 2 is no longer unimodal,\nbut bimodal. Thus, recurrent inhibition leads to activation of two units at stage 2.\nThey correspond to the two stimulus directions. A simulation is shown in Figure 6.\nStage 1\n\nStage 2\n\nact\n\n60 120\n180\ndirection 240\n\nFigure 6: Simulation of two distinct directions of motion. Stage 2 of the model\nregisters transparent motion during adaptation, but the MAE is unidirectional.\nFeedforward inhibition is tuned much broader than feedforward excitation, and as a\nconsequence the inhibitory signal during adaptation is unimodal, peaking at the unit\nof stage 2 coding the opposite direction of the average of the two previously active\ndirections. Therefore that unit receives the least amount of inhibition after motion\noffset. It receives the same activity from stage 1 as units coding nearby directions,\nsince the corresponding weights at stage 1 did not adapt. Due to recurrent activities\nat stage 2 that unit becomes active: non-transparent motion is registered.\n\n\x0cA Model of Transparent Motion and Non-transparent Motion Aftereffects\n\n4\n\n843\n\nDISCUSSION\n\nRecently Snowden, Treue, Erickson, and Andersen (1991) have studied the effect\nof transparent motion stimuli on neurons in areas VI and MT of macaque monkey.\nThey simultaneously presented two populations of dots, one of which was moving\nin the preferred direction of the neuron under study, and the other population was\nmoving in a different direction. They found that neurons in VI were barely affected\nby the second population of dots. Neurons in MT, on the other hand, were inhibited\nwhen the direction of the second population differed from the preferred direction,\nand inhibition was maximal when the second population was moving opposite to the\npreferred direction. These results support key mechanisms of the model. At stage\n1 there is no interaction between opposing directions of motion. The feedforward\ninhibition between stages 1 and 2 is maximal between opposite directions. Thus\nactivities of units at stage 1 parallel neural activities recorded at VI, and activities\nof units at stage 2 parallels those neural activities recorded in area MT.\nAcknowledgments\nThis research was carried out under HFSP grant SF-354/94.\n\nReference\nGrossberg, S. (1973). Contour enhancement, short term memory, and constancies in\nreverberating neural networks. Studies in Applied Mathematics, LII, 213-257.\nHiris, E., & Blake, R. (1992). Another perspective in the visual motion aftereffect.\nProceedings of the National Academy of Sciences USA, 89, 9025-9028.\nMather, G. (1980). The movement aftereffect and a distribution-shift model for\ncoding the direction of visual movement. Perception, 9, 379-392.\nRaymond, J. E. (1993). Movement direction analysers: independence and bandwidth. Vision Research, 33(5/6), 767-775.\nSnowden, R. J ., Treue, S., Erickson, R. G., & Andersen, R. A. (1991). The response\nof area MT and VI neurons to transparent motion. Journal of Neuroscience,\n11 (9), 2768-2785.\nSutherland, N. S. (1961). Figural after-effects and apparent size. Quarterly Journal\nof Experimental Psychology, 13, 222-228.\nVerstraten, F. A. J., Fredericksen, R. E., & van de Grind, W. A. (1994). Movement\naftereffect of bi-vectorial transparent motion. Vision Research, 34, 349-358.\nWilliams, D., Tweten, S., & Sekuler, R. (1991). Using metamers to explore motion\nperception. Vision Research, 31 (2), 275-286.\nWilliams, D. W., & Sekuler, R. (1984). Coherent global motion percept from\nstochastic local motions. Vision Research, 24 (1), 55-62.\nWohlgemuth, A. (1911). On the aftereffect of seen movement. British Journal of\nPsychology (Monograph Supplement), 1, 1-117.\n\n\x0c"
p83194
sg174
S'Onset-based Sound Segmentation\n\nLeslie S. Smith\nCCCN jDepartment of Computer Science\nUniversity of Stirling\nStirling FK9 4LA\nScotland\n\nAbstract\nA technique for segmenting sounds using processing based on mammalian early auditory processing is presented. The technique is\nbased on features in sound which neuron spike recording suggests\nare detected in the cochlear nucleus. The sound signal is bandpassed and each signal processed to enhance onsets and offsets.\nThe onset and offset signals are compressed, then clustered both in\ntime and across frequency channels using a network of integrateand-fire neurons. Onsets and offsets are signalled by spikes, and\nthe timing of these spikes used to segment the sound.\n\n1\n\nBackground\n\nTraditional speech interpretation techniques based on Fourier transforms, spectrum\nrecoding, and a hidden Markov model or neural network interpretation stage have\nlimitations both in continuous speech and in interpreting speech in the presence\nof noise, and this has led to interest in front ends modelling biological auditory\nsystems for speech interpretation systems (Ainsworth and Meyer 92; Cosi 93; Cole\net al 95).\nAuditory modelling systems use similar early auditory processing to that used in\nbiological systems. Mammalian auditory processing uses two ears, and the incoming\nsignal is filtered first by the pinna (external ear) and the auditory canal before it\ncauses the tympanic membrane (eardrum) to vibrate. This vibration is then passed\non through the bones of the middle ear to the oval window on the cochlea. Inside\nthe cochlea, the pressure wave causes a pattern of vibration to occur on the basilar\nmembrane. This appears to be an active process using both the inner and outer hair\ncells of the organ of Corti. The movement is detected by the inner hair cells and\nturned into neural impulses by the neurons of the spiral ganglion. These pass down\nthe auditory nerve, and arrive at various parts of the cochlear nucleus. From there,\nnerve fibres innervate other areas: the lateral and medial nuclei of the superior olive,\n\n\x0cL.S.SMITH\n\n730\n\nand the inferior colliculus, for example. (See (Pickles 88)).\nVirtually all modern sound or speech interpretation systems use some form of bandpass filtering, following the biology as far as the cochlea. Most use Fourier transforms to perform a calculation of the energy in each band over some time period,\nusually between 25 and 75 ms. This is not what the cochlea does. Auditory modelling front ends differ in the extent and length to which they follow animal early\nauditory processing, but the term generally implies at least that wideband filters\nare used, and that high temporal resolution is maintained in the initial stages. This\nmeans the use of filtering techniques. rather than Fourier transforms in the bandpass\nstage. Such filtering systems have been implemented by Patterson and Holdsworth\n(Patterson and Holdsworth 90; Slaney 93), and placed directly in silicon (Lazzaro\nand Mead 89; Lazzaro et al 93; Liu et al 93; Fragniere and van Schaik 94).\nSome auditory models have moved beyond cochlear filtering. The inner hair cell\nhas been modelled by either simple rectification (Smith 94) or has been based on\nthe work of (Meddis 88) for example (Patterson and Holdsworth 90; Cosi 93; Brown\n92). Lazzaro has experimented with a silicon version of Licklider\'s autocorrelation\nprocessing (Licklider 51; Lazzaro and Mead 89). Others such as (Wu et al 1989:\nBlackwood et al1990; Ainsworth and Meyer 92; Brown 92; Berthommier 93; Smith\n94) have considered the early brainstem nuclei, and their possible contribution,\nbased on the neurophysiology of the different cell types (Pickles 88; Blackburn and\nSachs 1989; Kim et al 90).\nAuditory model-based systems have yet to find their way into mainstream speech\nrecognition systems (Cosi 93). The work presented here uses auditory modelling\nup to onset cells in the cochlear nucleus. It adds a temporal neural network to\nclean up the segmentation produced. This part has been filed as a patent (Smith\n95). Though the system has some biological plausibility, the aim is an effective\ndata-driven segmentation technique implement able in silicon.\n\n2\n\nTechniques used\n\nDigitized sound was applied to an auditory front end, (Patterson and Holdsworth\n90), which bandpassed the sound into channels each with bandwidth 24.7{4.37Fr; +\nI)Hz, where Fe is the centre frequency (in KHz) of the band (Moore and Glasberg\n83). These were rectified, modelling the effect of the inner hair cells. The signals\nproduced bear some resemblance to that in the auditory nerve. The real system\nhas far more channels and each nerve channel carries spike-coded information. The\ncoding here models the signal in a population of neighboring auditory nerve fibres.\n\n2.1\n\nThe onset-offset filter\n\nThe signal present in the auditory nerve is stronger near the onset of a tone than\nlater (Pickles 88). This effect is much more pronounced in certain cell types of the\ncochlear nucleus. These fire strongly just after the onset of a sound in the band to\nwhich they are sensitive, and are then silent. This emphasis on onsets was modelled\nby convolving the signal in each band with a filter which computes two averages, a\nmore recent one, and a less recent one, and subtracts the less recent one from the\nmore recent one. One biologically possible justification for this is to consider that\na neuron is receiving the same driving input twice, one excitatorily, and the other\ninhibitorily; the excitatory input has a shorter time-constant than the inhibitory\ninput. Both exponentially weighted averages, and averages formed using a Gaussian\nfilter have been tried (Smith 94), but the former place too much emphasis on the\nmost recent part of the signal, making the latter more effective.\n\n\x0c731\n\nOnset-based Sound Segmentation\n\nThe filter output for input signal s(x) is\n\nO(t. k, \'f\') =\n\nlot (f(t - x, k) -\n\nf(t - x, k/,r))s(x)dx\n\n(1)\n\nwhere f(x, y) = vY exp( -yx 2 ). k and \'r determine the rise and fall times of the\npulses of sOlmd that the system is sensitive to. We used A: = 1000, \'r = 1.2, so\nthat the SD of the Gaussians are 24.49ms and 22.36ms. The convolving filter has\na positive peak at O. crosses 0 at 22.39ms. and is then negative. With these values.\nthe system is sensitive to energy rises and falls which occm in the envelopes of\neveryday sounds. A positive onset-offset signal implies that the bandpassed signal is\nincreasing in intensity, and a negative onset-offset signal implies that it is decreasing\nin intensity. The convolution used is a sound analog of the difference of Gaussians\noperator used to extract black/white and white/black edges in monochrome images\n(MalT and Hildreth 80). In (Smith 94) we performed sOlmd segmentation directly\non this signal.\n\n2.2\n\nCompressing the onset-offset signal\n\nThe onset-offset signal was divided into two positive-going signals, an onset signal\nconsisting of the positive-going part, and an offset signal consisting of the inverted\nnegative-going part. Both were compressed logarithmically (where log(x) was taken\nas 0 for 0 S x S 1). This increases the dynamical range of the system, and models\ncompressive biological effects. The compressed onset signal models the output of a\npopulation of onset cells. This technique for producing an onset signal is related to\nthat of (Wu et al 1989: Cosi 93).\n\n2.3\n\nThe integrate-and-fire neural network\n\nTo segment the sound using the onset and offset signals, they need to be integrated\nacross frequency bands and across time. This temporal and tonotopic clustering\nwas achieved using a network of integrate-and-fire units. An integrate-and-fire unit\naccumulates its weighted input over time. The activity of the unit A. is initially O.\nand alters according to\ndA\n(2)\n- = I(t) - "YA\ndt\nwhere I(t) is the input to the nemon and "Y, the dissipation, describes the leakiness\nof the integration. When A reaches a threshold. the unit fires (i.e. emits a pulse).\nand A is reset to O. After firing, there is a period of insensitivity to input, called the\nrefractory period. Such nemons are discussed in. e.g. (Mirolla and Strogatz 90).\nOne integrate-and-fire neuron was used per charmel: this neuron received input either from a single charmel, or from a set of adjacent charmels. all with equal positive\nweighting. The output of each neuron was fed back to a set of adjacent neurons,\nagain with a fixed positive weight, one time step (here 0.5ms) later. Because of the\nleaky nature of the accumulation of activity, excitatory input to the neuron arriving\nwhen its activation is near\' threshold has a lar\'ger effect on the next firing time than\nexcitatory input arriving when activation is lower. Thus, if similar input is applied\nto a set of neurons in adjacent charmels. the effect of the inter-neuron connections\nis that when the first one fires, its neighbors fire almost immediately. This allows\na network of such neurons to cluster the onset or offset signals, producing a sharp\nburst of spikes across a number of charmels providing unambiguous onsets or offsets.\nThe external and internal weights of the network were adjusted so that onset or\noffset input alone allowed neurons to fire, while internal input alone was not enough\n\n\x0cL. S. SMITH\n\n732\n\nto cause firing. The refractory period used was set to 50ms for the onset system,\nand 5ms for the offset system. For the onset system, the effect was to produce sharp\nonset firing responses across adjacent channels in response to a sudden increase in\nenergy in some channels, thus grouping onsets both tonotopically and temporally.\nThis is appropriate for onsets, as these are generally brief and clearly marked. The\noutput of this stage we call the onset map. Offsets tend to be more gradual. This\nis due to physical effects: for example, a percussive sound will start suddenly, as\nthe vibrating element starts to move. but die away slowly as the vibration ceases\n(see (Gaver 93) for a discussion). Even when the vibration does stop suddenly. the\nsound will die away more slowly due to echoes. Thus we cannot reliably mark the\noffset of a sound: instead. we reduce the refractory period of the offset neurons, and\nproduce a train of pulses marking the duration of the offset in this channel. We call\nthe output of this stage the offset map.\n\n3\n\nResults\n\nAs the technique is entirely data-driven. it can be applied to sound from any source.\nIt has been applied to both speech and musical sounds. Figure 1 shows the effect\nof applying the techniques discussed to a short piece of speech. Fig lc shows that\nthe neural network integrates the onset timings across the channels, allowing these\nonsets to be used for segmentation. The simplest technique is to divide up the\ncontinuous speech at each onset: however,to ensure that the occasional onset in a\nsingle channel does not confuse the system. and that onsets which occur near to\neach other do not result in very short segments we demanded that a segmentation\nboundary have at least 6 onsets inside a period of lOms. and the minimum segment\nlength was set to 25ms.\nThe utterance Ne\'Uml information processing systems has phonetic representation:\n/ njtlrl: anfarmeIan prosc:salJ ststalllS /\nand is segmented into the following 19 segments:\n/n/, jtl/, /r/, /la/, /a/, /nf/. /arm/, /e/,\n/t/, /st/, /am/, /s/\n\n/I/,\n\n/an/, /pro/, /os/, /c:s/ , /aIJ/, /s/,\n\nThe same text spoken more slowly (over 4.38s, rather than 2.31s) has phonetic\nrepresentation:\n/ njural:anftrmeIanprosc:stIJ ststams /\nSegmenting using this technique gives the following 25 segments:\n/n/ , /ju/ , /u/, /r/. /a/ , /al/, /1/, / /, /an/, /f/ , /um/, /e/,\n/ro/, /os/, /c:s/, /tIJ/, /s/, /t/ , /st/, /am/, /s/\n\n/I/.\n\n/an/, /n:/, /pr/,\n\nAlthough some phonemes are broken between segments, the system provides effective segmentation, and is relatively insensitive to speech rate. The system is also\neffective at finding speech inside certain types of noise (such as motor-bike noise) ,\nas can be seen in fig Ie and f.\nThe system has been used to segment sound from single musical instruments. Where\nthese have clear breaks between notes this is straightforward: in (Smith 94) correct\nsegmentation was achieved directly from the onset-offset signal but was not achieved\nfor slurred sounds, in which the notes change smoothly. As is visible in figure 2c,\nthe onsets here are clear using the network, and the segmentation produced is near-\n\n\x0c733\n\nOnset-based Sound Segmentation\n\n[E]J\n\n:\'.\n\n...\n...\n\'"\n\nGJ . . .\n,\n\n..\n\n..\n\n"\n\n. ... .... ..\n\n\'"\\N\',,,,,,.J/\'I~\'/w,. -"?\'\'\'\'\'\'\'\'\'\'\'~v..flt\'\'\'\'\'I/!fII~~...A..it.-./\'~f\\It\'\'\'v,i~~~~\';\'\'o~\\-J..J{iII\'\'r ~\'if\'I{I/\'}/i\'...J"\'\'\'\n\n~ \'W\'hlJ.,j.~~..f"\'\\/\' JJ.A~ "\'v.""./fI/<II\'~rJ~ M \'\\-\'.\'~\'f.."""v/\n\n"\'I\'jNflNlI/V\n\n\'1\'#.~N~I{\'f!II/W/W ?" /")~,\\/,,\n\n.. ; . .\n\'\n\nFigure 1: (a-d):Onset and Offset maps from author saying Neural information processing systems rapidly. a: envelope of original sound. b: onset map. from 28\nchannels. from 100Hz-6KHz. Onset filter parameters as in text; one neuron per\nchannel, with no interconnection. Neuron refractory period is 50ms. c: as b , but\nnetwork has input applied to 6 adjacent channels, and internal feedback to 10 channels. d: offset map produced similarly, with refractory period 5ms. e: envelope of\nsay, that\'s a nice bike with motorbike noise in background (lines mark utterance).\nf, g: onset, offset maps for e.\n\nperfect. Best results were obtained here when the input to the network is not spread\nacross channels.\n\n4\n\nConclusions and further work\n\nAn effective data driven segmentation technique based on onset feature detection\nand using integrate-and-fire neurons has been demonstrated. The system is relatively immune to broadband noise. Segmentation is not an end in itself: the\neffectiveness of any technique will depend on the eventual application.\n\n\x0cL. S.SMITH\n\n734\n\n..\'~----\n\n.\'\n\nFigure 2: a: slurred flute sound. with vertical lines showing boundary between\nnotes. b: onsets found using a single neuron per channel, and no interconnection.\nc: as b, but with internal feedback from each channel to 16 adjacent channels d:\noffsets found with refractory period 5ms.\n\nThe segmentation is currently not using the information on which bands the onsets\noccur in. We propose to extend this work by combining the segmentation described\nhere with work streaming bands sharing same-frequency amplitude modulation.\nThe aim of this is to extract sound segments from some subset of the bands, allowing\nsegmentation and streaming to run concurrently.\n\nAcknowledgements\nMany thanks are due to the members of the Centre for Cognitive and Computational\nNeuroscience at the University of Stirling.\n\nReferences\nAinsworth W. Meyer G. Speech analysis by means of a physiologically-based model\nof the cochlear nerve and cochlear nucleus. in Visual r\'e presentations of speech\nsignals. Cooke M. Beet S. eds. 1992.\nBerthommier F .. Modelling nelll\'rul\'eSpOllSes of t.he int.ermediate auditory system, in\nMathematics applied to biology and medicine, Demongeot .I, Capa..\'!so V, Wuertz\nPublishing, Canada, 1993.\nBlackburn C.C .. Sachs M.B. Classification of unit types in the anteroventral cochlear\nnucleus: PST hist.ograms and regularity analysis, . J. Neurophys\'iology, 62, 6,\n1989.\n\n\x0cOnset-based Sound Segmentation\n\n735\n\nBlackwood N .. Meyer G., Aimsworth W. A Model of the processing of voiced plosives\nin the auditory nerve and cochlear nucleus, Proceedings Inst of Acoustics, 12,\n10, 1990.\nBrown G. Computational Auditory Scene Analysis, TR CS-92-22, Department of\nComputing Science, University of Sheffield, England, 1992.\nCole R .. et al, The challenge of spoken language systems: research directions of the\n90\'s. IEEE Trans Speech and Audio Pmcessing, 3. 1, 1995.\nCosi P. On the use of auditory models in speech technology, in Intelligent Perceptual\nModels, LNCS 745, Springer Verlag, 1993.\nFragniere E., van Schaik A .. Lineal\' predictive coding of the speech signal using an\nanalog cochlear modeL MANTRA Internal Report, 94/2, MANTRA Center for\nNeuro-mimetic systems, EPFL, Lausanne, Switzerland, 1994.\nGaver W.W. What in the world do we hear?: an ecological approach to auditory\nevent perception. Ecological Psychology, 5(1). 1-29, 1993.\nKim D.O. ,Sirianni .T.G., Chang S.O .. Responses of DCN-PVCN neurons and auditory nerve fibres in lmanesthetized decerebrate cats to AM and pure tones:\nanalysis with autocorrelation/power-spectrum, Hearing Research. 45, 95-113.\n1990.\nLazzaro .T., Mead C., Silicon modelling of pitch perception, Proc Natl. Acad Sciences, USA, 86. 9597-9601, 1989.\nLazzaro .T., Wawrzynek .T .. Mahowald M. , Sivilotti M., Gillespie D .. Silicon auditory\nprocessors as computer peripherals. IEEE Trans on Neural Networks, 4, 3, May\n1993.\nLicklider .T.C.R, A Duplex theory of pitch perception, Experentia, 7. 128-133, 1951.\nLiu W .. Andreou A.G., Goldstein M.H., Analog cochlear model for multiresolution\nspeech analysis, Advances in Neural Information Processing Systems 5, Hanson\nS ..T., Cowan .T.D., Lee Giles C. (eds), Morgan Kaufmann, 1993.\nMarl\' D., Hildreth E. Theory of edge detection, Proc. Royal Society of London B,\n207. 187-217, 1980.\nMeddis R .. Simulation of auditory-neural transduction: further studies. J. Acollst\nSoc Am. 83. 3, 1988.\nMoore B.C ..J.. Glasberg B.R. Suggested formulae for calculating auditory-filter\nbandwidths and excitation patterns, J Acoust Soc America, 74. 3, 1983.\nMirollo RE. , Strogatz S.H. Synchronization of pulse-coupled biological oscillators,\nSIAM J. Appl Math, 50, 6, 1990.\nPatterson R. Holdsworth .T. (1990). An Introd\'IJ,ction to A1J,ditory Sensation Processing. in AAM HAP. Vol 1. No 1.\nPickles .T.O. (1988). An Introd\'u ction to the PhyS\'iology of Hearing, 2nd Edition,\nAcademic Press.\nSlaney M .. An efficient implementation of the Patterson-Holdsworth auditory filter\nbank, Apple technical report No 35, Apple Computer Inc, 1993.\nSmith L.S. SO\\illd segmentation using onsets and offsets, J of New Music Research,\n23, 1, 1994.\nSmith L.S. Onset/offset coding for interpretation and segmentation of sound, UK\npatent no 9505956.4. March 1995.\nWu Z.L., Schwartz .T.L .. Escudier P. A theoretical study of neural mechanisms\nspecialized in the detection of articulatory-acoustic events, Proc Eurospeech\n89. ed Tubach .T.P., Mariani .T ..T., Paris, 1989.\n\n\x0c'
p83195
sg440
S'REMAP: Recursive Estimation and\nMaximization of A Posteriori\nProbabilities - Application to\nTransition-Based Connectionist Speech\nRecognition\n\nYochai Konig, Herve Bourlard~ and Nelson Morgan\n{konig, bourlard,morgan }@icsi.berkeley.edu\nInternational Computer Science Institute\n1947 Center Street Berkeley, CA 94704, USA.\n\nAbstract\nIn this paper, we introduce REMAP, an approach for the training\nand estimation of posterior probabilities using a recursive algorithm\nthat is reminiscent of the EM-based Forward-Backward (Liporace\n1982) algorithm for the estimation of sequence likelihoods. Although very general, the method is developed in the context of a\nstatistical model for transition-based speech recognition using Artificial Neural Networks (ANN) to generate probabilities for Hidden Markov Models (HMMs). In the new approach, we use local\nconditional posterior probabilities of transitions to estimate global\nposterior probabilities of word sequences. Although we still use\nANNs to estimate posterior probabilities, the network is trained\nwith targets that are themselves estimates of local posterior probabilities. An initial experimental result shows a significant decrease\nin error-rate in comparison to a baseline system.\n\n1\n\nINTRODUCTION\n\nThe ultimate goal in speech recognition is to determine the sequence of words that\nhas been uttered. Classical pattern recognition theory shows that the best possible system (in the sense of minimum probability of error) is the one that chooses\nthe word sequence with the maximum a posteriori probability (conditioned on the\n\n*Also affiliated with with Faculte Poly technique de Mons, Mons, Belgium\n\n\x0cREMAP: Recursive Estimation and Maximization of A Posteriori Probabilities\n\n389\n\nevidence). If word sequence i is represented by the statistical model M i , and the\nevidence (which, for the application reported here, is acoustical) is represented by\na sequence X = {Xl, ... , X n , ... , X N }, then we wish to choose the sequence that\ncorresponds to the largest P(MiIX). In (Bourlard & Morgan 1994), summarizing\nearlier work (such as (Bourlard & Wellekens 1989)), we showed that it was possible to compute the global a posteriori probability P(MIX) of a discriminant form\nof Hidden Markov Model (Discriminant HMM), M, given a sequence of acoustic\nvectors X. In Discriminant HMMs, the global a posteriori probability P(MIX) is\ncomputed as follows: if r represents all legal paths (state sequences ql, q2, ... , qN)\nin Mi, N being the length of the sequence, then\n\nP(Mi IX) =\n\nL P(Mi, ql, q2, ... , qNIX)\nr\n\n=\n\nin which ~n represents the specific state hypothesized at time n, from the set Q\n{ql, ... , q , qk, ... , qK} of all possible HMM states making up all possible models\n\nMi. We can further decompose this into:\nP(Mi, ql, q2,???, qNIX) = P(ql, q2,???, qNIX)P(Milql, q2,???, qN, X)\nUnder the assumptions stated in (Bourlard & Morgan 1994) we can compute\nN\n\nP(ql, q2,???, qNIX)\n\n= II p(qnlqn-l, xn)\nn=l\n\nThe Discriminant HMM is thus described in terms of conditional transition probabilities p(q~lq~-l\' xn), in which q~ stands for the specific state ql of Q hypothesized\nat time n and can be schematically represented as in Figure 1.\nP(IkIIIkI, x)\n\np(/aell/ael, x)\n\nP(/aelllkl, x)\n\nP(ltIlltI, x)\n\nP(ltll/ael, x)\n\nFigure 1: An example Discriminant HMM for the word "cat". The variable\nto a specific acoustic observation Xn at time n.\n\nX\n\nrefers\n\nFinally, given a state sequence we assume the following approximation:\n\nP(Milql, q2,???, qN, X) : : : : P(Milql, q2,???, qN)\nWe can estimate the right side of this last equation from a phonological model (in\nthe case that a given state sequence can belong to two different models). All the\nrequired (local) conditional transition probabilities p(q~lq~-l> xn) can be estimated\nby the Multi-Layer Perceptron (MLP) shown in Figure 2.\nRecent work at lesl has provided us with further insight into the discriminant\nHMM, particularly in light of recent work on transition-based models (Konig &\nMorgan 1994j Morgan et al. 1994). This new perspective has motivated us to further\ndevelop the original Discriminant HMM theory. The new approach uses posterior\nprobabilities at both local and global levels and is more discriminant in nature.\nIn this paper, we introduce the Recursive Estimation-Maximization of A posteriori\n\n\x0c390\n\nY. KONIG, H. BOURLARD, N. MORGAN\nP(CurrenCstlte I Acoustics, Prevlous_stlte)\n\nt t t t\n\nt???? .. t\n0.1 ?? 0\nPrevious\nStlte\n\nAcoustics\n\nFigure 2: An MLP that estimates local conditional transition probabilities.\nProbabilities (REMAP) training algorithm for hybrid HMM/MLP systems. The\nproposed algorithm models a probability distribution over all possible transitions\n(from all possible states and for all possible time frames n) rather than picking a\nsingle time point as a transition target. Furthermore, the algorithm incrementally\nincreases the posterior probability of the correct model, while reducing the posterior\nprobabilities of all other models. Thus, it brings the overall system closer to the\noptimal Bayes classifier.\nA wide range of discriminant approaches to speech recognition have been studied\nby researchers (Katagiri et al. 1991; Bengio et al. 1992; Bourlard et al. 1994). A\nsignificant difficulty that has remained in applying these approaches to continuous\nspeech recognition has been the requirement to run computationally intensive algorithms on all of the rival sentences. Since this is not generally feasible, compromises\nmust always be made in practice. For instance, estimates for all rival sentences can\nbe derived from a list of the "N-best" utterance hypotheses, or by using a fully\nconnected word model composed of all phonemes.\n\n2\n2.1\n\nREMAP TRAINING OF THE DISCRIMINANT HMM\nMOTIVATIONS\n\nThe discriminant HMM/MLP theory as described above uses transition-based probabilities as the key building block for acoustic recognition. However, it is well known\nthat estimating transitions accurately is a difficult problem (Glass 1988). Due to\nthe inertia of the articulators, the boundaries between phones are blurred and overlapped in continuous speech. In our previous hybrid HMM/MLP system, targets\nwere typically obtained by using a standard forced Viterbi alignment (segmentation). For a transition-based system as defined above, this procedure would thus\nyield rigid transition targets, which is not realistic.\nAnother problem related to the Viterbi-based training of the MLP presented in\nFigure 2 and used in Discriminant HMMs, is the lack of coverage of the input space\nduring training. Indeed, during training (based on hard transitions), the MLP only\nprocesses inputs consisting of "correct" pairs of acoustic vectors and correct previous\nstate, while in recognition the net should generalize to all possible combinations of\n\n\x0cREMAP: Recursive Estimation and Maximization of A Posteriori Probabilities\n\n391\n\nacoustic vectors and previous states, since all possible models and transitions will be\nhypothesized for each acoustic input. For example, some hypothesized inputs may\ncorrespond to an impossible condition that has thus never been observed, such as\nthe acoustics of the temporal center of a vowel in combination with a previous state\nthat corresponds to a plosive. It is unfortunately possible that the interpolative\ncapabilities of the network may not be sufficient to give these "impossible" pairs a\nsufficiently low probability during recognition.\nOne possible solution to these problems is to use a full MAP algorithm to find transition probabilities at each frame for all possible transitions by a forward-backwardlike algorithm (Liporace 1982), taking all possible paths into account.\n\n2.2\n\nPROBLEM FORMULATION\n\nAs described above, global maximum a posteriori training of HMMs should find the\noptimal parameter set e maximizing\nJ\n\nII P(Mj IXj, e)\n\n(1)\n\nj=1\n\nin which Mj represents the Markov model associated with each training utterance\nXj, with j = 1, ... , J.\nAlthough in principle we could use a generalized back-propagation-like gradient\nprocedure in e to maximize (1) (Bengio et al. 1992), an EM-like algorithm should\nhave better convergence properties, and could preserve the statistical interpretation of the ANN outputs. In this case, training of the discriminant HMM by a\nglobal MAP criterion requires a solution to the following problem: given a trained\nMLP at iteration t providing a parameter set e t and, consequently, estimates of\nP(q~lxn\' q~-I\' et ), how can we determine new MLP targets that:\n1. will be smooth estimates of conditional transition probabilities q~-1\nVk,f E [1, K] and "In E [1, N],\n\n-+\n\nq~,\n\n2. when training the MLP for iteration t+ 1, will lead to new estimates of et+l\nand P(q~lxn\' q~-I\' et+1) that are guaranteed to incrementally increase the\nglobal posterior probability P(MiIX, e)?\nIn (Bourlard et al. 1994), we prove that a re-estimate of MLP targets that guarantee\nconvergence to a local maximum of (1) is given by1:\n\n(2)\nwhere we have estimated the left-hand side using a mapping from the previous\nstate and the local acoustic data to the current state, thus making the estimator\nrealizable by an MLP with a local acoustic window .2 Thus, we will want to estimate\n1 In most of the following, we consider only one particular training sequence X associated\nwith one particular model M. It is, however, easy to see that all of our conclusions remain\nvalid for the case of several training sequences Xj, j\n1, ... , J. A simple way to look\nat the problem is to consider all training sequences as a single training sequence obtained\nby concatenating all the X,\'s with boundary conditions at every possible beginning and\nending point.\n2Note that, as done in our previous hybrid HMM/MLP systems, all conditional on Xn\ncan be replaced by X;::!: = {x n - c , ., ?. , X n , .?? , Xn+d} to take some acoustic context into\naccount.\n\n=\n\n\x0c392\n\nY. KONIG, H. BOURLARD, N. MORGAN\n\nthe transition probability conditioned on the local data (as MLP targets) by using\nthe transition probability conditioned on all of the data.\nIn (Bourlard et al. 1994), we further prove that alternating MLP target estimation\n(the "estimation" step) and MLP training (the" maximization" step) is guaranteed\nto incrementally increase (1) over t. 3 The remaining problem is to find an efficient\nalgorithm to express P(q~IX, q~-l\' M) in terms of P(q~lxn, q~-l) so that the next\niteration targets can be found. We have developed several approaches to this estimation, some of which are described in (Bourlard et al. 1994). Currently, we are\nimplementing this with an efficient recursion that estimates the sum of all possible\npaths in a model, for every possible transition at each possible time. From these\nvalues we can compute the desired targets (2) for network training by\n\nP( t IX M k )\nqn , , qn-l\n\n2.3\n\n=\n\nP(M, q~, ~~_lIX)\nJ\nk\nIX)\nDJ\n,qn,\nqn-l\n~ . P(M\n\n(3)\n\nREMAP TRAINING ALGORITHM\n\nThe general scheme of the REMAP training of hybrid HMM/MLP systems can be\nsummarized as follow:\n1. Start from some initial net providing P(q~lxn\' q~-l\' e t ), t = 0, V possible\n(k,?)-pairs4.\n2. Compute MLP targets P(q~IXj,q~_l,et,Mj) according to (3), V training\nsentences Xj associated with HMM Mj, V possible (k, ?) state transition\npairs in Mj and V X n , n\n1, ... , N in Xj (see next point).\n\n=\n\n3. For every Xn in the training database, train the MLP to minimize the\nrelative entropy between the outputs and targets. See (Bourlard et ai,\n1994) for more details. This provides us with a new set of parameters t ,\nfor t\nt + 1.\n\n=\n\ne\n\n4. Iterate from 2 until convergence.\nThis procedure is thus composed of two steps: an Estimation (E) step, corresponding to step 2 above, and a Maximization (M) step, corresponding to step 3 above.\nIn this regards, it is reminiscent of the Estimation-Maximization (EM) algorithm\nas discussed in (Dempster et al. 1977). However, in the standard EM algorithm,\nthe M step involves the actual maximization of the likelihood function. In a related\napproach, usually referred to as Generalized EM (GEM) algorithm, the M step does\nnot actually maximize the likelihood but simply increases it (by using, e.g., a gradient procedure). Similarly, REMAP increases the global posterior function during\nthe M step (in the direction of targets that actually maximize that global function),\nrather than actually maximizing it. Recently, a similar approach was suggested for\nmapping input sequences to output sequences (Bengio & Frasconi 1995).\n3Note here that one "iteration" does not stand for one iteration of the MLP training\nbut for one estimation-maximization iteration for which a complete MLP training will be\nrequired.\n4This can be done, for instance, by training up such a net from a hand-labeled database\nlike TIMIT or from some initial forward-backward estimator of equivalent local probabilities (usually referred to as "gamma" probabilities in the Baum-Welch procedure).\n\n\x0cREMAP: Recursive Estimation and Maximization of A Posteriori Probabilities\n\nSystem\nDHMM, pre-REMAP\n1 REMAP iteration\n2 REMAP iterations\n\n393\n\nError Rate\n14.9%\n13.6%\n13.2%\n\nTable 1: Training and testing on continuous numbers, no syntax, no durational\nmodels.\n\n3\n\nEXPERIMENTS AND RESULTS\n\nFor testing our theory we chose the Numbers\'93 corpus. It is a continuous speech\ndatabase collected by CSLU at the Oregon Graduate Institute. It consists of numbers spoken naturally over telephone lines on the public-switched network (Cole\net al. 1994). The Numbers\'93 database consists of 2167 speech files of spoken numbers produced by 1132 callers. We used 877 of these utterances for training and\n657 for cross-validation and testing (200 for cross-validation) saving the remaining\nutterances for final testing purposes. There are 36 words in the vocabulary, namely\nzero, oh, 1, 2, 3, ... ,20, 30, 40, 50, ... ,100, 1000, a, and, dash, hyphen, and double.\nAll our nets have 214 inputs: 153 inputs for the acoustic features, and 61 to represent the previous state (one unit for every possible previous state, one state per\nphoneme in our case). The acoustic features are combined from 9 frames with 17\nfeatures each (RASTA-PLP8 + delta features + delta log gain) computed with an\nanalysis window of 25 ms computed every 12.5 ms (overlapping windows) and with\na sampling rate of 8 Khz . The nets have 200 hidden units and 61 outputs.\nOur results are summarized in Table 1. The row entitled "DHMM, pre-REMAP"\ncorresponds to a Discriminant HMM using the same training approach, with hard\ntargets determined by the first system, and additional inputs to represent the previous state The improvement in the recognition rate as a result of REMAP iterations\nis significant at p < 0.05. However all the experiments were done using acoustic\ninformation alone. Using our (baseline) hybrid system under equal conditions, i.e.,\nno duration information and no language information, we get 31.6% word error;\nadding the duration information back we get 12.4% word error. We are currently\nexperimenting with enforcing minimum duration constraints in our framework.\n\n4\n\nCONCLUSIONS\n\nIn summary:\n? We have a method for MAP training and estimation of sequences.\n? This can be used in a new form of hybrid HMM/MLP. Note that recurrent\nnets or TDNNs could also be used. As with standard HMM/MLP hybrids,\nthe network is used to estimate local posterior probabilities (though in this\ncase they are conditional transition probabilities, that is, state probabilities\nconditioned on the acoustic data and the previous state). However, in the\ncase of REMAP these nets are trained with probabilistic targets that are\nthemselves estimates of local posterior probabilities.\n? Initial experiments demonstrate a significant reduction in error rate for this\nprocess.\n\n\x0c394\n\nY. KONIG, H. BOURLARD, N. MORGAN\n\nAcknowledgments\n\nWe would like to thank Kristine Ma and Su-Lin Wu for their help with the Numbers\'93 database. We also thank OGI, in particular to Ron Cole, for providing the\ndatabase. We gratefully acknowledge the support of the Office of Naval Research,\nURI No. N00014-92-J-1617 (via UCB), the European Commission via ESPRIT\nproject 20077 (SPRACH), and ICSI and FPMs in general for supporting this work.\n\nReferences\nBENGIO, Y., & P. FRASCONI. 1995. An input output HMM architecture.\nIn Advances in Neural Information Processing Systems, ed. by G. Tesauro,\nD. Touretzky, & T. Leen, volume 7. Cambridge: MIT press.\n- - , R. DE MORI, G. FLAMMIA, & R. KOMPE. 1992. Global optimization of a\nneural network-hidden Markov model hybrid. IEEE trans. on Neural Networks\n3.252-258.\nBOURLARD, H., Y. KONIG, & N. MORGAN. 1994. REMAP: Recursive estimation\nand maximization of a posteriori probabilities, application to transition-based\nconnectionist speech recognition. Technical Report TR-94-064, International\nComputer Science Institute, Berkeley, CA.\n--, & N. MORGAN. 1994. Connectionist Speech Recognition - A Hybrid Approach.\nKluwer Academic Publishers.\n--, & C. J. WELLEKENS. 1989. Links between Markov models and multilayer\nperceptrons. In Advances in Neural Information Processing Systems 1, ed. by\nD.J. Touretzky, 502-510, San Mateo. Morgan Kaufmann.\nCOLE, R.A., M. FANTY, & T. LANDER. 1994. Telephone speech corpus development at CSL U. In Proceedings Int \'I Conference on Spoken Language Processing,\nYokohama, Japan.\nDEMPSTER, A. P., N. M. LAIRD, & D. B. RUBIN. 1977. Maximum likelihood\nfrom incomplete data via the EM algorithm. Journal of the Royal Statistical\nSociety, Series B 34.1-38.\nGLASS, J. R., 1988. Finding Acoustic Regularities in Speech Applications to Phonetic Recognition. M.LT dissertation.\nKATAGIRI, S., C.H. LEE, & JUANG B.H. 1991. New discriminative training\nalgorithms based on the generalized probabilistic decent method. In Proc. of\nthe IEEE Workshop on Neural Netwroks for Signal Processing, ed. by RH.\nJuang, S.Y. Kung, & C.A. Kamm, 299-308 .\nKONIG, Y., & N. MORGAN. 1994. Modeling dynamics in connectionist speech\nrecognition - the time index model. In Proceedings Int\'l Conference on Spoken\nLanguage Processing, 1523-1526, Yokohama, Japan.\nLIPORACE, L. A. 1982. Maximum likelihood estimation for multivariate observations of markov sources. IEEE Trans. on Information Theory IT-28.729-734.\nMORGAN, N., H. BOURLARD, S. GREENBERG, & H. HERMANSKY. 1994. Stochastic perceptual auditory-event-based models for speech recognition. In Proceedings Int\'l Conference on Spoken Language Processing, 1943-1946, Yokohama,\nJapan.\n\n\x0c'
p83196
sg332
S'A MODEL OF AUDITORY STREAMING\nSusan L. McCabe & Michael J. Denham\nNeurodynamics Research Group\nSchool of Computing\nUniversity of Plymouth\nPlymouth PL4 8AA, u.K.\n\nABSTRACT\nAn essential feature of intelligent sensory processing is the ability to\nfocus on the part of the signal of interest against a background of\ndistracting signals, and to be able to direct this focus at will. In this\npaper the problem of auditory scene segmentation is considered and a\nmodel of the early stages of the process is proposed. The behaviour of\nthe model is shown to be in agreement with a number of well known\npsychophysical results. The principal contribution of this model lies in\ndemonstrating how streaming might result from interactions between\nthe tonotopic patterns of activity of input signals and traces of previous\nactivity which feedback and influence the way in which subsequent\nsignals are processed.\n\n1 INTRODUCTION\nThe appropriate segmentation and grouping of incoming sensory signals is important in\nenabling an organism to interact effectively with its environment (Llinas, 1991). The\nformation of associations between signals, which are considered to arise from the same\nexternal source, allows the organism to recognise significant patterns and relationships\nwithin the signals from each source without being confused by accidental coincidences\nbetween unrelated signals (Bregman, 1990). The intrinsically temporal nature of sound\nmeans that in addition to being able to focus on the signal of interest, perhaps of equal\nsignificance, is the ability to predict how that signal is expected to progress; such\nexpectations can then be used to facilitate further processing of the signal. It is important\nto remember that perception is a creative act (Luria, 1980). The organism creates its\ninterpretation of the world in response to the current stimuli, within the context of its\ncurrent state of alertness, attention, and previous experience. The creative aspects of\nperception are exemplified in the auditory system where peripheral processing\ndecomposes acoustic stimuli. Since the frequency spectra of complex sounds generally\n\n\x0cA Model of Auditory Streaming\n\n53\n\noverlap, this poses a complicated problem for the auditory system : which parts of the\nsignal belong together, and which of the subgroups should be associated with each other\nfrom one moment to the next, given the extra complication of possible discontinuities\nand occlusion of sound signals? The process of streaming effectively acts to to associate\nthose sounds emitted from the same source and may be seen as an accomplishment,\nrather than the breakdown of some integration mechanism (Bregman, 1990).\nThe cognitive model of streaming, proposed by (Bregman, 1990), is based primarily on\nGestalt principles such as common fate, proximity, similarity and good continuation.\nStreaming is seen as a mUltistage process, in which an initial, preattentive process\npartitions the sensory input, causing successive sounds to be associated depending on the\nrelationship between pitch proximity and presentation rate. Further refinement of these\nsound streams is thought to involve the use of attention and memory in the processing of\nsingle streams over longer time spans.\nRecently a number of computational models which implement these concepts of\nstreaming have been developed. A model of streaming in which pitch trajectories are\nused as the basis of sequential grouping is proposed by (Cooke, 1992). In related work,\n(Brown, 1992) uses data-driven grouping schema to form complex sound groups from\nfrequency components with common periodicity and simultaneous onset. Sequential\nassociations are then developed on the basis of pitch trajectory. An alternative approach\nsuggests that the coherence of activity within networks of coupled oscillators, may be\ninterpreted to indicate both simultaneous and sequential groupings (Wang, 1995),\n(Brown, 1995), and can, therefore, also model the streaming of complex stimuli. Sounds\nbelonging to the same stream, are distinguished by synchronous activity and the\nrelationship between frequency proximity and stream formation is modelled by the\ndegree of coupling between oscillators.\nA model, which adheres closely to auditory physiology, has been proposed by (Beauvois,\n1991). Processing is restricted to two frequency channels and the streaming of pure\ntones. The model uses competitive interactions between frequency channels and leaky\nintegrator model neurons in order to replicate a number of aspects of human\npsychophysical behaviour. The model, described here, used Beauvois\' work as a starting\npoint, but has been extended to include multichannel processing of complex signals. It\ncan account for the relationship streaming and frequency difference and time interval\n(Beauvois, 1991), the temporal development and variability of streaming perceptions\n(Anstis, 1985), the influence of background organisation on foreground perceptions\n(Bregman, 1975), as well as a number of other behavioural results which have been\nomitted due to space limitations.\n\n2\n\nTHEMODEL\n\nWe assume the existence of tonotopic maps, in which frequency is represented as a\ndistributed pattern of activity across the map. Interactions between the excitatory\ntonotopic patterns of activity reflecting stimulus input, and the inhibitory tonotopic\nmasking patterns, resulting from previous activity, form the basis of the model. In order\nto simulate behavioural experiments, the relationship between characteristic frequency\nand position across the arrays is determined by equal spacing within the ERB scale\n(Glasberg, 1990). The pattern of activation across the tonotopic axis is represented in\nterms of a Gaussian function with a time course which reflects the onset-type activity\nfound frequently within the auditory system.\n\n\x0cs. L. MCCABE, M. J. DENHAM\n\n54\n\nInput signals therefore take the form :\ni(x,t) = CI (t - t Onset)e-c2(t-t Ortut )e 2~2lfc(x}-r.)2\n[1]\nwhere i(x.t) is the probability of input activity at position x, time t. C} and C; are\nconstants, tan.m is the starting time of the signal, fc (x) is the characteristic frequency at\nposition x,/. is the stimulus frequency, and a determines the spread of the activation.\n\nIn models where competitive interactions within a single network are used to model the\nstreaming process, such as (Beauvois, 1991), it is difficult to see how the organisation of\nbackground sounds can be used to improve foreground perceptions (Bregman, 1975)\nsince the strengthening of one stream generally serves to weaken others. To overcome\nthis problem, the model of preattentive streaming proposed here, consists of two\ninteracting networks, the foreground and background networks, F and B; illustrated in\nfigure 1. The output from F indicates the activity, if any, in the foreground, or attended\nstream, and the output from B reflects any other activity. The interaction between the\ntwo eventually ensures that those signals appearing in the output from F, i.e. in the\nforeground stream, do not appear in the output from B, the background; and vice versa.\nIn the model, strengthening of the organisation of the background sounds, results in the\n\'sharpening\' of the foreground stream due to the enhanced inhibition produced by a more\ncoherent background.\nrre\n\nrnR\n\nFigure 1 : Connectivity of the Streaming Networks.\nNeurons within each array do not interact with each other but simply perform a\nsummation of their input activity. A simplified neuron model with low-pass filtering of\nthe inputs, and output representing the probability offiring, is used:\np(x, t) = cr[~ Vj(x, t)], where cr(y)\n\n= I+~_Y\n\n[2]\n\nJ\n\nThe inputs to the foreground net are :\nVI (x,t) = (1- ::)VI (x,t-dt) + VI . ~(i(x,t?.dt\n\n[3]\n\nV2(X,t)\n\n= (1- ::)V2(X, t- dt) + V2?mFi(x,t- dt?\n\n.dt\n\n[4]\n\nV3(X, t)\n\n= (1- ~)v3(x,t-dt) + V3 . ~(mB(x,t- dt?.dt\n\n[5]\n\nwhere x is the position across the array, time t, sampling rate dt. "tj are time constants\nwhich determine the rate of decay of activity, V; are weights on each of the inputs, and\nt/J(y) is a function used to simulate the stochastic properties of nerve firing which returns\na value of J or 0 with probability y.\n\n\x0c55\n\nA Model of Auditory Streaming\n\nThe output activity pattern in the foreground net and its \'inverse\', mF(x,f) and mFi(x,t),\nare found by :\nmF(x, t) = cr[v\\ (x, t) -l\'\\(V2(X, t), n) -l\'\\(V3(X, t), n)]\n\n[6]\n\nN\n\nmFi(x, t) = max {[~ ~ mF(xi\' t - dt)] - mF(x, t- dt), O}\n\n[7]\n\ni=\\\n\nwhere 17(v(x,f),n) is the mean of the activity within neighbourhood n of position x at time\nt and N is the number of frequency channels. Background inputs are similarly calculated.\nTo summarise, the current activity in response to the acoustic stimulus forms an\nexcitatory input to both the foreground and background streaming arrays, F and B. In\naddition, F receives inhibitory inputs reflecting the current background activity, and the\ninverse of the current foreground activity. The interplay between the excitatory and\ninhibitory activities causes the model to gradually focus the foreground stream and\nexclude extraneous stimuli. Since the patterns of inhibitory input reflect the distributed\npatterns of activity in the input, the relationship between frequency difference and\nstreaming, results simply from the graded inhibition produced by these patterns. The\nrelationship between tone presentation rate and streaming is determined by the time\nconstants in the model which can be tuned to alter the rate of decay of activity.\nTo enable comparisons with psychophysical results, we view the judgement of coherence\nor streaming made by the model as the difference between the strength of the foreground\nresponse to one set of tones compared to the other. The strength of the response to a\ngiven frequency, Resp(f,t), is a weighted sum of the activity within a window centred on\nthe frequency :\nRespif, t) =\n\nW\n\n~\n\ni=-W\n\nmF(x(j) + i, t) * e\n\n_k...\n2(12\n\n[8]\n\nwhere W determines the size of the window centred on position, x(/), the position in the\nmap corresponding to frequency f, and a determines the spread of the weighting\nfunction about position x(/).\nThe degree of coherence between two tones, say hand h\' is assumed to depend on the\ndifference in strength of foreground response to the two :\nC hif\n\no\n\nI:\n\\,j 2,\n\nt) = 1 _/ Resp(fj .t)--Resp{j2,t) /\nResp(fj . t}+Resp(/2,t)\n\n[9]\n\nwhere Coh(f;,h,t) ranges between 0, when Resp(f;,t) or Resp(h,t) vanishes and the\ndifference between the responses is a maximum, indicating maximum streaming, and 1,\nwhen the responses are equal and maximally coherent. Values between these limits are\ninterpreted as the degree of coherence, analogous to the probability of human subjects\nmaking ajudgement of coherence (Anstis, 1985), (Beauvois, 1991).\n\n3\n\nRESULTS\n\nExperiments exploring the effect of frequency interval and tone presentation rate and\nstreaming are described in (Beauvois, 1991). Subjects were required to listen to an\nalternating sequence of tones, ABABAB ... for 15 seconds, and then to judge whether at\nthe end of the sequence they perceived an oscillating, trill-like, temporally coherent\nsequence, or two separate streams, one of interrupted high tones, the other of interrupted\n\n\x0cs. L. MCCABE, M. J. DENHAM\n\n56\n\nlow tones. Their results showed clearly an increasing tendency towards stream\nsegmentation both with increasing frequency difference between A and B, and\nincreasing tone presentation rate, results the model manages substantially to reproduce;\nas 100r---~c_--~--~--------_,\nmay be seen in figure 2.\n100r---~~--------~--~----\'\n4.76 tones/sec\n\nI eo\n\n60\n\n~ 60\n~\n\n11\n\n~\n\nE20\n0\n1000\n\nOL---~----~----~--~--~\n\n1100\n\n1200\n\n1300\n\n1<400\n\n1500\n\n1000\n\n1100\n\n1200\n\n!\n\n60\n\n1500\n\n7.69 tones/sec\n\n5.88 tones/sec\n~\n\n1?>0\n\n100~~~----------~--------\'\n\n100\n80\n\n1300\n\n80\n\n~\n\n?\n\n\'li\n~\n\n?>\n\nE 20\n0\n1000\n\n20\noL---~----~----~--~----~\n\n1100\n\n1200\n\n1?Xl\n\n1500\n\n1000\n\n1100\n\n1200\n\n1300\n\n1?>0\n\n1500\n\n100r---~----------~--~----\'\n\n100\n\n20 tones/sec\n\n11.11 tones/sec\n\n?\nS\n? .co\nl!\n\n1300\n\n80\n\n~\n\n60\n\n1J\n~\n\neo\n\n~\n\n20\n\n20\n\no\n1000\n\nOL---~----~----~--~--~\n\n1100\n\n1200\n\n1300\n\n1?Xl\n\n1500\n\n1000\n\n1100\n\n1200\n\n1300\n\n1.ao\n\n1500\n\nFigure 2 : Mean Psychophysical \'0\' and Model ,*, Responses to the Stimulus ABAB ...\n(A=lOOO Hz, B as indicated along X axis (Hz), tone presentation rates, as shown.)\nIn investigating the temporal development of stream segmentation, (Anstis, 1985) used a\nsimilar stimulus to the experiment described above, but in this case subjects were\nrequired to indicate continuously whether they were perceiving a coherent or streaming\nsignal. As can be seen in figure 3, the model clearly reproduces the principal features\nfound in their experiments, i.e. the probability of hearing a single, fused, stream declines\nduring each run, the more rapid the tone presentation rate, the quicker stream\nsegmentation occurs, and the judgements made were quite variable during each run.\nIn an experiment to investigate whether the organisation of the background sounds\naffects the foreground, subjects were required to judge whether tone A was higher or\nlower than B (Bregman, 1975). This judgement was easy when the two tones were\npresented in isolation, but performance degraded significantly when the distractor tones,\nX, were included. However, when a series of \'captor\' tones, C, with frequency close to X\nwere added, the judgement became easier, and the degree of improvement was inversely\nrelated to the difference in frequency between X and C. In the experiment, subjects\nreceived an initial priming AB stimulus, followed by a set of 9 tones : CCCXABXCC.\nThe frequency of the captor tones, was manipulated to investigate how the proximity of\n\'captor\' to \'distractor\' tones affected the required AB order judgement.\n\n\x0c57\n\nA Model of Auditory Streaming\n\nFigure 3 : The Probability of Perceptual Coherence as a Function of Time in Response\nto Two Alternating Tones. Symbols: \'.\' 2 tones/s, \'0\' 4 tones/s, \'+\' 8 tones/so\nIn order to model this experiment and the effect of priming, an \'attentive\' input, focussed\non the region of the map corresponding to the A and B tones, was included. We assume,\nas argued by Bregman, that subjects\' performance in this task is related to the degree to\nwhich they are able to stream the AB pair separately. His D parameter is a measure of\nthe degree to which ABIBA can be discriminated. The model\'s performance is then\ngiven by the strength of the foreground response to the AB pair as compared to the\ndistractor tones, and Coh([A B],X) is used to measure this difference. The model exhibits\na similar sensitivity to the distractor/captor frequency difference to that of human\nsubjects, and it appears that the formation of a coherent background stream allows the\nmodel to distinguish the foreground group more clearly.\n\nA)\n\nB)\' ~------~------~----~\n09\n\nt.4eM toherente XAB.X.\n\n08\n\n2S00\n\n0)\n\nN\n\n06\n\nI!OO\n\nE\n\n05\n04\n_____ . . _____ e -- --\n\nG?????..??c?????????(???? ..?? ....?...._..........???co.... ?..??,\n\n03 " .. ? .. ?? .... &egm ..... Op...."...,.\n\n02\n01\n\no\n\n500\n\n\' 000\n\n\'500\n\nCap\'Of hoquoncy jHz)\n\nTIME\n\nFigure 4 : A) Experiment to Demonstrate the Formation ofMuItiple Streams,\n(Bregman, 1975). B) Model Response; \'?\'Mean Degree of Doherence to XABX, \'0\',\nBregman\'s D Parameter, \'+\' Model\'s Judgement of Coherence.\n\n4\n\nDISCUSSION\n\nThe model of streaming which we have presented here is essentially a very simple one,\nwhich can, nevertheless, successfully replicate a wide range of psychophysical\nexperiments. Embodied in the model is the idea that the characteristics of the incoming\nsensory signals result in activity which modifies the way in which subsequent incoming\n\n\x0c58\n\ns. L. MCCABE, M. J. DENHAM\n\nsignals are processed. The inhibitory feedback signals effectively comprise expectations\nagainst which later signals are processed. Processing in much of the auditory system\nseems to be restricted to processing within frequency \'channels\'. In this model, it is\nshown how local interactions, restricted almost entirely to within-channel activity, can\nform a global computation of stream formation. It is not known where streaming occurs\nin the auditory system, but feedback projections both within and between nuclei are\nextensive, perhaps allowing an iterative refinement of streams. Longer range projections,\noriginating from attentive processes or memory, may modify local interactions to\nfacilitate the extraction of recognised or interesting sounds.\nThe relationship between streaming and frequency interval, could be modelled by\nsystematically graded inhibitory weights between frequency channels. However, in the\nmodel this relationship arises directly from the distributed incoming activity patterns,\nwhich seems a more robust and plausible solution, particularly if one takes the need to\ncope with developmental changes into account. Although to simplify the simulations\nperipheral auditory processing was not included in the model, the activity patterns\nassumed as input can be produced by the competitive processing of the output from a\ncochlear model.\nAn important aspect of intelligent sensory processing is the ability to focus on signals of\ninterest against a background of distracting signals, thereby enabling the perception of\nsignificant temporal patterns. Artificial sensory systems, with similar capabilities, could\nact as robust pre-processors for other systems, such as speech recognisers, fault detection\nsystems, or any other application which required the dynamic extraction and temporal\nlinking of subsets of the overall signal.\nValues Used For Model Parameters\na=.005, c)=75, c2=100, V=[lOO 5 5 5 5], T=[.05 .6 .6 .6 .6], n=2, N=lOO\nReferences\nAnstis, S., Saida, S., J. (1985) Exptl Psych, 11(3), pp257-271\nBeauvois, M.W., Meddis, R (1991) J. Exptl Psych, 43A(3), pp517-541\nBregman, AS., Rudnicky, AI. (1975) J. ExptJ Psych, 1(3), pp263-267\nBregman, A.S. (1990) \'Auditory scene analysis\', MIT Press\nBrown, GJ. (1992) University of Sheffield Research Reports, CS-92-22\nBrown, GJ., Cooke, M. (1995) submitted to IJCAI workshop on Computational\nAuditory Scene Analysis\nCooke, M.P. (1992) Computer Speech and Language 6, pp 153-173\nGlasberg, B.R., Moore, B.C.J. (1990) Hearing Research, 47, pp103-138\nL1inas, RR, Pare, D. (1991) Neuroscience, 44(3), pp521-535\nLuria, A (1980) \'Higher cortical functions in man\', NY:Basic\nvan Noorden, L.P.AS. (1975) doctoral dissertation, published by Institute for Perception\nResearch, PO Box 513, Eindhoven, NL\nWang, D.L. (1995) in \'Handbook of brain theory and neural networks\', MIT Press\n\n\x0cPART II\nNEUROSCIENCE\n\n\x0c\x0c'
p83197
sg121
S'The Gamma MLP for Speech Phoneme\nRecognition\n\nSteve\n\nLawrence~\n\nAh Chung Tsoi, Andrew D. Back\n{lawrence,act,back}Oelec.uq.edu.au\n\nDepartment of Electrical and Computer Engineering\nUniversity of Queensland\nSt. Lucia Qld 4072 Australia\n\nAbstract\nWe define a Gamma multi-layer perceptron (MLP) as an MLP\nwith the usual synaptic weights replaced by gamma filters (as proposed by de Vries and Principe (de Vries and Principe, 1992)) and\nassociated gain terms throughout all layers. We derive gradient\ndescent update equations and apply the model to the recognition\nof speech phonemes. We find that both the inclusion of gamma\nfilters in all layers, and the inclusion of synaptic gains, improves\nthe performance of the Gamma MLP. We compare the Gamma\nMLP with TDNN, Back-Tsoi FIR MLP, and Back-Tsoi I1R MLP\narchitectures, and a local approximation scheme. We find that the\nGamma MLP results in an substantial reduction in error rates.\n\n1\n1.1\n\nINTRODUCTION\nTHE GAMMA FILTER\n\nInfinite Impulse Response (I1R) filters have a significant advantage over Finite Impulse Response (FIR) filters in signal processing: the length of the impulse response\nis uncoupled from the number of filter parameters. The length of the impulse response is related to the memory depth of a system, and hence I1R filters allow a\ngreater memory depth than FIR filters of the same order. However, I1R filters are\n*http://www.neci.nj.nec.com/homepages/lawrence\n\n\x0c786\n\nS. LAWRENCE, A. C. TSOI, A. D. BACK\n\nnot widely used in practical adaptive signal processing. This may be attributed\nto the fact that a) there could be instability during training and b) the gradient\ndescent training procedures are not guaranteed to locate the global optimum in the\npossibly non-convex error surface (Shynk, 1989).\nDe Vries and Principe proposed using gamma filters (de Vries and Principe, 1992),\na special case of IIR filters, at the input to an otherwise standard MLP. The gamma\nfilter is designed to retain the uncoupling of memory depth to the number of parameters provided by IIR filters, but to have simple stability conditions.\nThe output of a neuron in a multi-layer perceptron is computed using 1\nI\nf L--i=O WkiYi\nI\n1-1)\nDe Vries and Principe consider adding short\nYk =\n.\n\n("",Nr-l\n\nterm memory with delays: YkI --\n\nf\n\n("",Nr-l\nL--i=O "",K\nL--j=O 9kij (t I\n\nh\nJ.) Yi1-1 (t - J.)) were\n\n(r!i)!\n\n9~ij =\ntj-le-/-\'~it\nj = 1, ... , K . The depth of the memory is controlled\nby J.t, and K is the order of the filter. For the discrete time case, we obtain the\nrecurrence relation: zo(t) = x(t) and Zj(t) = (1 - J.t)Zj(t - 1) + J.tZj-l (t - 1) for\nj = 1, ... , K. In this form, the gamma filter can be interpreted as a cascaded series\nof filter modules, where each module is a first order IIR filter with the transfer function q-(I-/-,) , where qZj(t) ~ Zj(t + 1). We have a filter with K poles, all located\nat 1 - J.t. Thus, the gamma filter may be considered as a low pass filter for J.t < 1.\nThe value of J.t can be fixed, or it can be adapted during training.\n\n2\n\nNETWORK MODELS\n\nFigure 1: A gamma filter synapse with an associated gain term \'c\'.\nWe have defined a gamma MLP as a multi-layer perceptron where every synapse\ncontains a gamma filter and a gain term, as shown in figure 1. The motivation\nbehind the inclusion of the gain term is discussed later. A separate J.t parameter\nis used for each filter. Update equations are derived in a manner analogous to the\nstandard MLP and can be found in Appendix A. The model is defined as follows.\n\nlwhere yi is the output of neuron k in layer I, Nl is the number of neurons in layer I,\nis the weight connecting neuron k in layer I to neuron i in layer I - 1, yb = 1 (bias),\nand / is commonly a sigmoid function.\n\nWii\n\n\x0cThe Gamma MLP for Speech Phoneme Recognition\n\n787\n\nDefinition 1 A Gamma MLP with L layers excluding the input layer (0,1, ... , L),\ngamma filters of order K, and No, N 1 , ... , NL neurons per layer, is defined as\n\nf (x~ (t))\nN\'-l\n\nK\n\ni=O\n\nj=O\n\nL C~i(t) L wL j (t)Zkij (t)\n\nZiij (t)\nZiij (t)\n\n(1- ILL(t))zkij(t -1) + ILL(t)zki(j_I)(t -1)\ny!-l (t)\n\n(1)\n\n1\n\n~j ~\n\nK\n\nj=O\neO / 2 _e- o / 2\n\nwhere y(t) = neuron output, c\'ki = synaptic gain, f(a) = eO/2+e 0/2, k\n1,2, ... ,N, (neuronindex), I = 0,1, ... ,L(layer), and Ziijli=O = 1, W~ij li=O,#O\n0, C~ij li=O = 1(bias).\n\no\nFor comparison purposes, we have used the TDNN (Time Delay Neural Network)\narchitecture2 , the Back-Tsoi FIR3 and I1R MLP architectures (Back and Tsoi,\n1991a) where every synapse contains an FIR or I1R filter and a gain term, and the\nlocal approximation algorithm used by Casdagli (k-NN LA) (Casdagli, 1991)4. The\nGamma MLP is a special case of the I1R MLP.\n\n3\n3.1\n\nTASK\nMOTIVATION\n\nAccurate speech recognition requires models which can account for a high degree\nof variability in the data. Large amounts of data may be available but it may be\nimpractical to use all of the information in standard neural network models.\nHypothesis: As the complexity of a problem increases (higher dimensionality, greater\n\nvariety of training data), the error surface of a neural network becomes more complex. It may contain a number of local minima5 many of which may be much worse\nthan the global minimum. The training (parameter estimation) algorithms become\n"stuck" in local minima which may be increasingly poor compared to the global\noptimum. The problem suffers from the so called "curse of dimenSionality" and the\n2We use TDNN to refer to an MLP with a time window of inputs, not the replicated\narchitecture introduced by Lang (Lang et al., 1990) .\n3We distinguish the Back-Tsoi FIR network from the Wan FIR network in that the\nWan architecture has no synaptic gains, and the update algorithms are different. The\nBack-Tsoi update algorithm has provided better convergence in previous experiments.\n4Casdagli created an affine model of the following form for each test pattern: yi =\naD + L~=l ai~, where k is the number of neighbors, j = 1, ... , k, and n is the input\ndimension. The resulting model is used to find y for the test pattern.\n5We note that it can be difficult to distinguish a true local minimum from a long plateau\nin the standard backpropagation algorithm.\n\n\x0c788\n\nS. LAWRENCE, A. C. TSOI, A. D. BACK\n\ndifficulty in optimizing a function with limited control over the nature of the error\nsurface.\nWe can identify two main reasons why the application of the Gamma MLP may\nbe superior to the standard TDNN for speech recognition: a) the gamma filtering\noperation allows consideration of the input data using different time resolutions and\ncan account for more past history of the signal which can only be accounted for in\nan FIR or TDNN system by increasing the dimensionality of the model, and b)\nthe low pass filtering nature of the gamma filter may create a smoother function\napproximation task, and therefore a smoother error surface for gradient descent 6 .\n\n3.2\n\nTASK DETAILS\nModel Input Window\n\n[~\n\nNetworl( Output 1\nTarget Function\n\nClassification 0\nNetworl( Output 2\n\nII\n\n;\n\n~}\n!\n\nFrames of RASTA data\n\n...::.. ...:::\'.!\'}\n\nj :\n\n~.} ,.;:!.. ""::\'I\'}\n. ; i\n\n~\n\nl I~\n\n~\n\nSequence End\n\n~\n\nFigure 2: PLP input data format and the corresponding network target functions for the\nphoneme "aa" .\nOur data consists of phonemes extracted from the TIMIT database and organized\nas a number of sequences as shown in figure 2 (example for the phoneme "aa").\nOne model is trained for each phoneme. Note that the phonemes are classified in\ncontext, with a number of different contexts, and that the surrounding phonemes\nare labelled only as not belonging to the target phoneme class. Raw speech data\nwas pre-processed into a sequence of frames using the RASTA-PLP v2.0 software7 .\nWe used the default options for PLP analysis. The analysis window (frame) was\n20 ms. Each succeeding frame overlaps with the preceding frame by 10 ms. 9\nPLP coefficients together with the signal power are extracted and used as features\ndescribing each frame of data. Phonemes used in the current tests were the vowel\n"aa" and the fricative "s" . The phonemes were extracted from speakers coming\nfrom the same demographic region in the TIMIT database. Multiple speakers were\nused and the speakers used in the test set were not contained in the training set.\nThe training set contained 4000 frames, where each phoneme is roughly 10 frames.\nThe test set contained 2000 frames, and an additional validation set containing 2000\nframes was used to control generalization.\n6If we consider a very simple network and derive the relationship of the smoothness of\nthe required function approximation to the smoothness of the error surface this statement\nappears to be valid. However, it is difficult to show a direct relationship for general\nnetworks.\n7 Obtained from ftp:/ /ftp.icsi.berkeley.edu/pub/speech/rasta2.0.tar.Z.\n\n\x0cThe Gamma MLP for Speech Phoneme Recognition\n\n4\n\n789\n\nRESULTS\n\nTwo outputs were used in the neural networks as shown by the target functions in\nfigure 2, corresponding to the phoneme being present or not. A confidence criterion\nwas used: Ymax x (Ymax - Ymin) (for soft max outputs). The initial learning rate was\n0.1, 10 hidden nodes were used, FIR and Gamma orders were 5 (6 taps), the TDNN\nand k-NN models had an input window of 6 steps in time, the tanh activation function was used, target outputs were scaled between -0.8 and 0.8, stochastic update\nwas used, and initial weights were chosen from a set of candidates based on training\nset performance. The learning rate was varied over time according to the schedule:\n\n= \'TIo/ (N/2 + max (1,(cI- ",~!(o.Cj(n\n?)) where\'TI = learning rate, \'TIo = initial\n(I\nlearning rate, N = total epochs, n = current epoch, Cl = 50, C2 = 0.65. This is\n\'TI\n\nC2 N\nC2)N\n\nsimilar to the schedule proposed in (Darken and Moody, 1991) with an additional\nterm to decrease the learning rate towards zero over the final epochs 8 .\nI\n\nTrain Error %\nFIR MLP\nGamma MLP\nTDNN\nk-NN LA\n\nI\n\n2-NN\n\nI\n\nTest Error %\nFIR MLP\nGamma MLP\nTDNN\nk-NN LA\n\nI\n\n2-NN\n\nI\n\nTest False +ve\nFIR MLP\nGamma MLP\nTDNN\nk-NN LA\n\nI\n\nTest False -ve\nFIR MLP\nGamma MLP\nTDNN\nk-NN LA\n\nI\n\n5-NN\n\n1st layer\n17.6\n0.43\n0 .39\n7.78\n\nI\n\nAll layers\n14.5\n1.5\n5.73\n0 .88\n\nI\n\nGains , 1st layer\n27.2\n0 .59\n6 .07\n0 .12\n\nI\n\nGains , all layers\n40 .9\n19.8\n5.63\n1.68\n14.4\n0.86\n\nI\n\n5-NN l i s t layer\n22.2\n0.97\n0.16\n14 .7\n\nI\n\nAll layers\n20.4\n0 .61\n13.5\n0 . 33\n\nI\n\nGams , 1st layer\n29\n0.14\n12.8\n1.0\n\nI\n\nGams , all layers\n41\n21\n12.7\n0.50\n24.5\n0 .68\n\nI\n\nAll layers\n2.0\n11.4\n7 .01\n0.47\n\nI\n\nAll layers\n44.1\n5 .6\n2.2\n30.4\n\n0\n\n0\n\n31\n\nI\n\n2-NN\n\n2-NN\n\n53\n\nI\n\n28 .4\n\nI\n\n22.6\n\nI\n\nI\n\n5-NN l i s t layer\n13.5\n0 .67\n7 .94\n0.45\n\nI\n\nGams , 1st layer\n4.5\n0 .77\n6.83\n0 .34\n\nI\n\nGams , all layers\n31.3\n49.0\n8.05\n1.8\n13\n0 .27\n\nI\n\n17.4\n\nI\n\n5-NN l i s t layer\n44.9\n2 .6\n32 .2\n1.2\n\nI\n\nGams , 1st layer\n92.9\n2.4\n2 .8\n28.4\n\nI\n\nGams , all\n66.4\n24.7\n54.6\n\nlayers\n53\n4.4\n1.8\n\nI\n\n56.8\n\nTable 1: Results comparing the architectures and the use of filters in all layers and\nsynaptic gains for the FIR and Gamma MLP models. The NMSE is followed by the\nstandard deviation. The TDNN results are listed under an arbitrary column heading\n(gains and 1st layer/alilayers does not apply).\nThe results of the simulations are shown in table 19 . Each result represents an\naverage over four simulations with different random seeds - the standard deviation\nof the four individual results is also shown. The FIR and Gamma MLP networks\nhave been tested both with and without synaptic gains, and with and without\nfilters in the output layer synapses. These results are for the models trained on\nthe "s" phoneme, results for the "aa" phoneme exhibit the same trend. "Test false\nnegative" is probably the most important result here, and is shown graphically\nin figure 3. This is the percentage of times a true classification (ie. the current\n8Without this term we have encountered considerable parameter fluctuation over the\nlast epoch.\n9NMSE\n\n= 2:~=1 (d(k) -\n\ny(k))2\n\nI\n\n(2:~=1 (d(k) - (2:~=1 d(k)) INr) IN.\n\n\x0c790\n\nS. LAWRENCE, A. C. TSOI, A. D. BACK\n60\n\n--"\n\n55\nQ)\n\n~\n\n~\n\n45\n\nQ)\n\n40\n\n\'"\n\nLL\n\n35\n\ni\n\n30\n\nI-\n\nk-NN LA _._._ ..\n\nf------ f\n\nZ\n\n.!!2\n\n~ Ga:~~TDNN\n~t~ -_=-=-~-\'\n.. _.-\n\n50\n\nI - - -__ I\n\n1\n\n-r-?---- ----+ --- -\n\n25\n20\n2-NN\n\n5-NN\n\nNG 1 L NG AL\n\nG lL\n\nGAL\n\nFigure 3: Percentage of false negative classifications on the test set. NG=No gains,\nG=Gains, lL=filters in the first layer only, AL=filters in all layers. The error bars show\nplus and minus one standard deviation. The synaptic gains case for the FIR MLP is not\nshown as the poor performance compresses the remainder of the graph. Top to bottom,\nthe lines correspond to: k-NN LA (left), TDNN, FIR MLP, and Gamma MLP.\nphoneme is present) is incorrectly reported as false. From the table we can see\nthat the Gamma MLP performs Significantly better than the FIR MLP or standard\nTDNN models for this problem. Synaptic gains and gamma filters in all layers\nimprove the performance of the Gamma MLP, while the inclusion of synaptic gains\npresented difficulty for the FIR MLP. Results for the IIR MLP are not shown - we\nhave been unable to obtain significant convergence lO . We investigated values of k\nnot listed in the table for the k-NN LA model, but it performed poorly in all cases.\n\n5\n\nCONCLUSIONS\n\nWe have defined a Gamma MLP as an MLP with gamma filters and gain terms in\nevery synapse. We have shown that the model performs significantly better on our\nspeech phoneme recognition problem when compared to TDNN, Back-Tsoi FIR and\nIIR MLP architectures, and Casdagli\'s local approximation model. The percentage\nof times a phoneme is present but not recognized for the Gamma MLP was 44%\nlower than the closest competitor, the Back-Tsoi FIR MLP model.\nThe inclusion of gamma filters in all layers and the inclusion of synaptic gains improved the performance of the Gamma MLP. The improvement due to the inclusion\nof synaptic gains may be considered non-intuitive to many - we are adding degrees\nof freedom, but no additional representational power. The error surface will be different in each case, and the results indicate that the surface for the synaptic gains\ncase is more amenable to gradient descent. One view of the situation is seen by\nBack & Tsoi with their FIR and IIR MLP networks (Back and Tsoi, 1991b): From\na signal processing perspective the response of each synapse is determined by polezero positions. With no synaptic gains, the weights determine both the static gain\nand the pole-zero positions of the synapses. In an experimental analysis performed\nby Back & Tsoi it was observed that some synapses devoted themselves to modellOTheoretically, the IIR MLP model is the most powerful model used here. Though it\nis prone to stability problems, the stability of the model can and was controlled in the\nsimulations performed here (basically, by reflecting poles that move outside the unit circle\nback inside). The most obvious hypothesis for the difficulty in training the model is related\nto the error surface and the nature of gradient descent. We expect the error surface to be\nconsiderably more complex for the IIR MLP model, and for gradient descent update to\nexperience increased difficulty optimizing the function.\n\n\x0cThe Gamma MLP for Speech Phoneme Recognition\n\n791\n\ning the dynamics of the system in question, while others "sacrificed" themselves to\nprovide the necessary static gains l l to construct the required nonlinearity.\n\nAPPENDIX A: GAMMA MLP UPDATE EQUATIONS\n~W~i;(t)\n\n=\n\n8J(t)\n-\'1 8\n\nI\n\nI\n\n()\n\nw",; t\n\nI\n\nI\n\n= \'1 6" (t)c", (t)Z"i; (t)\n\n(2)\n\n~C~i(t)\n~J\'~i (t)\n\n(3)\n\n=\n\n(4)\n\no\n\n=\n\nj=O\n\n(1 - J\'~i(t))a~,;(t -1) + J\'~i(t)a~iC;_I)(t - 1)\n+z~,(;_I)(t -1) - Z~i;(t - 1)\n\n(5)\n1 $j $ K\n\nI=L\n\n(6)\n\n1 $j $ K\n\n1\n\n(1 - J\';,,(t)).B;,,;(t -1)\n\nj=O\n\n+ J\';,,(t).B~"(;_l) (t -\n\n1)\n\n1 $j $K\n\n(7)\n\nAcknowledgments\nThis work has been partially supported by the Australian Research Council (ACT and\nADB) and the Australian Telecommunications and Electronics Research Board (SL).\n\nReferences\nBack, A. and Tsoi, A. (1991a). FIR and IIR synapses, a new neural network architecture\nfor time series modelling. Neural Computation, 3(3):337-350.\nBack, A. D. and Tsoi, A. C. (1991b). Analysis of hidden layer weights in a dynamic locally\nrecurrent network. In Simula, 0., editor, Proceedings International Conference on\nArtificial Neural Networks, ICANN-91, volume 1, pages 967-976, Espoo, Finland.\nCasdagli, M. (1991). Chaos and deterministic versus stochastic non-linear modelling. J.R.\nStatistical Society B, 54(2):302-328.\nDarken, C. and Moody, J. (1991). Note on learning rate schedules for stochastic optimization. In Neural Information Processing Systems 3, pages 832-838. Morgan Kaufmann.\nde Vries, B. and Principe, J. (1992). The gamma model- a new neural network for temporal\nprocessing. Neural Networks, 5(4):565-576.\nLang, K. J., Waibel, A. H., and Hinton, G. E. (1990). A time-delay neural network\narchitecture for isolated word recognition. Neural Networks, 3:23-43.\nShynk, J . (1989). Adaptive IIR filtering. IEEE ASSP Magazine, pages 4-21.\nllThe neurons were observed to have gone into saturation, providing a constant output.\n\n\x0c\x0cPART VII\nVISION\n\n\x0c\x0c'
p83198
sg4
S'A Computational Model of Prefrontal\nCortex Function\nTodd S. Braver\nDept. of Psychology\nCarnegie Mellon Univ.\nPittsburgh, PA 15213\n\nJonathan D. Cohen\nDept. of Psychology\nCarnegie Mellon Univ .\nPittsburgh , PA 15213\n\nDavid Servan-Schreiber\nDept. of Psychiatry\nUniv . of Pittsburgh\nPittsburgh , PA 15232\n\nAbstract\nAccumulating data from neurophysiology and neuropsychology\nhave suggested two information processing roles for prefrontal cortex (PFC): 1) short-term active memory; and 2) inhibition. We\npresent a new behavioral task and a computational model which\nwere developed in parallel. The task was developed to probe both\nof these prefrontal functions simultaneously, and produces a rich\nset of behavioral data that act as constraints on the model. The\nmodel is implemented in continuous-time , thus providing a natural\nframework in which to study the temporal dynamics of processing\nin the task. We show how the model can be used to examine the behavioral consequences of neuromodulation in PFC . Specifically, we\nuse the model to make novel and testable predictions regarding the\nbehavioral performance of schizophrenics, who are hypothesized to\nsuffer from reduced dopaminergic tone in this brain area.\n\n1\n\nIntroduction\n\nPrefrontal cortex (PFC) is an area of the human brain which is significantly expanded relative to other animals. There is general consensus that the PFC is centrally involved in higher cognitive activities such as planning , problem solving and\nlanguage. Recently, the PFC has been associated with two specific information processing mechanisms : short-term active memory and inhibition . Active memory is\nthe capacity of the nervous system to maintain information in the form of sustained\nactivation states (e.g. , cell firing) for short periods of time. This can be distinguished from forms of memory that are longer in duration and are instantiated as\n\n\x0c142\n\nTodd S. Braver, Jonathan D. Cohen, David Servan-Schreiber\n\nmodified values of physiological parameters (e.g., synaptic strength). Over the last\ntwo decades, there have been a large number of neurophysiological studies focusing\non the cellular basis of active memory in prefrontal cortex. These studies have revealed neurons in PFC that fire selectively to specific stimuli and response patterns,\nand that remain active during a delay between these. Investigators such as Fuster\n(1989) and Goldman-Rakic (1987) have argued from this data that PFC maintains\ntemporary information needed to guide behavioral responses through sustained patterns of neural activity. This hypothesis is consistent with behavioral findings from\nboth animal and human lesion studies, which suggest that PFC is required for tasks\ninvolving delayed responses to prior stimuli (Fuster, 1989; Stuss & Benson, 1986).\nIn addition to its role in active memory, many investigators have focused on the\ninhibitory functions of PFC. It has been argued that PFC representations are required to overcome reflexive or previously reinforced response tendencies in order\nto mediate a contextually appropriate - but otherwise weaker - response (Cohen &\nServan-Schreiber, 1992). Clinically, it has been observed that lesions to PFC are often associated with a syndrome of behavioral disinhibition, in which patients act in\nimpulsive and often socially inappropriate ways (Stuss & Benson, 1986). This syndrome has often been cited as evidence that PFC plays an important role inhibiting\nbehaviors which are compelling but socially inappropriate.\nWhile the involvement of PFC in both active memory and inhibition is generally\nagreed upon, computational models can play an important role in providing mechanisms by which to explain how these two information processing functions arise.\nThere are several computational models now in the literature which have focused\non either the active memory (Zipser, 1991), or inhibitory (Levine & Pruiett, 1989)\nfunctions of PFC, or both functions together (Dehaene & Changeux, 1989; Cohen & Servan-Schreiber, 1992). These models have been instrumental in explaining\nthe role of PFC in a variety of behavioral tasks (e.g., the Wisconsin Card Sort and\nStroop). However, these earlier models are limited by their inability to fully capture the dynamical processes underlying active memory and inhibition. Specifically,\nnone of the simulations have been tightly constrained by the temporal parameters\nfound in the behavioral tasks (e.g., durations of stimuli, delay periods, and response\nlatencies). This limitation is not found solely in the models, but is also a feature of\nthe behavioral tasks themselves. The tasks simulated were not structured in ways\nthat could facilitate a dynamical analysis of processing.\nIn this paper we address the limitations of the previous work by describing both a\nnew behavioral task and a computational model of PFC. These have been developed\nin parallel and, together, provide a useful framework for exploring the temporal\ndynamics of active memory and inhibition and their consequences for behavior. We\nthen go on to describe how this framework can be used to examine neuromodulatory\neffects in PFC, which are believed to playa critical role in both normal functioning\nand in psychiatric disorders, such as schizophrenia.\n\n2\n\nBehavioral Assessment of Human PFC Function\n\nWe have developed a task paradigm which incorporates two components central to\nthe function of prefrontal cortex - short-term active memory and inhibition - and\nthat can be used to study the dynamics of processing. The task is a variant of the\ncontinuous performance test (CPT), which is commonly used to study attention in\n\n\x0cA Computational Model of Prefrontal Cortex Function\n\n143\n\nbehavioral and clinical research. In a standard version of the task (the CPT-AX),\nletters are presented one at a time in the middle of a computer screen. Subjects are\ninstructed to press the target button to the letter X (probe stimulus) but only when\nit is preceded by an A (the cue stimulus). In previous versions of the CPT, subjects\nonly responded on target trials. In the present version of the task, a two response\nforced-choice procedure is employed; on non-A-X trials subjects are asked to press\nthe non-target button. This procedure allows for response latencies to be evaluated\non every trial , thus providing more information about the temporal dimensions of\nprocessing in the task .\nTwo additional modifications were made to the standard paradigm in order to\nmaximally engage PFC activity. The memory function of PFC is tapped by manipulating the delay between stimuli. In the CPT-AX , the prior stimulus (cue or\nnon-cue) provides the context necessary to decide how to respond to the probe letter . However, with a short delay (750 msec .), there is little demand on memory\nfor the prior stimulus. This is supported by evidence that PFC lesions have been\nshown to have no effect on performance when there is only a short delay (Stuss &\nBenson, 1986). With a longer delay (5000 msec.), however, it becomes necessary to\nmaintain a representation of the prior stimulus in order for it to be used as context\nfor responding to the current one. The ability of the PFC to sustain contextual\nrepresentations over the delay period can be determined behaviorally by comparing\nperformance on short delay trials (50%) against those with long delays (50%).\nThe inhibitory function of PFC is probed by introducing a prepotent response\ntendency that must be overcome to respond correctly. This tendency is introduced\ninto the task by increasing the frequency of target trials (A followed by X). In the\nremaining trials, there are three types of distractors: 1) a cue followed by a nontarget probe letter (e.g. , A-Y); 2) a non-cue followed by the target probe letter (e.g .,\nB-X); and a non-cue followed by a non-target probe letter (e.g., B-Y). Target trials\noccur 70% of the time, while each type of distract or trial occurs only 10% of the\ntime. The frequ ency of targets promotes the development of a strong tendency to\nrespond to the target probe letter whenever it occurs , regardless of the identity of\nthe cue (since a response to the X itself is correct 7 out of 8 times).\nThe ability to inhibit this response tendency can be examined by comparing accuracy on trials when the target occurs in the absence of the cue (B-X trials) , with\nthose made when neither the cue nor target occurs (i.e., B-Y trials , which provide a\nmeasure of non-specific response bias and random responding). Trials in which the\ncue but not the target probe appears (A-Y trials) are also particularly interesting\nwith respect to PFC function. These trials measure the cumulative influence of\nactive representations of context in guiding responses. In a normally functioning\nsystem, context representations should stabilize and increase in strength as time\nprogresses. Thus , it is expected that A- Y accuracy will tend to decrease for long\ndelay trials relative to short ones .\nAs mentioned above, the primary benefit of this paradigm is that it provides a\nframework in which to simultaneously probe the inhibitory and memory functions\nassociated with PFC. This is supported by preliminary neuroimaging data from\nour laboratory (using PET) which suggests that PFC is, in fact, activated during\nperformance of the task. Although it is simple in structure, the task also generates\na rich set of behavioral data. There are four stimulus conditions crossed with two\ndelay conditions for which both accuracy and reaction time performance can be\n\n\x0c144\n\nTodd S. Braver, Jonathan D. Cohen, David Servan-Schreiber\n\n100\n90\n80\n70\n\nAccurac, (Short Delay)\n\nAccurac, (Long Delay)\n\nV\n\nV\n\n60\n\n1-\n\nMODEL (Ace)\n.DATA (Ace)\n\nRT(ShortDelay)\n\nJ\nRT (Long Delay)\n\n750\n650\n\n!\n\nI\n\n550\n\n"\n\n.Ii\n\n1 450\n\n..:\n\n350\n250\n\n~\n-~~\n\nAX\n\n,\n\n,\n\n,\n\nAY\nBX\nBY\nTrial Condition\n\n09\n\'\'\'J!.\'\n\nAX\n\n-~~-~\n\nAY\nBX\nBY\nTrial Condition\n\nMODEL (Correct)\n--- MODEL (Incorrect)\n.. DATA (Correct)\n\'V DATA (Incorrect)\n\nFigure 1:\n\nSubjecl beha.viora.1 da.la. with model performa.nce s uperimpos ed . Top Panels: Acc ura.cy a.c ross\n\nboth dela.ys in a.1I four condilion s. Bottom Panels: Rea.ction times for both correc t a.nd incorrec t res pon se s in\na.1I condition s . Ba.r s repre sent s ta.nda.rd error of mea.s ure ment for the empirica.l da.ta..\n\nmeasured. Figure 1 shows data gathered from 36 college-age subjects performing\nthis task.\nIn brief, we found that: 1) Accuracy was relatively unchanged in the long delays\ncompared to the short, demonstrating that active memory was adequately supporting performance; 2) A-Y accuracy, however, did slightly decrease at long delays,\nreflecting the normal build-up of context representations over time; 3) Accuracy\non B-X trials was relatively high, supporting the assumption that subjects could\neffectively use context representations to inhibit prepotent responses ; 4) A distinct\npattern emerged in the latencies of correct and incorrect responses , providing information on the temporal dynamics of processing (i .e. , responses to A-Y trials are\nslow on correct trials and fast on incorrect ones; the pattern is reversed for B-X trials) . Taken together, the data provides specific, detailed information about normal\nPFC functioning, which act as constraints on the development and evaluation of a\ncomputational model.\n\n3\n\nA Computational Model of the CPT-AX\n\nWe have developed a recurrent network model which produces detailed information\nregarding the temporal course of processing in the CPT-AX task. The network is\ncomposed of three modules: an input module, a memory module, and an output\nmodule. The memory module implements the memory and inhibitory functions\nbelieved to be carried out by PFC. Figure 2 shows a diagram of the model.\nEach unit in the input module represents a different stimulus condition: A, B, X &\n\n\x0cA Computational Model of Prefrontal Cortex Function\n\n145\n\nOUTPUT LAYER\n\n~~L0~\nINPUT LAYER\n\nFigure 2:\n\nA diagram of the CPT?AX model.\n\nY. Units in the input module make excitatory connections on the response module,\nboth directly and indirectly through the memory module. Lateral inhibition within\neach layer produces competition for representations . Activity from the cue stimulus\nflows to the memory module, which is responsible for maintaining a trace of the\nrelevant context in each trial. Units in the memory module have self-excitatory\nconnections, which allow for the activity generated by the cue to be sustained in\nthe absence of input. The recurrent connectivity utilized by each unit in this module\nis assumed to be a simpler, but formally equivalent analogue of a fully connected\nrecurrent cell assembly. Further, Zipser (1991) has used this type of connectivity to\nproduce temporal activity patterns which are highly similar to the firing patterns\nof neurons in memory-associated areas of cortex, such as PFC. Activity from the\ninput and memory modules is integrated in the output module. The output of this\nmodule determines whether a target (T) or non-target (N) response is made.\nTo simulate the CPT-AX task we have purposefully kept the network architecture\nand size as simple as possible in order to maximize the model\'s interpretability. We\nhave therefore not attempted to simulate neural information processing in a neuronby-neuron manner. Rather, the populations of a few units are seen as capturing the\ninformation processing characteristics of much larger populations of real neurons.\nIn this way, it is possible to capture the stochastic, distributed, and dynamical\nproperties of real neural networks with small and analytically tractable simulations.\nThe simulation is run in a temporally continuous framework in which processing is\ngoverned by the following difference equation:\n(1 )\nwhere\n\n1\n\n(2)\n\nis the state of unit j, Ij is the total input to j , dt is the time-step of integration, \'Y\nis the gain and f3 is the bias. The continuous framework is preferable to a discrete\nevent-based one in that it allows for a plausible way to scale events appropriately\nto the exact temporal specifications of the task (i.e., the duration of stimuli and\nthe delay between cue and probe). In addition, the continuous character of the\nsimulation naturally provides a framework for inferring the reaction times in the\nvarious conditions.\n\n\x0c146\n\n4\n\nTodd S. Braver, Jonathan D. Cohen, David Servan-Schreiber\n\nSimulations of Behavioral Performance\n\nWe used a continuous recurrent generalization of backpropagation (Pearlmutter ,\n1989) to train the network to perform the CPT-AX. All of the connection weights\nwere developed entirely by the training procedure , with the constraint that that all\nself and between layer weights were forced to be positive and all within layer weights\nwere forced to be negative. Training consisted of repeated presentation of each of\nthe 8 conditions in the task (A-X,A-Y,B-X ,B-Y, at both long and short delays), with\nthe presentation frequency of each condition matching that of the behavioral task .\nWeights were updated after the presentation of each trial, biases ({3) were fixed at\n-2.5, and dt was set at 0.1. The network was trained deterministically ; completion\nof training occurred when network accuracy reached 100% for each condition.\nFollowing training, weights were fixed. Errors and reaction time distributions were\nthen simulated by adding zero-mean Gaussian noise to the net input of each unit\nat every time step during trial presentation. A trial consisted of the presentation\nof the cue stimulus, a delay period and then the probe stimulus. As mentioned\nabove, the duration of these events was appropriately scaled to match the temporal\nparameters of the task (e.g ., 300 msec. duration for cue and probe presentation,\n750 msec . for short delays, 5000 msec. for long delays). A time constant (1") of 50\nmsec. was used for simulation in the network. This scaling factor provided sufficient\ntemporal resolution to capture the relationship between the two task delays while\nstill permitting a tractable way of simulating the events .\nResponses were determined by noting which output unit reached a threshold value\nfirst following presentation of the probe stimulus. Response latency was determined\nby calculating the number of time steps taken by the model to reach threshold\nmultiplied by the time constant 1". To facilitate comparisons with the experimental\nreaction times, a constant k was added to all values produced . This parameter might\ncorrespond to the time required to execute a motor response. The value of k was\ndetermined by a least mean squares fit to the data. 1000 trials of each condition\nwere run in order to obtain a reliable estimate of performance under stochastic\nconditions. The standard deviation of the noise distribution (0\') and the threshold\n(T) of the response units were adjusted to produce the best fit to the subject data.\nFigure 1 compares the results of the simulation against the behavioral data.\nAs can be seen in the figure, the model provides a good fit to the behavioral data\nin both the pattern of accuracy and reaction times . The model not only matches\nthe qualitative pattern of errors and reaction times but produces very similar quantitative results as well. The match between model and experimental results is particularly striking when it is considered that there are a total of 24 data points that\nthis model is fitting, with only 4 free parameters (O\',T,1" ,k). The model\'s ability to\nsuccessfully account for the pattern of behavioral performance provides convincing\nevidence that it captures the essential principles of processing in the task. We can\nthen feel confident in not only examining normal processing, but also in extending\nthe model to explore the effects of specific disturbances to processing in PFC .\n\n5\n\nBehavioral Effects of Neuromodulation in PFC\n\nIn a previous meeting of this conference a simulation of a simpler version of the CPT\nwas discussed (Servan-Schreiber, Printz, & Cohen, 1990). In this simulation the\n\n\x0c147\n\nA Computational Model of Prefrontal Cortex Function\n\nAccuracy (Short Delay)\n\nAccuracy (Long Delay)\n\n100\n\n....CJ\n\n......\n~\n\n,,\nI\n\n90\n\nI\n\nI\n\nQ\n\nI\n\nU\n\n....\n==\n~\n\n...\n\n,\nI\n\n,\n\nCJ\n~\n\n=-\n\nI\n,\n\n80\n\nI\n\n~\n\n70\n60\n\nAX\n\nAY\n\nBX\n\nBY\n\nAX\n\nAY\n\nBX\n\nBY\n\nMODEL (Normal Gain)\n-- - MODEL (Reduced Gain)\n.DATA (Controls)\nComparision of of model performance with normal and redu ced gain . The graph illustrates ~he effec~\nof reducing gain in the memory layer on task performance. In the baseline network "1=1 , in ~he reduced-gain\nnetwork "1=0.8.\n\nFigure 3:\n\neffects of system-wide changes in catecholaminergic tone were captured by changing\nthe gain (-r) parameter of network units. Changes in gain are thought correspond to\nthe action of modulatory neurotransmitters in modifying the responsivity of neurons\nto input signals (Servan-Schreiber et aI. , 1990; Cohen & Servan-Schreiber, 1992).\nThe current simulation of the CPT offers the opportunity to explore the effects\nof neuromodulation on the information processing functions specific to PFC. The\ntransmitter dopamine is known to modulate activity in PFC , and manipulations\nto prefrontal dopamine have been shown to have effects on both memory-related\nneuronal activity and behavioral performance (Sawaguchi & Goldman-Rakic, 1991).\nFurthermore, it has been hypothesized that reductions of the neuromodulatory effects of dopamine in PFC are responsible for some of the information processing\ndeficits seen in schizophrenia. To simulate the behavior of schizophrenic subjects,\nwe therefore reduce the gain (\'Y) of units in the memory module of the network.\nWith reduced gain in the memory module, there are striking changes in the model\'s\nperformance of the task. As can be seen in Figure 3, in the short delay conditions\nthe performance of the reduced-gain model is relatively similar to that of control\nsubjects (and the intact model). However, at long delays , the reduced-gain model\nproduces a qualitatively different pattern of performance. In this condition, the\nmodel has a high B-X error rate but a low A-Y error rate, a pattern which is opposite\nto that seen in the control subjects. This double dissociation in performance is a\nrobust effect of the reduced-gain simulation (i.e. , it seems relatively uninfluenced\nby other parameter adjustments) .\nThus , the model makes clear-cut predictions which are both novel and highly\ntestable. Specifically, the model predicts that: 1) Differences in performance be-\n\n\x0c148\n\nTodd S. Braver, lonatMn D. Cohen, David Servan-Schreiber\n\ntween control and schizophrenic subjects will be most apparent at long delays ; 2)\nSchizophrenics will perform significantly worse than control subjects on B-X trials\nat long delays; 3) Schizophrenics will perform significantly better than control subjects on A-Y trials at long delays. This last prediction is especially interesting given\nthe fact that tasks in which schizophrenics show superior performance relative to\ncontrols are relatively rare in experimental research.\nFurthermore, the model not only makes predictions regarding schizophrenic behavioral performance, but also offers explanations as to their mechanisms. Analyses of\nthe trajectories of activation states in the memory module reveals that both of the\ndissociations in performance are due to failures in maintaining representations of\nthe context set up by the cue stimulus. Reducing gain in the memory module blurs\nthe distinction between signal and noise , and causes the context representations to\ndecay over time. As a result, in the long delay trials , there is a higher probability\nthat the model will show both failures of inhibition (more B-X errors) and memory\n(less A- Y errors) .\n\n6\n\nConclusions\n\nThe results of this paper show how a computational analysis of the temporal dynamics of PFC information processing can aid in understanding both normal and disturbed behavior. We have developed a behavioral task which simultaneously probes\nboth the inhibitory and active memory functions of PFC. We have used this task in\ncombination with a computational model to explore the effects of neuromodulatory\ndysfunction, making specific predictions regarding schizophrenic performance in the\nCPT-AX. Confirmation of these predictions now await further testing.\n\nReferences\nCohen, J. & Servan-Schreiber, D. (1992). Context , cortex, and dopamine: A connectionist\napproach to behavior and biology in schizophrenia. Psychological Review, 99 , 45- 77.\nDehaene, S. & Changeux, J. (1989). A simple model of prefrontal cortex function\ndelayed-response tasks. Journal of Cognitive Neuroscience, 1 (3), 244- 261.\n\nIII\n\nFuster, J . (1989). The prefrontal cortex. New York: Raven Press.\nGoldman-Rakic, P. (1987). Circuitry of primate prefrontal cortex and regulation of behavior by representational memory. In F. Plum (Ed .) , Handbook of physiology-the nervous\nsystem, v. Bethesda, MD: American Physiological Society, 373-417.\nLevine, D. & Pruiett, P. (1989). Modeling some effects of frontal lobe damage: novelty\nand perseveration. Neural Networks, 2 , 103-116.\nPearlmutter, B. (1989). Learning state space trajectories in recurrent neural networks.\nNeural Computation, 1 , 263-269.\nSawaguchi, T. & Goldman-Rakic, P. (1991). D1 dopamine receptors in prefrontal cortex:\nInvolvement in working memory. Science , 251 , 947-950.\nServan-Schreiber, D., Printz, H., & Cohen, J. (1990). The effect of catecholamines on\nperformance: From unit to system behavior. In D. Touretzky (Ed.), Neural information\nprocessing systems 2. San Mateo, GA: Morgan Kaufman , 100-108.\nStuss, D. & Benson , D. (1986) . The frontal lobes. New York: Raven Press.\nZipser, D. (1991). Recurrent network model of the neural mechanism of short-term active\nmemory. Neural Computation, 3,179- 19.3.\n\n\x0c'
p83199
sg6
S'Correlated Neuronal Response:\nTime Scales and Mechanisms\nWyeth Bair\nHoward Hughes Medical Inst.\nNYU Center for Neural Science\n4 Washington PI., Room 809\nNew York, NY 10003\n\nEhud Zohary\nDept. of Neurobiology\nInstitute of Life Sciences\nThe Hebrew University, Givat Ram\nJerusalem, 91904 ISRAEL\n\nChristof Koch\nComputation and Neural Systems\nCaltech, 139-74\nPasadena, CA 91125\n\nAbstract\nWe have analyzed the relationship between correlated spike count\nand the peak in the cross-correlation of spike trains for pairs of simultaneously recorded neurons from a previous study of area MT\nin the macaque monkey (Zohary et al., 1994). We conclude that\ncommon input, responsible for creating peaks on the order of ten\nmilliseconds wide in the spike train cross-correlograms (CCGs),\nis also responsible for creating the correlation in spike count observed at the two second time scale of the trial. We argue that\nboth common excitation and inhibition may play significant roles\nin establishing this correlation.\n\n1\n\nINTRODUCTION\n\nIn a previous study of pairs of MT neurons recorded using a single extracellular\nelectrode, it was found that the spike count during two seconds of visual motion\nstimulation had an average correlation coefficient of r = 0.12 and that this correlation could significantly limit the usefulness of pooling across increasingly large\npopulations of neurons (Zohary et aI., 1994). However, correlated spike count between two neurons could in principle occur at several time-scales. Correlated drifts\n\n\x0cCorrelated Neuronal Response: Time Scales and Mechanisms\n\n69\n\nin the excitability of the cells, for example due to normal biological changes or\nelectrode induced changes, could cause correlation at a time scale of many minutes. Alternatively, attentional or priming effects from higher areas could change\nthe responsivity of the cells at the time scale of an experimental trial. Or, as suggested here, common input that changes on the order of milliseconds could cause\ncorrelation in spike count. The first section determines the time scale at which the\nneurons are correlated by analyzing the relationship between the peak in the spike\ntrain cross-correlograms (CCGs) and the correlation between the spike counts using\na construct we call the trial CCG. The second section examines temporal structure\nthat is indicative of correlated suppression of firing, perhaps due to inhibition, which\nmay also contribute to the spike count correlation.\n\n2\n\nTHE TIME SCALE OF CORRELATION\n\nAt the time scale of the single trial, the correlation, r se, of spike counts x and y from\ntwo neurons recorded during nominally identical two second stimuli was computed\nusing Pearson\'s correlation coefficient,\nrse\n\n=\n\nE[xy] - ExEy\n,\nuxuy\n\n(1)\n\nwhere E is expected value and u 2 is variance. If spike counts are converted to\nz-scores, i.e., zero mean and unity variance, then rse = E[xy], and rse may be\ninterpreted as the zero-lag value of the cross-correlation of the z-scored spike counts.\nThe trial CCGs resulting from this procedure are shown for two pairs of neurons in\nFig. l.\nTo distinguish between cases like the two shown in Fig. 1, the correlation was broken\ninto a long-term component, rlt, the average value (computed using a Gaussian\nwindow of standard deviation 4 trials) surrounding the zero-lag value, and a shortterm component, rst, the difference between the zero-lag value and rlt. Across 92\npairs of neurons from three monkeys, the average rst was 0.10 (s.d. 0.17) while rlt\nwas not significantly different from zero (mean 0.01, s.d. 0.11). The mean of rst\nwas similar to the overall correlation of 0.12 reported by Zohary et al. (1994).\nUnder certain assumptions, including that the time scale of correlation is less than\nthe trial duration, rst can be estimated from the area under the spike train CCG\nand the areas under the autocorrelations (derivation omitted). Under the additional\nassumption that the spike trains are individually Poisson and have no peak in the\nautocorrelation except that which occurs by definition at lag zero, the correlation\ncoefficient for spike count can be estimated by\nrpeak\n\n~ j.AA.ABArea,\n\n(2)\n\nwhere .AA and .AB are the mean firing rates of neurons A and B, and Area is the area\nunder the spike train CCG peak, like that shown in Fig. 2 for one pair of neurons.\nTaking Area to be the area under the CCG between ?32 msec gives a good estimate\nof short-term rst, as shown in Fig. 3. In addition to the strong correlation (r = 0.71)\nbetween rpeak and rst, rpeak is a less noisy measure, having standard deviation (not\nshown) on average one fourth as large as those of rst.\nWe conclude that the common input that causes the peaks in the spike train CCGs is\nalso responsible for the correlation in spike count that has been previously reported.\n\n\x0cW. BAIR. E. ZOHARY. C. KOCH\n\n70\n\no\n\n80\n\n160\n\n240\n\n320 0\n\nTrial Number\n\n400\n\n800\n\n1200\n\nTrial Number\n\n0.3\n0.2\n\nd\nU\nU 0.1\n";3\n.~\n\n~\n\n0\n\n~~------------------~~\n\nernu090\n-0.1 +\'-r-~..:...-,:......-~-.-,-~~---,-,~,.--,--,\n-100\n-50\n0\n50\n100 -50\n-25\n\nLag (Trials)\n\n0\n\n25\n\n50\n\nLag (Trials)\n\nFigure 1: Normalized responses for two pairs of neurons and their trial crosscorrelograms (CCGs). The upper traces show the z-scored spike counts for all\ntrials in the order they occurred. Spikes were counted during the 2 sec stimulus,\nbut trials occurred on average 5 sec apart, so 100 trials represents about 2.5 minutes. The lower traces show the trial CCGs. For the pair of cells in the left panel,\nresponsivity drifts during the experiment. The CCG (lower left) shows that the drift\nis correlated between the two neurons over nearly 100 trials. For the pair of cells\nin the right panel, the trial CCG shows a strong correlation only for simultaneous\ntrials. Thus, the measured correlation coefficient (trial CCG at zero lag) seems to\noccur at a long time scale on the left but a short time scale (less than or equal to one\ntrial) on the right. The zero-lag value can be broken into two components, T st and\nTlt (short term and long term, respectively, see text). The short-term component,\nT st, is the value at zero lag minus the weighted average value at surrounding lag\ntimes. On the left, Tst ~ 0, while on the right, Tlt ~ O.\n\n\x0cCorrelated Neuronal Response: Time Scales and Mechanisms\n\n71\n\n5\n\no\n\n8\n\n16\n\n24\n\n32\n\nWidth at HH (msec)\n\n1\n\no\n\nemu064P\n\n-100\n\no\n\n-50\n\n50\n\n100\n\nTime Lag (msec)\nFigure 2: A spike train CCG with central peak. The frequency histogram of widths\nat half-height is shown (inset) for 92 cell pairs from three monkeys. The area of the\ncentral peak measured between ?32 msec is used to predict the correlation coefficients, rp eak. plotted in Fig. 3. The y-axis indicates the probability of a coincidence\nrelative to that expected for Poisson processes at the measured firing rates .\n\n0.8\n\n?\n\n0.6\n~\n\n~\n(1)\n~\n\n\'-"\n~\n\n? ?\n? ?\n\n0.4\n\n?\n\n?\n0.2 ?\n\n?\n\n.....\n\n? ?\n\n?\n\n?\n\n? ?\n?\n\n?\n\n?\n\n?\n\n0 ?\n??\n-0.2\n-0.2\n\no\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\nr (Short Term)\nFigure 3: The area of the peak of the spike train CCG yields a prediction, rpeak (see\nEqn. 2) , that is strongly correlated (r = 0.71, p < 0.00001), with the short-term\nspike count correlation coefficient , rst . The absence of points in the lower right\ncorner of the plot indicates that there are no cases of a pair of cells being strongly\ncorrelated without having a peak in the spike train CCG.\n\n\x0cw. BAIR, E. ZOHARY, C. KOCH\n\n72\n\nIn Fig. 3, there are no pairs of neurons that have a short-term correlation and yet\ndo not have a peak in the ?32 msec range of the spike train CCG.\n\n3\n\nCORRELATED SUPPRESSION\n\nThere is little doubt that common excitatory input causes peaks like the one shown\nin Fig. 2 and therefore results in the correlated spike count at the time scale of the\ntrial. However, we have also observed correlated periods of suppressed firing that\nmay point to inhibition as another contribution to the CCG peaks and consequently\nto the correlated spike count.\nFig. 4 A and B show the response of one neuron to coherent preferred and null\ndirection motion, respectively. Excessively long inter-spike intervals (ISIs), or gaps,\nappear in the response to preferred motion, while bursts appear in the response\nto null motion. Across a database of 84 single neurons from a previous study\n(Britten et aI., 1992), the occurrence of the gaps and bursts has a symmetrical\ntime course-both are most prominent on average from 600-900 msec post-stimulus\nonset, although there are substantial variations from cell to cell (Bair, 1995). The\ngaps, roughly 100 msec long, are not consistent with the slow, steady adaptation\n(presumably due to potassium currents) which is observed under current injection\nin neocortical pyramidal neurons, e.g., the RS 1 and RS2 neurons of Agmon and\nConnors (1992).\nFig. 4 C shows spike trains from two simultaneously recorded neurons stimulated\nwith preferred direction motion. The longest gaps appear to occur at about the\nsame time. To assess the correlation with a cross-correlogram, we first transform\nthe spike trains to interval trains, shown in Fig. 4 D for the spike trains in C.\nThis emphasizes the presence of long ISIs and removes some of the information\nregarding the precise occurrence times of action potentials. The interval crosscorrelation (ICC) between each pair of interval trains is computed and averaged\nover all trials, and the average shift predictor is subtracted. Fig. 4 E and F show\nICCs (thick lines) for two different pairs of neurons. In 17 of 31 pairs (55%), there\nwere peaks in the raw ICC that were at least 4 standard errors above the level of the\nshift predictor. The peaks were on average centered (mean 4.3 msec, SD 54 msec)\nand had mean width at half-height of 139 msec (SD 59 msec).\nTo isolate the cause of the peaks, the long intervals in the trains were set to the\nmean of the short intervals. Long intervals were defined as those that accounted\nfor 30% of the duration of the data and were longer than all short intervals. Note\nthat this is only a small fraction of the number of ISIs in the spike train (typically\nless than about 10%), since a few long intervals consume the same amount of time\nas many short intervals. Data from 300-1950 msec was processed, avoiding the\non-transient and the lack of final interval. With the longest intervals neutralized,\nthe peaks were pushed down to the level of the noise in the ICC (thin lines, Fig. 4\nE, F). Thus, 90% of the action potentials may serve to set a mean rate, while a few\nperiods of long ISIs dominate the ICC peaks.\nThe correlated gaps are consistent with common inhibition to neurons in a local\nregion of cortex, and this inhibition adds area to the spike train CCG peaks in\nthe form of a broader base (not shown). The data analyzed here is from behaving animals, so the gaps may be related to small saccades (within the 0.5 degree\n\n\x0c73\n\nCorrelated Neuronal Response: Time Scales and Mechanisms\n\n""11"1\'"\'\'\'\'\'\'"\'\n\n""\'11111"\'"\'\'\n\n1111 """111111111111"\'""\n11111\'""111111111"11\n""II.!IIIIIIIIIIII\n"11"""1111111110111111111111111111111""111\'1\nII\n11111111\'"11111111111111111111\n111111111\'"\'"111 IIItIlI!\n\'"IIIIIII!!I1I11\'"1I1I 1I1"\'"tll ""UIU\n""\'"\'"\n\'""""\n\n111"\'."111111111111\n\nIII\n1111111 \'"\'\nIlIg" " \' 111111111111111"11111\n111111 1111111111\'"1111111 1111111111\n\n111\'"""""1111\n\n1111"111\'"\'"1111111\n\n\'"""""\'"11\'"111111\'\n11111111\'"1111111111\'"\'"""\'"\'\n11""\'"1111111\nII\'\nIIIIIU"\'"IIIIIIIIIII.II\'\n"\'\'\'\'\'1111111111 11,.""""\'"\n111111111111111\'\n\n1111111111111111111\'"\'"11111"11111111\'11"11"11111111111"1 \' " \' \' \' \' \' "\n1111111111111"\'""1111111 "\'"111\n\'"\'"1111111\'"111111111111111\'\n\nI\n\nI\n\n""1111111111.11. IIUII\n\n111\'\'\'.,11111111111111111\n1111111\'111111"""11\n1111\n\nI\n\n"11\'"" 11111111"1111\'"111 111""\n"""\'"111 "" II 111111""" 1111\'\n1111 111\'11\'111111111 I III 1M II , \' \' \' \'\nI"\n! III\n\'\'\'.1\'111111111111111,,11111111111111111111111111111111111111111111\'\'"1111"\nI 11111\nI l " l I l l ! III\n1111\n\n"11"11\n\nA\n\n"I\n\n1111111 II! I 1111 III II! I ! ! " I I! " \' " \' " III III. II 11.11 III II\n111M"" tI III. " 111"111 I ? " I ""I I I 11111\n.11111111111111 II\n11111111111111\'",\n"""""""\'"111\'"1111111111"1"1\n"11\'11111\'1111111111\n1111\n1111.1.11111111 \'"\'1 II!!\' I I ,,\'\'\'\'\'HIII\'\'\'!I1\'\'!\n""\n11111111111"1\n"\'"111\'"\'" I I I \' " ! I I " I II!\n1111111.111111111111111111111\'\n11111111""\'"1111111111111\'1111111111 \'1IIIII\'"IIIIIIII""""""YlI.""111I\n\n11""\n\n,""\'\',\'\'1\'\'\',\'\' ,,\'.\',\nII\n\nI\nI\nII\n\nI\n\n\'I\n\n?\n"\n?\n\nI\nI\n\n"\'I I\n\nta\'\n\nI\n\nIII\n\nN 1\n\n.111\n\nIII\n\ni\n\nII\n\n0\n\n1111\nI\n\n"" "\'\n\nI I\n\nIII\n\nI\n\n"11\n\n.1\n\nII\nI I\n\nII\n\n\'"~\n\nII\nII\n\nI"\n\n"\'\n\nI\'"\n\n1111\n\n"II\'"\'~IIIIII,II!\'"\'\'\'\',,\\\n\nIII\n\n,.\n\nII\n\nII\n\n"I II !\n\nI!\n11\n\nII.\n\nt""\'IIIIII\n\n\'11I\'1~I\'M\'i"IlI\'III"\'I,IIII"II\'"III,III\'\n\nI\n\n11111\n\nII\n\n\'111\'\n\n"\'\n\nI\n\n500\n\n1000\n\nB\n\n, I\n\n?\n\n\'I!\n\nI\'\n\nIII\n\n""\nI\'\n\nIII\n\n1500\n\n2000\n\nmsec\n\n1\n2\n\nII\n\n11111111111111 1111111111111111 11111111 II 01 n11111111 11111111111111111111111111111111111111111111111111118 11111\n11.1111 1111\n1111 1 11111111 111111\nI 1I11 III I 11I1111I111I111I111I111111I11Im1l11111 II\n\nc\n\n1\n\nD\n1000\n\n2000\n\nTime (msec)\n\nE\n\n-1000 -500\n\nF\n\no\n\n500\n\n1000 -1000 -500\n\no\n\n500\n\n1000\n\nTime Lag (msec)\nFigure 4: (A) The brisk response to coherent preferred direction motion is interrupted by occasional excessively long inter-spike intervals, i.e., gaps. (B) The\nsuppressed response to null direction motion is interrupted by bursts of spikes. (C)\nSimultaneous spike trains from two neurons show correlated gaps in the preferred\ndirection response. (D) The interval representation for the spike trains in C. (E,F)\nInterval cross-correlograms have peaks indicating that the gaps are correlated (see\ntext).\n\n\x0cw. BAIR. E. ZOHARY. C. KOCH\n\n74\n\nfixation window) or eyelid blink. It has been hypothesized that blink suppression\nand saccadic visual suppression may operate through the same pathways and are\nof neuronal origin (Ridder and Tomlinson, 1993). An alternative hypothesis is that\nthe gaps and bursts arise in cortex from intrinsic circuitry arranged in an opponent\nfashion.\n\n4\n\nCONCLUSION\n\nCommon input that causes central peaks on the order of tens of milliseconds wide in\nspike train CCGs is also responsible for causing the correlation in spike count at the\ntime scale of two second long trials. Long-term correlation due to drifts in responsivity exists but is zero on average across all cell pairs and may represent a source of\nnoise which complicates the accurate measurement of cell-to-cell correlation. The\narea of the peak of the spike train CCG within a window of ?32 msec is the basis of\na good prediction of the spike count correlation coefficient and provides a less noisy\nmeasure of correlation between neurons. Correlated gaps observed in the response\nto coherent preferred direction motion is consistent with common inhibition and\ncontributes to the area of the spike train CCG peak, and thus to the correlation\nbetween spike count. Correlation in spike count is an important factor that can\nlimit the useful pool-size of neuronal ensembles (Zohary et al., 1994; Gawne and\nRichmond, 1993).\nAcknowledgements\n\nWe thank William T. Newsome, Kenneth H. Britten, Michael N. Shadlen, and J.\nAnthony Movshon for kindly providing data that was recorded in previous studies\nand for helpful discussion. This work was funded by the Office of Naval Research\nand the Air Force Office of Scientific Research. W. B. was supported by the L. A.\nHanson Foundation and the Howard Hughes Medical Institute.\nReferences\n\nAgmon A, Connors BW (1992) Correlation between intrinsic firing patterns and\nthalamocortical synaptic responses of neurons in mouse barrel cortex. J N eurosci 12:319-329.\nBair W (1995) Analysis of Temporal Structure in Spike Trains of Visual Cortical\nArea MT. Ph.D. thesis, California Institute of Technology.\nBritten KH, Shadlen MN, Newsome WT, Movshon JA (1992) The analysis of visual\nmotion: a comparison of neuronal and psychophysical performance. J Neurosci\n12:4745-4765.\nGawne T J, Richmond BJ (1993) How independent are the messages carried by\nadjacent inferior temporal cortical neurons? J Neurosci 13:2758-2771.\nRidder WH, Tomlinson A (1993) Suppression of contrasts sensitivity during eyelid\nblinks. Vision Res 33: 1795- 1802.\nZohary E, Shadlen MN, Newsome WT (1994) Correlated neuronal discharge rate\nand its implications for psychophysical performance. Nature 370:140-143.\n\n\x0c'
p83200
sg8
S'A Multiscale Attentional Framework for\nRelaxation Neural Networks\nDimitris I. Tsioutsias\nDept. of Electrical Engineering\nYale University\nNew Haven, CT 06520-8285\n\nEric Mjolsness\nDept. of Computer Science & Engineering\nUniversity of California, San Diego\nLa Jolla, CA 92093-0114\n\ntsioutsias~cs.yale.edu\n\nemj~cs.ucsd.edu\n\nAbstract\nWe investigate the optimization of neural networks governed by\ngeneral objective functions. Practical formulations of such objectives are notoriously difficult to solve; a common problem is the\npoor local extrema that result by any of the applied methods. In\nthis paper, a novel framework is introduced for the solution oflargescale optimization problems. It assumes little about the objective\nfunction and can be applied to general nonlinear, non-convex functions; objectives in thousand of variables are thus efficiently minimized by a combination of techniques - deterministic annealing ,\nmultiscale optimization, attention mechanisms and trust region optimization methods.\n\n1\n\nINTRODUCTION\n\nMany practical problems in computer vision, pattern recognition , robotics and other\nareas can be described in terms of constrained optimization . In the past decade,\nresearchers have proposed means of solving such problems with the use of neural\nnetworks [Hopfield & Tank, 1985; Koch et ai., 1986], which are thus derived as\nrelaxation dynamics for the objective functions codifying the optimization task.\nOne disturbing aspect of the approach soon became obvious , namely the apparent inability of the methods to scale up to practical problems , the principal reason\nbeing the rapid increase in the number of local minima present in the objectives as\nthe dimension of the problem increases. Moreover most objectives, E( v), are highly\nnonlinear, non-convex functions of v , and simple techniques (e.g. steepest descent)\n\n\x0cD. I. TSIOUTSIAS, E. MJOLSNESS\n\n634\n\nwill , in general , locate the first minimum from the starting point.\nIn this work, we propose a framework for solving large-scale instances of such optimization problems. We discuss several techniques which assist in avoiding spurious\nminima and whose combined result is an objective function solution that is computationallyefficient, while at the same time being globally convergent. In section 2.1\nwe discuss the use of deterministic annealing as a means of avoiding getting trapped\ninto local minima. Section 2.2 describes multiscale representations of the original\nobjective in reduced spatial domains. In section 2.3 we present a scheme for reducing the computational requirements of the optimization method used, by means of\na focus of attention mechanism. Then, in section 2.4 we introduce a trust region\nmethod for the relaxation phase of the framework, which uses second order information (i.e. curvature) of the objective function. In section 3 we present experimental\nresults on the application of our framework to a 2-D region segmentation objective\nwith discontinuities. Finally, section 4 summarizes our presentation.\n\n2\n\nTHEORETWALFRAMEWORK\n\nOur optimization framework takes the form of a list of nested loops indicating the\norder of conceptual (and computational) phases that occur: from the outer to the\ninner loop we make use of deterministic annealing, a multiscale representation , an\nattentional mechanism and a trust region optimization method.\n\n2.1\n\nANNEALING NETS\n\nThe usefulness of statistical mechanics for designing optimization procedures has\nrecently been established; prime examples are simulated annealing and its various\nmean field theory approximations [Hopfield & Tank, 1985; Durbin & Willshaw,\n1987]. The success of such methods is primarily due to entropic terms included in\nthe objective (i .e. syntactic terms), but the price to pay is their highly nonlinear\nform. Interestingly, those terms can effectively be convexified by the use of a "temperature" parameter, T , allowing for a reduction in the number of minima and the\nability to track the solution through "temperature".\n\n2.2\n\nMULTISCALE REPRESENTATION\n\nTo solve large-scale problems in thousands of variables , we need to speed up the\nconvergence of the method while still retaining valid state-space trajectories. To\naccomplish this we introduce smaller, approximate versions of the problem at coarser\nspatial scales [Mjolsness et al. , 1991] ; the nonlinearity of the original objective is\nmaintained at all scales, as opposed to other approaches where the objectives and\ntheir derivatives are either approximated by the use of finite difference methods ,\nor solved for by multigrid techniques where a quadratic objective is still assumed .\nConsequently, the multiscale representation exploits the effective smoothness in the\nobjectives: by alternating relaxation phases between coarser and finer scales, we\nuse the former to identify extrema and the latter to localise them.\n\n2.3\n\nFOCUS OF ATTENTION\n\nTo further reduce the computational requirements of larg~scale optimization (and\nindirectly control its temporal behavior), we use a focus of attention (FoA) mechanism [Mjolsness & Miranker , 1993], reminiscent of the spotlight hypothesis argued\n\n\x0cA Multiscale Attentional Framework for Relaxation Neural Networks\n\n635\n\nto exist in early vision systems [Koch & Ullman, 1985; Olshausen et al., 1993]. The\neffect of a FoA is to support efficient, responsive analysis: it allows resources to be\nfocused on selected areas of a computation and can rapidly redirect them as the\ntask requirements evolve.\nSpecifically, the FoA becomes a characteristic function, 7l\'(X) , determining which\nof the N neurons are active and which are clamped during relaxation, by use of a\ndiscrete-valued vector, X, and by the rule: 7l\'i(X) = 1 if neuron Vi is in the FoA, and\nzero otherwise. Moreover, a limited number, n, of neurons Vi are active at any given\ninstant: I:i 7l\'i(X) = n, with n? Nand n chosen as an optimal FoA size. To tie the\nattentional mechanism to the multiscale representation, we introduce a partition\nof the neurons Vi into blocks indexed by a (corresponding to coarse-scale blockneurons), via a sparse rectangular matrix Bia E {O, I} such that I:a Bia = 1, Vi,\nwith i = 1, ... ,N, a = 1,oo.,K and K?N. Then 7l\'i(X) = I:aBiaXa, and we use\neach component of X for switching a different block of the partition; thus, a neuron\nVi is in the FoA iff its coarse scale block a is in the FoA, as indicated by Xa. As\na result, our FoA need not necessarily have a single region of activity: it may well\nhave a distributed activity pattern as determined by the partitions Bia. 1\n\nClocked objective function notation [Mjolsness & Miranker, 1993] makes the task\nmore apparent: during the active-x phase the FoA is computed for the next activev phase, determining the subset of neurons Vi on which optimization is to be carried\nout. We introduce the quantity E ;dv] == g~ ~ (Ti is a time axis for Vi) [Mjolsness\n& Miranker, 1993] as an estimate of the predicted dE arising from each Vi if it joins\nthe FoA. For HopfieldjGrossberg dynamics this measure becomes:\nE ;d v ] =\n\n_g~(gi1(Vi)) (~~)\n\n2\n\n(1)\n\n== -gH U i)(E,i)2\n\nwi th E,i ~f \'V\'i E, and gi the transfer function for neuron Vi (e.g. a sigmoid function). Eq. (1) is used here analogously to saliency measures introduced into neurophysiological work [Koch & Ullman, 1985]; we propose it as a global measure\nof conspicuousness. As a result, attention becomes a k-winner-take-all (kWTA)\nnetwork:\n\na\n\na\n\nwhere I refers to the scale for which the FoA is being determined (I = 1, ... , L), EEl\nconforms with the clocked objective notation, and the last summand corresponds\nto the subspace on which optimization is to be performed, as determined by the\ncurrent FoA.2 Periodically, an analogous FoA through spatial scales is run, allowing\nre-direction of system resources to the scale which seems to be having the largest\ncombined benefit and cost effect on the optimization [Tsioutsias & Mjolsness, 1995].\nThe combined effect of multiscale optimization and FoA is depicted schematically in\nFig. 1: reduced-dimension functionals are created and a FoA beam "shines" through\nscales picking the neurons to work on.\nPreferably, Bia will be chosen to minimize the number of inter-block connections.\nBefore computing a new FoA we update the neighbors of all neurons that were included\nin the last focus; this has a similar effect to an implicit spreading of activation.\n1\n\n2\n\n\x0cD. I. TSIOUTSIAS, E. MJOLSNESS\n\n636\n\nLayer 3\n\nLayer 1\n\nFigure 1: Multiscale Attentional Neural Nets: FoA on a layer (e.g. L=l) competes\nwith another FoA (e.g . L=2) to determine both preferable scale and subspace.\n2.4\n\nOPTIMIZATION PHASE\n\nTo overcome the problems generally associated with the steepest descent m ethod,\nother techniques have been devised . Newton \'s method , although successful in small\nto medium-sized problems, does not scale well in large non-convex instances and is\ncomputationally intensive. Quasi-Newton methods are efficient to compute , have\nquadratic termination but are not globally convergent for general nonlinear, nonconvex functions. A method that guarantees global convergence is the trust region\nmethod [Conn et al., 1993] . The idea is summarized as follows : Newton\'s method\nsuffers from non-positive definite Hessians; in such a case, the underlying function\nm(k)(6) obtained from the 2nd order Taylor expansion of E(Vk + 6) does not have\na minimum and the method is not defined, or equivalently, the region around the\ncurrent point Vk in which the Taylor series is adequate does not include a minimizing\npoint of m(k)(6). To resolve this, we can define a neighborhood Ok of Vk such that\nm(k)(6) agrees with E(Vk + 6) in some sense; then, we pick Vk+l\nVk + 6 k , where\n6 k minimizes m(k)(6) , V(Vk + 6) E Ok . Thus , we seek a solution to the resulting\nsubproblem:\n\n=\n\n(3)\nwhere 1I ?lIp is any kind of norm (for instance, the L2 norm leads to the LevenbergMarquardt methods) , and ~k is the radius of Ok, adaptively modified based on an\n\n=\n\n+\n\naccuracy ratio Tk = (~E(k)/~m(k)\n(E(k ) - E(Vk\n6k?/(m(k)(O) - m(k)(6 k ?;\n~E(k) is the "actual reduction" in E(k) when step 6 k is taken, and ~m(k) the\n"predicted reduction" . The closer Tk is to unity, the better the agreement between\nthe local quadratic model of E (k) and the objective itself is , and ~k is modified\n\nadaptively to reflect this [Conn et al., 1993].\nWe need to make some brief points here (a complete discussion will be given elsewhere [Tsioutsias & Mjolsness, 1995]):\n\n\x0cA Multiscale Attentional Framework for Relaxation Neural Networks\n\n637\n\n? At each spatial scale of our multiscale representation, we optimize the corresponding objective by applying a trust region method. To obtain sufficient\nrelaxation progress as we move through scales we have to maintain meaningful region sizes, Llk; to that end we use a criterion based on the curvature\nof the functionals along a searching direction.\n? The dominant relaxation computation within the algorithm is the solution\nof eq. (3). We have chosen to solve this subproblem with a preconditioned\nconjugate gradient method (PCG) that uses a truncated Newton step to\nspeed up the computation; steps are accepted when a sufficiently good\napproximation to the quasi-Newton step is found. 3 In our case, the norm\nin eq. (3) becomes the elliptical norm 1I~llc = ~tc~, where a diagonal\npreconditioner to the Hessian is used as the scaling matrix C.\n\n? If the neuronal connectivity pattern of the original objective is sparse (as\nhappens for most practical combinatorial optimization problems), the pattern of the resulting Hessian can readily be represented by sparse static data\nstructures,4 as we have done within our framework. Moreover, the partition\nmatrices, Bia, introduce a moderate fill-in in the coarser objectives and the\nsparsity of the corresponding Hessians is again taken into account.\n\n3\n\nEXPERIMENTS\n\nWe have applied our proposed optimization framework to a spatially structured\nobjective from low-level vision, namely smooth 2-D region segmentation with the\ninclusion of discontinuity detection processes:\n\nij\n\nij\n\nij\n\nij\n\nij\n\nwhere d is the set of image intensities, j is the real-valued smooth surface to be fit to\nthe data, lV and lh are the discrete-valued line processes indicating a non-zero value\nin the intensity gradient, and ?(x) = -(2g o)-1[lnx+ln(1-x)] is a barrier function\nrestricting each variable into (0,1) by infinite barriers at the borders. Eq. (4) is\na mixed-nonlinear objective involving both continuous and binary variables ; our\nframework optimizes vectors j, lh and lV simultaneously at any given scale as continuous variables, instead of earlier two-step, alternate continuous/discrete-phase\napproaches [Terzopoulos, 1986].\nWe have tested our method on gradually increasing objectives, from a "small" size\nof N=12,288 variables for a 64x64 image, up to a large size of N=786 ,432 variables\nfor a 512x512 image; the results seem to coincide with our theoretical expectations:\na significant reduction in computational cost was observed and consistent convergence towards the optimum of the objective was found for various numbers of coarse\nscales and FoA sizes. The dimension of the objective at any scale I was chosen via\na power law: N(L-l+1)! L, where L is the total number of scales and N the size of\n3\n\n4\n\nThe algorithm can also handle directions of negative curvature.\nThis property becomes important in a neural net implementation.\n\n\x0cD. I. TSIOUTSIAS, E. MJOLSNESS\n\n638\n\nthe original objective.\nThe effect of our multiscale optimization with and without a FoA is shown in Fig. 2\nfor the 128x128 and the 512x512 nets, where E( v*) is the best final configuration\nwith a one-level no-FoA net , and cumulative cost is an accumulated measure in the\nnumber of connection updates at each scale; a consistent scale-up in computational\nefficiency can be noted when L > 1, while the cost measure also reflects the relative\ntotal wall-clock times needed for convergence. Fig. 3 shows part of a comparative\nstudy we made for saliency measures alternative to eq. (1) (e.g. g~IE,il), in order\nto investigate the validity of eq. (1) as a predictor of l:!..E: the more prominent\n"linearity" in the left scatterplot seems to justify our choice of saliency.\n104\n\nM-\'S-\'/-_A_T_N_e_t_s,..,,(_12_8_t2-,)_\n: _L_=--,1,_2\'-,3_ _ _---,\n\n. - -_ _ _\n\n10\'\n\nMS/ AT Nets (512t2) : L=1,2,3,4\n\n10\'\n\n10\'\n\n10\'\n10\'\n10\'\n10\'\n10\'\n\n~ 10 l\n~\n\n2\n\n\'"\n\nNl\n\nI\n\n10\'\n\n#1\n\n>\'\n\ng10 - 1\n\n10-\'\n10-\'\n\n10"\n10"\n10-4\n10-110\n\n10-\'\n\n2000\n\n10-\' 0\n\nFigure 2: Multiscale Optimization (curves labeled by number of scales used): #numbered curves correspond to nets without a FoA , simply-numbered ones to nets\nwith a FoA used at all scales. The lowest costs result from the combined use of\nmultiscale optimization and FoA.\n\n4\n\nCONCLUSION\n\nWe have presented a framework for the optimization of large-scale objective functions using neural networks that incorporate a multiscale attentional mechanism.\nOur method allows for a continuous adaptation of the system resources to the computational requirements of the relaxation problem through the combined use of\nseveral techniques. The framework was applied to a 2-D image segmentation objective with discontinuities; formulations of this problem with tens to hundreds of\nthousands of variables were then successfully solved.\nAcknow ledgements\nThis work was supported partly by AFOSR-F49620-92-J-0465 and the Yale Center\nof Theoretical and Applied Neurosci ence.\n\n60000\n\n\x0c639\n\nA Multiscale Attentional Framework for Relaxation Neural Networks\n10\'\n\n(128t2) : Focus on 1st level - proposed saliency\n\n10 \'\n\n.. ..\n\n8\n~\n\no\n\n8\n\n,.\n\n00\n00\n\n10?\n\n",00\n\n10\'\n\n0\n\n~ 10-\'\n.!!\n\no\no\n\no\n\n0\n\n,.,10-\'\no\n\no\n\no\n\n.."\n\no\n\no\n\n~\n\n0\n0\n0\n\n8 o 8\n\n~0o;\n/.0\n\n: 10\'\n\n"\n\n0.\n\nc\n.!!\nOJ\n11l 10-3\n\n0\n\n3\n\n..\no\n\n0\n0\n\no\n\n:0\n\n.8-\n\n0\n\n10\'\n\no\n\no\n\n:0\n\n(128t2) : Focus on 1st level - absolute gradient\n\n.\n\n0\n\n"1:,00 0\n0"\n\n~\nc\n.!!\n\n. .0\n\n.\n\n~10-1\n\n~\n\n~\n\n!10- 4\n\n10-\'\n\n"I\n10 -~0~-.-\'-\'-\'u.tl~Oo.,- J....Ll.J"!\'1*=0-.-\'-\'-~1\nO:=.-.l.....L..Ll.\';t\'!loOr-\'-~.tO!:.-r-u~1~0-:r\'-\'~\n100\n\n(Average Della-E per block)\n\n10-~0b.--\'"-U.~I~"ol_:r\'-\'.w.m~I"O~\nI _r-u.li;~\nlo.L_:r\'-\'-,-"~uI"O~I_?.-\'-\'-l.l..lLU~l\nulo,,,"\n_? .l.....L..Lu.;I"~ol-cr\'-\'-~\n1 00\n(Average Della-E per block)\n\nFigure 3: Saliency Comparison: (left), saliency as in eq. (1); (right), the absolute\ngradient was used instead.\nReferences\nA. Conn , N. Gould, A. Sartanaer, & Ph . Toint. (1993) Global Convergence of a\nClass of Trust Region Algorithms for Optimization Using Inexact Projections on\nConvex Constraints. SIAM J. of Optimization, 3(1) :164-221.\nR. Durbin & D. Willshaw. (1987) An Analogue Approach to the TSP Problem\nUsing an Elastic Net Method. Nature , 326:689-691.\nJ. Hopfield & D. W. Tank. (1985) Neural Computation of Decisions in Optimization\nProblems. Bioi. Cybernei., 52:141-152.\n\nC. Koch , J. Marroquin & A. Yuille. (1986) Analog \'Neuronal \' Networks in Early\nVision . Proc . of the National Academy of Sciences USA, 83:4263-4267.\nC . Koch, & S. Ullman . (1985) Shifts in Selective Visual Attention : Towards the\nUnderlying Neural Circuitry. Human Neurobiology , 4 :219-227 .\nE. Mjolsness, C. Garrett, & W. Miranker. (1991) Multiscale Optimization in Neural\nNets. IEEE Trans. on Neural Networks , 2(2):263-274 .\nE. Mjolsness & W. Miranker. (1993) Greedy Lagrangians for Neural Networks:\nThree Levels of Optimization in Relaxation Dynamics. YALEU/DCS/TR-945.\n(URL file:!!cs.ucsd.edu!pub!emj!papers!yale-TR-945.ps.Z)\nB. Olshausen, C. Anderson, & D. Van Essen. (1993) A Neurobiological Model of\nVisual Attention and Invariant Pattern Recognition Based on Dynamic Routing of\nInformation. The Journal of Neuroscience , 13(11):4700-4719 .\nD. Terzopoulos. (1986) Regularization of Inverse Visual Problems Involving Discontinuities. IEEE Trans. PAMI, 8:419-429 .\nD. I. Tsioutsias & E. Mjolsness. (1995) Global Optimization in Neural Nets: A\nNovel Relaxation Framework . To appear as a UCSD-CSE-TR, Dec. 1995.\n\n\x0c'
p83201
sg126
S'A Practical Monte Carlo Implementation\nof Bayesian Learning\n\nCarl Edward Rasmussen\nDepartment of Computer Science\nUniversity of Toronto\nToronto, Ontario, M5S 1A4, Canada\ncarl@cs.toronto.edu\n\nAbstract\nA practical method for Bayesian training of feed-forward neural\nnetworks using sophisticated Monte Carlo methods is presented\nand evaluated. In reasonably small amounts of computer time this\napproach outperforms other state-of-the-art methods on 5 datalimited tasks from real world domains.\n\n1\n\nINTRODUCTION\n\nBayesian learning uses a prior on model parameters, combines this with information\nfrom a training set , and then integrates over the resulting posterior to make predictions. With this approach, we can use large networks without fear of overfitting,\nallowing us to capture more structure in the data, thus improving prediction accuracy and eliminating the tedious search (often performed using cross validation) for\nthe model complexity that optimises the bias/variance tradeoff. In this approach\nthe size of the model is limited only by computational considerations.\nThe application of Bayesian learning to neural networks has been pioneered by\nMacKay (1992), who uses a Gaussian approximation to the posterior weight distribution. However, the Gaussian approximation is poor because of multiple modes in\nthe posterior. Even locally around a mode the accuracy of the Gaussian approximation is questionable, especially when the model is large compared to the amount\nof training data.\nHere I present and test a Monte Carlo method (Neal, 1995) which avoids the\nGaussian approximation. The implementation is complicated, but the user is not required to have extensive knowledge about the algorithm. Thus, the implementation\nrepresents a practical tool for learning in neural nets.\n\n\x0c599\n\nA Practical Monte Carlo Implementation of Bayesian Learning\n\n1.1\n\nTHE PREDICTION TASK\n\n=\n\nThe training data consists of n examples in the form of inputs x\n{x(i)} and\ncorresponding outputs y = {y(i)} where i = 1 ... n. For simplicity we consider\nonly real-valued scalar outputs. The network is parametrised by weights w, and\nhyperparameters h that control the distributions for weights, playing a role similar\nto that of conventional weight decay. Weights and hyperparameters are collectively\ntermed 0, and the network function is written as F/I (x), although the function value\nis only indirectly dependent on the hyperparameters (through the weights).\nBayes\' rule gives the posterior distribution for the parameters in terms of the likelihood, p(ylx, 0), and prior, p(O):\n\np\n\n(Olx\n\n,y\n\n) = p(O)p(ylx, O)\np(ylx)\n\nTo minimize the expected squared error on an unseen test case with input\nwe use the mean prediction\n\nx(n+l),\n\n(1)\n\n2\n\nMONTE CARLO SAMPLING\n\nThe following implementation is due to Neal (1995). The network weights are\nupdated using the hybrid Monte Carlo method (Duane et al. 1987). This method\ncombines the Metropolis algorithm with dynamical simulation. This helps to avoid\nthe random walk behavior of simple forms of Metropolis, which is essential if we\nwish to explore weight space efficiently. The hyperparameters are updated using\nGibbs sampling.\n\n2.1\n\nNETWORK SPECIFICATION\n\nThe networks used here are always of the same form: a single linear output unit, a\nsingle hidden layer of tanh units and a task dependent number of input units. All\nlayers are fully connected in a feed forward manner (including direct connections\nfrom input to output). The output and hidden units have biases.\nThe network priors are specified in a hierarchical manner in terms of hyperparameters; weights of different kinds are divided into groups, each group having it\'s own\nprior. The output-bias is given a zero-mean Gaussian prior with a std. dev. of\nu = 1000, so it is effectively unconstrained.\nThe hidden-biases are given a two layer prior: the bias b is given a zero-mean\nGaussian prior b \'" N(O, ( 2 ); the value of u is specified in terms of precision r = u- 2 ,\nwhich is given a Gamma prior with mean p = 400 (corresponding to u = 0.05) and\nshape parameter a = 0.5; the Gamma density is given by p(r) \'" Gamma(p, a) ex:\nr Ol / 2 - 1 exp( -ra/2p). Note that this type of prior introduces a dependency between\nthe biases for different hidden units through the common r. The prior for the\nhidden-to-output weights is identical to the prior for the hidden-biases, except that\nthe variance of these weights under the prior is scaled down by the square root\nof the number of hidden units, such that the network output magnitude becomes\nindependent of the number of hidden units. The noise variance is also given a\nGamma prior with these parameters.\n\n\x0c600\n\nC. E. RASMUSSEN\n\nThe input-to-hidden weights are given a three layer prior: again each weight is\ngiven a zero-mean Gaussian prior w rv N(O, (12); the corresponding precision for\nthe weights out of input unit i is given a Gamma prior with a mean J.l and a shape\nparameter a1 = 0.5: Ti rv Gamma(J.l, a1). The mean J.l is determined on the top\nlevel by a Gamma distribution with mean and shape parameter ao = 1: J.li rv\nGamma(400,ao). The direct input-to-output connections are also given this prior.\nThe above-mentioned 3 layer prior incorporates the idea of Automatic Relevance\nDetermination (ARD), due to MacKay and Neal, and discussed in Neal (1995) . The\nhyperparameters, Ti, associated with individual inputs can adapt according to the\nrelevance of the input; for an unimportant input, Ti can grow very large (governed\nby the top level prior), thus forcing (1i and the associated weights to vanish.\n2.2\n\nMONTE CARLO SPECIFICATION\n\nSampling from the posterior weight distribution is performed by iteratively updating\nthe values of the network weights and hyperparameters. Each iteration involves two\ncomponents: weight updates and hyperparameter updates. A cursory description\nof these steps follows.\n2.2.1\n\nWeight Updates\n\nWeight updates are done using the hybrid Monte Carlo method . A fictitious dynamical system is generated by interpreting weights as positions, and augmenting\nthe weights w with momentum variables p. The purpose of the dynamical system\nis to give the weights "inertia" so that slow random walk behaviour can be avoided\nduring exploration of weight space. The total energy, H, of the system is the sum\nof the kinetic energy, I<, (a function of the momenta) and the potential energy, E.\nThe potential energy is defined such that p(w) ex exp( -E). We sample from the\njoint distribution for wand p given by p(w,p) ex exp(-E - I<), under which the\nmarginal distribution for w is given by the posterior. A sample of weights from the\nposterior can therefore be obtained by simply ignoring the momenta.\nSampling from the joint distribution is achieved by two steps: 1) finding new points\nin phase space with near-identical energies H by simulating the dynamical system\nusing a discretised approximation to Hamiltonian dynamics, and 2) changing the\nenergy H by doing Gibbs sampling for the momentum variables.\nHamiltonian Dynamics. Hamilton\'s first order differential equations for Hare\napproximated by a series of discrete first order steps (specifically by the leapfrog\nmethod). The first derivatives of the network error function enter through the\nderivative of the potential energy, and are computed using backpropagation. In\nthe original version of the hybrid Monte Carlo method the final position is then\naccepted or rejected depending on the final energy H\'" (which is not necessarily\nequal to the initial energy H because of the discretisation). Here we use a modified\nversion that uses an average over a window of states instead. The step size of the\ndiscrete dynamics should be as large as possible while keeping the rejection rate\nlow. The step sizes are set individually using several heuristic approximations, and\nscaled by an overall parameter c. We use L = 200 iterations, a window size of 20\nand a step size of c = 0.2 for all simulations.\nGibbs Sampling for Momentum Variables. The momentum variables are\nupdated using a modified version of Gibbs sampling, allowing the energy H to\nchange. A "persistence" of 0.95 is used; the new value of the momentum is a\nweighted sum of the previous value (weight 0.95) and the value obtained by Gibbs\nsampling (weight (1 - 0.95 2)1/2). With this form of persistence, the momenta\n\n\x0cA Practical Monte Carlo Implementation of Bayesian Learning\n\n601\n\nchanges approx. 20 times more slowly, thus increasing the "inertia" of the weights,\nso as to further help in avoiding random walks. Larger values of the persistence will\nfurther increase the weight inertia, but reduce the rate of exploration of H. The\nadvantage of increasing the weight inertia in this way rather than by increasing L is\nthat the hyperparameters are updated at shorter intervals, allowing them to adapt\nto the rapidly changing weights.\n2.2.2\n\nHyperparameter Updates\n\nThe hyperparameters are updated using Gibbs sampling. The conditional distributions for the hyperparameters given the weights are of the Gamma form, for which\nefficient generators exist, except for the top-level hyperparameter in the case of the\n3 layer priors used for the weights from the inputs; in this case the conditional\ndistribution is more complicated and a form of rejection sampling is employed.\n2.3\n\nNETWORK TRAINING AND PREDICTION\n\nThe network training consists of two levels of initialisation before sampling for\nnetworks used for prediction. At the first level of initialisation the hyperparameters\n(variance of the Gaussians) are kept constant at 1, allowing the weights to grow\nduring 1000 leapfrog iterations. Neglecting this phase can cause the network to get\ncaught for a long time in a state where weights and hyperparameters are both very\nsmall.\nThe scheme described above is then invoked and run for as long as desired, eventually producing networks from the posterior distribution. The initial 1/3 of these\nnets are discarded, since the algorithm may need time to reach regions of high posterior probability. Networks sampled during the remainder of the run are saved for\nmaking predictions.\nThe predictions are made using an average of the networks sampled from the posterior as an approximation to the integral in eq. (1). Since the output unit is linear\nthe final prediction can be seen as coming from a huge (fully connected) ensemble\nnet with appropriately scaled output weights. All the results reported here were\nfor ensemble nets with 4000 hidden units. The size of the individual nets is given\nby the rule that we want at least as many network parameters as we have training\nexamples (with a lower limit of 4 hidden units). We hope thereby to be well out of\nthe underfitting region. Using even larger nets would probably not gain us much\n(in the face of the limited training data) and is avoided for computational reasons.\nAll runs used the parameter values given above. The only check that is necessary\nis that the rejection rate stays low, say below 5%; if not, the step size should\nbe lowered. In all runs reported here, c = 0.2 was adequate. The parameters\nconcerning the Monte Carlo method and the network priors were all selected based\non intuition and on experience with toy problems. Thus no parameters need to be\nset by the user.\n\n3\n\nTESTS\n\nThe performance of the algorithm was evaluated by comparing it to other state-ofthe-art methods on 5 real-world regression tasks. All 5 data sets have previously\nbeen studied using a 10-way cross-validation scheme (Quinlan 1993). The tasks\nin these domains is to predict price or performance of an object from various discrete and real-valued attributes. For each domain the data is split into two sets\nof roughly equal size, one for training and one for testing. The training data is\n\n\x0c602\n\nC. E. RASMUSSEN\n\nfurther subdivided into full-, half-, quarter- and eighth-sized subsets, 15 subsets in\ntotal. Networks are trained on each of these partitions, and evaluated on the large\ncommon test set . On the small training sets, the average performance and one\nstd. dev. error bars on this estimate are computed.\n\n3.1\n\nALGORITHMS\n\nThe Monte Carlo method was compared to four other algorithms. For the three\nneural network methods nets with a single hidden layer and direct input-output\nconnections were used. The Monte Carlo method was run for 1 hour on each of the\nsmall training sets, and 2,4 and 8 hours respectively on the larger training sets. All\nsimulations were done on a 200 MHz MIPS R4400 processor. The Gaussian Process\nmethod is described in a companion paper (Williams & Rasmussen 1996).\nThe Evidence method (MacKay 1992) was used for a network with separate hyperparameters for the direct connections, the weights from individual inputs (ARD),\nhidden biases, and output biases. Nets were trained using a conjugate gradient\nmethod, allowing 10000 gradient evaluations (batch) before each of 6 updates of\nthe hyperparameters. The network Hessian was computed analytically. The value\nof the evidence was computed without compensating for network symmetries, since\nthis can lead to a vastly over-estimated evidence for big networks where the posterior Gaussians from different modes overlap. A large number of nets were trained for\neach task, with the number of hidden units computed from the results of previous\nnets by the following heuristics: The min and max number of hidden units in the 20%\nnets with the highest evidences were found. The new architecture is picked from a\nGaussian (truncated at 0) with mean (max - min)/2 and std. dev. 2 + max - min,\nwhich is thought to give a reasonable trade-off between exploration and exploitation. This procedure is run for 1 hour of cpu time or until more than 1000 nets have\nbeen trained. The final predictions are made from an ensemble of the 20% (but a\nmaximum of 100) nets with the highest evidence.\nAn ensemble method using cross-validation to search over a 2-dimensional grid for\nthe number of hidden units and the value of a single weight decay parameter has\nbeen included, as an attempt to have a thorough version of "common practise".\nThe weight decay parameter takes on the values 0, 0.01, 0.04, 0.16 , 0.64 and 2.56.\nUp to 6 sizes of nets are used, from 0 hidden units (a linear model) up to a number\nthat gives as many weights as training examples. Networks are trained with a\nconjugent gradient method for 10000 epochs on each of these up to 36 networks,\nand performance was monitored on a validation set containing 1/3 of the examples,\nselected at random. This was repeated 5 times with different random validation\nsets, and the architecture and weight decay that did best on average was selected.\nThe predictions are made from an ensemble of 10 nets with this architecture, trained\non the full training set. This algorithm took several hours of cpu time for the largest\ntraining sets.\nThe Multivariate Adaptive Regression Splines (MARS) method (Friedman 1991)\nwas included as a non-neural network approach. It is possible to vary the maximum\nnumber of variables allowed to interact in the additive components of the model.\nIt is common to allow either pairwise or full interactions. I do not have sufficient\nexperience with MARS to make this choice. Therefore, I tried both options and\nreported for each partition on each domain the best performance based on the\ntest error, so results as good as the ones reported here might not be obtainable in\npractise. All other parameters of MARS were left at their default values. MARS\nalways required less than 1 minute of cpu time.\n\n\x0cA Practical Monte Carlo Implementation of Bayesian Learning\n\n603\n\nAuto price\n\nCpu\n\n0.6\n\n2\n\n0.5\n1.5\n\n0.4\n\n+\n\n0.3\n\n1\n0*\n\n+\n\n0.5\n\no~------~----~----~---\n\n20\n\n40\n\nIS!\n\n*\n\nX\n\n0.1\n\nx\n10\n\no\n\n0.2\n\n80\n\nOL-~----~------~----~--\n\n13\n\n26\n\nHouse\n\n52\n\n104\n\nMpg\n\n0.25\n0.6\n\nt\n\n0.5\n\n0.4\n\n0.2\n0.15\n\n0.3\n\n0.1\n\n>?1>*\n+ IS!\n\n0.2\n\n*\n\nXo+\n\nIS!\n\n0.05\n\n0.1\no~~----~------~----~--\n\n32\n\n64\n\n128\n\nOL-~----~----~----~--\n\n256\n\n24\n\n48\n\n96\n\n192\n\nServo\nGeometric mean\n\n1\nx Monte Carlo\n\n0.283\n\no Gaussian Evidence\n\n0.364\n\n0.6\n\n+ Backprop\n\n0.339\n\n0.4\n\n*\n\nMARS\n\n0.371\n\nIS!\n\nGaussian Process\n\n0.304\n\n0.8\n\nOtIS!\n\n0.2\n\nX\n\n*\n\no~~------~----~----~---\n\n11\n\n22\n\n44\n\n88\n\nFigure 1: Squared error on test cases for the five algorithms applied to the five problems.\nErrors are normalized with respect to the variance on the test cases. The x-axis gives the\nnumber of training examples; four different set sizes were used on each domain. The error\nbars give one std. dev. for the distribution of the mean over training sets. No error bar is\ngiven for the largest size, for which only a single training set was available. Some of the\nlarge error bars are cut of at the top. MARS was unable to run on the smallest partitions\nfrom the Auto price and the servo domains; in these cases the means of the four other\nmethods were used in the reported geometric mean for MARS.\n\n\x0c604\n\nC. E. RASMUSSEN\n\ndomain\nAuto Price\nCpu\nHouse\nMpg\nServo\n\n3.2\n\nTable 1: Data Sets\n# training cases # test cases # binary inputs\n80\n104\n256\n192\n88\n\n79\n105\n250\n200\n79\n\n0\n0\n1\n6\n10\n\n# real inputs\n16\n6\n12\n3\n2\n\nPERFORMANCE\n\nThe test results are presented in fig . 1. On the servo domain the Monte Carlo\nmethod is uniformly better than all other methods, although the difference should\nprobably not always be considered statistically significant. The Monte Carlo method\ngenerally does well for the smallest training sets. Note that no single method does\nwell on all these tasks. The Monte Carlo method is never vastly out-performed by\nthe other methods.\nThe geometric mean of the performances over all 5 domains for the the 4 different\ntraining set sizes is computed. Assuming a Gaussian distribution of prediction\nerrors, the log of the error variance can (apart from normalising constants) be\ninterpreted as the amount of information unexplained by the models. Thus, the\nlog of the geometric means in fig. 1 give the average information unexplained by\nthe models. According to this measure the Monte Carlo method does best, closely\nfollowed by the Gaussian Process method . Note that MARS is the worst, even\nthough the decision between pairwise and full interactions were made on the basis\nof the test errors.\n\n4\n\nCONCLUSIONS\n\nI have outlined a black-box Monte Carlo implementation of Bayesian learning in\nneural networks, and shown that it has an excellent performance. These results suggest that Monte Carlo based Bayesian methods are serious competitors for practical\nprediction tasks on data limited domains.\n\nAcknowledgements\nI am grateful to Radford Neal for his generosity with insight and software. This research\nwas funded by a grant to G. Hinton from the Institute for Robotics and Intelligent Systems.\n\nReferences\nS. Duane, A. D. Kennedy, B. J. Pendleton & D. Roweth (1987) "Hybrid Monte Carlo",\nPhysics Letters B, vol. 195, pp. 216-222.\nJ . H. Friedman (1991) "Multivariate adaptive regression splines" (with discussion) , Annals\nof Statistics , 19,1-141 (March) . Source: http://lib.stat.cmu.edu/general/mars3.5.\n\nD. J. C. MacKay (1992) "A practical Bayesian framework for backpropagation networks",\nNeural Computation, vol. 4, pp. 448- 472.\n\nR. M. Neal (1995) Bayesian Learning for Neural Networks, PhD thesis, Dept. of Computer\nScience, University of Toronto, ftp: pub/radford/thesis. ps. Z from ftp. cs . toronto. edu.\nJ. R. Quinlan (1993) "Combining instance-based and model-based learning", Proc . ML \'93\n\n(ed P.E. Utgoff), San Mateo: Morgan Kaufmann.\nC. K. I. Williams & C. E. Rasmussen (1996). "Regression with Gaussian processes", NIPS\n8, editors D. Touretzky, M. Mozer and M. Hesselmo. (this volume) .\n\n\x0c'
p83202
sg341
S'Exponentially many local minima for single\nneurons\n\nPeter Auer\n\nManfred K. Warmuth\n\nMark Herbster\n\nDepartment of Computer Science\nSanta Cruz, California\n{pauer,mark,manfred} @cs.ucsc.edu\n\nAbstract\nWe show that for a single neuron with the logistic function as the transfer\nfunction the number of local minima of the error function based on the\nsquare loss can grow exponentially in the dimension.\n\n1 INTRODUCTION\nConsider a single artificial neuron with d inputs. The neuron has d weights w E Rd. The\noutput of the neuron for an input pattern x E Rd is y = ?(x? w), where ? : R -+ R\nis a transfer function. For a given sequence of training examples ((Xt, Yt))I<t<m, each\nconsisting of a pattern Xt E R d and a desired output Yt E R, the goal of the training phase\nfor neural networks consists of minimizing the error function with respect to the weight\nvector w E Rd. This function is the sum of the losses between outputs of the neuron and\nthe desired outputs summed over all training examples. In notation, the error function is\nm\n\nE(w) =\n\nL L(Yt, ?(Xt . w))\n\n,\n\nt=1\n\nwhere L : R x R\n\n-+\n\n[0,00) is the loss function.\n\nA common example of a transfer function is the logistic function logistic( z) = I+!-\' which\nhas the bounded range (0, 1). In contrast, the identity function id(z) = z has unbounded\n(y - Y)2. Other\nrange. One of the most common loss functions is the square loss L(y, y)\nexamples are the absolute loss Iy - yl and the entropic1oss yin? + (1 - y) In\n\n=\n\n::::l\n\nWe show that for the square loss and the logistic function the error function of a single\nneuron for n training examples may have Ln / dJ d local minima. More generally, this holds\nfor any loss and transfer function for which the composition of the loss function with the\ntransfer function (in notation L(y, ?(x . w)) is continuous and has bounded range. This\n\n\x0cExponentially Many Local Minima for Single Neurons\n\nFigure 1:\n\n317\n\nError Function with 25 Local Minima (16 Visible), Generated by 10 TwoDimensional Examples.\n\nproves that for any transfer function with bounded range exponentially many local minima\ncan occur when the loss function is the square loss.\nThe sequences of examples that we use in our proofs have the property that they are nonrealizable in the sense that there is no weight vector W E R d for which the error function\nis zero, i.e. the neuron cannot produce the desired output for all examples. We show with\nsome minimal assumptions on the loss and transfer functions that for a single neuron there\ncan be no local minima besides the global minimum if the examples are realizable.\n\nIf the transfer function is the logistic function then it has often been suggested in the\nliterature to use the entropic loss in artificial neural networks in place of the square loss\n[BW88, WD88, SLF88, Wat92]. In that case the error function of a single neuron is\nconvex and thus has only one minimum even in the non-realizable case. We generalize this\nobservation by defining a matching loss for any differentiable increasing transfer functions\n?:\n\n1\n\n,p-l(y)\n\nL</>(y, f)) =\n\n(?(z) - y) dz .\n</>-l(y)\nThe loss is the area depicted in Figure 2a. If ? is the identity function then L</> is the square\nloss likewise if ? is the logistic function then L</> is the entropic loss. For the matching loss\nthe gradient descent update for minimizing the error function for a sequence of examples\nis simply\nWnew := Wold\n\n-1]\n\n(f)?(Xt .\n\nWold) - Yt)Xt) ,\n\nt=1\n\nwhere 1] is a positive learning rate. Also the second derivatives are easy to calculate for\nthis general setting: L4>(Y~:v~<:Wt;W)) = ?\'(Xt . W)Xt,iXt,j. Thus, if Ht(w) is the Hessian\nof L</>(Yt, ?(Xt . w)) with respect to W then v T Ht(w)v = ?\'(Xt . w)(v . Xt)2. Thus\n\n\x0c318\n\nP. AUER. M. HERBSTER, M. K. WARMUTH\n\n0.8\n\nwO.I\n\n0.4\n\n0.2\n\n.-1\n\n(9)\n\n(a)\n\nFigure 2:\n\n=w? x\n\n...\n\n-2\n\n...o\n(b)\n\n(a) The Matching Loss Function L</>.\n(b) The Square Loss becomes Saturated, the Entropic Loss does not.\n\nH t is positive semi-definite for any increasing differentiable transfer function. Clearly\nL:~I Ht(w) is the Hessian of the error function E(w) for a sequence of m examples and\n\nit is also positive semi-definite. It follows that for any differentiable increasing transfer\nfunction the error function with respect to the matching loss is always convex.\nWe show that in the case of one neuron the logistic function paired with the square loss\ncan lead to exponentially many minima. It is open whether the number of local minima\ngrows exponentially for some natural data. However there is another problem with the\npairing of the logistic and the square loss that makes it hard to optimize the error function\nwith gradient based methods. This is the problem of flat regions. Consider one example\n(x, y) consisting of a pattern x (such that x is not equal to the all zero vector) and the\ndesired output y. Then the square loss (Iogistic(x . w) - y)2, for y E [0, I] and w E R d ,\nturns flat as a function of w when f) = logistic( x . w) approaches zero or one (for example\nsee Figure 2b where d = I and y = 0). It is easy to see that for all bounded transfer\nfunctions with a finite number of minima and corresponding bounded loss functions, the\nof the square\nsame phenomenon occurs. In other words, the composition L(y, ?(x .\nloss with any bounded transfer function ? which has a finite number of extrema turns flat as\nIx . w I becomes large. Similarly, for multiple examples the error function E( w) as defined\nabove becomes flat. In flat regions the gradients with respect to the weight vector w are\nsmall, and thus gradient-based updates of the weight vector may have a hard time moving\nthe weight vector out of these flat regions. This phenomenon can easily be observed in\npractice and is sometimes called "saturation" [Hay94]. In contrast, if the logistic function\nis paired with the entropic loss (see Figure 2b), then the error function turns flat only at the\nglobal minimum. The same holds for any increasing differentiable transfer function and its\nmatching loss function.\n\nw?\n\nA number of previous papers discussed conditions necessary and sufficient for mUltiple\nlocal minima of the error function of single neurons or otherwise small networks [WD88,\nSS89, BRS89, Blu89, SS91, GT92]. This previous work only discusses the occurrence of\nmultiple local minima whereas in this paper we show that the number of such minima can\ngrow exponentially with the dimension. Also the previous work has mainly been limited\nto the demonstration of local minima in networks or neurons that have used the hyperbolic\ntangent or logistic function with the square loss. Here we show that exponentially many\nminima occur whenever the composition of the loss function with the transfer function is\ncontinuous and bounded.\nThe paper is outlined as follows. After some preliminaries in the next section, we gi ve formal\n\n\x0cExponentially Many Local Minima for Single Neurons\n\n319\n\n11\n\n04$\n\nO.\n0.9\n036\n0.1\n03\n07\n026\nWOI\n\n02\n\nas -- --- ------------ __\n\n0.t5\n\nO.\n\n0.1\n\n03\n\n0.06\n\n02\n\n0\n-2\n\n,-\n\n,\n\n,\n\'\n\n,\n\\,\n\n\'-\n\n,/ \' ..\n\n"\n\n01 L......L-~~-~~~~-\'--~-~\n-8-e-~-2\n0\n\n-1\n\nlog ..\n\n(b)\n\n(a)\n\nFigure 3:\n\n(a) Error Function for the Logistic Transfer Function and the\nSquare Loss with Examples ((10, .55), (.7, .25?)\n(b) Sets of Minima can be Combined.\n\nstatements and proofs of the results mentioned above in Section 3. At first (Section 3.1) we\nshow that n one-dimensional examples might result in n local minima of the error function\n(see e.g. Figure 3a for the error function of two one-dimensional examples). From the local\nminima in one dimension it follows easily that n d-dimensional examples might result in\nLn/ dJ d local minima of the error function (see Figure 1 and discussion in Section 3.2).\nWe then consider neurons with a bias (Section 4), i.e. we add an additional input that is\n((Xt, Yt?)I<t<m is\nclamped to one. The error function for a sequence of examples S\nnow\n\n=\n\nm\n\nEs(B, w) =\n\nI: L(Yt, r/>(B + WXt?,\nt=1\n\nwhere B denotes the bias, i.e. the weight of the input that is clamped to one. We can prove\nthat the error function might have Ln/2dJ d local minima if loss and transfer function are\nsymmetric. This holds for example for the square loss and the logistic transfer function .\nThe proofs are omitted due to space constraints. They are given in the full paper [AHW96] ,\ntogether with additional results for general loss and transfer functions.\nFinally we show in Section 5 that with minimal assumptions on transfer and loss functions\nthat there is only one minimum of the error function if the sequence of examples is realizable\nby the neuron.\nThe essence of the proofs is quite simple. At first observe that ifloss and transfer function are\nbounded and the domain is unbounded, then there exist areas of saturation where the error\nfunction is essentially flat. Furthermore the error function is "additive" i.e. the error function\nproduced by examples in SUS\' is simply the error function produced by the examples in\nS added to the error function produced by the examples in S\', Esusl Es + ESI. Hence\nthe local minima of Es remain local minima of Esus 1 if they fall into an area of saturation\nof Es. Similarly, the local minima of ESI remain local minima of Esusl as well (see\nFigure 3b). In this way sets of local minima can be combined.\n\n=\n\n2 PRELIMINARIES \'\nWe introduce the notion of minimum-containing set which will prove useful for counting\nthe minima of the error function.\n\n\x0c320\n\nP. AUER, M. HERBSTER, M. K. WARMUTH\n\nDefinition 2.1 Let f : Rd_R be a continuous function. Then an open and bounded set\nU E Rd is called a minimum-containing set for f if for each w on the boundary of U there\nis a w\'" E U such that f(w"\') < f(w).\nObviously any minimum-containing set contains a local minimum of the respecti ve function.\nFurthermore each of n disjoint minimum-containing sets contains a distinct local minimum.\nThus it is sufficient to find n disjoint minimum-containing sets in order to show that a\nfunction has at least n local minima.\n\n3 MINIMA FOR NEURONS WITHOUT BIAS\nWe will consider transfer functions ? and loss functions L which have the following\nproperty:\n(PI): The transfer function ? : R-R is non-constant. The loss function L : ?(R) x\n?(R)-[O, 00) has the property that L(y, y) = 0 and L(y, f)) > 0 for all y f.\nf) E ?(R). FinallythefunctionL(?,?(?)): ?(R) x R-[O,oo) is continuous and\n\nbounded.\n\n3.1\n\nONE MINIMUM PER EXAMPLE IN ONE DIMENSION\n\nTheorem 3.1 Let ? and L satisfy ( PI). Then for all n ~ I there is a sequence of n\nexamples S = (XI, y), ... , (x n , y)), Xt E R, y E ?(R), such that Es(w) has n distinct\nlocal minima.\nSince L(y, ?( w)) is continuous and non-constant there are w- , w"\', w+ E R such that the\nvalues ?( w-), ?( w"\'), ?( w+) are all distinct. Furthermore we can assume without loss\nof generality that 0 < w- < w\'" < w+. Now set y = ?(w"\'). If the error function\nL(y, ?(w)) has infinitely many local minima then Theorem 3.1 follows immediately, e.g.\nby setting XI = ... = Xn = 1. If L(y, ?(w)) has only finitely many minima then\nlimw ..... oo L(y, ?(w)) = L(y, ?(oo)) exists since L(y, ?(w)) is bounded and continuous.\nWe use this fact in the following lemma. It states that we get a new minimum-containing\nset by adding an example in the area of saturation of the error function.\nLemma 3.2 Assume that limw..... oo L(y, ?( w)) exists. Let S = (XI, YI), ... , (x n , Yn))\nbe a sequence of examples and 0 < WI < wi < wt < ... < w;; < w~ < w~\nsuch that Es(w t ) > Es(wn and Es(wn < Es(wt) for t = 1, ... , n. Let S\' =\n(xo, y} (XI, Yd, ... , (x n, Yn)) where Xo is sufficiently large. Furthermoreletwo = w\'" /xo\nand Wo = w?/xo (where w-, w"\', w+, Y = ?(w"\') are as above). Then 0 < we; < Wo <\nwt < WI < wi < wt < ... < w;; < w~ < w~ and\n\nProof. We have to show that for all Xo sufficiently large condition (l) is satisfied, i.e. that\n\n(2)\nWe get\nlim ESI(WO) = L(y, ?(w"\'))\n\n~\n\n..... oo\n\nrecalling that Wo\n\n+\n\nlim Es(w\'" /xo) = L(y, ?(w"\'))\n\n~-oo\n\n= w\'" /xo and S\' = S u (xo, y) . Analogously\nlim ESI(w~) = L(y,?(w?)) + Es(O).\nx\n0"\'" 00\n\n+ Es(O),\n\n\x0c321\n\nExponentially Many Local Minima for Single Neurons\n\nThus equation (2) holds for t = 0. For t = 1, ... , n we get\nlim ESI(w;) = lim L(y, ?(w;xo))\n:1:0-+00\n\n:1:0-+00\n\n+ Es(wn\n\n= L(y, ?(oo))\n\n+ Es(wn\n\nand\n\nSince Es (w;)\n\n< Es (w;) for t\n\n= 1, ... , n, the lemma follows.\n\no\n\nProof of Theorem 3.1. The theorem follows by induction from Lemma 3.2 since each\n0\ninterval ( wi, wi) is a minimum-containing set for the error function .\nRemark. Though the proof requires the magnitude of the examples to be arbitrarily large I\nin practice local minima show up for even moderately sized w (see Figure 3a).\n3.2\n\nCURSE OF DIMENSIONALITY: THE NUMBER OF MINIMA MIGHT\nGROW EXPONENTIALLY WITH THE DIMENSION\n\nWe show how the I-dimensional minima of Theorem 3.1 can be combined to obtain ddimensional minima.\nLemma 3.3 Let I : R -+ R be a continuous function with n disjoint minimum-containing\nsets UI , .?. ,Un. Then the sets UtI x ... X Utd , tj E {I, ... , n}, are n d disjoint minimumcontaining sets for the function 9 : Rd -+ R, g(XI, . .. , Xd) = l(xI) + ... + I(xd).\n\no\n\nProof. Omitted.\n\nTheorem 3.4 Let ? and L satisfy ( PI). Then for all n ~ 1 there is a sequence of examples\nS = (XI,Y),""(xn,y)), Xt E Rd, y E ?(R), such that Es(w) has l~Jd distinct local\nminima.\nBy Lemma 3.2 there exists a sequence of one-dimensional examples S\' =\n(xI,y)"" , (xLcrJ\'Y)) such that ESI(w) has L~J disjoint minimum-containing sets.\nThus by Lemma 3.3 the error function Es (w) has l ~ Jd disjoint minimum-containing\nsets where S = ((XI, 0, .. . ,0), y), ... , ?xLcrJ\' 0, . .. ,0), y), .. . , ?0, ... , xI), y), .. . ,\n?0, .. . , xLcrJ), y)).\n0\nProof.\n\n4 MINIMA FOR NEURONS WITH A BIAS\nTheorem 4.1 Let the transfer function ? and the loss function L satisfy ?( Bo + z) - ?o =\n?o - ?(Bo - z) and L(?o + y, ?o + y)\nL(?o - y, ?o - y)for some B o, ?o E R and all\nz E R, y, Y E ?(R). Furthermore let ? have a continuous second derivative and assume\nthat the first derivative of ? at Bo is non-zero. At last let ~L(y, y) be continuous in y\n\n=\n\nThen for all n ~ 1\nthere is a sequence of examples S = (XI, YI), . .. , (xn, Yn)), Xt E R d, Yt E ?(R), such\n\nand y, L(y, y) = 0for all y E ?(R), and\nthat Es (B, w) has\n\n(~L(Y, y)) (?o, ?o) > 0.\n\nl ~ Jd distinct local minima.\n\nNote that the square loss along with either the hyperbolic or logistic transfer function\nsatisfies the conditions of the theorem.\nIThere is a parallel proof where the magnitudes of the examples may be arbitrarily small.\n\n\x0cP. AUER, M. HERBSTER, M. K. WARMUTH\n\n322\n\n5 ONE MINIMUM IN THE REALIZABLE CASE\nWe show that when transfer and loss function are monotone and the examples are realizable\nthen there is only a single minimal surface. A sequence of examples S is realizable if\nEs(w) = 0 for some wE Rd.\n\nTheorem 5.1 Let 4> and L satisfy (P1). Furthermore let 4> be mOriotone and L such that\nL(y, y + rl) ~ L(y, y + r2) for 0 ~ rl ~ r2 or 0 ~ rl ~ r2. Assume that for some\nsequence of examples S there is a weight vectorwo E Rd such that Es(wo) = O. Thenfor\neach WI E Rd the function h( a) = Es (( 1 - a )wo + aWl) is increasing for a ~ O.\nThus each minimum WI can be connected with Wo by the line segment WOWI such that\nEs(w) = 0 for all W on WOWI.\n\nProof of Theorem 5.1.\nLet S = ((XI, yd, ... , (xn, Yn)).\nThen h(a)\nE~=I L(yt, 4>(WOXt + a(wl - wo)xt}). Since Yt = 4>(WOXt) it suffices to show that\nL(4)(z), 4>(z+ar)) is monotonically increasing in a ~ ofor all Z, r E R. Let 0 ~ al ~ a2.\nSince 4> is monotone we get 4>(z + aIr) = 4>(z) + rl, 4>(z + a2r)\n4>(z) + r2 where\no ~ rl ~ r2 or 0 ~ rl ~ r2? Thus L(4)(z), 4>(z + aIr)) ~ L(4)(z), 4>(z + a2r)).\n0\n\n=\n\nAcknowledgments\nWe thank Mike Dooley, Andrew Klinger and Eduardo Sontag for valuable discussions. Peter Auer\ngratefully acknowledges support from the FWF, Austria, under grant J01028-MAT. Mark Herbster\nand Manfred Warmuth were supported by NSF grant IRI-9123692.\n\nReferences\n[AHW96] P. Auer, M. Herbster, and M. K. Warmuth. Exponentially many local minima for single\nneurons. Technical Report UCSC-CRL-96-1, Univ. of Calif. Computer Research Lab,\nSanta Cruz, CA, 1996. In preperation.\n[Blu89]\n\nE.K. Blum. Approximation of boolean functions by sigmoidal networks: Part i: Xor and\nother two-variable functions . Neural Computation, 1:532-540, February 1989.\n\n[BRS89]\n\nM.L. Brady, R. Raghavan, and J. Slawny. Back propagation fails to separate where\nperceptrons succeed. IEEE Transactions On Circuits and Systems, 36(5):665-674, May\n1989.\n\n[BW88]\n\nE. Baum and F. Wilczek. Supervised learning of probability distributions by neural\nnetworks . In D.Z. Anderson, editor, Neural Information Processing Systems, pages 5261, New York, 1988. American Insitute of Physics.\n\n[GT92]\n\nMarco Gori and Alberto Tesi. On the problem of local minima in backpropagation. IEEE\nTransaction on Pattern Analysis and Machine Intelligence, 14(1):76-86, 1992.\n\n[Hay94]\n\nS. Haykin. Neural Networks: a Comprehensive Foundation. Macmillan, New York, NY,\n1994.\n\n[SLF88]\n\nS. A. Solla, E. Levin, and M. Fleisher. Accelerated learning in layered neural networks.\nComplex Systems, 2:625-639,1988.\n\n[SS89]\n\nE.D. Sontag and H.l. Sussmann. Backpropagation can give rise to spurious local minima\neven for networks without hidden layers. Complex Systems, 3(1):91-106, February 1989.\n\n[SS91]\n\nE.D. Sontag and H.l. Sussmann. Back propagation separates where perceptrons do. Neural\nNetworks,4(3),1991.\n\n[Wat92]\n\nR. L. Watrous. A comparison between squared error and relative entropy metrics using\nseveral optimization algorithms. Complex Systems, 6:495-505, 1992.\n\n[WD88]\n\nB.S. Wittner and J .S. Denker. Strategies for teaching layered networks classification tasks.\nIn D.Z. Anderson, editor, Neural Information Processing Systems, pages 850--859, New\nYork, 1988. American Insitute of Physics.\n\n\x0c'
p83203
sg30
S'Family Discovery\n\nStephen M. Omohundro\nNEC Research Institute\n4 Independence Way, Princeton, NJ 08540\nom@research.nj.nec.com\n\nAbstract\n"Family discovery" is the task of learning the dimension and structure of a parameterized family of stochastic models. It is especially appropriate when the training examples are partitioned into\n"episodes" of samples drawn from a single parameter value. We\npresent three family discovery algorithms based on surface learning and show that they significantly improve performance over two\nalternatives on a parameterized classification task.\n\n1\n\nINTRODUCTION\n\nHuman listeners improve their ability to recognize speech by identifying the accent\nof the speaker. "Might" in an American accent is similar to "mate" in an Australian\naccent. By first identifying the accent, discrimination between these two words is\nimproved. We can imagine locating a speaker in a "space of accents" parameterized\nby features like pitch, vowel formants, "r" -strength, etc. This paper considers the\ntask of learning such parameterized models from data.\nMost speech recognition systems train hidden Markov models on labelled speech\ndata. Speaker-dependent systems train on speech from a single speaker. Speakerindependent systems are usually similar, but are trained on speech from many\ndifferent speakers in the hope that they will then recognize them all. This kind of\ntraining ignores speaker identity and is likely to result in confusion between pairs of\nwords which are given the same pronunciation by speakers with different accents.\nSpeaker-independent recognition systems could more closely mimic the human approach by using a learning paradigm we call "family discovery". The system would\nbe trained on speech data partitioned into "episodes" for each speaker. From this\ndata, the system would construct a parameterized family of models representing dif-\n\n\x0cFamily Discovery\n\nAffine\nFamily\n\n403\n\nAffine Patch\nFamily\n\nCoupled Map\nFamily\n\nFigure 1: The structure of the three family discovery algorithms.\n\nferent accents. The learning algorithms presented in this paper could determine the\ndimension and structure of the parameterization. Given a sample of new speech,\nthe best-fitting accent model would be used for recognition.\nThe same paradigm applies to many other recognition tasks. For example, an OCR\nsystem could learn a parameterized family of font models (Revow, et. al., 1994).\nGiven new text, the system would identify the document\'s font parameters and use\nthe corresponding character recognizer.\nIn general, we use "family discovery" to refer to the task of learning the dimension\nand structure of a parameterized family of stochastic models. The methods we\npresent are equally applicable to parameterized density estimation, classification,\nregression, manifold learning, reinforcement learning, clustering, stochastic grammar learning, and other stochastic settings. Here we only discuss classification and\nprimarily consider training examples which are explicitly partitioned into episodes.\nThis approach fits naturally into the neural network literature on "meta-learning"\n(Schmidhuber, 1995) and "network transfer" (Pratt, 1994). It may also be considered as a particular case of the "bias learning" framework proposed by Baxter at\nthis conference (Baxter, 1996).\nThere are two primary alternatives to family discovery: 1) try to fit a single model\nto the data from all episodes or 2) use separate models for each episode. The first\napproach ignores the information that the different training sets came from distinct\nmodels. The second approach eliminates the possibility of inductive generalization\nfrom one set to another.\nIn Section 2, we present three algorithms for family discovery based on techniques\nfor "surface learning" (Bregler and Omohundro, 1994 and 1995). As shown in Figure\n1, the three alternative representations of the family are: 1) a single affine subspace\nof the parameter space, 2) a set of local affine patches smoothly blended together,\nand 3) a pair of coupled maps from the parameter space into the model space and\nback. In Section 3, we compare these three approaches to the two alternatives on a\nparameterized classification task.\n\n\x0c404\n\n2\n\nS. M. OMOHUNDRO\n\nTHE FIVE ALGORITHMS\n\nLet the space of all classifiers under consideration be parameterized by 0 and assume\nthat different values of 0 correspond to different classifiers (ie. it is identifiable). For\nexample, 0 might represent the means, covariances, and class priors of a classifier\nwith normal class-conditional densities. O-space will typically have a much higher\ndimension than the parameterized family we are seeking. We write P9(X) for the\ntotal probability that the classifier 0 assigns to a labelled or unlabelled example x.\nThe true models are drawn from a d-dimensional family parameterized by , . Let the\ntraining set be partitioned into N episodes where episode i consists of Ni training\nexamples tij, 1 :S j :S Ni drawn from a single underlying model with parameter\nA family discovery learning algorithm uses this training data to estimate the\nunderlying parameterized family.\n\n0:.\n\nFrom a parameterized family, we may define the projection operator P from O-space\nto itself which takes each 0 to the closest member of the family. Using this projection\noperator, we may define a "family prior" on O-space which dies off exponentially\nwith the square distance of a model from the family mp(O) ex e-(9-P(9))2. Each\nof the family discovery algorithms chooses a family so as to maximize the posterior\nprobability of the training data with respect to this prior. If the data is very\nsparse, this MAP approximation to a full Bayesian solution can be supplemented\nby "Occam" terms (MacKay, 1995) or by using a Monte Carlo approximation.\nThe outer loop of each of the algorithms performs the optimization of the fit of the\ndata by re-estimation in a manner similar to the Expectation Maximization (EM)\napproach (Jordan and Jacobs, 1994). First, the training data in each episode i is\nindependently fit by a model Oi. Then the dimension of the family is determined\nas described later and the family projection operator P is chosen to maximize the\nprobability that the episode models Oi came from that family\ni mp(Oi). The\nepisode models Oi are then re-estimated including the new prior probability mp.\nThese newly re-estimated models are influenced by the other episodes through mp\nand so exhibit training set "transfer". The re-estimation loop is repeated until\nnothing changes.\n\nn\n\nThe learned family can then be used to classify a set of N test unlabelled test examples Xk, 1 :S k :S N test drawn from a model O;est in the family. First, the parameter\nOtest is estimated by selecting the member of the family with the highest likelihood\non the test samples. This model is then used to perform the classification. A good\napproximation to the best-fit family member is often to take the image of the best-fit\nmodel in the entire O-space under the projection operator P.\nIn the next five sections, we describe the two alternative approaches and the three\nfamily discovery algorithms. They differ only in their choice of family representation\nas encoded in the projection operator P.\n\n2.1\n\nThe Single Model Approach\n\nThe first alternative approach is to train a single model on all of the training data.\nIt selects 0 to maximize the total likelihood L( 0) = n~l n~l P9 (tij ). New test\ndata is classified by this single selected model.\n\n\x0cFamily Discovery\n\n2.2\n\n405\n\nThe Separate Models Approach\n\nThe second alternative approach fits separate models for each training }?isode. It\nchooses Bi for 1::; i::; N to maximize the episode likelihood Li(Bi ) = TIj~IPIJ(tij).\nGiven new test data, it determines which of the individual models Bi fit best and\nclassifies the data with it.\n2.3\n\nThe Affine Algorithm\n\nThe affine model represents the underlying model family as an affine subspace of\nthe model parameter space. The projection operator Pal line projects a parameter\nvector B orthogonally onto the affine subspace. The subspace is determined by\nselecting the top principal vectors in a principal components analysis of the bestfit episode model parameters. As described in (Bregler & Omohundro, 1994) the\ndimension is chosen by looking for a gap in the principal values.\n2.4\n\nThe Affine Patch Algorithm\n\nThe second family discovery algorithm is based on the "surface learning" procedure described in (Bregler and Omohundro, 1994). The family is represented by\na collection of local affine patches which are blended together using Gaussian influence functions. The projection mapping Ppatch is a smooth convex combination\nof projections onto the affine patches Ppatch(B) = 2::=1 10: (B)Ao: (B) where Ao: is\nthe projection operator for an affine patch and Io:(B) =\nis a normalized\n\nE:"J:)(IJ)\n\nGaussian blending function.\nThe patches are initialized using k-means clustering on the episode models to choose\nk patch centers. A local principal components analysis is performed on the episode\nmodels which are closest to each center. The family dimension is determined by\nexamining how the principal values scale as successive nearest neighbors are considered. Each patch may be thought of as a "pancake" lying in the surface. Dimensions\nwhich belong to the surface grow quickly as more neighbors are considered while\ndimensions across the surface grow only because of the curvature of the surface.\nThe Gaussian influence functions and the affine patches are then updated by the\nEM algorithm (Jordan and Jacobs, 1994). With the affine patches held fixed, the\nGaussians Go: are refit to the errors each patch makes in approximating the episode\nmodels. Then with the Gaussians held fixed, the affine patches Ao: are refit to the\nepsiode models weighted by the the corresponding Gaussian Go:. Similar patches\nmay be merged together to form a more parsimonious model.\n2.5\n\nThe Coupled Map Algorithm\n\nThe affine patch approach has the virtue that it can represent topologically complex\nfamilies (eg. families representing physical objects might naturally be parameterized\nby the rotation group which is topologically a projective plane). It cannot, however,\nprovide an explicit parameterization of the family which is useful in some applications (eg. optimization searches). The third family discovery algorithm therefore\nattempts to directly learn a parameterization of the model family.\nRecall that the model parameters define B-space, while the family parameters de-\n\n\x0c406\n\nS. M. OMOHUNDRO\n\nfine \'Y-space. We represent a family by a mapping G from B-space to \'Y-space together with a mapping F from \'Y-space back to B-space. The projection operation\nis Pmap(B) = F(G(B)). The map G(O) defines the family parameter l\' on the full\nO-space.\nThis representation is similar to an "auto-associator" network in which we attempt\nto "encode" the best-fit episode parameters Oi in the lower dimensional \'Y-space\nby the mapping G in such a way that they can be correctly reconstructed by the\nfunction F. Unfortunately, if we try to train F and G using back-propagation on\nthe identity error function, we get no training data away from the family. There is\nno reason for G to project points away from the family to the closest family member.\nWe can rectify this by training F and G iteratively. First an arbitrary G is chosen\nand F is trained to send the images \'Yi = G(Oi) back to 0i\' G is trained, however,\non images under F corrupted by additive spherical Gaussian noise! This provides\nsamples away from the family and on average the training signal sends each point\nin B space to the closest family member.\nTo avoid iterative training, our experiments used a simpler approach. G was taken to\nbe the affine projection operator defined by a global principal components analysis\nof the best-fit episode model parameters. Once G is defined, F is chosen to minimize\nthe difference between F(G(Oi)) and Oi for each best-fit episode parameter Oi.\nAny form of trainable nonlinear mapping could be used for F (eg. backprop neural\nnetworks or radial basis function networks). We represent F as a mixture of experts\n(Jordan and Jacobs, 1994) where each expert is an affine mapping and the mixture\ncoefficients are Gaussians. The mapping is trained by the EM algorithm.\n\n3\n\nALGORITHM COMPARISON\n\nTo compare these five algorithms, we consider a two-class classification task with\nunit-variance normal class-conditional distributions on a 5-dimensional feature\nspace. The means of the class distributions are parameterized by a nonlinear twoparameter family:\nml\nm2\n\n= (1\'1\n\n= (\'Yl\n\n+ ~cos??e~1 + (\'Y2 + ~sin??e~2\n- ~ cos ?>) e~1 + (\'Y2 - ~ sin ?>) l2 .\n\nwhere 0 ~ 1\'1, 1\'2 ~ 10 and ?> = (\'Yl + 1\'2)/3. The class means are kept at a unit\ndistance apart, ensuring significant class overlap over the whole family. The angle\n?> varies with the parameters so that the correct classification boundary changes\norientation over the family. This choice of parameters introduces sufficient nonlinearity in the task to distinguish the non-linear algorithms from the linear one.\nFigure 1 shows the comparative performance of the 5 algorithms. The x-axis is the\ntotal number of training examples. Each set of examples consisted of approximately\nN =\nepisodes of approximately Ni =\nexamples each. The classifier parameters for an episode were drawn uniformly from the classifier family. The episode\ntraining examples were then sampled from the chosen classifier according to the\nclassifier\'s distribution. Each of the 5 algorithms was then trained on these examples. The number of patches in the surface patch algorithm and the number of affine\ncomponents in the surface map algorithm were both taken to be the square-root of\n\n..;x\n\n..;x\n\n\x0cFamily Discovery\n0.52\n\n407\n\nr---.---.---""T""----r----,-----r---r---~-__,\n\nSingle model\nSeparate models\nAffine family\nAffine Patch family\nMap Mixture family\n\n0.5\n\n-+-+-_.\n\n-EJ -??x????\n-A-.-\n\n0.48\n0.46\nI!?\n\ng\n\n0.44\n\n\'0\n\n0.42\n\nw\nc:\n0\n\n:uI!!\n\nu.\n\n0.4\n0.38\n0 .36\n0 .34\n\n400\n\n600\n\n800\n\n1000\n1200\nNumber of Examples\n\n1400\n\n1600\n\n1800\n\n2000\n\nFigure 2: A comparison of the 5 family discovery algorithms on the classification\ntask.\nthe number of training episodes.\nThe y-axis shows the percentage correct for each algorithm on an independent test\nset. Each test set consisted of 50 episodes of 50 examples each. The algorithms\nwere presented with unlabelled data and their classification predictions were then\ncompared with the correct classification label.\nThe results show significant improvement through the use of family discovery for\nthis classification task. The single model approach performed significantly worse\nthan any of the other approaches, especially for larger numbers of episodes (where\nthe family discovery becomes possible). The separate model approach improves with\nthe number of episodes, but is nearly always bested by the approaches which take\nexplicit account of the underlying parameterized family. Because of the nonlinearity\nin this task, the simple affine model performs more poorly than the two nonlinear\nmethods. It is simple to implement, however, and may well be the method of choice\nwhen the parameters aren \'t so nonlinear. From this data, there is not a clear winner\nbetween the surface patch and surface map approaches.\n\n4\n\nTRAINING SET DISCOVERY\n\nThroughout this paper, we have assumed that the training set was partitioned into\nepisodes by the teacher. Agents interacting with the world may not be given this\nexplicit information. For example, a speech recognition system may not be told\nwhen it is conversing with a new speaker. Similarly, a character recognition system\n\n\x0c408\n\ns. M. OMOHUNDRO\n\nwould probably not be given explicit information about font changes. Learners can\nsometimes use the data itself to detect these changes, however. In many situations\nthere is a strong prior that successive events are likely to have come from a single\nmodel with only occasional model changes. The EM algorithm is often used for\nsegmenting unlabelled speech. It may be used in a similar manner to find the\ntraining set episode boundaries. First, a clustering algorithm is used to partition\nthe training examples into episodes. A parameterized family is then fit to these\nepisodes. The data is then repartitioned according to the similarity of the induced\nfamily parameters and the process is repeated until it converges. A similar approach\nmay be applied when the model parameters vary slowly with time rather than\noccasionally jumping discontinously.\nAcknowledgements\n\nI\'d like to thank Chris Bregler for work on the affine patch approach to surface\nlearning, Alexander Linden for suggesting coupled maps for surface learning, and\nPeter Blicher for discussions.\nReferences\n\nBaxter, J. (1995) Learning model bias. This volume.\nBregler, C. & Omohundro, S. (1994) Surface learning with applications to lipreading. In J. Cowan, G. Tesauro and J. Alspector (eds.), Advances in Neural Information Processing Systems 6, pp. 43-50. San Francisco, CA: Morgan Kaufmann\nPublishers.\nBregler, C. & Omohundro, S. (1995) Nonlinear image interpolation using manifold\nlearning. In G. Tesauro, D. Touretzky and T. Leen (eds .), Advances in Neural\nInformation Processing Systems 7. Cambridge, MA: MIT Press.\nBregler, C. & Omohundro, S. (1995) Nonlinear manifold learning for visual speech\nrecognition. In W . Grimson (ed.), Proceedings of the Fifth International Conference\non Computer Vision.\nJordan, M. & Jacobs, R. (1994) Hierarchical mixtures of experts and the EM algorithm. Neural Computation, 6:181-214.\nMacKay, D. (1995) Probable networks and plausible predictions - a review of practical Bayesian methods for supervised neural networks. Network, to appear.\nPratt, L. (1994) Experiments on the transfer of knowledge between neural networks.\nIn S. Hanson, G. Drastal, and R. Rivest (eds.) , Computational Learning Theory and\nNatural Learning Systems, Constraints and Prospects, pp. 523-560. Cambridge,\nMA: MIT Press.\nRevow, M., Williams, C. and Hinton, G. (1994) Using generative models for handwritten digit recognition. Technical report, University of Toronto.\nSchmidhuber, J. (1995) On learning how to learn learning strategies. Technical\nReport FKI-198-94, Fakultat fur Informatik, Technische Universitat Munchen.\n\n\x0c'
p83204
sg287
S'Neural Networks with Quadratic VC\nDimension\nPascal Koiran*\nLab. de l\'Informatique du Paraltelisme\nEcole Normale Superieure de Lyon - CNRS\n69364 Lyon Cedex 07, France\n\nEduardo D. Sontag t\nDepartment of Mathematics\nRutgers University\nNew Brunswick, NJ 08903, USA\n\nAbstract\nThis paper shows that neural networks which use continuous activation functions have VC dimension at least as large as the square\nof the number of weights w. This result settles a long-standing\nopen question, namely whether the well-known O( w log w) bound,\nknown for hard-threshold nets, also held for more general sigmoidal\nnets. Implications for the number of samples needed for valid generalization are discussed.\n\n1\n\nIntroduction\n\nOne of the main applications of artificial neural networks is to pattern classification\ntasks. A set of labeled training samples is provided, and a network must be obtained\nwhich is then expected to correctly classify previously unseen inputs. In this context,\na central problem is to estimate the amount of training data needed to guarantee\nsatisfactory learning performance. To study this question, it is necessary to first\nformalize the notion of learning from examples.\nOne such formalization is based on the paradigm of probably approximately correct\n(PAC) learning, due to Valiant (1984). In this framework, one starts by fitting some\nfunction /, chosen from a predetermined class F, to the given training data. The\nclass F is often called the "hypothesis class" , and for purposes of this discussion it\nwill be assumed that the functions in F take binary values {O, I} and are defined on a\ncommon domain X. (In neural networks applications, typically F corresponds to the\nset of all neural networks with a given architecture and choice of activation functions.\nThe elements of X are the inputs, possibly multidimensional.) The training data\nconsists of labeled samples (Xi,ci), with each Xi E X and each Ci E {O, I}, and\n*koiranGlip. ens-lyon. fr.\ntsontagGhilbert.rutgers.edu.\n\n\x0c198\n\nP. KOIRAN, E. D. SONTAG\n\n"fitting" by an f means that f(xj) = Cj for each i. Given a new example x, one\nuses f( x) as a guess of the "correct" classification of x. Assuming that both training\ninputs and future inputs are picked according to the same probability distribution\non X, one needs that the space of possible inputs be well-sampled by the training\ndata, so that f is an accurate fit. We omit the details of the formalization of\nPAC learning, since there are excellent references available, both in textbook (e.g.\nAnthony and Biggs (1992), Natarajan (1991)) and survey paper (e.g. Maass (1994))\nform, and the concept is by now very well-known.\nAfter the work of Vapnik (1982) in statistics and of Blumer et. al. (1989) in computationallearning theory, one knows that a certain combinatorial quantity, called\nthe Vapnik-Chervonenkis (VC) dimension VC(F) of the class F of interest completely characterizes the sample sizes needed for learnability in the PAC sense. (The\nappropriate definitions are reviewed below. In Valiant\'s formulation one is also interested in quantifying the computational effort required to actually fit a function\nto the given training data, but we are ignoring that aspect in the current paper.)\nVery roughly speaking, the number of samples needed in order to learn reliably is\nproportional to VC(F). Estimating VC(F) then becomes a central concern. Thus\nfrom now on, we speak exclusively of VC dimension, instead of the original PAC\nlearning problem.\nThe work of Cover (1988) and Baum and Haussler (1989) dealt with the computation of VC(F) when the class F consists of networks built up from hard-threshold\nactivations and having w weights; they showed that VC(F)= O(wlogw). (Conversely, Maass (1993) showed that there is also a lower bound of this form.) It\nwould appear that this definitely settled the VC dimension (and hence also the\nsample size) question.\nHowever, the above estimate assumes an architecture based on hard-threshold\n("Heaviside") neurons. In contrast, the usually employed gradient descent learning\nalgorithms ("backpropagation" method) rely upon continuous activations, that is,\nneurons with graded responses. As pointed out in Sontag (1989), the use of analog activations, which allow the passing of rich (not just binary) information among\nlevels, may result in higher memory capacity as compared with threshold nets. This\nhas serious potential implications in learning, essentially because more memory capacity means that a given function f may be able to "memorize" in a "rote" fashion\ntoo much data, and less generalization is therefore possible. Indeed, Sontag (1992)\nshowed that there are conceivable (though not very practical) neural architectures\nwith extremely high VC dimensions. Thus the problem of studying VC(F) for analog networks is an interesting and relevant issue. Two important contributions in\nthis direction were the papers by Maass (1993) and by Goldberg and Jerrum (1995),\nwhich showed upper bounds on the VC dimension of networks that use piecewise\npolynomial activations. The last reference, in particular, established for that case\nan upper bound of O(w2), where, as before, w is the number of weights. However\nit was an open problem (specifically, "open problem number 7" in the recent survey\nby Maass (1993) if there is a matching w 2 lower bound for such networks, and more\ngenerally for arbitrary continuous-activation nets. It could have been the case that\nthe upper bound O( w 2 ) is merely an artifact of the method of proof in Goldberg\nand Jerrum (1995), and that reliable learning with continuous-activation networks\nis still possible with far smaller sample sizes, proportional to O( w log w). But this is\nnot the case, and in this paper we answer Maass\' open question in the affirmative.\nAssume given an activation (T which has different limits at ?oo, and is such that\nthere is at least one point where it has a derivative and the derivative is nonzero\n(this last condition rules out the Heaviside activation). Then there are architectures with arbitrary large numbers of weights wand VC dimension proportional\n\n\x0cNeural Networks with Quadratic VC Dimension\n\n199\n\nto w 2 ? The proof relies on first showing that networks consisting of two types of\nactivations, Heavisides and linear, already have this power. This is a somewhat\nsurprising result, since purely linear networks result in VC dimension proportional\nto w, and purely threshold nets have, as per the results quoted above, VC dimension\nbounded by w log w. Our construction was originally motivated by a related one,\ngiven in Goldberg and Jerrum (1995), which showed that real-number programs (in\nthe Blum-Shub-Smale (1989) model of computation) with running time T have VC\ndimension O(T2). The desired result on continuous activations is then obtained,\napproximating Heaviside gates by IT-nets with large weights and approximating linear gates by IT-nets with small weights. This result applies in particular to the\nstandard sigmoid 1/(1 + e- X ). (However, in contrast with the piecewise-polynomial\ncase, there is still in that case a large gap between our O( w 2 ) lower bound and\nthe O( w 4 ) upper bound which was recently established in Karpinski and Macintyre (1995).) A number of variations, dealing with Boolean inputs, or weakening\nthe assumptions on IT, are discussed. The full version of this paper also includes\nsome remarks on thresholds networks with a constant number of linear gates, and\nthreshold-only nets with "shared" weights.\n\nBasic Terminology and Definitions\nFormally, a (first-order, feedforward) architecture or network A is a connected directed acyclic graph together with an assignment of a function to a subset of its\nnodes. The nodes are of two types: those of fan-in zero are called input nodes and\nthe remaining ones are called computation nodes or gates. An output node is a node\nof fan-out zero. To each gate g there is associated a function IT g : IR. -!- IR., called the\nactivation or gate function associated to g.\nThe number of weights or parameters associated to a gate 9 is the integer ng equal\nto the fan-in of 9 plus one. (This definition is motivated by the fact that each input\nto the gate will be multiplied by a weight, and the results are added together with\na "bias" constant term , seen as one more weight; see below.) The (total) number\nof weights (or parameters) of A is by definition the sum of the numbers n g , over all\nthe gates 9 of A. The number of inputs m of A is the total number of input nodes\n(one also says that "A has inputs in IR.m,,); it is assumed that m > O. The number\nof outputs p of A is the number of output nodes (unless otherwise mentioned, we\nassume by default that all nets considered have one-dimensional outputs, that is,\np = 1).\nTwo examples of gate functions that are of particular interest are the identity or\nlinear gate: Id( x)\nx for all x, and the threshold or H eaviside function: H (x) 1\nif x ~ 0, H(x) = 0 if x < O.\n\n=\n\n=\n\nLet A be an architecture. Assume that nodes of A have been linearly ordered as\n11"1, ... , 11"m, gl, ... , gl, where the 1I"j \'s are the input nodes and the gj \'s the gates. For\nsimplicity, write nj := n g ., for each i = 1, ... , I. Note that the total number of\nparameters is n = L:~=1 nj and the fan-in of each gj is nj - 1. To each architecture\nA (strictly speaking, an architecture together with such an ordering of nodes) we\nassociate a function\nF : ]Rm x ]Rn -!-]RP ,\nwhere p is the number of outputs of A, defined by first assigning an "output" to\neach node, recursively on the distance from the the input nodes. Assume given\nan input x E ]Rm and a vector of weights w E ]Rn. We partition w into blocks\n(WI , ... , WI) of sizes nl, ... , nl respectively. First the coordinates of x are assigned\nas the outputs of the input nodes 11"1, ... , 1I"m respectively. For each of the other\ngates gj, we proceed as follows. Assume that outputs Yl, ... , Yn. -1 have already\n\n\x0cP. KOIRAN, E. D. SONTAG\n\n200\n\nbeen assigned to the predecessor nodes of gi (these are input and/or computation\nnodes, listed consistently with the order fixed in advance). Then the output of gi\nis by definition\n(1\'g.\n\n(Wi,O\n\n+ Wi , lYI + Wi ,2Y2 + ... + wi,n.-lYn.-d\n\n,\n\nwhere we are writing Wi = (Wi,O, Wi,l, Wi ,2, ... , wi,n.-d. The value of F(x, w) is\nthen by definition the vector (scalar if p = 1) obtained by listing the outputs of the\noutput nodes (in the agreed-upon fixed ordering of nodes). We call F the function\ncomputed by the architecture A. For each choice of weights W E IRn, there is a\nfunction Fw : IR m _ IRP defined by Fw(x) := F(x, w) ; by abuse of terminology we\nsometimes call this also the function computed by A (if the weight vector has been\nfixed).\nAssume that A is an architecture with inputs in IR m and scalar outputs, and that\nthe (unique) output gate has range {O, 1}. A subset A ~ IR m is said to be shattered\nby A if for each Boolean function 13 : A - {O, 1} there is some weight W E IRn so\nthat Fw(x) = f3(x) for all x EA . The Vapnik-Chervonenkis (VC) dimension of A\nis the maximal size of a subset A ~ IRm that is shattered by A. If the output gate\ncan take non-binary values, we implicitly assume that the result of the computation\nis the sign of the output. That is, when we say that a subset A ~ IR m is shattered\nby A , we really mean that A is shattered by the architecture H(A) in which the\noutput of A is fed to a sign gate .\n\n2\n\nNetworks Made up of Linear and Threshold Gates\n\nProposition 1 For every n ;::: 1, there is a network architecture A with inputs in\nIR 2 and O( VN) weights that can shatter a set of size N = n 2. This architecture is\n\nmade only of linear and threshold gates.\nProof. Our architecture has n parameters WI , ... , W n ; each of them is an element\nofT {O.WI . .. Wn ;Wi E {O, 1}}. The shattered set will be S = [n]2\n{1, .. . ,nF.\n\n=\n\n=\n\nFor a given choice of W = (WI\' ... \' W n ), A will compute the boolean function\nfw : S - {O, 1} defined as follows: fw(x, y) is equal to the x-th bit of W y . Clearly,\nfor any boolean function f on S, there exists a (unique) W such that f = fw.\nWe first consider the obvious architecture which computes the function:\nn\n\n(1)\n- Wz-dH(y - z + 1/2)\nz=2\nsending each point Y E [n] to W y. This architecture has n - 1 threshold gates,\n3(n - 1) + 1 weights, and just one linear gate.\n\nflv(Y) = WI\n\n+ I)Wz\n\nNext we define a second multi-output net which maps wET to its binary representation j2(w) = (WI\' . .. \' wn ). Assume by induction that we have a net N?\nthat maps W to (WI, ... ,Wi,O.Wi+l ... Wn) . Since Wi+l = H(O .Wi+l . .. Wn -1/2)\nand o.Wi+2 ... Wn = 2 x o. Wi+1 . .. Wn - Wi+!, .N;;\'l can be obtained by adding one\nthreshold gate and one linear gate to .N;2 (as well as 4 weights). It follows that N~\nhas n threshold gates, n linear gates and 4n weights.\nFinally, we define a net N3 which takes as input x E [n] and W = (WI , ... , w n ) E\n{O, l}n, and outputs W X ? We would like this network to be as follows:\nn\n\nf3(X , w) = WI\n\n+L\nz=2\n\nn\n\nwzH(x - z + 1/2) -\n\nL wz_IH(x z=2\n\nz\n\n+ 1/2).\n\n\x0c201\n\nNeural Networks with Quadratic VC Dimension\n\nThis is not quite possible, because the products between the Wi\'S (which are inputs\nin this context) and the Heavisides are not allowed. However, since we are dealing\nwith binary variables one can write uv = H(u + v - l.5). Thus N3 has one linear\ngate, 4(n - 1) threshold gates and 12(n - 1) + n weights. Note that fw(x, y) =\np (x, P Ulv (y)). This can be realized by means of a net that has n + 2 linear gates,\n(n-l)+n+4(n-l) = 6n-5 threshold gates, and (3n-2)+4n+(12n-ll) = 19n-13\nweights. 0\nThe following is the main result of this section:\nTheorem 1 For every n ;::: 1, there is a network architecture A with inputs in IR.\nand O( VN) weights that can shatter a set of size N = n 2. This architecture is\nmade only of linear and threshold gates.\nProof. The shattered set will be S = {O, 1, .. . ,n 2 -I}. For every xES, there\nare unique integers x, y E {O, 1, ... , n - I} such that u = nx + y. The idea of the\nconstruction is to compute x and y, and then feed (x + 1, y + 1) to the network\nconstructed in Proposition 1. Note that x is the unique integer such that u - nx E\n{O, 1, .. . , n - I}. It can therefore by computed by brute force search as follows:\nn-1\nX\n\n=\n\nL kH[H(u -\n\nnk)\n\n+ H(n -\n\n1 - (u - nk)) - l.5].\n\nk=O\n\nThis network has 3n threshold gates, one linear gate and 8n weights. Then of course\ny = u - nx. 0\nA Boolean version is as follows.\nTheorem 2 For every d ;::: 1, there is a network architecture A with O( VN)\nweights that can shatter the N = 22d points of {O, 1F d . This architecture is made\nonly of linear and threshold gates.\n\nd, one can compute x = 1 + 2::=1 2i-1ui and y 1 +\nProof. Given u E {O,\n2:1=12i-1Ui+d with two linear gates. Then (x, y) can be fed to the network of\nProposition 1 (with n 2d ). 0\n\n=\n\nIF\n\n=\n\nIn other words, there is a network architecture with 2d weights that can compute\nall boolean functions on 2d variables.\n\n3\n\nArbitrary Sigmoids\n\nWe now extend the preceding VC dimension bounds to networks that use just\none activation function tr (instead of both linear and threshold gates). All that is\nrequired is that the gate function have a sigmoidal shape and satisfy a very weak\nsmoothness property:\nl. tr is differentiable at some point Xo (i.e., tr(xo+h) = tr(xo)+tr\'(xo)h+o(h))\nwhere tr\'(xo)# 0.\n2. limx __ oo tr(x) = and limx _+ oo tr(x) = 1 (the limits and 1 can be\nreplaced by any distinct numbers).\n\n?\n\n?\n\nA function satisfying these two conditions will be called sigmoidal. Given any such\n\ntr, we will show that networks using only tr gates provide quadratic VC dimension.\n\n\x0cP. KOIRAN, E. D. SONTAG\n\n202\n\nTheorem 3 Let tT be an arbitrary sigmoidal function. There exist architectures Al\nand A2 with O( VN) weights made only of tT gates such that:\n\n? Al can shatter a subset ofIR of cardinality N = n 2 ,-\n\n? A2 can shatter the N = 22d points of {O, 1}2d.\nThis follows directly from Theorems 1 and 2, together with the following simulation\nresult:\n\nTheorem 4 Let tT be a an arbitrary sigmoidal function. Let N be a network of\n\nT threshold and L linear gates, with a threshold gate at the output. Then N can\n\nbe simulated on any given finite set of inputs by a network N\' of T + L gates that\nall use the activation function tT (except the output gate which is still a threshold).\nMoreover, if N has n weights then N\' has O( n) weights.\n\nProof. Let S be a finite set of inputs. We can assume, by changing the thresholds of\nthreshold gates if necessary, that the net input Ig (x) to any threshold gate 9 of N\nis different from for all inputs xES.\n\n?\n\nGiven ? > 0, let N( be the net obtained by replacing the output functions of all gates\nby the new output function x 1--+ tT( X / ?) if this output function is the sign function ,\nand by x 1--+ tT(x) = [tT(xo+?x)-tT(xo))/[?tT\'(xo)] ifit is the identity function. Note\nthat for any a > 0, lim(_o+ tT(x/?) = H(x) uniformly for x E) - 00, -a] U [a, +00]\nand limHo tT(x) = x uniformly for x E [-l/a, l/a].\nThis implies by induction on the depth of 9 that for any gate 9 of N and any input\nXES, the net input Ig,(x) to 9 in the transformed net N( satisfies li~_o IgAx) =\nIg(x) (here, we use the fact that the output function of every 9 is continuous at\nIg(x)). In particular, by taking 9 to be the output gate of N, we see that Nand\nN( compute the same function on S if ? is small enough. Such a net N( can be\ntransformed into an equivalent net N\' that uses only tT as gate function by a simple\ntransformation of its weights and thresholds. The number of weights remains the\nsame, except at most for a constant term that must be added to each net input to\na gate; thus if N has n weights, N\' has at most 2n weights. 0\n\n4\n\nMore General Gate Functions\n\nThe objective of this section is to establish results similar to Theorem 3, but for\neven more arbitrary gate functions, in particular weakening the assumption that\nlimits exist at infinity. The main result is, roughly, that any tT which is piecewise\ntwice (continuously) differentiable gives at least quadratic VC dimension, save for\ncertain exceptional cases involving functions that are almost everywhere linear.\nA function tT : IR --+ IR is said to be piecewise C 2 if there is a finite sequence\nal < a2 < ... < a p such that on each interval I of the form] - 00, al [, )ai, ai+1 [ or\n]a p , +00[, tTll is C2.\n\n(Note: our results hold even if it is only assumed that the second derivative exists in\neach of the above intervals; we do not use the continuity of these second derivatives.)\n\nTheorem 5 Let tT be a piecewise C2 function. For every n ~ 1, there exists an\narchitecture made of tT-gates, and with O( n) weights, that can shatter a subset of\nIR 2 of cardinality n 2 , except perhaps in the following cases:\n\n1. tT is piecewise-constant, and in this case the VC dimension of any architecture of n weights is O( n log n),-\n\n\x0cNeural Networks with Quadratic VC Dimension\n\n203\n\n2. u is affine, and in this case the VC dimension of any architecture of n\nweights is at most n.\n3. there are constants af; 0 and b such that u( x) = ax + b except at a finite\nnonempty set of points. In this case, the VC dimension of any architecture of n weights is O(n 2 ), and there are architectures of VC dimension\n\nO(nlogn).\nDue to the lack of space, the proof cannot be included in this paper. Note that\nthe upper bound of the first special case is tight for threshold nets, and that of the\nsecond special case is tight for linear functions in ]R n.\nAcknowledgements\n\nPascal Koiran was supported by an INRIA fellowship , DIMACS, and the International Computer Science Institute. Eduardo Sontag was supported in part by US\nAir Force Grant AFOSR-94-0293 .\nReferences\nM . ANTHONY AND N.L. BIGGS (1992) Computational Learning Th eory: An Introduction,\nCambridge U. Press.\nE .B. BAUM AND D . HAUSSLER (1989) What size net gives valid generalization?, Neural\nComputation 1, pp. 151-160.\nL. BLUM, M. SHUB AND S. SMALE (1989) On the theory of computation and complexity over the real numbers: NP-completeness, recursive functions and universal machines,\nBulletin of the AMS 21 , pp. 1- 46 .\nA. BLUMER, A . EHRENFEUCHT, D . HAUSSLER, AND M . WARMUTH (1989) Learnability\nand the Vapnik- Chervonenkis dimension , J. of the ACM 36, pp. 929-965.\nT.M. COVER (1988) Capacity problems for linear machines, in: Pattern Recognition , L.\nKanal ed. , Thompson Book Co., pp. 283-289.\nP. GOLDBERG AND M . JERRUM (1995) Bounding the Vapnik-Chervonenkis dim ension of\nconcept classes parametrized by real numbers, Machine Learning 18, pp. 131-148.\nM . KARPINSKI AND A. MACINTYRE (1995) Polynomial bounds for VC dimension of sigmoidal neural networks, in Proc. 27th ACM Symposium on Theory of Computing, pp. 200208.\nW. MAASS (1993) Bounds for the computational power and learning complexity of analog\nneural nets, in Proc. of the 25th ACM Symp. Theory of Computing, pp. 335-344.\nW . MAASS (1994) Perspectives of current research about the complexity of learning in neural nets, in Theoretical Advances in N eural Computation and Learning , V.P. Roychowdhury, K.Y. Siu, and A . Orlitsky, editors, Kluwer, Boston , pp. 295-336.\nB.K . NATARAJAN (1991) Machine Learning : A Theoretical Approach, M . Kaufmann Publishers, San Mateo , CA.\nE .D. SONTAG (1989) Sigmoids distinguish better than Heavisides, Neural Computation 1,\npp. 470-472.\nE.D. SONTAG (1992) Feedforward nets for interpolation and classification, J. Comp o\nSyst. Sci 45 , pp. 20-48.\nL.G. VALIANT (1984) A th eory of the learnable, Comm. of the ACM 27, pp. 1134-1142\nV .N. VAPNIK (1982) Estimation of Dependencies Based on Empirical Data, Springer,\nBerlin.\n\n\x0c'
p83205
sg74
S'Learning the structure of similarity\n\nJoshua B. Tenenbaum\nDepartment of Brain and Cognitive Sciences\nMassachusetts Institute of Technology\nCambridge, MA 02139\njbt~psyche.mit.edu\n\nAbstract\nThe additive clustering (ADCL US) model (Shepard & Arabie, 1979)\ntreats the similarity of two stimuli as a weighted additive measure\nof their common features. Inspired by recent work in unsupervised\nlearning with multiple cause models, we propose anew, statistically\nwell-motivated algorithm for discovering the structure of natural\nstimulus classes using the ADCLUS model, which promises substantial gains in conceptual simplicity, practical efficiency, and solution\nquality over earlier efforts. We also present preliminary results with\nartificial data and two classic similarity data sets.\n\n1\n\nINTRODUCTION\n\nThe capacity to judge one stimulus, object, or concept as similado another is thought\nto play a pivotal role in many cognitive processes, including generalization , recognition, categorization, and inference. Consequently, modeling subjective similarity\njudgments in order to discover the underlying structure of stimulus representations\nin the brain/mind holds a central place in contemporary cognitive science. Mathematical models of similarity can be divided roughly into two families: spatial models,\nin which stimuli correspond to points in a metric (typically Euclidean) space and\nsimilarity is treated as a decreasing function of distance; and set-theoretic models, in\nwhich stimuli are represented as members of salient subsets (presumably corresponding to natural classes or features in the world) and similarity is treated as a weighted\nsum of common and distinctive subsets.\nSpatial models, fit to similarity judgment data with familiar multidimensional scaling (MDS) techniques, have yielded concise descriptions of homogeneous, perceptual\ndomains (e.g. three-dimensional color space), often revealing the salient dimensions\nof stimulus variation (Shepard, 1980). Set-theoretic models are more general , in\nprinciple able to accomodate discrete conceptual structures typical of higher-level\ncognitive domains, as well as dimensional stimulus structures more common in per-\n\n\x0c4\n\n1. B. TENENBAUM\n\nception (Tversky, 1977). In practice, however, the utility of set-theoretic models is\nlimited by the hierarchical clustering techniques that underlie conventional methods\nfor discovering the discrete features or classes of stimuli. Specifically, hierarchical\nclustering requires that any two classes of stimuli correspond to disjoint or properly\ninclusive subsets, while psychologically natural classes may correspond in general to\narbitrarily overlapping subsets of stimuli. For example, the subjective similarity of\ntwo countries results from the interaction of multiple geographic and cultural factors, and there is no reason a priori to expect the subsets of communist, African, or\nFrench-speaking nations to be either disjoint or properly inclusive.\nIn this paper we consider the additive clustering (ADCL US) model (Shepard & Arabie, 1979), the simplest instantiation of Tversky \'s (1977) general contrast model that\naccommodates the arbitrarily overlapping class structures associated with multiple\ncauses of similarity. Here, the similarity of two stimuli is modeled as a weighted\nadditive measure of their common clusters:\nK\n\nSij\n\n=\n\nI:\n\nwkfikfJk\n\n+ C,\n\n(1)\n\nk=l\n\nwhere Sij is the reconstructed similarity of stimuli i and j, the weight Wk captures\nthe salience of cluster k, and the binary indicator variable fik equals 1 if stimulus i\nbelongs to cluster k and 0 otherwise. The additive constant c is necessary because the\nsimilarity data are assumed to be on an interval scale. 1 As with conventional clustering models, ADCLUS recovers a system of discrete subsets of stimuli, weighted by\nsalience, and the similarity of two stimuli increases with the number (and weight)\nof their common subsets. ADCLUS, however, makes none of the structural assumptions (e.g. that any two clusters are disjoint or properly inclusive) which limit the\napplicability of conventional set-theoretic models. Unfortunately this flexibility also\nmakes the problem of fitting the ADCL US model to an observed similarity matrix\nexceedingly difficult.\nPrevious attempts to fit the model have followed a heuristic strategy to minimize a\nsquared-error energy function ,\n\nE\n\n= I:(Sij - Sij)2 = I:(Sij itj\n\nitj\n\nI:\n\nwklikfJk)2,\n\n(2)\n\nk\n\nby alternately solving for the best cluster configurations fik given the current weights\nWk and solving for the best weights given the current clusters (Shepard & Arabie,\n1979; Arabie & Carroll, 1980). This strategy is appealing because given the cluster configuration, finding the optimal weights becomes a simple linear least-squares\nproblem.2 However, finding good cluster configurations is a difficult problem in combinatorial optimization, and this step has always been the weak point in previous\nwork . The original ADCLUS (Shepard & Arabie, 1979) and later MAPCLUS (Arabie & Carroll, 1980) algorithms employ ad hoc techniques of combinatorial optimization that sometimes yield unexpected or uninterpretable final results. Certainly, no\nrigorous theory exists that would explain why these approaches fail to discover the\nunderlying structure of a stimulus set when they do.\nEssentially, the ADCL US model is so challenging to fit because it generates similarities from the interaction of many independent underlying causes . Viewed this way,\nmodeling the structure of similarity looks very similar to the multiple-cause learning\nIn the remainder of this paper, we absorb c into the sum over k, taking the sum over\n== c, and fixing !iO = 1, (Vi) .\n2Strictly speaking, because the weights are typically constrained to be nonnegative, more\nelaborate techniques than standard linear least-squares procedures may be required.\n1\n\nk\n\n= 0, ... , K , defining Wo\n\n\x0c5\n\nLearning the Structure of Similarity\n\nproblems that are currently a major focus of study in the neural computation literature (Ghahramani, 1995; Hinton, Dayan, et al., 1995; Saund, 1995; Neal, 1992). Here\nwe propose a novel approach to additive clustering, inspired by the progress and\npromise of work on multiple-cause learning within the Expectation-Maximization\n(EM) framework (Ghahramani, 1995; Neal, 1992). Our BM approach still makes\nuse of the basic insight behind earlier approaches, that finding {wd given {lid is\neasy, but obtains better performance from treating the unknown cluster memberships\nprobabilistically as hidden variables (rather than parameters of the model), and perhaps more importantly, provides a rigorous and well-understood theory. Indeed, it\nis natural to consider {/ik} as "unobserved" features of the stimuli, complementing the observed data {Sij} in the similarity matrix. Moreover, in some experimental\nparadigms, one or more of these features may be considered observed data, if subjects\nreport using (or are requested to use) certain criteria in their similarity judgments.\n\n2\n\nALGORITHM\n\n2.1\n\nMaximum likelihood formulation\n\nWe begin by formulating the additive clustering problem in terms of maximum likelihood estimation with unobserved data. Treating the cluster weights w\n{Wk}\nas model parameters and the unobserved cluster memberships I = {lik} as hidden\ncauses for the observed similarities S {Sij}, it is natural to consider a hierarchical\ngenerative model for the "complete data" (including observed and unobserved components) of the form p(s, Ilw) = p(sl/, w)p(flw). In the spirit of earlier approaches\nto ADCLUS that seek to minimize a squared-error energy function, we take p(sl/, w)\nto be gaussian with common variance u 2 :\n\n=\n\n=\n\np(sl/, w) ex: exp{ -~ \'L:(Sij - Sij )2} = exp{ -~ \'L:(Sij 2u itj\n2u itj\n\n\'L: wklik/ik)2}.\n\n(3)\n\nk\n\nNote that logp(sl/, w) is equivalent to -E/(2u 2 ) (ignoring an additive constant),\nwhere E is the energy defined above. In general, priors p(flw) over the cluster\nconfigurations may be useful to favor larger or smaller clusters, induce a dependence\nbetween cluster size and cluster weight, or bias particular kinds of class structures,\nbut only uniform priors are considered here. In this case -E /(2u 2 ) also gives the\n"complete data" loglikelihood logp(s, Ilw).\n\n2.2\n\nThe EM algorithm for additive clustering\n\nGiven this probabilistic model, we can now appeal to the EM algorithm as the basis\nfor a new additive clustering technique. EM calls for iterating the following twostep procedure, in order to obtain successive estimates of the parameters w that are\nguaranteed never to decrease in likelihood (Dempster et al., 1977). In the E-step, we\ncalculate\n\nQ(wlw(n)) =\n\nL,: p(f\' Is, wen)) logp(s,f/lw) =\nl\'\n\n2 \\ (-E}3,w(n).\n\n(4)\n\nu\n\nQ(wlw(n) is equivalent to the expected value of E as a function of w, averaged over\n\nall possible configurations I\' of the N K binary cluster memberships, given the observed data s and the current parameter estimates wen). In the M-step, we maximize\nQ(wlw(n) with respect to w to obtain w(n+l).\nEach cluster configuration I\' contributes to the mean energy in proportion to its\nprobability under the gaussian generative model in (3). Thus the number of configurations making significant contributions depends on the model variance u 2 . For large\n\n\x0c6\n\nJ. B. TENENBAUM\n\nthe probability is spread over many configurations. In the limiting case u 2 ---+ 0,\nonly the most likely configuration contributes, making EM effectively equivalent to\nthe original approaches presented in Section 1 that use only the single best cluster\nconfiguration to solve for the best cluster weights at each iteration.\nU2 ,\n\nIn line with the basic insight embodied less rigorously in these earlier algorithms, the\nM-step still reduces to a simple (constrained) linear least-squares problem, because\nthe mean energy (E} = L:i#j (srj - 2Sij L:k Wk(fik!ik} + L:kl WkWl(fik!jk!il!il}) ,\nlike the energy E, is quadratic in the weights Wk. The E-step, which amounts to\ncomputing the expectations mijk = (fik!ik} and mijkl = (fik !ik!il/j I} , is much\nmore involved , because the required sums over all possible cluster configurations f\'\nare intractable for any realistic case. We approximate these calculations using Gibbs\nsampling, a Monte Carlo method that has been successfully applied to learning similar\ngenerative models with hidden variables (Ghahramani, 1995; Neal 1992).3\nFinally, the algorithm should produce not only estimates of the cluster weights, but\nalso a final cluster configuration that may be interpreted as the psychologically natural\nfeatures or classes of the relevant domain. Consider the expected cluster memberships\nPik = (fik}$ w(n) , which give the probability that stimulus i belongs to cluster k, given\nthe observed similarity matrix and the current estimates of the weights. Only when\nall Pik are close to 0 or 1, i.e. when u 2 is small enough that all the probability becomes\nconcentrated on the most likely cluster configuration and its neighbors, can we fairly\nassert which stimuli belong to which classes.\n2.3\n\nSimulated annealing\n\nTwo major computational bottlenecks hamper the efficiency of the algorithm as described so far. First, Gibbs sampling may take a very long time to converge to the\nequilibrium distribution, particularly when u 2 is small relative to the typical energy\ndifference between neighboring cluster configurations. Second, the likelihood surfaces\nfor realistic data sets are typically riddled with local maxima. We solve both problems\nby annealing on the variance. That is, we run Gibbs sampling using an effective variance\ninitially much greater than the assumed model variance 2 , and decrease\ntowards u 2 according to the following two-level scheme. We anneal within the\nnth iteration of EM to speed the convergence of the Gibbs sampling E-step (Neal,\n1993) , by lowering u;jJ from some high starting value down to a target U~arg(n) for\nthe nth EM iteration . We also anneal between iterations of EM to avoid local maxima\n(Ros~ et al., 1990), by intializing U~arg(o) at a high value and taking U~arg(n) ---+ u 2\nas n Increases.\n\nu;"\n\n3\n\nu;"\n\nu\n\nRESULTS\n\nIn all of the examples below, one run of the algorithm consisted of 100-200 iterations\nof EM, annealed both within and between iterations. Within each E-step, 10-100\ncycles of Gibbs sampling were carried out at the target temperature UTarg while the\nstatistics for mik and mijk were recorded. These recorded cycles were preceeded\nby 20-200 unrecorded cycles, during which the system was annealed from a higher\ntemperature (e.g. 8u~arg) down to U~arg, to ensure that statistics were collected as\nclose to equilibrium as possible. The precise numbers of recorded and unrecorded\niterations were chosen as a compromise between the need for longer samples as the\n3We generally also approximate\nsults with much greater efficiency.\n\nmiJkl\n\n~\n\nmiJkmi;"l,\n\nwhich usually yields satisfactory re-\n\n\x0c7\n\nLearning the Structure of Similarity\n\nTable 1: Classes and weights recovered for the integers 0-9.\nRank\n1\n2\n3\n4\n5\n6\n7\n8\n\nWeight\n.444\n.345\n.331\n.291\n.255\n.216\n.214\n.172\n\nVariance accounted for\n\nStimuli in class\n2\n4\n\n8\n\n012\n\n3\n\n9\n6\n6 789\n2 345 6\n1\n3\n5\n7\n9\n1 2 3 4\n4 5 6 7 8\n\n= 90.9% with\n\nInterpretation\npowers of two\nsmall numbers\nmultiples of three\nlarge numbers\nmiddle numbers\nodd numbers\nsmallish numbers\nlargish numbers\n\n8 clusters (additive constant\n\n= .148).\n\nnumber of hidden variables is increased and the need to keep computation times\npractical.\n3.1\n\nArtificial data\n\nWe first report results with artificial data, for which the true cluster memberships and\nweights are known, to verify that the algorithm does in fact find the desired structure.\nWe generated 10 data sets by randomly assigning each of 12 stimuli independently\nand with probability 1/2 to each of 8 classes, and choosing random weights for the\nclasses uniformly from [0.1,0.6]. These numbers are grossly typical of the real data\nsets we examine later in this section. We then calculated the observed similarities\nfrom (1), added a small amount of random noise (with standard deviation equal to\n5% of the mean noise-free similarity), and symmeterized the similarity matrix.\nThe crucial free parameter is K, the assumed number of stimulus classes. When the\nalgorithm was configured with the correct number of clusters (K = 8), the original\nclasses and weights were recovered during the first run of the algorithm on all 10 data\nsets, after an average of 58 EM iterations (low 30, high 92). When the algorithm\nwas configured with K = 7 clusters, one less than the correct number, the seven\nclasses with highest weight were recovered on 9/10 first runs. On these runs, the\nrecovered weights and true weights had a mean correlation of 0.948 (p < .05 on each\nrun). When configured with K = 5, the first run recovered either four of the top\nfive classes (6/10 trials) or three of the top five (4/10 trials). When configured with\ntoo many clusters (K = 12), the algorithm typically recovered only 8 clusters with\nsignificantly non-zero weights, corresponding to the 8 correct classes. Comparable\nresults are not available for ADCLUS or MAPCLUS, but at least we can be satisfied\nthat our algorithm achieves a basic level of competence and robustness.\n3.2\n\nJudged similarities of the integers 0-9\n\nShepard et al. (1975) had subjects judge the similarities of the integers 0 through\n9, in terms of the "abstract concepts" of the numbers. We analyzed the similarity\nmatrix (Shepard, personal communication) obtained by pooling data across subjects\nand across three conditions of stimulus presentation (verbal, written-numeral, and\nwritten-dots). We chose this data set because it illustrates the power of additive\nclustering to capture a complex, overlapping system of classes, and also because\nit serves to compare the performance of our algorithm with the original ADCL US\nalgorithm. Observe first that two kinds of classes emerge in the solution. Classes\n1, 3, and 6 represent familiar arithmetic concepts (e.g. "multiples of three", "odd\nnumbers"), while the remaining classes correspond to subsets of consecutive integers\n\n\x0c8\n\n1. B. TENENBAUM\n\nTable 2: Classes and weights recovered for the 16 consonant phonemes.\nRank\n1\n2\n3\n4\n5\n6\n7\n8\n\nWeight\n.800\n.572\n.463\n.424\n.357\n.292\n.169\n.132\n\nStimuli in class\nf 0\ndg\n\np k\nb\n\nv {t\n\npt k\nmn\ndgvCTz2\n\nptkfOs\n\nInterpretation\nfront unvoiced fricatives\nback voiced stops\nunvoiced stops (omitting t)\nfront voiced\nunvoiced stops\nnasals\nvoiced (omitting b)\nunvoiced (omittings)\n\nVariance accounted for = 90.2% with 8 clusters (additive constant = .047).\n\nand thus together represent the dimension of numerical magnitude. In general, both\narithmetic properties and numerical magnitude contribute to judged similarity, as\nevery number has features of both types (e.g. 9 is a "large" "odd" "multiple of three"),\nexcept for 0, whose only property is "small." Clearly an overlapping clustering model\nis necessary here to accomodate the multiple causes of similarity.\nThe best solution reported for these data using the original ADCLUS algorithm\nconsisted of 10 classes, accounting for 83.1% of the variance of the data (Shepard &\nArabie, 1979).4 Several of the clusters in this solution differed by only one or two\nmembers (e.g. three of the clusters were {0,1}, {0,1,2}, and {0,1,2,3,4}), which led\nus to suspect that a better fit might be obtained with fewer than 10 classes. Table 2\nshows the best solution found in five runs of our algorithm, accounting for 90.9% of\nthe variance with eight classes. Compared with our solution, the original ADCLUS\nsolution leaves almost twice as much residual variance unaccounted for, and with 10\nclasses, is also less parsimonious.\n\n3.3\n\nConfusions between 16 consonant phonemes\n\nFinally, we examine Miller & Nicely\'s (1955) classic data on the confusability of 16\nconsonant phonemes, collected under varying signal/noise conditions with the original intent of identifying the features of English phonology (compiled and reprinted\nin Carroll & Wish, 1974). Note that the recovered classes have reasonably natural\ninterpretations in terms of the basic features of phonological theory, and a very different overall structure from the classes recovered in the previous example. Quite\nsignificantly, the classes respect a hierarchical structure almost perfectly, with class\n3 included in class 5, classes 1 and 5 included in class 8, and so on. Only the absence\nof /b / in class 7 violates the strict hierarchy.\nThese data also provide the only convenient oppportunity to compare our algorithm\nwith the MAPCLUS approach to additive clustering (Arabie & Carroll, 1980). The\npublished MAPCLUS solution accounts for 88.3% of the variance in this data, using\neight clusters. Arabie & Carroll (1980) report being "substantively pe...turbed" (p.\n232) that their algorithm does not recover a distinct cluster for the nasals /m n/,\nwhich have been considered a very salient subset in both traditional phonology (Miller\n& Nicely, 1955) and other clustering models (Shepard, 1980). Table 3 presents our\neight-cluster solution, accounting for 90.2% of the variance. While this represents\nonly a marginal improvement, our solution does contain a cluster for the nasals, as\nexpected on theoretical grounds.\n4Variance accounted for = 1- Ej Ei#j(SiJ - 8)2, where\n\ns is\n\nthe mea.n of the set {Sij}.\n\n\x0cLearning the Structure of Similarity\n\n3.4\n\n9\n\nConclusion\n\nThese examples show that ADCLUS can discover meaningful representations of stimuli with arbitrarily overlapping class structures (arithmetic properties), as well as dimensional structure (numerical magnitude) or hierarchical structure (phoneme families) when appropriate. We have argued that modeling similarity should be a natural\napplication of learning generative models with multiple hidden causes, and in that\nspirit, presented a new probabilistic formulation of the ADCLUS model and an algorithm based on EM that promises better results than previous approaches. We\nare currently pursuing several extensions: enriching the generative model, e.g. by\nincorporating significant prior structure, and improving the fitting process, e.g. by\ndeveloping efficient and accurate mean field approximations . More generally, we hope\nthis work illustrates how sophisticated techniques of computational learning can be\nbrought to bear on foundational problems of structure discovery in cognitive science.\nAcknowledgements\nI thank P. Dayan, W. Richards, S. Gilbert, Y. Weiss, A. Hershowitz, and M. Bernstein\nfor many helpful discussions, and Roger Shepard for generously supplying inspiration and\nunpublished data. The author is a Howard Hughes Medical Institute Predoctoral Fellow.\n\nReferences\nArabie, P. & Carroll, J. D. (1980). MAPCLUS: A mathematical programming approach to\nfitting the ADCLUS model. Psychometrika 45, 211-235.\nCarroll, J. D. & Wish, M. (1974) Multidimensional perceptual models and measurement\nmethods. In Handbook of Perception, Vol. 2. New York: Academic Press, 391-447.\nDempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood estimation from\nincomplete data via the EM Algorithm (with discussion). J. Roy. Stat. Soc. B39, 1-38.\nGhahramani, Z. (1995). Factorial learning and the EM algorithm. In G. Tesauro, D. S.\nTouretzky, & T . K. Leen (eds.), Advances in Neural Information Processing Systems 7.\nCambridge, MA: MIT Press, 617-624.\nHinton, G. E., Dayan, P., Frey, B. J., & Neal, R. M. (1995) The ((wake-sleep" algorithm for\nunsupervised neural networks. Science 268, 1158-1161.\nMiller, G. A. & Nicely, P. E. (1955). An analysis of perceptual confusions among some\nEnglish consonants. J. Ac. Soc. Am. 27, 338-352.\nNeal, R . M. (1992). Connectionist learning of belief networks. Arti/. Intell. 56, 71-113.\nNeal, R. M. (1993). Probabilistic inference using Markov chain Monte Carlo methods.\nTechnical Report CRG-TR-93-1, Dept. of Computer Science, U. of Toronto.\nRose, K., Gurewitz, F., & Fox, G. (1990). Statistical mechanics and phase transitions in\nclustering. Physical Review Letters 65, 945-948.\nSaund, E. (1995). A multiple cause mixture model for unsupervised learning. Neural Computation 7, 51-71.\nShepard, R. N. & Arabie, P. (1979). Additive clustering: Representation of similarities as\ncombinations of discrete overlapping properties. Psychological Review 86, 87-123.\nShepard, R. N., Kilpatric, D. W., & Cunningham, J. P., (1975). The internal representation\nof numbers. Cognitive Psychology 7, 82-138.\nShepard, R. N. (1980) . Multidimensional scaling, tree-fitting, and clustering. Science 210,\n390-398.\nTversky, A. (1977). Features of similarity. Psychological Review 84, 327-352.\n\n\x0c'
p83206
sg176
S'Reorganisation of Somatosensory Cortex after\nTactile Training\n\nRasmus S. Petersen\nJohn G. Taylor\nCentre for Neural Networks, King\'s College London\nStrand, London WC2R 2LS, UK\n\nAbstract\nTopographic maps in primary areas of mammalian cerebral cortex reorganise as a result of behavioural training. The nature of this reorganisation seems consistent with the behaviour of competitive neural networks, as has been demonstrated in the past by computer simulation.\nWe model tactile training on the hand representation in primate somatosensory cortex, using the Neural Field Theory of Amari and his colleagues. Expressions for changes in both receptive field size and magnification factor are derived, which are consistent with owl monkey experiments and make a prediction which goes beyond them.\n\n1. INTRODUCTION\nThe primary cortical areas of mammals are now known to be plastic throughout life; reviewed recently by Kaas(1995). The problem of how and why the underlying learning\nprocesses work is an exciting one, for which neural network modelling appears well\nsuited. In this contribution, we model the long-term effects of tactile training (Jenkins et\nai, 1990) on the functional organisation of monkey primary somatosensory cortex, by\nperturbing a topographic net (Takeuchi and Amari, 1979).\n\n1.1 ADAPTATION IN ADULT SOMATOSENSORY CORTEX\nLight touch activates skin receptors which in primates are mapped, largely topographically, in area 3b. In a series of papers, Merzenich and colleagues describe how area 3b\nbecomes reorganised following peripheral nerve damage (Merzenich et ai, 1983a; 1983b)\nor digit amputation (Merzenich et ai, 1984). The underlying learning processes may also\nexplain the phenomenon of phantom limb "telescoping" (Haber, 1955). Recent advances\nin brain scanning are beginning to make them observable even in the human brain\n(Mogilner et ai, 1993).\n\n1.2 ADAPTATION ASSOCIATED WITH TACTILE TRAINING\nJenkins et al trained owl monkeys to maintain contact with a rotating disk. The apparatus\nwas arranged so that success eventually involved touching the disk with only the digit\ntips. Hence these regions received selective stimulation. Some time after training had\nbeen completed electro-physiological recordings were made from area 3b. These revealed an increase in Magnification Factor (MF) for the stimulated skin and a decrease in\n\n\x0c83\n\nReorganization of Somatosensory Cortex after Tactile Training\n\nthe size of Receptive Fields (RFs) for that region. The net territory gained for light touch\nof the digit tips came from area 3a and/or the face region of area 3b, but details of any\nchanges in these representations were not reported.\n\n2. THEORETICAL FRAMEWORK\n2.1 PREVIOUS WORK\nTakeuchi and Amari(1979), Ritter and Schulten(1986), Pearson et al(1987) and Grajski\nand Merzenich( 1990) have all modelled amputationldenervation by computer simulation\nof competitive neural networks with various Hebbian weight dynamics. Grajski and\nMerzenich(1990) also modelled the data of Jenkins et al. We build on this research\nwithin the Neural Field Theory framework (Amari, 1977; Takeuchi and Amari, 1979;\nAmari, 1980) of the Neural Activity Model of Willshaw and von der Malsburg(1976).\n\n2.2 NEURAL ACTIVITY MODEL\nConsider a "cortical" network of simple, laterally connected neurons. Neurons sum inputs linearly and output a sigmoidal function of this sum. The lateral connections are\nexcitatory at short distances and inhibitory at longer ones. Such a network is competitive: the steady state consists of blobs of activity centred around those neurons locally receiving the greatest afferent input (Amari, 1977). The range of the competition is limited\nby the range of the lateral inhibition.\nSuppose now that the afferent synapses adapt in a Hebbian manner to stimuli that are localised in the sensory array; the lateral ones are fixed. Willshaw and von der Malsburg(1976) showed by computer simulation that this network is able to form a topographic map of the sensory array. Takeuchi and Amari( 1979) amended the WillshawMalsburg model slightly: neurons possess an adaptive firing threshold in order to prevent\nsynaptic weight explosion, rather than the more usual mechanism of weight normalisation. They proved that a topographic mapping is stable under certain conditions.\n\n2.3 TAKEUCHI-AMARI THEORY\nConsider a one-dimensional model. The membrane dynamics are:\n\nau(~y,t) = -u(x,y,t)+ f s(x,y\' ,t)a(y- y\')dy\'-\n\n(1)\n\nf\n\nso(x,t)ao + w(x-x\')f[u(x\' ,y,t)]dx\'-h\nHere u(x,y,t) is the membrane potential at time I for point x when a stimulus centred at y is\nbeing presented; h is a positive resting potential; w(z) is the lateral inhibitory weight between two points in the neural field separated by a distance z - positive for small Izl and\nnegative for larger Izl; s(x,y,t) is the excitatory synaptic weight from y to x at time I and\nsr/X,I) is an inhibitory weight from a tonically active inhibitory input aD to x at time t - it is\nthe adaptive firing threshold . f[u] is a binary threshold function that maps positive membrane potentials to 1 and non-positive ones to O.\nIdealised, point-like stimuli are assumed, which "spread out" somewhat on the sensory\nsurface or subcortically. The spreading process is assumed to be independent of y and is\ndescribed in the same coordinates. It is represented by the function a(y-y\'), which describes the effect of a point input at y spreading to the point y\'. This is a decreasing, positive, symmetric function of Iy-y\'l. With this type of input, the steady-state activity of the\nnetwork is a single blob, localised around the neuron with maximum afferent input.\n\n\x0cR. S. PETERSEN, J. O. TAYLOR\n\n84\n\nThe afferent synaptic weights adapt in a leaky Hebbian manner but with a time constant\nmuch larger than that of the membrane dynamics (1). Effectively this means that learning\noccurs on the steady state of the membrane dynamics. The following averaged weight\ndynamics can be justified (Takeuchi and Amari, 1979; Geman 1979):\n\nJ) (\n\nas( x,aty, t) =-s(x,y,t)+b p(y\' a Y-Y\')f [Au(x,y\' )]dy\'\n(2)\n\naso(~y,t) =-so(x,y,t)+b\' aoJ p(y\')f[u(x,y\')]dy\'\n\nwhere r1(x,y\') is the steady-state of the membrane dynamics at x given a stimulus at y\' and\np(y\') is the probability of a stimulus at y \'; b, b\' are constants.\nEmpirically, the "classical" Receptive Field (RF) of a neuron is defined as the region of\nthe input field within which localised stimulation causes change in its activity. This concept can be modelled in neural field theory as: the RF of a neuron at x is the portion of the\ninput field within which a stimulus evokes a positive membrane potential (inhibitory RFs\nare not considered). If the neural field is a continuous map of the sensory surface then the\nRF of a neuron is fully described by its two borders rdx), rix), defined formally:\ni\n\n= 1,2\n\n(3)\n\nwhich are illustrated in figure 1.\nLet RF size and RF position be denoted respectively by the functions rex) and m(x), which\nrepresent experimentally measurable quantities. In terms of the border functions they can\nbe expressed:\n\nr(x) = r2 (x) - r1 (x)\n\n(4)\n\nm(x) =-} (rl {x} + r2 (x))\ny\n\n~---------------------------\n\nx\n\nFigure 1:\nRF\nboundaries as a\nfunction of position\nin the neural field,\nfor a topographically ordered network. Only the region\nin-between\nrdx) and rix) has\npositive\nsteadystate\nmembrane\npotential\nr1(x,y).\nrdx) and rix) are\ndefined\nby\nthe\ncondition\nr1(x,r;(x))=O\nfor\ni=J,2.\n\nUsing (1), (2) and the definition (3), Takeuchi and Amari(1979) derived dynamical equations for the change in RF borders due to learning. In the case of uniform stimulus probability, they found solutions for the steady-state RF border functions. With periodic\nboundary conditions, the basic solution is a linear map with constant RF size:\n\n\x0c85\n\nReorganization of Somatosensory Cortex after Tactile Training\n\nr(x) = ro = const\nm(x) = px ++ro\n\n= px\nr~tni (x) = px+ ro\n\nr l uni ( x )\n\n(5)\n\nThis means that both RF size and activity blob size are uniform across the network and\nthat RF position m(x) is a linear function of network location. (The value of p is determined by boundary conditions; ro is then determined from the joint equilibrium of (I),\n(2?. The inverse of the RF position function, denoted by m-l(y), is the centre of the cortical active region caused by a stimulus centred at y. The change in m-l(y) over a unit interval in the input field is, by empirical definition, the cortical magnification factor (MF).\nHere we model MF as the rate of change of m-l(y). The MF for the system described by\n(5) is:\nd\n_I ( )\n-m\ny =p -I\n\n(6)\n\ndy\n\n3. ANALYSIS OF TACTILE TRAINING\n3.1 TRAINING MODEL AND ASSUMPTIONS\nJenkins et aI\'s training sessions caused an increase in the relative frequency of stimulation\nto the finger tips, and hence a decrease in relative frequency of stimulation elsewhere.\nOver a long time, we can express this fact as a localised change in stimulus probability\n(figure 2). (This is not sufficient to cause cortical reorganisation - Recanzone et al( 1992)\nshowed that attention to the stimulation is vital. We consider only attended stimulation in\nthis model). To account for such data it is clearly necessary to analyse non-uniform\nstimulus probabilities, which demands extending the results of Takeuchi and Amari. Unfortunately, it seems to be hard to obtain general results. However, a perturbation analysis around the uniform probability solution (5) is possible.\nTo proceed in this way, we must be able to assume that the change in the stimulus probability density function away from uniformity is small. This reasoning is expressed by the\nfollowing equation:\n\np(y) = Po + E p(y)\n\n(7)\n\nwhere pry) is the new stimulus probability in terms of the uniform one and a perturbation\ndue to training: E is a small constant. The effect of the perturbation is to ease the weight\ndynamics (2) away from the solution (5) to a new steady-state. Our goal is to discover the\neffect of this on the RF border functions, and hence for RF size and MF.\n\np(y)\n\nFigure 2: The type\nof\nchange\nin\nstimulus probability density that we\nassume to model\nthe effects of behavioural training.\n\no\n\ny\n\n\x0c86\n\nR. S. PETERSEN, J. G. TAYLOR\n\n3.2 PERTURBATION ANALYSIS\n3.2.1 General Case\nFor a small enough perturbation, the effect on the RF borders and on the activity blob size\nought also to be small. We consider effects to first order in E, seeking new solutions of\nthe form:\ni\n\n= 1,2\n\n,{x} = r; {x} - ~ {x}\nm{x} = +(~ (X}+\'2 (x})\n\n(8)\n\nwhere the superscript peT denotes the new, perturbed equilibrium and uni denotes the unperturbed, uniform probability equilibrium. Using (1) and (2) in (3) for the post-training\nRF borders, expanding to first order in E, a pair of difference equations may be obtained\nfor the changes in RF borders. It is convenient to define the following terms:\n\nJ\n\nrt \'(x)\n\no\n\nr,"no (x)\n\nro\n\nAt (x) = p(y+ px)k(y)dy-b\' a~\no\n\nJp(y)dy\nr;-n\' (x )\n\nJp(y + px + TO )k(y)dy - b\' a~ Jp(y)dy\nk(y) = bJa(y - y\' )a(y\' )dy\'\n\nA2 {x} =\n\n(9)\n\nB = b\' a~p() -k(ro)po > 0\nC=\n\nw(p-tTo)p-t <0\n\nwhere the signs of Band C arise due to stability conditions (Amari, 1977; Takeuchi and\nAmari, 1979). In terms of RF size and RF position (4), the general result is:\n\n= ~(~ + I)At (x) - M2 (x)\nBC~2m{X) = (B- C -+ C~)(~+ I}A t (x) + (C- B++(C -2B)~)A2 (x)\nB~2 ,(X}\n\n(10)\n\nwhere ~ is the difference operator:\n~ f{ x) = f( x + p - t To) - f( x)\n\n(11 )\n\n3.2.2 Particular Case\nThe second order difference equations (l0) are rather opaque. This is partly due to coupling in y caused by the auto-correlation function key): (10) simplifies considerably if very\nnarrow stimuli are assumed - a(y)=O(y) (see also Amari, 1980). For periodic boundary\nconditions:\n\n(12)\n\nwhere:\n\n\x0cReorganization of Somatosensory Cortex after Tactile Training\n\nm -I P(W (y)\n\n87\n\n= m -I pre (y) + Em -I (y)\n=p-l(y_+ro)+Em-l(y)\n\n(13)\n\nand we have used the crude approximation:\n\nt;:\n\nd _() 1\n(\ndx m x "" ~m x -\n\n1\n\n2" P\n\n_I\n\nro\n\n)\n\n(14)\n\nwhich demands smoothness on the scale of 10 . However, for perturbations like that\nsketched in figure 2, this is sufficient to tell us about the constant regions of MF. (We\nwould not expect to be able to model the data in the transition region in any case, as its\nform is too dependent upon fine detail of the model).\nOur results (12) show that the change in RF size of a neuron is simply minus the total\nchange in stimulus probability over its RF. Hence RF size decreases where p(y) increases\nand vice versa. Conversely, the change in MF at a given stimulus location is roughly the\nlocal average change in stimulus probability there. Note that changes in RF size correlate\ninversely with changes in MF. Figure 3 is a sketch of these results for the perturbation of\nfigure 2.\nMF\n\nRF\n\no\n\no\n\ny\n\nI\n\n\\\n\nI\n\nL.J\n\nFigure 3: Results of perturbation analysis for how behavioural training (figure 2) changes\nRF size and MF respectively, in the case where stimulus width can be neglected. For MF\n- due to the approximation (14) - predictions do not apply near the transitions.\n\n4. DISCUSSION\nEquations (12) are the results of our model for RF size and MF after area 3b has fully\nadapted to the behavioural task, in the case where stimulus width can be neglected. They\nappear to be fully consistent with the data of Jenkins et al described above: RF size decreases in the region of cortex selective for the stimulated body part and the MF for this\nbody part increases. Our analysis also makes a specific prediction that goes beyond\nJenkins et aI\'s data, directly due to the inverse relationship between changes in RF size\nand those in MF. Within the regions that surrender territory to the entrained finger tips\n(sometimes the face region), for which MF decreases, RF sizes should increase.\nSurprisingly perhaps, these changes in RF size are not due to adaptation of the afferent\nweights s(x,y). The changes are rather due to the adaptive threshold term six). This\npoint will be discussed more fully elsewhere.\nA limitation of our analysis is the assumption that the change in stimulus probability is in\nsome sense small. Such an approximation may be reasonable for behavioural training but\nseems less so as regards important experimental protocols like amputation or denervation.\nEvidently a more general analysis would be highly desirable.\n\n\x0c88\n\nR. S. PETERSEN,J. O. TAYLOR\n\n5. CONCLUSION\nWe have analysed a system with three interacting features: lateral inhibitory interactions;\nHebbian adaptivity of afferent synapses and an adaptive firing threshold. Our results indicate that such a system can account for the data of Jenkins et aI, concerning the response of adult somatosensory cortex to the changing environmental demands imposed by\ntactile training. The analysis also brings out a prediction of the model, that may be testable.\n\nAcknowledgements\nRSP is very grateful for a travel stipend from the NIPS Foundation and for a Nick\nHughes bursary from the School of Physical Sciences and Engineering, King\'s College\nLondon, that enabled him to participate in the conference.\n\nReferences\nAmari S. (1977) BioI. Cybern. 2777-87\nAmari S. (1980) Bull. Math. Biology 42339-364\nGeman S. (1979) SIAM 1. App. Math. 36 86-105\nGrajski K.A., Merzenich M.M. (1990) in Neural Information Processing Systems 2\nTouretzky D.S. (Ed) 52-59\nHaberW.B. (1955)1. Psychol. 40115-123\nJenkins W.M ., Merzenich M.M., Ochs M.T., Allard T., Gufc-Robles E. (1990) 1. Neurophysiol. 63 82-104\nKaas J.H. (1995) in The Cognitive Neurosciences Gazzaniga M.S. (Ed ic) 51-71\nMerzenich M.M., Kaas J.H., Wall J.T., Nelson R.J., Sur M., Felleman DJ. (1983a) Neuroscience 8 35-55\nMerzenich M .M., Kaas J.H., Wall J.T., Sur M., Nelson R.I., Felleman DJ . (1983b) Neuroscience 10639-665\nMerzenich M.M., Nelson R.I., Stryker M.P., Cynader M.S., Schoppmann A., Zook J.M.\n(1984) 1. Compo Neural. 224591-605\nMogilner A., Grossman A.T., Ribrary V., Joliot M., Vol mann J., Rapaport D., Beasley R.,\nL1inas R. (1993) Proc. Natl. Acad. Sci. USA 90 3593-3597\nPearson J.e., Finkel L.H., Edelman G.M. (1987) 1. Neurosci. 124209-4223\nRecanzone G.H., Merzenich M.M., Jenkins W.M., Grajski K.A., Dinse H.R. (1992) 1.\nNeurophysiol. 67 1031-1056\nRitter H., Schulten K. (1986) BioI. Cybern. 5499-106\nTakeuchi A., Amari S. (1979) BioI. Cybern. 35 63-72\nWillshaw DJ., von der Malsburg e. (1976) Proc. R. Soc. Lond. B194 203-243\n\n\x0c'
p83207
sg145
S'Implementation Issues in the Fourier\nTransform Algorithm\n\nYishay Mansour" Sigal Sahar t\nComputer Science Dept.\nTel-Aviv University\nTel-Aviv, ISRAEL\n\nAbstract\nThe Fourier transform of boolean functions has come to play an\nimportant role in proving many important learnability results. We\naim to demonstrate that the Fourier transform techniques are also\na useful and practical algorithm in addition to being a powerful\ntheoretical tool. We describe the more prominent changes we have\nintroduced to the algorithm, ones that were crucial and without\nwhich the performance of the algorithm would severely deteriorate. One of the benefits we present is the confidence level for each\nprediction which measures the likelihood the prediction is correct.\n\n1\n\nINTRODUCTION\n\nOver the last few years the Fourier Transform (FT) representation of boolean functions has been an instrumental tool in the computational learning theory community. It has been used mainly to demonstrate the learnability of various classes of\nfunctions with respect to the uniform distribution . The first connection between the\nFourier representation and learnability of boolean functions was established in [6]\nwhere the class ACo was learned (using its FT representation) in O(nPoly-log(n))\ntime. The work of [5] developed a very powerful algorithmic procedure: given a\nfunction and a threshold parameter it finds in polynomial time all the Fourier coefficients of the function larger than the threshold. Originally the procedure was\nused to learn decision trees [5], and in [8, 2, 4] it was used to learn polynomial size\nDNF. The FT technique applies naturally to the uniform distribution, though some\nof the learnability results were extended to product distribution [1, 3] .\n.. e-mail: manSQur@cs.tau.ac.il\nt e-mail: gales@cs.tau .ac.il\n\n\x0cImplementation Issues in the Fourier Transform Algorithm\n\n261\n\nA great advantage of the FT algorithm is that it does not make any assumptions\non the function it is learning. We can apply it to any function and hope to obtain\n"large" Fourier coefficients. The prediction function simply computes the sum of\nthe coefficients with the corresponding basis functions and compares the sum to\nsome threshold. The procedure is also immune to some noise and will be able to\noperate even if a fraction of the examples are maliciously misclassified. Its drawback\nis that it requires to query the target function on randomly selected inputs.\nWe aim to demonstrate that the FT technique is not only a powerful theoretical\ntool, but also a practical one. In the process of implementing the Fourier algorithm\nwe enhanced it in order to improve the accuracy of the hypothesis we generate while\nmaintaining a desirable run time. We have added such feartures as the detection\nof inaccurate approximations "on the fly" and immediate correction of the errors\nincurred at a minimal cost. The methods we devised to choose the "right" parameters proved to be essential in order to achieve our goals. Furthermore, when making\npredictions, it is extremely beneficial to have the prediction algorithm supply an\nindicator that provides the confidence level we have in the prediction we made. Our\nalgorithm provides us naturally with such an indicator as detailed in Section 4.1.\nThe paper is organized as follows: section 2 briefly defines the FT and describes\nthe algorithm. In Section 3 we describe the experiments and their outcome and in\nSection 4 the enhancements made. We end with our conclusions in Section 5.\n\n2\n\nFOURIER TRANSFORM (FT) THEORY\n\nIn this section we briefly introduce the FT theory and algorithm. its connection to\nlearning and the algorithm that finds the large coefficients. A comprehensive survey\nof the theoretical results and proofs can be found in [7].\nWe consider boolean functions of n variables: f : {O, l}n - t {-I, I}. We define the\ninner product: < g, f >= 2- n L::XE{O,l}R f(x)g(x) = E[g . f], where E is the expected value with respect to the uniform distribution . The basis is defined as follows:\nfor each z E {O,l}n, we define the basis function :\\:z(Xl,???,X n ) = (_1)L::~=lx;z ?.\nAny function of n boolean inputs can be uniquely expressed as a linear combination\nof the basis functions . For a function f, the zth Fourier coefficient of f is denoted\nby j(z) , i.e. , f(x) = L::zE{O,l}R j(z)XAx) . The Fourier coefficients are computed\nby j(z) =< f, Xz > and we call z the coefficient-name of j(z). We define at-sparse\nfunction to be a function that has at most t non-zero Fourier coefficients.\n2.1\n\nPREDICTION\n\nOur aim is to approximate the target function f by a t-sparse function h. In many\ncases h will simply include the "large" coefficients of f. That is, if A = {Zl\' ... , zm}\nis the set of z\'s for which j(Zi) is "large", we set hex) = L::z;EA aiXz;(x), where\nat is our approximation of j(Zi). The hypothesis we generate using this process,\nhex), does not have a boolean output. In order to obtain a boolean prediction\nwe use Sign(h(x)), i.e., output +1 if hex) 2 0 and -1 if hex) < o. We want to\nbound the error we get from approximating f by h using the expected error squared,\nE[(J - h )2]. It can be shown that bounding it bounds the boolean prediction error\nprobability, i.e., Pr[f(x) f. sign(h(x))] ~ E[(J - h)2] . For a given t, the t-sparse\n\n\x0cY. MANSOUR, S. SAHAR\n\n262\n\nhypothesis h that minimizes E[(J - h)2] simply includes the t largest coefficients of\nf. Note that the more coefficients we include in our approximation and the better\nwe approximate their values, the smaller E[(J - h)2] is going to be. This provides\nus with the motivation to find the "large" coefficients.\n2.2\n\nFINDING THE LARGE COEFFICIENTS\n\nThe algorithm that finds the "large" coefficients receives as inputs a function 1 (a\nblack-box it can query) and an interest threshold parameter (J > 0. It outputs a list\nof coefficient-names that (1) includes all the coefficients-names whose corresponding coefficients are "large", i.e., at least (J , and (2) does not include "too many"\ncoefficient-names. The algorithm runs in polynomial time in both 1/() and n .\nSUBROUTINE search( a)\nIF TEST[J, a, II] THEN IF\n\nlal\n\n=n\n\nTHEN OUTPUT a\nELSE search(aO); search(al);\n\nFigure 1: Subroutine search\nThe basic idea of the algorithm is to perform a search in the space of the coefficientnames of I. Throughout the search algorithm (see Figure (1)) we maintain a prefix\nof a coefficient-name and try to estimate whether any of its extensions can be\na coefficient-name whose value is "large". The algorithm commences by calling\nsearch(A) where A is the empty string. On each invocation it computes the predicate TEST[/, a, (J]. If the predicate is true, it recursively calls search(aO) and\nsearch(al). Note that if TEST is very permissive we may reach all the coefficients, in which case our running time will not be polynomial; its implementation\nis therefore of utmost interest. Formally, T EST[J, a, (J] computes whether\n\nE xe {O,l}n-"E;e{O,lP.[J(YX)Xa(Y)] 2: (J2,\n\nwhere k = Iiali .\n\n(1)\n\nDefine la(x) = L:,ae{O,l}n-" j(aj3)x.,a(x). It can be shown that the expected value\nin (1) is exactly the sum of the squares of the coefficients whose prefix is a , i.e.,\nE xe {o,l}n-"E;e{o,l}d/(yx)x.a(Y)] = Ex[/~(x)] = L:,ae{o,l}n-" p(aj3), implying\nthat if there exists a coefficient Ii( a,8)1 2: (), then E[/;] 2: (J2 . This condition\nguarantees the correctness of our algorithm, namely that we reach all the "large"\ncoefficients. We would like also to bound the number of recursive calls that search\nperforms. We can show that for at most 1/(J2 of the prefixes of size k, TEST[!, a , (J]\nis true. This bounds the number of recursive calls in our procedure by O(n/(J2).\nIn TEST we would like to compute the expected value, but in order to do so\nefficiently we settle for an approximation of its value. This can be done as follows:\n(1) choose ml random Xi E {a, l}n-k, (2) choose m2 random Yi,j E {a, l}k , (3)\nquery 1 on Yi,jXi (which is why we need the query model-to query f on many\npoints with the same prefix Xi) and receive I(Yi,j xd, and (4) compute the estimate\nas, Ba =\n\n3\n\n\';1 L:~\\ (~~ L:~l I(Yi,iXdXa(Yi,j)f\n\n. Again , for more details see [7].\n\nEXPERIMENTS\n\nWe implemented the FT algorithm (Section 2.2) and went forth to run a series of\nexperiments. The parameters of each experiment include the target function , (J , ml\n\n\x0cImplementation Issues in the Fourier Transform Algorithm\n\n263\n\nand m2. We briefly introduce the parameters here and defer the detailed discussion.\nThe parameter () determines the threshold between "small" and "large" coefficients,\nthus controlling the number of coefficients we will output. The parameters wI and\nw2 determine how accurately we approximate the TEST predicate. Failure to approximate it accurately may yield faulty, even random, results (e.g., for a ludicrous\nchoice of m1 = 1 and m2 = 1) that may cause the algorithm to fail (as detailed in\nSection 4.3). An intelligent choice of m1 and m2 is therefore indispensable. This\nissue is discussed in greater detail in Sections 4.3 and 4.4.\n\nFigure 2:\n\nTypical frequency plots and typical errors . Errors occur in two cases: (1) the algorithm\npredicts a +1 response when the actual response is -1 (the lightly shaded area), and (2) the algorithm\npredicts a -1 response , while the true response is +1 (the darker shaded area) .\n\nFigures (3)-(5) present representative results of our experiments in the form of\ngraphs that evaluate the output hypothesis of the algorithm on randomly chosen\ntest points. The target function, I, returns a boolean response, ?1, while the FT\nhypothesis returns a real response. We therefore present, for each experiment, a\ngraph constituting of two curves: the frequency of the values of the hypothesis,\nh( x), when I( x) = +1, and the second curve for I( x) = -1. If the two curves\nintersect, their intersection represents the inherent error the algorithm makes.\n\nFigure 3: Decision trees of depth 5 and 3 with 41 variables . The 5-deep (3-deep) decision tree\nreturns -1 about 50% (62.5%) of the time . The results shown above are for values (J\n0.03, ml\n100\nand m2 = 5600 ?(J = 0.06, ml = 100 and m2 = 1300). Both graphs are disjoint, signifying 0% error.\n\n=\n\n4\n4.1\n\n=\n\nRESULTS AND ALGORITHM ENHANCEMENTS\nCONFIDENCE LEVELS\n\nOne of our most consistent and interesting empirical findings was the distribution\nof the error versus the value of the algorithm\'s hypothesis: its shape is always that\nof a bell shaped curve. Knowing the error distribution permits us to determine with\na high (often 100%) confidence level the result for most of the instances, yielding\nthe much sought after confidence level indicator. Though this simple logic thus far\nhas not been supported by any theoretical result, our experimental results provide\noverwhelming evidence that this is indeed the case.\nLet us demonstrate the strength of this technique: consider the results of the 16-term\nDNF portrayed in Figure (4) . If the algorithm\'s hypothesis outputs 0.3 (translated\n\n\x0c264\n\nY. MANSOUR, S. SAHAR\n\nFigure 4: 16 terlD DNF. This (randomly generated) DNF of 40 variables returns -1 about 61 % of\nthe time. The results shown above are for the values of 9\n0 .02 , m2\n12500 and ml\n100. The\nhypothesis uses 186 non-zero coefficients . A total of 9 .628% error was detected.\n\n=\n\n=\n\n=\n\ninto 1 in boolean terms by the Sign function), we know with an 83% confidence\nlevel that the prediction is correct. If the algorithm outputs -0.9 as its prediction,\nwe can virtually guarantee that the response is correct. Thus, although the total\nerror level is over 9% we can supply a confidence level for each prediction. This is\nan indispensable tool for practical usage of the hypothesis .\n\n4.2\n\nDETERMINING THE THRESHOLD\n\nOnce the list of large coefficients is built and we compute the hypothesis h( x), we\nstill need to determine the threshold, a, to which we compare hex) (i.e., predict +1\niff hex) > a). In the theoretical work it is assumed that a = 0, since a priori one\ncannot make a better guess . We observed that fixing a\'s value according to our\nhypothesis, improves the hypothesis. a is chosen to minimize the error with respect\nto a number of random examples.\n\nFigure 5:\n\n8 terlD DNF . This (randomly generated) DNF of 40 variables returns -1 about 43% of the\ntime. The results shown above are for the values of 9\n0 .03, m2\n5600 and ml\n100. The hypothesis\nconsists of 112 non-zero coefficients.\n\n=\n\n=\n\n=\n\nFor example, when trying to learn an 8-term DNF with the zero threshold we will\nreceive a total of 1.22% overall error as depicted in Figure (5). However, if we\nchoose the threshold to be 0.32, we will get a diminished error of 0.068%.\n\n4.3\n\nERROR DETECTION ON THE FLY - RETRY\n\nDuring our experimentations we have noticed that at times the estimate Ba for\nE[J~] may be inaccurate. A faulty approximation may result in the abortion of the\ntraversal of "interesting" subtreees, thus decreasing the hypothesis\' accuracy, or in\ntraversal of "uninteresting" subtrees, thereby needlessly increasing the algorithm\'s\nruntime. Since the properties of the FT guarantee that E[J~] = E[f~o] + E[J~d,\nwe expect Ba :::::: Bao + Bal . Whenever this is not true, we conclude that at least\none of our approximations is somewhat lacking. We can remedy the situation by\n\n\x0c265\n\nImplementation Issues in the Fourier Transform Algorithm\n\nrunning the search procedure again on the children, i.e., retry node a. This solution increases the probability of finding all the "large" coefficients. A brute force\nimplementation may cost us an inordinate amount of time since we may retraverse\nsubtrees that we have previously visited. However, since any discrepancies between\nthe parent and its children are discovered-and corrected-as soon as they appear,\nwe can circumvent any retraversal. Thus, we correct the errors without any superfluous additions to the run time.\n\n-J:\n,-\n\nFigure 6:\nand\n\n(J\n\ni\\"\no\n" .......\n\nMajority function of 41 variables. The result portrayed are for values m1 = 100 , m2 = 800\n\n=0 .08 . Note the majority-function characteristic distribution of the results 1 .\n\nWe demonstrate the usefulness of this approach with an example of learning the\nmajority function of 41 boolean variables . Without the retry mechanism, 8 (of a\ntotal of 42) large coefficients were missed, giving rise to 13.724% error represented by\nthe shaded area in Figure (6). With the retries all the correct coefficients were found,\nyielding perfect (flawless) results represented in the dotted curve in Figure (6).\n4.4\n\nDETERMINING THE PARAMETERS\n\nOne of our aims was to determine the values of the different parameters, m1, m2 and\n(}. Recall that in our algorithm we calculate B a , the approximation of Ex[f~(x)]\nwhere m1 is the number of times we sample x in order to make this approximation.\nWe sample Y randomly m2 times to approximate fa(Xi) = Ey[f(YXih:a(Y)), for each\nXi ? This approximation of fa(Xi) has a standard deviation of approximately\nAssume that the true value is 13i, i.e. f3i = fa(Xi), then we expect the contribution\nof the ith element to Ba to be (13i ? )n;? = 131 ?\n+ rr!~. The algorithm tests\nBa =\nL 131 ? (}2, therefore, to ensure a low error, based on the above argument,\nwe choose m2 = (J52 ?\n\nA.\n\nJ&;\n\nrr!1\n\nChoosing the right value for m2 is of great importance. We have noticed on more\nthan one occasion that increasing the value of m2 actually decreases the overall run\ntime. This is not obvious at first : seemingly, any increase in the number of times we\nloop in the algorithm only increases the run time. However, a more accurate value\nfor m2 means a more accurate approximation of the TEST predicate, and therefore\nless chance of redundant recursive calls (the run time is linear in the number of\nrecursive calls) . We can see this exemplified in Figure (7) where the number of\nrecursive calls increase drastically as m2 decreases. In order to present Figure (7) ,\n1The "peaked" distribution of the results is not coincidental. The FT of the majority function has 42 large\nequal coefficients, labeled cmaj\' one for each singleton (a vector of the form 0 .. 010 .. 0) and one for parity (the\nall-ones vector). The zeros of an input vector with z zeros we will contribute ?1(2z - 41). cmajl to the result\nand the parity will contribute ?cma ) (depending on whether z is odd or even), so that the total contribution is\nan even factor of c ma )\' Since c ma ) =\naround the peaks is due to the\n\nf~ct\n\n(~g);tcr\n\n- 0 .12, we have peaks around factors of 0.24 . The distribution\n\nwe only approximate each coefficient and get a value close to c ma )\'\n\n\x0cY. MANSOUR, S. SAHAR\n\n266\n\nwe learned the same 3 term DNF always using e = 0.05 and mr\nThe trials differ in the specific values chosen in each trial for m2.\n\n* m2\n\n100000.\n\nFigure 7: Deter01ining 012\' Note that the number of recursive calls grows dramatically as m2 \'s\nvalue decreases. For example, for m2\n400, the number of recursive calls is 14,433 compared with only\n1,329 recursive calls for m2\n500 .\n\n=\n\n=\n\nSPECIAL CASES: When k = 110\'11 is either very small or very large, the values we\nchoose for ml and m2 can be self-defeating: when k ,..... n we still loop ml (~ 2n - k )\ntimes, though often without gaining additional information. The same holds for very\nsmall values of k, and the corresponding m2 (~ 2k) values. We therefore add the\nfollowing feature: for small and large values of k we calculate exactly the expected\nvalue thereby decreasing the run time and increasing accuracy.\n\n5\n\nCONCLUSIONS\n\nIn this work we implemented the FT algorithm and showed it to be a useful practical\ntool as well as a powerful theoretical technique. We reviewed major enhancements\nthe algorithm underwent during the process. The algorithm successfully recovers\nfunctions in a reasonable amount of time. Furthermore, we have shown that the\nalgorithm naturally derives a confidence parameter. This parameter enables the user\nin many cases to conclude that the prediction received is accurate with extremely\nhigh probability, even if the overall error probability is not negligible.\nAcknowledgements\nThis research was supported in part by The Israel Science Foundation administered by The Israel\nAcademy of Science and Humanities and by a grant of the Israeli Ministry of Science and Technology.\n\nReferences\n[1) Mihir Bellare. A technique for upper bounding the spectral norm with applications to learning.\nAnnual Work&hop on Computational Learning Theory, pages 62-70, July 1992.\n\nIn 5 th\n\n(2) Avrim Blum, Merrick Furst, Jeffrey Jackson, Michael Kearns, Yishay Mansour, and Steven Rudich. Weakly\nlearning DNF and characterizing statistical query learning using fourier analysis. In The 26 th Annual AC M\nSympo&ium on Theory of Computing, pages 253 - 262, 1994 .\n(3) Merrick L . Furst , Jeffrey C. Jackson, and Sean W. Smith. Improved learning of AC O functions .\nAnnual Work&hop on Computational Learning Theory, pages 317-325, August 1991.\n\nIn 4th\n\n(4) J. Jackson . An efficient membership-query algorithm for learning DNF with respect to the uniform distribution. In Annual Sympo&ium on Switching and Automata Theory, pages 42 - 53, 1994.\n(5) E. Kushilevitz and Y . Mansour. Learning decision trees using the fourier spectrum. SIAM Journal on\nComputing 22(6): 1331-1348, 1993.\n(6) N. Linial, Y. Mansour, and N . Nisan. Constant depth circuits, fourier transform and learnability.\n\nJACM\n\n40(3):607-620, 1993.\n\n(7) Y. Mansour . Learning Boolean Functions via the Fourier Transform. Advance& in Neural Computation,\nedited by V.P. Roychodhury and K-Y. Siu and A. Orlitsky, Kluwer Academic Pub. 1994. Can be accessed\nvia Up :/ /ftp .math.tau.ac.iJ/pub/mansour/PAPERS/LEARNING/fourier-survey.ps.Z.\n(8) Yishay Mansour. An o(nlog log n) learning algorihm for DNF under the uniform distribution . J. of Computer\nand Sy&tem Science, 50(3):543-550, 1995.\n\n\x0c'
p83208
sg256
S'Adaptive Retina with Center-Surround\nReceptive Field\n\nShih-Chii Lin and Kwabena Boahen\nComputation and Neural Systems\n139-74 California Institute of Technology\nPasadena, CA 91125\nshih@pcmp.caltech.edu, buster@pcmp.caltech.edu\n\nAbstract\nBoth vertebrate and invertebrate retinas are highly efficient in extracting contrast independent of the background intensity over five\nor more decades. This efficiency has been rendered possible by\nthe adaptation of the DC operating point to the background intensity while maintaining high gain transient responses. The centersurround properties of the retina allows the system to extract information at the edges in the image. This silicon retina models the\nadaptation properties of the receptors and the antagonistic centersurround properties of the laminar cells of the invertebrate retina\nand the outer-plexiform layer of the vertebrate retina. We also illustrate the spatio-temporal responses of the silicon retina on moving\nbars. The chip has 59x64 pixels on a 6.9x6.8mm2 die and it is\nfabricated in 2 J-tm n-well technology.\n\n1\n\nIntroduction\n\nIt has been observed previously that the initial layers of the vertebrate and invertebrate retina systems perform very similar processing functions on the incoming\ninput signal[1]. The response versus log intensity curves of the receptors in invertebrate and vertebrate retinas look similar. The curves show that the receptors\nhave a larger gain for changes in illumination than to steady illumination, i.e, the\nreceptors adapt. This adaptation property allows the receptor to respond over a\nlarge input range without saturating.\nAnatomically, the eyes of invertebrates differ greatly from that of vertebrates. Ver-\n\n\x0cAdaptive Retina with Center-Surround Receptive Field\n\n679\n\ntebrates normally have two simple eyes while insects have compound eyes. Each\ncompound eye in the fly consists of 3000-4000 ommatidia and each ommatidium\nconsists of 8 photoreceptors. Six of these receptors (which are also called RI-R6)\nare in a single spectral class. The other two receptors, R7 and R8 provide channels\nfor wavelength discrimination and polarization.\nThe vertebrate eye is divided into the outer-plexiform layer and the inner-plexiform\nlayer. The outer-plexiform layer consists of the rods and cones, horizontal cells\nand bipolar cells. Invertebrate receptors depolarise in response to an increase in\nlight, in contrast to vertebrate receptors, which hyperpolarise to an increase in light\nintensity. Both vertebrate and invertebrate receptors show light adaptation over at\nleast five decades of background illumination. This adaptation property allows the\nretina to maintain a high transient gain to contrast over a wide range of background\nintensities.\nThe invertebrate receptors project to the next layer which is called the lamina layer.\nThis layer consists primarily of monopolar cells which show a similar response versus log intensity curve to that of vertebrate bipolar cells in the outer-plexiform\nlayer. Both cells respond with graded potentials to changes in illumination. These\ncells also show a high transient gain to changes in illumination while ignoring the\nbackground intensity and they possess center-surround receptive fields. In vertebrates, the cones which are excited by the incoming light, activate the horizontal\ncells which in tum inhibit the cones. The horizontal cells thus mediate the lateral\ninhibition which produces the center-surround properties. In insects, a possible\nprocess of this lateral inhibition is done by current flow from the photoreceptors\nthrough the epithelial glial cells surrounding an ommatidium or the modulation\nof the local field potential in the lamina to influence the transmembrane potential\nof the photoreceptor[2]. The center-surround receptive fields allow contrasts to be\naccentuated since the surround computes a local mean and subtracts that from the\ncenter signal.\nMahowald[3] previously described a silicon retina with adaptive photoreceptors and\nBoahen et al.[4] recently described a compact current-mode analog model of the\nouter-plexiform layer of the vertebrate retina and analysed the spatio-temporal\nprocessing properties of this retina[5]. A recent array of photoreceptors from\nDelbriick[6] uses an adaptive photoreceptor circuit that adapts its operating point\nto the background intensity so that the pixel shows a high transient gain over 5\ndecades of background illumination. However this retina does not have spatial\ncoupling between pixels.\nThe pixels in the silicon retina described here has a compact circuit that incorporates both spatial and temporal filtering with light adaptation over 5 decades\nof background intensity. The network exhibits center-surround behavior. Boahen\net al.[4] in their current-mode diffusor retina, draw an analogy between parts of\nthe diffusor circuit and the different cells in the outer-plexiform layer. While the\nsame analogy cannot be drawn from this silicon retina to the invertebrate retina\nsince the function of the cells are not completely understood, the output responses\nof the retina circuit are similar to the output responses of the photoreceptor and\nmonopolar cells in invertebrates.\nThe circuit details are described in Section 2 and the spatio-temporal processing\nperformed by the retina on stimulus moving at different speeds is shown in Section\n\n\x0cS.-C. LIU, K. BOAHEN\n\n680\n\n3.\n\n2\n\nCircuit\n-----VI\n\nVb\n\nVI\n\n1\n\np1\n\n1\n\nVI\n\nM4\nVI?I\n\nVh\n\nVI+I\n\nVh\n\n.1.\n\n.bel\n\n.1.\n\nVr\n\n---------\n\nMI\n\n(a)\n\nim.l\n\niia\n\nrrr\n\niI...I\n\nrrr\n\nrrr\n\n\'II\n\n\'II\n\n(b)\n\nFigure 1: (a) One-dimensional version of the retina. (b) Small-signal equivalent of\ncircuit in (a).\nA one-dimensional version of the retina is shown in Figure l(a). The retina consists\nof an adaptive photoreceptor circuit at each pixel coupled together with diffusors,\ncontrolled by voltages, Vg and Vh. The output of this network can either be obtained\nat the voltage output, V, or at the current output, 10 but the outputs have different\nproperties. Phototransduction is obtained by using a reverse-biased photodiode\nwhich produces current that is proportional to the incident light. The logarithmic\nproperties are obtained by operating the feedback transistor shown in Figure l(a)\nin the subthreshold region. The voltage change at the output photoreceptor, V r , is\nproportional to a small contrast since\n\nUT\n\nVr\n\nUTdI\n\nU\n\ni\n\nT\n= -d(logl)\n==K,\nK,\n1\nK, h\ng\n\nCO:rCd \'\n\nwhere UT is the thermal voltage, K, =\nCoz is the oxide capacitance and\nCd is the depletion capacitance of a transistor. The circuit works as follows: If\nthe photocurrent through the photodiode increases, Vr will be pulled low and the\noutput voltage at V, increases by VI = AVr where A is the amplifier gain of the\noutput stage. This output change in V, is coupled into Vel through a capacitor\n\n\x0cAdaptive Retina with Center-Surround Receptive Field\n\n681\n\ndivider ratio, Cl~2C2. The feedback transistor, M4, operates in the subthreshold\nregion and supplies the current necessary to offset the photocurrent. The increase\nin Vel (i.e. the gate voltage of M4) causes the current supplied by M3 to increase\nwhich pulls the node voltage, Vr , back to the voltage level needed by Ml to sink\nthe bias current from transistor, M2.\n\n3.5\n\n3.45\n\n...-=\n\n3.4\n\n??c\n?\na:?\n\n3.35\n\n0\n\n~\n\n-2\n\n0\n\nQ.\n\n3.3\n-1\n\n3.25\n0\n3.2\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nTime (Sec)\n\nFigure 2: This figure shows the output response of the receptor to a variation of\nabout 40% p-p in the intensity of a flickering LED light incident on the chip. The\nresponse shows that the high sensitivity of the receptor to the LED is maintained\nover 5 decades of differing background intensities. The numbers on the section of\nthe curve indicate the log intensity of the mean value. 0 log is the absolute intensity\nfrom the LED.\nThe adaptive element, M3, has an I-V curve which looks like a hyperbolic sine.\nThe small slope of the I-V curve in the middle means that for small changes of\nvoltages across M3, the element looks like an open-circuit. With large changes of\nvoltage across M3, the current through M3 becomes exponential and Vel is charged\nor discharged almost instantaneously.\nFigure 2 shows the output response of the photoreceptor to a square-wave variation\nof about 40% p-p in the intensity of a red LED (635 nm). The results show that\nthe circuit is able to discern the small contrast over five decades of background intensity while the steady-state voltage of the photoreceptor output varies only about\n15mV. Further details of the photoreceptor circuit and its adaptation properties\nare described in Delbriick[6].\n\n3\n\nSpatio-Temporal Response\n\nThe spatio-temporal response of the network to different moving stimuli is explored\nin this section. The circuit shown in Figure l(a) can be transferred to an equivalent\nnetwork of resistors and capacitors as shown in Figure l(b) to obtain the transfer\nfunction of the circuit. The capacitors at each node are necessary to model the\n\n\x0cS.-C. LIU, K. BOAHEN\n\n682\n\n8.5\n\ni\n\n1lJ;\n\n~ 7.5\n\n:;\n:;\n\n...\n\no\n\ni\n\nI\n\n~\n\n...\nr.\n\n0.4\n\n(a)\n\n0.6\n\n0.8\n\n1.2\n\n1.4\n\nTime (Sec)\n\n3.8 ~_---:-":--_--::\'=_ _\':"\':-_--::\':-_--::,\'::-_--.J\n0.3\n0.4\n0.5\n0 .6\n0.7\n0.8\n\n(b)\n\nTime (Sec)\n\nFigure 3: (a) Response of a pixel to a grey strip 2 pixels wide of gray-level "0.4"\non a dark background of level "0" moving past the pixel at different speeds. (b)\nResponse of a pixel to a dark strip of gray-level "0.6" on a white background of level\n"1" moving past the pixel at different speeds. The voltage shown on these curves is\nnot the direct measurement of the voltage at V, but rather V, drives a current-sensing\ntransistor and this current is then sensed by an offchip current sense-amplifier.\n\n\x0cAdaptive Retina with Center-Surround Receptive Field\n\n683\n\ntemporal responses of the circuit.\nThe chip results from the experiments below illustrate the center-surround properties of the network and the difference in time-constants between the surround and\ncenter.\n3.1\n\nChip Results\n\nData from the 2D chip is shown in the next few figures. In these experiments, we\nare only looking at one pixel of the 2D array. A rotating circular fly-wheel stimulus\nwith strips of alternating contrasts is mounted above the chip. The stimulus was\ncreated using Mathematica. Figure 3a shows the spati~temporal impulse response\nof one pixel measured at V, with a small strip at level "0.4" on a dark background of\nlevel "0" moving past the pixels on the row. At slow speeds, the impulse response\nshows a center-surround behavior where the pixel first receives inhibition from the\npreceding pixels which are excited by the stimulus. When the stimulus moves by\nthe pixel of interest, it is excited and then it is inhibited by the subsequent pixels\nseeing the stimulus.\n\nI\no\n\nf\n\nI\ni\n\nTim. (Sec)\n\nFigure 4: Response of a pixel to a strip of varying contrasts on a dark background\nmoving past the pixel at a constant speed.\nAt faster speeds, the initial inhibition in the response grows smaller until at some\neven faster speed, the initial inhibition is no longer observed. This response comes\nabout because the inhibition from the surround has a longer-time constant than the\ncenter. When the stimulus moves past the pixel of interest, the inhibition from the\npreceding pixels excited by the stimulus does not have time to inhibit the pixel of\ninterest. Hence the excitation is seen first and then the inhibition comes into place\nwhen the stimulus passes by. Note that in these figures (Figures 3-4), the curves\nhave been displaced to show the pixel response at different speeds of the moving\nstimulus. The voltage shown on these curves is not the direct measurement of the\nvoltage at V, but rather V, drives a current-sensing transistor and this current is\nthen sensed by an off-chip current sense-amplifier.\nFigure 3b shows the\n\nspati~temporal\n\nimpulse response of one pixel with a similar\n\n\x0cs.-c. LlU, K. BOAHEN\n\n684\n\nsize strip of level "0.6" on a light background of level "1" moving past the row of\npixels. The same inhibition behavior is seen for increasing stimulus speeds. Figure 4 shows the output response at V, for the same stimulus of gray-levels varying\nfrom "0.2" to "0.8" on a dark background of level "0" moving at one speed. The\npeak excitation response is plotted against the contrast in Figure 5. A level of "0.2"\ncorresponds to a irradiance of 15mW/m2 while a level of "0.8" corresponds to a irradiance of 37.4mW/m2. These measurements are done with a photometer mounted\nabout 1.5in above a piece of paper with the contrast which is being measured. The\nirradiance varies exponentially with increasing level.\n\n4\n\nConclusion\n\nIn this paper, we described an adaptive retina with a center-surround receptive\nfield. The system properties of this retina allows it to model functionally either the\nresponses of the laminar cells in the invertebrate retina or the outer-plexiform layer\nof vertebrate retina. We show that the circuit shows adaptation to changes over\n5 decades of background intensities. The center-surround property of the network\ncan be seen from its spatio-temporal response to different stimulus speeds. This\nproperty serves to remove redundancy in space and time of the input signal.\nAcknowledgements\n\nWe thank Carver Mead for his support and encouragement. SC Liu is supported by\nan NIMH fellowship and K Boahen is supported by a Sloan fellowship. We thank\nTobias Delbriick for the inspiration and help in testing the design. We also thank\nRahul Sarpeshkar and Bradley Minch for comments. Fabrication was provided by\nMOSIS.\n\nReferences\n[1] S. B. Laughlin, "Coding efficiency and design in retinal processing", In: Facets\nof Vision (D. G. Stavenga and R. C. Hardie, eds) pp. 213-234. Springer, Berlin,\n1989.\n[2] S. R. Shaw, "Retinal resistance barriers and electrica1lateral inhibition", Nature, Lond.255,: 480-483, 1975.\n[3] M. A. Mahowald, "Silicon Retina with Adaptive Photoreceptors" in\nSPIE/SPSE Symposium on Electronic Science and Technology: From Neurons\nto Chips. Orlando, FL, April 1991.\n[4] K. A. Boahen and A. G. Andreou, "A Contrast Sensitive Silicon Retina with\nReciprocal Synapses", In D. S. Touretzky (ed.), Advances in Neural Information Processing Systems 4, 764-772. San Mateo, CA: Morgan Kaufmann, 1992.\n[5] K. A. Boahen, "Spatiotemporal sensitivity of the retina: A physical model",\nCNS Memo CNS-TR-91-06, California Institute of Technology, Pasadena, CA\n91125, June 1991.\n[6] T. Delbriick, "Analog VLSI Phototransduction by continous-time, adaptive,\nlogarithmic photoreceptor circuits", CNS Memo No.30, California Institute of\nTechnology, Pasadena, CA 91125, 1994.\n\n\x0c'
p83209
sg76
S'Forward-backward retraining of recurrent\nneural networks\nAndrew Senior ?\nTony Robinson\nCambridge University Engineering Department\nTrumpington Street, Cambridge, England\n\nAbstract\nThis paper describes the training of a recurrent neural network\nas the letter posterior probability estimator for a hidden Markov\nmodel, off-line handwriting recognition system. The network estimates posterior distributions for each of a series of frames representing sections of a handwritten word. The supervised training\nalgorithm, backpropagation through time, requires target outputs\nto be provided for each frame. Three methods for deriving these\ntargets are presented. A novel method based upon the forwardbackward algorithm is found to result in the recognizer with the\nlowest error rate.\n\n1\n\nIntroduction\n\nIn the field of off-line handwriting recognition, the goal is to read a handwritten\ndocument and produce a machine transcription. Such a system could be used\nfor a variety of purposes, from cheque processing and postal sorting to personal\ncorrespondence reading for the blind or historical document reading. In a previous\npublication (Senior 1994) we have described a system based on a recurrent neural\nnetwork (Robinson 1994) which can transcribe a handwritten document.\nThe recurrent neural network is used to estimate posterior probabilities for character classes, given frames of data which represent the handwritten word. These\nprobabilities are combined in a hidden Markov model framework, using the Viterbi\nalgorithm to find the most probable state sequence.\nTo train the network, a series of targets must be given. This paper describes three\nmethods that have been used to derive these probabilities. The first is a naive bootstrap method, allocating equal lengths to all characters, used to start the training\nprocedure. The second is a simple Viterbi-style segmentation method that assigns a\nsingle class label to each of the frames of data. Such a scheme has been used before\nin speech recognition using recurrent networks (Robinson 1994). This representation, is found to inadequately represent some frames which can represent two letters,\nor the ligatures between letters. Thus, by analogy with the forward-backward algorithm (Rabiner and Juang 1986) for HMM speech recognizers, we have developed a\n?Now at IDM T .J.Watson Research Center, Yorktown Heights NYI0598, USA.\n\n\x0c744\n\nA. SENIOR, T. ROBINSON\n\nforward-backward method for retraining the recurrent neural network. This assigns\na probability distribution across the output classes for each frame of training data,\nand training on these \'soft labels\' results in improved performance of the recognition\nsystem.\nThis paper is organized in four sections. The following section outlines the system\nin which the neural network is used, then section 3 describes the recurrent network\nin more detail. Section 4 explains the different methods of target estimation and\npresents the results of experiments before conclusions are presented in the final\nsection.\n\n2\n\nSystem background\n\nThe recurrent network is the central part of the handwriting recognition system.\nThe other parts are summarized here and described in more detail in another publication (Senior 1994). The first stage of processing converts the raw data into\nan invariant representation used as an input to the neural network. The network\noutputs are used to calculate word probabilities in a hidden Markov model.\nFirst, the scanned page image is automatically segmented into words and then normalized. Normalization removes variations in the word appearance that do not\naffect its identity, such as rotation, scale, slant, slope and stroke thickness. The\nheight of the letters forming the words is estimated, and magnifications, shear and\nthinning transforms are applied, resulting in a more robust representation of the\nword. The normalized word is represented in a compact canonical form encoding\nboth the shape and salient features. All those features falling within a narrow vertical strip across the word are termed a frame. The representation derived consists\nof around 80 values for each of the frames, denoted Xt. The T frames (Xl,\' .. , x r )\nfor a whole word are written xl\' Five frames would typically be enough to represent a single character. The recurrent network takes these frames sequentially and\nestimates the posterior character probability distribution given the data: P(Ai IxD,\nfor each of the letters, a, .. ,z, denoted Ao, ... , A25 ? These posterior probabilities are\nscaled by the prior class probabilities, and are treated as the emission probabilities\nin a hidden Markov model.\nA separate model is created for each word in the vocabulary, with one state per\nletter. Transitions are allowed only from a state to itself or to the next letter in the\nword. The set of states in the models is denoted Q = {ql, ... , qN} and the letter\nrepresented by qi is given by L(qi), L : Q 1-+ Ao, ... , A 25 ?\nWord error rates are presented for experiments on a single-writer task tested with\na 1330 word vocabulary!. Statistical significance of the results is evaluated using\nStudent\'s t-test, comparing word recognition rates taken from a number of networks\ntrained under the same conditions but with different random initializations. The\nresults of the t-test are written: T( degrees of freedom) and the tabulated values:\ntsignificance (degrees of freedom).\n\n3\n\nRecurrent networks\n\nThis section describes the recurrent error propagation network which has been used\nas the probability distribution estimator for the handwriting recognition system.\nRecurrent networks have been successfully applied to speech recognition (Robinson 1994) but have not previously been used for handwriting recognition, on-line\nor off-line. Here a left-to-right scanning process is adopted to map the frames of\na word into a sequence, so adjacent frames are considered in consecutive instants.\nlThe experimental data are available in ftp:/ /svr-ftp.eng.cam.ac.uk/pub/data\n\n\x0cForward-backward Retraining of Recurrent Neural Networks\n\n745\n\nA recurrent network is well suited to the recognition of patterns occurring in a\ntime-series because series of arbitrary length can be processed, with the same processing being performed on each section of the input stream. Thus a letter \'a\'\ncan be recognized by the same process, wherever it occurs in a word. In addition, internal \'state\' units are available to encode multi-frame context information\nso letters spread over several frames can be recognized. The recurrent network\nInput Frames\n\n_\n\nIT\n\n:J ._- ---.\n\nTT,\n\nNetwork\n\n,;\n\n,\n\n-- JD\n:!\n,\n\nOutput\n(Characlcrprobabllllles )\n\n( .. vv le e W WW II ... )\n\ni\n:\nf\n---l-- _. ;---------r\n\ninpu tiOulpul Uni ts\n\n-ie~~;k-Um, s\n\nUntt Time Iklay\n\nFigure 1: A schematic of the recurrent error propagation network.\nFor clarity only a few of the units and links are shown.\narchitecture used here is a single layer of standard perceptrons with nonlinear activation functions. The output 0 i of a unit i is a function of the inputs aj and\nthe network parameters, which are the weights of the links Wij with a bias bi :\n\nbi + Lakwik.\n(2)\nThe network is fully connected - that is, each input is connected to every output. However, some of the input units receive no external input and are connected one-to-one to corresponding output units through a unit time-delay (figure 1). The remaining input units accept a single frame of parametrized input and the remaining 26 output units estimate letter probabilities for the 26\ncharacter classes. The feedback units have a standard sigmoid activation function (3), but the character outputs have a \'softmax\' activation function (4).\n0i\n\n!i({O"j}),\n\n(1)\n\nO"i\n\neO\' ?\n\n(3)\n\nL: j\n\neO" ?\n\n(4)\n\nDuring recognition (\'forward propagation\'), the first frame is presented at the input\nand the feedback units are initialized to activations of 0.5. The outputs are calculated (1 and 2) and read off for use in the Markov model. In the next iteration, the\noutputs of the feedback units are copied to the feedback inputs, and the next frame\npresented to the inputs. Outputs are again calculated, and the cycle is repeated for\neach frame of input, with a probability distribution being generated for each frame.\nTo allow the network to assimilate context information, several frames of data are\npassed through the network before the probabilities for the first frame are read\noff, previous output probabilities being discarded. This input/output latency is\nmaintained throughout the input sequence, with extra, empty frames of inputs\nbeing presented at the end to give probability distributions for the last frames of\ntrue inputs. A latency of two frames has been found to be most satisfactory in\nexperiments to date.\n\n3.1\n\nTraining\n\nTo be able to train the network the target values (j (t) desired for the outputs\n= 0, ... ,25 for frame Xt must be specified. The target specification is dealt\n\nOJ (Xt) j\n\n\x0c746\n\nA. SENIOR. T. ROBINSON\n\nwith in the next section. It is the discrepancy between the actual outputs and these\ntargets which make up the objective function to be maximized by adjusting the\ninternal weights of the network. The usual objective function is the mean squared\nerror, but here the relative entropy, G, of the target and output distributions is\nused:\nG\n\n(j(t)-)\'\n- "\nL- "\nL- (j (t) log -.-(\nt\n\nj\n\n(5)\n\noJ Xt\n\nAt the end of a word, the errors between the network\'s outputs and the targets\nare propagated back using the generalized delta rule (Rumelhart et al. 1986) and\nchanges to the network weights are calculated. The network at successive time\nsteps is treated as adjacent layers of a multi-layer network. This process is generally known as \'back-propagation through time\' (Werbos 1990). After processing T\nframes of data with an input/output latency, the network is equivalent to a (T +\nlatency) layer perceptron sharing weights between layers. For a detailed description\nof the training procedure, the reader is referred elsewhere (Rumelhart et al. 1986;\nRobinson 1994).\n\n4\n\nTarget re-estimation\n\nThe data used for training are only labelled by word. That is, each image represents\na single word, whose identity is known, but the frames representing that word are\nnot labelled to indicate which part of the word they represent. To train the network,\na label for each frame\'s identity must be provided. Labels are indicated by the state\nSt E Q and the corresponding letter L(St) of which a frame Xt is part.\n4.1\n\nA simple solution\n\nTo bootstrap the network, a naive method was used, which simply divided the word\nup into sections of equal length, one for each letter in the word. Thus, for an Nletter word of T frames, xI, the first letter was assumed to be represented by frames\n\nxr, the next by k+\nas follows:\n..\n\n2r\n\nx\n\n1\n\nand so on. The segmentation is mapped into a set of targets\n\nI\'J.(t)\n{ 1 if L(St) = Aj\n(6)\n..\n0 otherwise.\nFigure 2a shows such a segmentation for a single word. Each line, representing\n(j(t) for some j, has a broad peak for the frames representing letter Aj. Such a\nsegmentation is inaccurate, but can be improved by adding prior knowledge. It\nis clear that some letters are generally longer than others, and some shorter. By\nweighting letters according to their a priori lengths it is possible to give a better,\nbut still very simple, segmentation. The letters Ii, I\' are given a length of and\n\'m, w\' a length ~ relative to other letters. Thus in the word \'wig\', the first half\nof the frames would be assigned the label \'w\', the next sixth Ii\' and the last third\nthe label \'g\'. While this segmentation is constructed with no regard for the data\nbeing segmented, it is found to provide a good initial approximation from which it\nis possible to train the network to recognize words, albeit with high error rates.\n\n!\n\n4.2\n\nViterbi re-estimation\n\nHaving trained the network to some accuracy, it can be used to calculate a good\nestimate of the probability of each frame belonging to any letter. The probability\nof any state sequence can then be calculated in the hidden Markov model, and\nthe most likely state sequence through the correct word S* found using dynamic\nprogramming. This best state sequence S* represents a new segmentation giving a\nlabel for each frame. For a network which models the probability distributions well,\nthis segmentation will be better than the automatic segmentation of section 4.1\n\n\x0cForward-backward Retraining of Recurrent Neural Networks\n\n747\n\n" Each line represents\nFigure 2: Segmentations of the word \'butler\'.\nP(St = AilS) for one letter ~ and is high for framet when S; = Ai.\n(a) is the equal-length segmentation discussed in section 4.1 (b) is\na segmentation of an untrained network. (c) is the segmentation\nre-estimated with a trained network.\nsince it takes the data into account. Finding the most probable state sequence S? is\ntermed a forced alignment. Since only the correct word model need be considered,\nsuch an alignment is faster than the search through the whole lexicon that is required\nfor recognition. Training on this automatic segmentation gives a better recognition\nrate, but still avoids the necessity of manually segmenting any of the database.\nFigure 2 shows two Viterbi segmentations of the word \'butler\'. First, figure 2b\nshows the segmentation arrived at by taking the most likely state sequence before\ntraining the network. Since the emission probability distributions are random, there\nis nothing to distinguish between the state sequences, except slight variations due\nto initial asymmetry in the network, so a poor segmentation results. After training the network (2c), the durations deviate from the prior assumed durations to\nmatch the observed data. This re-estimated segmentation represents the data more\naccurately, so gives better targets towards which to train. A further improvement\nin recognition accuracy can be obtained by using the targets determined by the reestimated segmentation. This cycle can be repeated until the segmentations do not\nchange and performance ceases to improve. For speed, the network is not trained\nto convergence at each iteration.\nIt can be shown (Santini and Del Bimbo 1995) that, assuming that the network has\nenough parameters, the network outputs after convergence will approximate the\nposterior probabilities P(~lxD. Further, the approximation P(AilxD ~ P(Adxt)\nis made. The posteriors are scaled by the class priors P(Ai) (Bourlard and Morgan\n1993), and these scaled posteriors are used in the hidden Markov model in place of\ndata likelihoods since, by Bayes\' rule,\n\nP(XtIAi)\n\n()(\n\nP(~lxt)\n\nP(Ai)?\n\n(7)\n\nTable 1 shows word recognition error rates for three 80-unit networks trained towards fixed targets estimated by another network, and then retrained, re-estimating\nthe targets at each iteration. The retraining improves the recognition performance\n(T(2) = 3.91, t.9s(2) = 2.92).\n\n4.3\n\nForward-backward re-estimation\n\nThe system described above performs well and is the method used in previous recurrent network systems, but examining the speech recognition literature, a potential\nmethod of improvement can be seen. Viterbi frame alignment has so far been used\nto determine targets for training. This assigns one class to each frame, based on\nthe most likely state sequence. A better approach might be to allow a distribution across all the classes indicating which are likely and which are not, avoiding a\n\n\x0c748\n\nA. SENIOR, T. ROBINSON\n\nTable 1: Error rates for 3 networks with 80 units trained with fixed\nalignments, and retrained with re-estimated alignments.\nTraining\nError (%)\n(7\nmethod\nJ.I.\nFixed targets 21.2 1.73\n17.0 0.68\nRetraining\n\'hard\' classification at points where a frame may indeed represent more than one\nclass (such as where slanting characters overlap), or none (as in a ligature). A \'soft\'\nclassification would give a more accurate portrayal of the frame identities.\n<?\n\nSuch a distribution, \'Yp(t) = P(St = qplxI, W), can be calculated with the forwardbackward algorithm (Rabiner and Juang 1986). To obtain \'Yp(t), the forward probabilities Ctp(t) = P(St = qp, xD must be combined with the backward probabilities\nf3p(t) = P(St = qp, x;+l)\' The forward and backward probabilities are calculated\nrecursively in the same manner.\nCtr(t + 1)\nCtp(t)P(xtIL(qp))ap,r,\n(8)\n\nL\n\n/3p(t - 1)\n\n(9)\nr\n\nSuitable initial distributions Ctr(O) = 7l\'r and f3r(r + 1) = Pr are chosen, e.g. 7l\' and\nP are one for respectively the first and last character in the word, and zero for the\nothers. The likelihood of observing the data Xl and being in state qp at time t is\nthen given by:\ne (t) = Ctp(t)/3p(t).\n(10)\nThen the probabilities \'Yp(t) of being in state qp at time t are obtained by normalization and used as the targets (j (t) for the recurrent network character probability\noutputs:\nep(t)\n(11)\n(j (t)\n\'Yp(t). (12)\n\nL\n\nl:r er(t)\'\n\np:L(qp)=Aj\n\nFigure 3a shows the initial estimate of the class probabilities for a sample of the\nword\' butler\'. The probabilities shown are those estimated by the forward-backward\nalgorithm when using an untrained network, for which the P(XtISt = qp) will be\nindependent of class. Despite the lack of information, the probability distributions\ncan be seen to take reasonable shapes. The first frame must belong to the first\nletter, and the last frame must belong to the last letter, of course, but it can also\nbe seen that half way through the word, the most likely letters are those in the\nmiddle of the word. Several class probabilities are non-zero at a time, reflecting\nthe uncertainty caused since the network is untrained. Nevertheless, this limited\ninformation is enough to train a recurrent network, because as the network begins\nto approximate these probabilities, the segmentations become more definite. In\ncontrast, using Viterbi segmentations from an untrained network, the most likely\nalignment can be very different from the true alignment (figure 2b). The segmentation is very definite though, and the network is trained towards the incorrect\ntargets, reinforcing its error. Finally, a trained network gives a much more rigid\nsegmentation (figure 3b), with most of the probabilities being zero or one, but with\na boundary of uncertainty at the transitions between letters. This uncertainty,\nwhere a frame might truly represent parts of two letters, or a ligature between\ntwo, represents the data better. Just as with Viterbi training, the segmentations\ncan be re-estimated after training and retraining results in improved performance.\nThe final probabilistic segmentation can be stored with the data and used when\nsubsequent networks are trained on the same data. Training is then significantly\nquicker than when training towards the approximate bootstrap segmentations and\nre-estimating the targets.\n\n\x0cForward-backward Retraining of Recurrent Neural Networks\n\n749\n\nFigure 3: Forward-backward segmentations of the word \'butler\'.\n(a) is the segmentation of an untrained network with a uniform\nclass prior. (b) shows the segmentation after training.\nThe better models obtained with the forward-backward algorithm give improved\nrecognition results over a network trained with Viterbi alignments. The improvement is shown in table 2. It can be seen that the error rates for the networks\ntrained with forward-backward targets are lower than those trained on Viterbi targets (T(2) 5.24, t.97S(2) 4.30).\n\n=\n\n=\n\nTable 2: Error rates for networks with\nor Forward-Backward alignments.\nTraining\nmethod\nViterbi\nForward-Backward\n\n5\n\n80 units trained with Viterbi\nError 1%)\nJ.I.\n\n(7\n\n17.0\n15.4\n\n0.68\n0.74\n\nConclusions\n\nThis paper has reviewed the training methods used for a recurrent network, applied\nto the problem of off-line handwriting recognition. Three methods of deriving target probabilities for the network have been described, and experiments conduded\nusing all three. The third method is that of the forward-backward procedure, which\nhas not previously been applied to recurrent neural network training. This method\nis found to improve the performance of the network, leading to reduced word error\nrates. Other improvements not detailed here (including duration models and stochastic language modelling) allow the error rate for this task to be brought below\n10%.\nAcknowledgments\nThe authors would like to thank Mike Hochberg for assistance in preparing this\npaper.\n\nReferences\nBOURLARD, H. and MORGAN, N. (1993) Connectionist Speech Recognition: A Hybrid\nApproach . Kluwer .\nRABINER, L. R. and JUANG, B . H. (1986) An introduction to hidden Markov models.\nIEEE ASSP magazine 3 (1): 4-16.\nROBINSON, A. (1994) The application ofrecuIIent nets to phone probability estimation.\nIEEE \'lransactions on Neural Networks.\nRUMELHART, D. E ., HINTON, G. E. and WILLIAMS, R. J. (1986) Learning internal\nrepresentations by eIIor propagation. In Parallel Distributed Processing: Explorations\nin the Microstructure of Cognition, ed. by D . E. Rumelhart and J . L. McClelland,\nvolume 1, chapter 8, PE. 318-362. Bradford Books.\nSANTINI, S. and DEL BIMBO, A . (1995) RecuIIent neural networks can be trained to\nbe maximum a posteriori probability classifiers. Neural Networks 8 (1): 25-29.\nSENIOR, A . W ., (1994) Off-line Cursive Handwriting Recognition using Recurrent\nNeural Networks. Cambridge University Engineering Department Ph.D. thesis. URL:\n~_~: / / svr-ft.p . enK. cam. ac . uk/pub/reports/senioLthesis . ps . gz.\nWERBOS, P. J. (1990) Backpropagation through time: What it does and how to do it.\nProceedings of the IEEE 78: 1550-60.\n\n\x0c'
p83210
sg262
S'When is an Integrate-and-fire Neuron\nlike a Poisson Neuron?\n\nCharles F. Stevens\nSalk Institute MNL/S\nLa Jolla, CA 92037\ncfs@salk.edu\n\nAnthony Zador\nSalk Institute MNL/S\nLa Jolla, CA 92037\nzador@salk.edu\n\nAbstract\n\nIn the Poisson neuron model, the output is a rate-modulated Poisson process (Snyder and Miller, 1991); the time varying rate parameter ret) is an instantaneous function G[.] of the stimulus,\nret) = G[s(t)]. In a Poisson neuron, then, ret) gives the instantaneous firing rate-the instantaneous probability of firing at any\ninstant t-and the output is a stochastic function of the input. In\npart because of its great simplicity, this model is widely used (usually with the addition of a refractory period) , especially in in vivo\nsingle unit electrophysiological studies, where set) is usually taken\nto be the value of some sensory stimulus. In the integrate-and-fire\nneuron model, by contrast, the output is a filtered and thresholded\nfunction of the input: the input is passed through a low-pass filter\n(determined by the membrane time constant T) and integrated until the membrane potential vet) reaches threshold 8, at which point\nvet) is reset to its initial value. By contrast with the Poisson model,\nin the integrate-and-fire model the ouput is a deterministic function\nof the input. Although the integrate-and-fire model is a caricature\nof real neural dynamics, it captures many of the qualitative features, and is often used as a starting point for conceptualizing the\nbiophysical behavior of single neurons. Here we show how a slightly\nmodified Poisson model can be derived from the integrate-and-fire\nmodel with noisy inputs yet) = set) + net). In the modified model,\nthe transfer function G[.] is a sigmoid (erf) whose shape is determined by the noise variance /T~. Understanding the equivalence\nbetween the dominant in vivo and in vitro simple neuron models\nmay help forge links between the two levels.\n\n\x0cc. F. STEVENS. A. ZADOR\n\n104\n\n1\n\nIntroduction\n\nIn the Poisson neuron model, the output is a rate-modulated Poisson process; the\ntime varying rate parameter ret) is an instantaneous function G[.] of the stimulus, ret) = G[s(t)]. In a Poisson neuron, then, ret) gives the instantaneous firing\nrate-the instantaneous probability of firing at any instant t-and the output is a\nstochastic function of the input. In part because of its great simplicity, this model\nis widely used (usually with the addition of a refractory period), especially in in\nvivo single unit electrophysiological studies, where set) is usually taken to be the\nvalue of some sensory stimulus.\nIn the integrate-and-fire neuron model, by contrast, the output is a filtered and\nthresholded function of the input: the input is passed through a low-pass filter\n(determined by the membrane time constant T) and integrated until the membrane\npotential vet) reaches threshold 0, at which point vet) is reset to its initial value.\nBy contrast with the Poisson model, in the integrate-and-fire model the ouput is\na deterministic function of the input. Although the integrate-and-fire model is a\ncaricature of real neural dynamics, it captures many of the qualitative features, and\nis often used as a starting point for conceptualizing the biophysical behavior of single\nneurons (Softky and Koch , 1993; Amit and Tsodyks, 1991; Shadlen and Newsome,\n1995; Shadlen and Newsome, 1994; Softky, 1995; DeWeese, 1995; DeWeese, 1996;\nZador and Pearlmutter, 1996).\nHere we show how a slightly modified Poisson model can be derived from the\nintegrate-and-fire model with noisy inputs yet) = set) + net). In the modified\nmodel, the transfer function G[.] is a sigmoid (erf) whose shape is determined by\nthe noise variance (j~ . Understanding the equivalence between the dominant in vivo\nand in vitro simple neuron models may help forge links between the two levels.\n\n2\n\nThe integrate-and-fire model\n\nHere we describe the the forgetful leaky integrate-and-fire model. Suppose we add\na signal set) to some noise net),\n\nyet) = net) + set),\nand threshold the sum to produce a spike train\n\nz(t) = F[s(t) + net)],\nwhere F is the thresholding functional and z(t) is a list of firing times generated by\nthe input. Specifically, suppose the voltage vet) of the neuron obeys\nvet) = - vet)\n\n+ yet)\n\n(1)\n\nT\n\nwhere T is the membrane time constant. We assume that the noise net) has O-mean\nand is white with variance (j~. Thus yet) can be thought of as a Gaussian white\nprocess with variance (j~ and a time-varying mean set) . If the voltage reaches the\nthreshold 00 at some time t, the neuron emits a spike at that time and resets to\nthe initial condition Vo. This is therefore a 5 parameter model: the membrane\ntime constant T, the mean input signal Il, the variance of the input signal 17 2 , the\nthreshold 0, and the reset value Vo. Of course, if net) = 0, we recover a purely\ndeterministic integrate-and-fire model.\n\n\x0cWhen Is an Integrate-and-fire Neuron like a Poisson Neuron?\n\n105\n\nIn order to forge the link between the integrate-and-fire neuron dynamics and the\nPoisson model, we will treat the firing times T probabilistically. That is, we will\nexpress the output of the neuron to some particular input set) as a conditional\ndistribution p(Tls(t?, i.e. the probability of obtaining any firing time T given\nsome particular input set) .\nUnder these assumptions, peT) is given by the first passage time distribution\n(FPTD) of the Ornstein-Uhlenbeck process (Uhlenbeck and Ornstein, 1930; Tuckwell, 1988). This means that the time evolution of the voltage prior to reaching\nthreshold is given by the Fokker-Planck equation (FPE),\n\n8\n8t g(t, v)\n\nu; 8v8\n\n=2\n\n8\nvet)\nv) - av [(set) - --;:- )g(t, v)],\n\n2\n\n2 get,\n\nwhere uy = Un and get, v) is the distribution at time t of voltage\nThen the first passage time distribution is related to g( v, t) by\n\npeT) = -\n\n81\n\nat\n\n90\n\n-00\n\n(2)\n-00\n\n< v ::;\n\n(}o.\n\n(3)\n\nget, v)dv.\n\nThe integrand is the fraction of all paths that p.ave not yet crossed threshold. peT)\nis therefore just the interspike interval (lSI) distribution for a given signal set). A\ngeneral eigenfunction expansion solution for the lSI distribution is known, but it\nconverges slowly and its terms offer little insight into the behavior (at least to us) .\nWe now derive an expression for the probability of crossing threshold in some very\nshort interval ~t, starting at some v. We begin with the "free" distribution of g\n(Tuckwell, 1988): the probability of the voltage jumping to v\' at time t\'\nt + ~t,\ngiven that it was at v at time t, assuming von Neumann boundary conditions at\nplus and minus infinity,\n\n=\n\n1\n\nget\', v\'lt, v) =\n\n[\n\nJ27r q(~t;Uy)\n\nexp -\n\n?)2]\n\n(v\' - m( ~t; u y ,\n2 q(~t;Uy)\n\n(4)\n\nwith\nand\nm(~t)\n\n= ve- at / + set) * T(l _ e- at /\nT\n\nT ),\n\nwhere * denotes convolution. The free distribution is a Gaussian with a timedependent mean m(~t) and variance q(~t; u y ). This expression is valid for all ~t.\nThe probability of making a jump\n~v\n\nin a short interval\n\n~t ~ T\n\ndepends only on\n\nga(~t, ~v; uy) =\nFor small\n\n~t,\n\n= v\' 1\n\nv\n\n~v\n\n..j27r qa(u y )\n\nand\n\n~t,\n\nexp [_\n\n~~2\n\n)].\n\n2 qa uy\n\n(5)\n\nwe expand to get\n\nqa(uy) :::::: 2u;~t,\nwhich is independent of T, showing that the leak can be neglected for short times.\n\n\x0cc. F. STEVENS, A. ZADOR\n\n106\n\nNow the probability Pt>, that the voltage exceeds threshold in some short Ilt, given\nthat it started at v, depends on how far v is from threshold; it is\n\nPr[v + Ilv\n\n~\n\n0] = Pr[llv\n\n~\n\n0 - v].\n\nThus\n\n(Xl dvgt>,(llt, v; O"y)\n\nJ9-v\n\n1\n-erfc\n2\n1\n-erfc\n2\n\n(6)\n\n(o-v)\n(o-v)\nJ2qt>,(O"y)\n\nO"yJ21lt\n\nI;\n\nwhere erfc(x) = 1 - -j;\ne-t~ dt goes from [2 : 0]. This then is the key result:\nit gives the instantaneous probability of firing as a function of the instantaneous\nvoltage v. erfc is sigmoidal with a slope determined by O"y, so a smaller noise yields\na steeper (more deterministic) transfer function; in the limit of 0 noise, the transfer\nfunction is a step and we recover a completely deterministic neuron.\nNote that Pt>, is actually an instantaneous function of v(t), not the stimulus itself\ns(t). If the noise is large compared with s(t) we must consider the distribution\ng$ (v, t; O"y) of voltages reached in response to the input s(t):\n\n(7)\n\nPy(t)\n\n3\n\nEnsemble of Signals\n\nWhat if the inputs s(t) are themselves drawn from an ensemble? If their distribution\nis also Gaussian and white with mean Jl and variance\nand if the firing rate is\nlow (E[T] ~ T), then the output spike train is Poisson. Why is firing Poisson only\nin the slow firing limit? The reason is that, by assumption, immediately following\na spike the membrane potential resets to 0; it must then rise (assuming Jl > 0) to\nsome asymptotic level that is independent of the initial conditions. During this rise\nthe firing rate is lower than the asymptotic rate, because on average the membrane\nis farther from threshold, and its variance is lower. The rate at which the asymptote\nis achieved depends on T. In the limit as t ~ T, some asymptotic distribution of\nvoltage qoo(v), is attained. Note that if we make the reset Vo stochastic, with a\ndistribution given by qoo (v), then the firing probability would be the same even\nimmediately after spiking, and firing would be Poisson for all firing rates.\n\n0";,\n\nA Poisson process is characterized by its mean alone. We therefore solve the FPE\n(eq. 2) for the steady-state by setting ?tg(t, v) = 0 (we consider only threshold\ncrossings from initial values t ~ T; negYecting the early events results in only a\nsmall error, since we have assumed E{T} ~ T). Thus with the absorbing boundary\n\n\x0c107\n\nWhen Is an Integrate-and-fire Neuron like a Poisson Neuron?\n\nat 0 the distribution at time t\n\n~ T\n\n(given here for JJ\n\n= 0) is\n\n(8)\ng~(Vj uy)= kl (1 - k2 erfi [uyfi]) exp [~i:] ,\nwhere u; = u; + u~, erfi(z) = -ierf(iz), kl determines the normalization (the sign\nof kl determines whether the solution extends to positive or negative infinity) and\nk2 = l/erfi(O/(uy Vr)) is determined by the boundary. The instantaneous Poisson\nrate parameter is then obtained through eq. (7),\n\n(9)\n\nFig. 1 tests the validity of the exponential approximation. The top graph shows\nthe lSI distribution near the "balance point" , when the excitation is in balance with\nthe inhibition and the membrane potential hovers just subthreshold. The bottom\ncurves show the lSI distribution far below the balance point. In both cases, the\nexponential distribution provides a good approximation for t ~ T.\n\n4\n\nDiscussion\n\nThe main point of this paper is to make explicit the relation between the Poisson\nand integrate-and-fire models of neuronal acitivity. The key difference between\nthem is that the former is stochastic while the latter is deterministic. That is, given\nexactly the same stimulus, the Poisson neuron produces different spike trains on\ndifferent trials, while the integrate-and-fire neuron produces exactly the same spike\ntrain each time. It is therefore clear that if some degree of stochasticity is to be\nobtained in the integrate-and-fire model, it must arise from noise in the stimulus\nitself.\nThe relation we have derived here is purely formalj we have intentionally remained\nagnostic about the deep issues of what is signal and what is noise in the inputs to a\nneuron. We observe nevertheless that although we derive a limit (eq. 9) where the\nspike train of an integrate-and-fire neuron is a Poisson process-i.e. the probability\nof obtaining a spike in any interval is independent of obtaining a spike in any other\ninterval (except for very short intervals)-from the point of view of information\nprocessing it is a very different process from the purely stochastic rate-modulated\nPoisson neuron. In fact, in this limit the spike train is deterministically Poisson\nif u y = u., i. e. when n( t) = OJ in this case the output is a purely deterministic\nfunction of the input, but the lSI distribution is exponential.\n\n\x0c108\n\nC. F. STEVENS, A. ZADOR\n\nReferences\nAmit, D. and Tsodyks, M. (1991). Quantitative study of attractor neural network retrieving at low spike rates. i. substrate-spikes, rates and neuronal gain.\nNetwork: Computation in Neural Systems , 2:259-273 .\nDeWeese, M. (1995). Optimization principles for the neural code. PhD thesis, Dept\nof Physics, Princeton University.\nDeWeese, M. (1996). Optimization principles for the neural code. In Hasselmo,\nM., editor, Advances in Neural Information Processing Systems, vol. 8. MIT\nPress, Cambridge, MA.\nShadlen, M. and Newsome, W. (1994) . Noise, neural codes and cortical organization.\nCurrent Opinion in Neurobiology, 4:569-579.\nShadlen, M. and Newsome, W. (1995) . Is there a signal in the noise? [comment].\nCurrent Opinion in Neurobiology, 5:248-250.\nSnyder, D. and Miller, M. (1991). Random Point Processes in Time and Space, 2 nd\nedition. Springer-Verlag.\nSoftky, W. (1995) . Simple codes versus efficient codes. Current Opinion in Neurobiology, 5:239-247 .\nSoftky, W. and Koch, C. (1993). The highly irregular firing of cortical cells is\ninconsistent with temporal integration of random epsps. J. Neuroscience . ,\n13:334-350.\nTuckwell, H. (1988). Introduction to theoretical neurobiology (2 vols.). Cambridge.\nUhlenbeck, G. and Ornstein, L. (1930). On the theory of brownian motion. Phys.\nRev., 36:823-84l.\nZador, A. M. and Pearlmutter, B. A. (1996) . VC dimension of an integrate and fire\nneuron model. Neural Computation, 8(3) . In press.\n\n\x0cWhen Is an Integrate-and-fire Neuron like a Poisson Neuron?\n\n109\n\nlSI distributions at balance point and the exponential limit\n0.02\n0.015\n.~\n\n15\n\n.8\n\nea.\n\n0.01\n0.005\n\n50\n\n100\n\n150\n\n200\n\n300\n250\nTime (msec)\n\n350\n\n400\n\n450\n\n500\n\n200\n\n400\n\n600\n\n800\n\n1000 1200\nlSI (msec)\n\n1400\n\n1600\n\n1800\n\n2000\n\n2 x 10-3\n\n1.5\n\n~\n\n~\n\n.0\n\n...0a.\n\n1\n0.5\n0\n\n0\n\nFigure 1: lSI distributions. (A; top) lSI distribution for leaky integrate-and-fire\nmodel at the balance point, where the asymptotic membrane potential is just subthreshold, for two values of the signal variance (1\'2 . Increasing (1\'2 shifts the distribution to the left . For the left curve, the parameters were chosen so that E{T} ~ T,\ngiving a nearly exponential distribution; for the right curve, the distribution would\nbe hard to distinguish experimentally from an exponential distribution with a refractory period. (T = 50 msec; left: E{T} = 166 msec; right: E{T} = 57 msec).\n(B; bottom) In the subthreshold regime, the lSI distribution (solid} is nearly exponential (dashed) for intervals greater than the membrane time constant. (T = 50\nmsec; E{T} = 500 msec)\n\n\x0c'
p83211
sg295
S'From Isolation to Cooperation:\nAn Alternative View of a System of Experts\nStefan Schaal:!:*\nsschaal@cc.gatech.edu\nhttp://www.cc.gatech.eduifac/Stefan.Schaal\n\nChristopher C. Atkeson:!:\ncga@cc.gatech.edu\nhttp://www.cc.gatech.eduifac/Chris.Atkeson\n\n+College of Computing, Georgia Tech, 801 Atlantic Drive, Atlanta, GA 30332-0280\n\n*ATR Human Infonnation Processing, 2-2 Hikaridai, Seiko-cho, Soraku-gun, 619-02 Kyoto\n\nAbstract\nWe introduce a constructive, incremental learning system for regression\nproblems that models data by means of locally linear experts. In contrast\nto other approaches, the experts are trained independently and do not\ncompete for data during learning. Only when a prediction for a query is\nrequired do the experts cooperate by blending their individual predictions. Each expert is trained by minimizing a penalized local cross validation error using second order methods. In this way, an expert is able to\nfind a local distance metric by adjusting the size and shape of the receptive field in which its predictions are valid, and also to detect relevant input features by adjusting its bias on the importance of individual input\ndimensions. We derive asymptotic results for our method. In a variety of\nsimulations the properties of the algorithm are demonstrated with respect\nto interference, learning speed, prediction accuracy, feature detection,\nand task oriented incremental learning.\n\n1. INTRODUCTION\nDistributing a learning task among a set of experts has become a popular method in computationallearning. One approach is to employ several experts, each with a global domain of\nexpertise (e.g., Wolpert, 1990). When an output for a given input is to be predicted, every\nexpert gives a prediction together with a confidence measure. The individual predictions\nare combined into a single result, for instance, based on a confidence weighted average.\nAnother approach-the approach pursued in this paper-of employing experts is to create\nexperts with local domains of expertise. In contrast to the global experts, the local experts\nhave little overlap or no overlap at all. To assign a local domain of expertise to each expert,\nit is necessary to learn an expert selection system in addition to the experts themselves.\nThis classifier determines which expert models are used in which part of the input space.\nFor incremental learning, competitive learning methods are usually applied. Here the experts compete for data such that they change their domains of expertise until a stable configuration is achieved (e.g., Jacobs, Jordan, Nowlan, & Hinton, 1991). The advantage of\nlocal experts is that they can have simple parameterizations, such as locally constant or locally linear models. This offers benefits in terms of analyzability, learning speed, and robustness (e.g., Jordan & Jacobs, 1994). For simple experts, however, a large number of experts is necessary to model a function. As a result, the expert selection system has to be\nmore complicated and, thus, has a higher risk of getting stuck in local minima and/or of\nlearning rather slowly. In incremental learning, another potential danger arises when the\ninput distribution of the data changes. The expert selection system usually makes either\nimplicit or explicit prior assumptions about the input data distribution. For example, in the\nclassical mixture model (McLachlan & Basford, 1988) which was employed in several local expert approaches, the prior probabilities of each mixture model can be interpreted as\n\n\x0c606\n\nS. SCHAAL. C. C. ATKESON\n\nthe fraction of data points each expert expects to experience. Therefore, a change in input\ndistribution will cause all experts to change their domains of expertise in order to fulfill\nthese prior assumptions. This can lead to catastrophic interference.\nIn order to avoid these problems and to cope with the interference problems during incremental learning due to changes in input distribution, we suggest eliminating the competition among experts and instead isolating them during learning. Whenever some new data is\nexperienced which is not accounted for by one of the current experts, a new expert is created. Since the experts do not compete for data with their peers, there is no reason for them\nto change the location of their domains of expertise. However, when it comes to making a\nprediction at a query point, all the experts cooperate by giving a prediction of the output\ntogether with a confidence measure. A blending of all the predictions of all experts results\nin the final prediction. It should be noted that these local experts combine properties of\nboth the global and local experts mentioned previously. They act like global experts by\nlearning independently of each other and by blending their predictions, but they act like local experts by confining themselves to a local domain of expertise, i.e., their confidence\nmeasures are large only in a local region.\nThe topic of data fitting with structurally simple local models (or experts) has received a\ngreat deal of attention in nonparametric statistics (e.g., Nadaraya, 1964; Cleveland, 1979;\nScott, 1992, Hastie & Tibshirani, 1990). In this paper, we will demonstrate how a nonparametric approach can be applied to obtain the isolated expert network (Section 2.1),\nhow its asymptotic properties can be analyzed (Section 2.2), and what characteristics such\na learning system possesses in terms of the avoidance of interference, feature detection,\ndimensionality reduction, and incremental learning of motor control tasks (Section 3).\n\n2. RECEPTIVE FIELD WEIGHTED REGRESSION\nThis paper focuses on regression problems, i.e., the learning of a map from 9t n ~ 9t m ?\nEach expert in our learning method, Receptive Field Weighted Regression (RFWR), consists of two elements, a locally linear model to represent the local functional relationship,\nand a receptive field which determines the region in input space in which the expert\'s\nknowledge is valid. As a result, a given data set will be modeled by piecewise linear elements, blended together. For 1000 noisy data points drawn from the unit interval of the\nfunction z == max[exp(-10x 2 ),exp(-50l),1.25exp(-5(x 2 + l)], Figure 1 illustrates an\nexample of function fitting with RFWR. This function consists of a narrow and a wide\nridge which are perpendicular to each other, and a Gaussian bump at the origin. Figure 1b\nshows the receptive fields which the system created during the learning process. Each experts\' location is at the center of its receptive field, marked by a $ in Figure 1b. The recep1.5\n0 .5\n0\n-0.5\n\n-1\n1.5\n\n0.5\n,1\n\n,.,\n10. 5%\n\n0\n-0.5\n\n0\n\nI\n1- 0 .5\n1\n\n-1\n\n0\n\n-1.5\n-1.5\n\n- 0 .5\n\n(a)\n\n-1\n\nx\n\n(b)\n\n-1\n\n-0.5\n\no\n\n0.5\n\n1.5\n\nx\n\nFigure 1: (a) result of function approximation with RFWR. (b) contour lines of 0.1 iso-activation of\neach expert in input space (the experts\' centers are marked by small circles).\n\n\x0cFrom Isolation to Cooperation: An Alternative View of a System of Experts\n\n607\n\ntive fields are modeled by Gaussian functions, and their 0.1 iso-activation lines are shown\nin Figure 1b as well. As can be seen, each expert focuses on a certain region of the input\nspace, and the shape and orientation of this region reflects the function\'s complexity, or\nmore precisely, the function\'s curvature, in this region. It should be noticed that there is a\ncertain amount of overlap among the experts, and that the placement of experts occurred on\na greedy basis during learning and is not globally optimal. The approximation result\n(Figure 1a) is a faithful reconstruction of the real function (MSE = 0.0025 on a test set, 30\nepochs training, about 1 minute of computation on a SPARC1O). As a baseline comparison,\na similar result with a sigmoidal 3-layer neural network required about 100 hidden units\nand 10000 epochs of annealed standard backpropagation (about 4 hours on a SPARC1O).\n\n2.1 THE ALGORITHM\n\n. .?... \'.\n\n~"" "\n\nWeighBd\' /\nAverage\n\nOutput\n\nli\'Iear\n\n~:~~\n\nGalng\n\nUnrt\n\nConnectIOn\n\ncentered at e\n\ny,\n\nFigure 2: The RFWR network\n\nRFWR can be sketched in network form as\nshown in Figure 2. All inputs connect to all expert networks, and new experts can be added as\nneeded. Each expert is an independent entity. It\nconsists of a two layer linear subnet and a receptive field subnet. The receptive field subnet has a\nsingle unit with a bell-shaped activation profile,\ncentered at the fixed location c in input space.\nThe maximal output of this unit is "I" at the center, and it decays to zero as a function of the distance from the center. For analytical convenience,\nwe choose this unit to be Gaussian:\n(1)\n\nx is the input vector, and D the distance metric, a positive definite matrix that is generated\nfrom the upper triangular matrix M. The output of the linear subnet is:\n(2)\ny=x Tb + bo=x-Tf3\nA\n\nThe connection strengths b of the linear subnet and its bias bO will be denoted by the d-dimensional vector f3 from now on, and the tilde sign will indicate that a vector has been\naugmented by a constant "I", e.g., i = (x T , Il. In generating the total output, the receptive\nfield units act as a gating component on the output, such that the total prediction is:\n(3)\n\nThe parameters f3 and M are the primary quantities which have to be adjusted in the learning process: f3 forms the locally linear model, while M determines the shape and orientation of the receptive fields . Learning is achieved by incrementally minimizing the cost\nfunction:\n\n(4)\nThe first term of this function is the weighted mean squared cross validation error over all\nexperienced data points, a local cross validation measure (Schaal & Atkeson, 1994). The\nsecond term is a regularization or penalty term. Local cross validation by itself is consistent, i.e., with an increasing amount of data, the size of the receptive field of an expert\nwould shrink to zero. This would require the creation of an ever increasing number of experts during the course of learning. The penalty term introduces some non-vanishing bias\nin each expert such that its receptive field size does not shrink to zero. By penalizing the\nsquared coefficients of D, we are essentially penalizing the second derivatives of the function at the site of the expert. This is similar to the approaches taken in spline fitting\n\n\x0c608\n\nS. SCHAAL, C. C. A TI(ESON\n\n(deBoor, 1978) and acts as a low-pass filter: the higher the second derivatives, the more\nsmoothing (and thus bias) will be introduced. This will be analyzed further in Section 2.2.\nThe update equations for the linear subnet are the standard weighted recursive least squares\nequation with forgetting factor A (Ljung & SOderstrom, 1986):\n\nf3 n+1 =f3n+wpn+lxe\n\n1(\nwherepn+1 =_ pn_\n\nA\n\ncv\'\n\npn- -Tpn )\nxx\nande =(y-x T f3n)\nAjw + xTpnx\ncv\n\n(5)\n\nThis is a Newton method, and it requires maintaining the matrix P, which is size\n0.5d x (d + 1) . The update of the receptive field subnet is a gradient descent in J:\n\nMn+l=Mn- a dJ!aM\n\n(6)\n\nDue to space limitations, the derivation of the derivative in (6) will not be explained here.\nThe major ingredient is to take this derivative as in a batch update, and then to reformulate\nthe result as an iterative scheme. The derivatives in batch mode can be calculated exactly\ndue to the Sherman-Morrison-Woodbury theorem (Belsley, Kuh, & Welsch, 1980; Atkeson, 1992). The derivative for the incremental update is a very good approximation to\nthe batch update and realizes incremental local cross validation.\nA new expert is initialized with a default M de! and all other variables set to zero, except the\nmatrix P. P is initialized as a diagonal matrix with elements 11 r/, where the ri are usually\nsmall quantities, e.g., 0.01. The ri are ridge regression parameters. From a probabilistic\nview, they are Bayesian priors that the f3 vector is the zero vector. From an algorithmic\nview, they are fake data points of the form [x = (0, ... , \'12 ,o, ... l,y = 0] (Atkeson, Moore, &\nSchaal, submitted). Using the update rule (5), the influence of the ridge regression parameters would fade away due to the forgetting factor A. However, it is useful to make the\nridge regression parameters adjustable. As in (6), rj can be updated by gradient descent:\n1\'I n+1\n\n= 1\'n I\n\na aJ/ar\nI\n\n(7)\n\nThere are d ridge regression parameters, one for each diagonal element of the P matrix. In\norder to add in the update of the ridge parameters as well as to compensate for the forgetting factor, an iterative procedure based on (5) can be devised which we omit here. The\ncomputational complexity of this update is much reduced in comparison to (5) since many\ncomputations involve multiplications by zero.\nIn sum, a RFWR expert consists of\nthree sets of parameters, one for\nthe locally linear model, one for\nend;\nthe size and shape of the receptive\nb)\nIr no expert was activated by more than Wgen :\n- create a new expert with c=x\nfields,\nand one for the bias. The\nend;\nc)\nIr two experts are acti vated more than W pn..~\nlinear model parameters are up- erase the expert with the smaller receptive field\ndated by a Newton method, while\nend;\nd)\ncalculate the mean, err""an\' and standard de viation errslIl of the\nthe other parameters are updated\nincrementally accumulated error er,! of all experts;\nby gradient descent. In our implee)\nFor k.= I to #experts:\nIr (Itrr! - err_I> 9 er\'Sld) reinitialize expert k with M = 2 ? Mdef\nmentations, we actually use second\nend;\nend;\norder gradient descent based on\nSutton (1992), since, with minor\nextra effort, we can obtain estimates of the second derivatives of the cost function with respect to all parameters. Finally, the logic of RFWR becomes as shown in the pseudo-code\nabove. Point c) and e) of the algorithm introduce a pruning facility. Pruning takes place either when two experts overlap too much, or when an expert has an exceptionally large\nmean squared error. The latter method corresponds to a simple form of outlier detection.\nLocal optimization of a distance metric always has a minimum for a very large receptive\nfield size. In our case, this would mean that an expert favors global instead of locally linear\nregression. Such an expert will accumulate a very large error which can easily be detected\nInitialize the RFWR network. with no expert;\nFor every new training sample (x,y):\na)\nFor k= I to #experts:\n- calculate the activation from (I)\n- update the expert\'s parameters according to (5), (6), and (7)\n\n\x0cFrom Isolation to Cooperation: An Alternative View of a System of Experts\n\n609\n\nin the given way. The mean squared error term, err, on which this outlier detection is\nbased, is a bias-corrected mean squared error, as will be explained below.\n2.2 ASYMPTOTIC BIAS AND PENALTY SELECTION\nThe penalty term in the cost function (4) introduces bias. In order to assess the asymptotic\nvalue of this bias, the real function f(x) , which is to be learned, is assumed to be represented as a Taylor series expansion at the center of an expert\'s receptive field. Without loss\nof generality, the center is assumed to be at the origin in input space. We furthermore assume that the size and shape of the receptive field are such that terms higher than 0(2) are\nnegligible. Thus, the cost (4) can be written as:\nJ\n\n~ (1w(f. +fTX+~XTFX-bo -bTxYdx )/(1wdx )+r~Dnm\n\n(8)\n\nwhere fo\' f, and F denote the constant, linear, and quadratic terms of the Taylor series\nexpansion, respectively. Inserting Equation (1), the integrals can be solved analytically after the input space is rotated by an orthonormal matrix transforming F to the diagonal matrix F\'. Subsequently, bo\' b, and D can be determined such that J is minimized:\n\nb~ =fa + bias = fa + ~075 ~ sgn(F:\')~IF;,:I,\n0.25\n\n(\n\n)\n\nb\'\n\n= f,\n\nD::\n\n~\n= (2r)2\n\n(9)\n\nThis states that the linear model will asymptotically acquire the correct locally linear\nmodel, while the constant term will have bias proportional to the square root of the sum of\nthe eigenvalues of F, i.e., the n ? The distance metric D, whose diagonalized counterpart\nis D\', will be a scaled image of the Hessian F with an additional square root distortion.\nThus, the penalty term accomplishes the intended task: it introduces more smoothing the\nhigher the curvature at an expert\'s location is, and it prevents the receptive field of an expert shrinking to zero size (which would obviously happen for r ~ 0). Additionally,\nEquation (9) shows how to determine rfor a given learning problem from an estimate of\nthe eigenvalues and a permissible bias. Finally, it is possible to derive estimates of the bias\nand the mean squared error of each expert from the current distance metric D:\n\nF:\n\nbiasesl = ~0. 5r IJeigenvalues(D)l.; en,,~, = r\n\nLn.mD;m\n\n(10)\n\nThe latter term was incorporated in the mean squared error, err, in Section 2.1. Empirical\nevaluations (not shown here) verified the validity of these asymptotic results.\n\n3. SIMULATION RESULTS\nThis section will demonstrate some of the properties of RFWR. In all simulations, the\nthreshold parameters of the algorithm were set to = 3.5, w prune = 0.9, and wmin = 0.1.\nThese quantities determine the overlap of the experts as well as the outlier removal threshold; the results below are not affected by moderate changes in these parameters.\n\ne\n\n3.1 AVOIDING INTERFERENCE\nIn order to test RFWR\'s sensitivity with respect to changes in input data distribution, the\ndata of the example of Figure 1 was partitioned into three separate training sets\n1; = {(x, y, z) 1-1.0 < x < -O.2} , 1; = {(x, y, z) 1-0.4 < x < OA}, 1; = {(x, y, z) I 0.2 < x < 1.0} .\nThese data sets correspond to three overlapping stripes of data, each having about 400 uniformly distributed samples. From scratch, a RFWR network was trained first on I; for 20\nepochs, then on T2 for 20 epochs, and finally on 1; for 20 epochs. The penalty was chosen\nas in the example of Figure 1 to be r = I.e - 7 , which corresponds to an asymptotic bias of\n\n\x0cS. SCHAAL, C. C. ATKESON\n\n610\n\n0.1 at the sharp ridge of the function. The default distance metric D was 50*1, where I is\nthe identity matrix. Figure 3 shows the results of this experiment. Very little interference\ncan be found. The MSE on the test set increased from 0.0025 (of the original experiment of\nFigure 1) to 0.003, which is still an excellent reconstruction of the real function.\ny\n\n0 .5\n-0 . 5\n\n-0.5\n\n(a)\n\n(b)\n\nFigure 3: Reconstructed function after training on (a)\n\n(c)\n\n7;, (b) then\n\n-1\n\n~,(c)\n\nand finally\n\n1;.\n\n3.2 LOCAL FEATURE DETECTION\nThe examples of RFWR given so far did not require ridge regression parameters. Their importance, however, becomes obvious when dealing with locally rank deficient data or with\nirrelevant input dimensions. A learning system should be able to recognize irrelevant input\ndimensions. It is important to note that this cannot be accomplished by a distance metric.\nThe distance metric is only able to decide to what spatial extent averaging over data in a\ncertain dimension should be performed. However, the distance metric has no means to exclude an input dimension. In contrast, bias learning with ridge regression parameters is able\nto exclude input dimensions. To demonstrate this, we added 8 purely noisy inputs\n(N(0,0.3)) to the data drawn from the function of Figure 1. After 30 epochs of training on a\n10000 data point training set, we analyzed histograms of the order of magnitude of the\nridge regression parameters in all 100bias input dimensions over all the 79 experts that had\nbeen generated by the learning algorithm. All experts recognized that the input dimensions\n3 to 8 did not contain relevant information, and correctly increased the corresponding ridge\nparameters to large values. The effect of a large ridge regression parameter is that the associated regression coefficient becomes zero. In contrast, the ridge parameters of the inputs 1,\n2, and the bias input remained very small. The MSE on the test set was 0.0026, basically\nidentical to the experiment with the original training set.\n\n3.3 LEARNING AN INVERSE DYNAMICS MODEL OF A ROBOT ARM\nRobot learning is one of the domains where incremental learning plays an important role. A\nreal movement system experiences data at a high rate, and it should incorporate this data\nimmediately to improve its performance. As learning is task oriented, input distributions\nwill also be task oriented and interference problems can easily arise. Additionally, a real\nmovement system does not sample data from a training set but rather has to move in order\nto receive new data. Thus, training data is always temporally correlated, and learning must\nbe able to cope with this. An example of such a learning task is given in Figure 4 where a\nsimulated 2 DOF robot arm has to learn to draw the figure "8" in two different regions of\nthe work space at a moderate speed (1.5 sec duration). In this example, we assume that the\ncorrect movement plan exists, but that the inverse dynamics model which is to be used to\ncontrol this movement has not been acquired. The robot is first trained for 10 minutes (real\nmovement time) in the region of the lower target trajectory where it performs a variety of\nrhythmic movements under simple PID control. The initial performance of this controller is\nshown in the bottom part of Figure 4a. This training enables the robot to learn the locally\nappropriate inverse dynamics model, a ~6 ~ ~2 continuous mapping. Subsequent per-\n\n\x0cFrom Isolation to Cooperation: An Alternative View of a System of Experts\n0.5\n\nt\n\n0.\'\n\nGralMy\n\n0.\'\n0.2\n0.1\n\n..,.~t\n~.\n\n~\n\n8\n\nZ\n\n8\n\n8\n\n?0.4\n\n(b)\n\n(a)\n\n(0)\n\n~.5\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.!5\n\nFigure 4: Learning to draw the figure "8" with a 2-joint\narm: (a) Performance of a PID controller before learning (the dimmed lines denote the desired trajectories,\nthe solid lines the actual performance); (b) Performance after learning using a PD controller with feedforward commands from the learned inverse model; (c)\nPerformance of the learned controller after training on\nthe upper "8" of (b) (see text for more explanations).\n\n611\n\nformance using this inverse model for\ncontrol is depicted in the bottom part\nof Figure 4b. Afterwards, the same\ntraining takes place in the region of the\nupper target trajectory in order to acquire the inverse model in this part of\nthe world. The figure "8" can then\nequally well be drawn there (upper\npart of Figure 4a,b). Switching back to\nthe bottom part of the work space\n(Figure 4c), the first task can still be\nperformed as before. No interference\nis recognizable. Thus, the robot could\nlearn fast and reliably to fulfill the two\ntasks. It is important to note that the\ndata generated by the training movements did not always have locally full\nrank. All the parameters of RFWR\nwere necessary to acquire the local inverse model appropriately. A total of\n\n39 locally linear experts were generated.\n\n4. DISCUSSION\nWe have introduced an incremental learning algorithm, RFWR, which constructs a network\nof isolated experts for supervised learning of regression tasks. Each expert determines a locally linear model, a local distance metric, and local bias parameters by incrementally\nminimizing a penalized local cross validation error. Our algorithm differs from other local\nlearning techniques by entirely avoiding competition among the experts, and by being\nbased on nonparametric instead of parametric statistics. The resulting properties of RFWR\nare a) avoidance of interference in the case of changing input distributions, b) fast incremental learning by means of Newton and second order gradient descent methods, c) analyzable asymptotic properties which facilitate the selection of the fit parameters, and d) local feature detection and dimensionality reduction. The isolated experts are also ideally\nsuited for parallel implementations. Future work will investigate computationally less\ncostly delta-rule implementations of RFWR, and how well RFWR scales in higher dimensions.\n\n5. REFERENCES\nAtkeson, C. G., Moore, A. W. , & Schaal, S.\n(submitted). "Locally weighted learning." Artificial Intelligence Review.\nAtkeson, C. G. (1992). "Memory-based approaches to\napproximating continuous functions." In: Casdagli, M.,\n& Eubank, S. (Eds.), Nonlinear Modeling and Forecasting, pp.503-521. Addison Wesley.\nBelsley, D. A., Kuh, E., & Welsch, R. E. (1980). Regression diagnostics: Identifying influential data and\nsources ofcollinearity. New York: Wiley.\nCleveland, W. S. (1979). "Robust locally weighted regression and smoothing scatterplots." J. American Stat.\nAssociation, 74, pp.829-836.\nde Boor, C. (1978). A practical guide to splines. New\nYork: Springer.\nHastie, T. J., & Tibshirani, R. J. (1990). Generalized\nadditive models. London: Chapman and Hall.\nJacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton,\nG. E. (1991). "Adaptive mixtures of local experts."\nNeural Computation, 3, pp.79-87.\n\nJordan, M. I., & Jacobs, R. (1994). "Hierarchical mixtures of experts and the EM algorithm." Neural Computation, 6, pp.79-87.\nLjung, L., & S_derstr_m, T. (1986). Theory and practice of recursive identification. Cambridge, MIT Press.\nMcLachlan, G. J., & Basford, K. E. (1988). Mixture\nmodels . New York: Marcel Dekker.\nNadaraya, E. A. (1964). "On estimating regression ."\nTheor. Prob. Appl., 9, pp.141-142.\nSchaal, S., & Atkeson, C. G. (l994b). "Assessing the\nquality of learned local models." In: Cowan, J. ,Tesauro, G., & Alspector, J. (Eds.), Advances in Neural\nInformation Processing Systems 6. Morgan Kaufmann.\nScott, D. W. (1992). Multivariate Density Estimation.\nNew York: Wiley.\nSutton, R. S. (1992). "Gain adaptation beats least\nsquares." In: Proc. of 7th Yale Workshop on Adaptive\nand Learning Systems, New Haven, CT.\nWolpert, D. H. (1990). "Stacked genealization." Los\nAlamos Technical Report LA-UR-90-3460.\n\n\x0c'
p83212
sg183
S'From Isolation to Cooperation:\nAn Alternative View of a System of Experts\nStefan Schaal:!:*\nsschaal@cc.gatech.edu\nhttp://www.cc.gatech.eduifac/Stefan.Schaal\n\nChristopher C. Atkeson:!:\ncga@cc.gatech.edu\nhttp://www.cc.gatech.eduifac/Chris.Atkeson\n\n+College of Computing, Georgia Tech, 801 Atlantic Drive, Atlanta, GA 30332-0280\n\n*ATR Human Infonnation Processing, 2-2 Hikaridai, Seiko-cho, Soraku-gun, 619-02 Kyoto\n\nAbstract\nWe introduce a constructive, incremental learning system for regression\nproblems that models data by means of locally linear experts. In contrast\nto other approaches, the experts are trained independently and do not\ncompete for data during learning. Only when a prediction for a query is\nrequired do the experts cooperate by blending their individual predictions. Each expert is trained by minimizing a penalized local cross validation error using second order methods. In this way, an expert is able to\nfind a local distance metric by adjusting the size and shape of the receptive field in which its predictions are valid, and also to detect relevant input features by adjusting its bias on the importance of individual input\ndimensions. We derive asymptotic results for our method. In a variety of\nsimulations the properties of the algorithm are demonstrated with respect\nto interference, learning speed, prediction accuracy, feature detection,\nand task oriented incremental learning.\n\n1. INTRODUCTION\nDistributing a learning task among a set of experts has become a popular method in computationallearning. One approach is to employ several experts, each with a global domain of\nexpertise (e.g., Wolpert, 1990). When an output for a given input is to be predicted, every\nexpert gives a prediction together with a confidence measure. The individual predictions\nare combined into a single result, for instance, based on a confidence weighted average.\nAnother approach-the approach pursued in this paper-of employing experts is to create\nexperts with local domains of expertise. In contrast to the global experts, the local experts\nhave little overlap or no overlap at all. To assign a local domain of expertise to each expert,\nit is necessary to learn an expert selection system in addition to the experts themselves.\nThis classifier determines which expert models are used in which part of the input space.\nFor incremental learning, competitive learning methods are usually applied. Here the experts compete for data such that they change their domains of expertise until a stable configuration is achieved (e.g., Jacobs, Jordan, Nowlan, & Hinton, 1991). The advantage of\nlocal experts is that they can have simple parameterizations, such as locally constant or locally linear models. This offers benefits in terms of analyzability, learning speed, and robustness (e.g., Jordan & Jacobs, 1994). For simple experts, however, a large number of experts is necessary to model a function. As a result, the expert selection system has to be\nmore complicated and, thus, has a higher risk of getting stuck in local minima and/or of\nlearning rather slowly. In incremental learning, another potential danger arises when the\ninput distribution of the data changes. The expert selection system usually makes either\nimplicit or explicit prior assumptions about the input data distribution. For example, in the\nclassical mixture model (McLachlan & Basford, 1988) which was employed in several local expert approaches, the prior probabilities of each mixture model can be interpreted as\n\n\x0c606\n\nS. SCHAAL. C. C. ATKESON\n\nthe fraction of data points each expert expects to experience. Therefore, a change in input\ndistribution will cause all experts to change their domains of expertise in order to fulfill\nthese prior assumptions. This can lead to catastrophic interference.\nIn order to avoid these problems and to cope with the interference problems during incremental learning due to changes in input distribution, we suggest eliminating the competition among experts and instead isolating them during learning. Whenever some new data is\nexperienced which is not accounted for by one of the current experts, a new expert is created. Since the experts do not compete for data with their peers, there is no reason for them\nto change the location of their domains of expertise. However, when it comes to making a\nprediction at a query point, all the experts cooperate by giving a prediction of the output\ntogether with a confidence measure. A blending of all the predictions of all experts results\nin the final prediction. It should be noted that these local experts combine properties of\nboth the global and local experts mentioned previously. They act like global experts by\nlearning independently of each other and by blending their predictions, but they act like local experts by confining themselves to a local domain of expertise, i.e., their confidence\nmeasures are large only in a local region.\nThe topic of data fitting with structurally simple local models (or experts) has received a\ngreat deal of attention in nonparametric statistics (e.g., Nadaraya, 1964; Cleveland, 1979;\nScott, 1992, Hastie & Tibshirani, 1990). In this paper, we will demonstrate how a nonparametric approach can be applied to obtain the isolated expert network (Section 2.1),\nhow its asymptotic properties can be analyzed (Section 2.2), and what characteristics such\na learning system possesses in terms of the avoidance of interference, feature detection,\ndimensionality reduction, and incremental learning of motor control tasks (Section 3).\n\n2. RECEPTIVE FIELD WEIGHTED REGRESSION\nThis paper focuses on regression problems, i.e., the learning of a map from 9t n ~ 9t m ?\nEach expert in our learning method, Receptive Field Weighted Regression (RFWR), consists of two elements, a locally linear model to represent the local functional relationship,\nand a receptive field which determines the region in input space in which the expert\'s\nknowledge is valid. As a result, a given data set will be modeled by piecewise linear elements, blended together. For 1000 noisy data points drawn from the unit interval of the\nfunction z == max[exp(-10x 2 ),exp(-50l),1.25exp(-5(x 2 + l)], Figure 1 illustrates an\nexample of function fitting with RFWR. This function consists of a narrow and a wide\nridge which are perpendicular to each other, and a Gaussian bump at the origin. Figure 1b\nshows the receptive fields which the system created during the learning process. Each experts\' location is at the center of its receptive field, marked by a $ in Figure 1b. The recep1.5\n0 .5\n0\n-0.5\n\n-1\n1.5\n\n0.5\n,1\n\n,.,\n10. 5%\n\n0\n-0.5\n\n0\n\nI\n1- 0 .5\n1\n\n-1\n\n0\n\n-1.5\n-1.5\n\n- 0 .5\n\n(a)\n\n-1\n\nx\n\n(b)\n\n-1\n\n-0.5\n\no\n\n0.5\n\n1.5\n\nx\n\nFigure 1: (a) result of function approximation with RFWR. (b) contour lines of 0.1 iso-activation of\neach expert in input space (the experts\' centers are marked by small circles).\n\n\x0cFrom Isolation to Cooperation: An Alternative View of a System of Experts\n\n607\n\ntive fields are modeled by Gaussian functions, and their 0.1 iso-activation lines are shown\nin Figure 1b as well. As can be seen, each expert focuses on a certain region of the input\nspace, and the shape and orientation of this region reflects the function\'s complexity, or\nmore precisely, the function\'s curvature, in this region. It should be noticed that there is a\ncertain amount of overlap among the experts, and that the placement of experts occurred on\na greedy basis during learning and is not globally optimal. The approximation result\n(Figure 1a) is a faithful reconstruction of the real function (MSE = 0.0025 on a test set, 30\nepochs training, about 1 minute of computation on a SPARC1O). As a baseline comparison,\na similar result with a sigmoidal 3-layer neural network required about 100 hidden units\nand 10000 epochs of annealed standard backpropagation (about 4 hours on a SPARC1O).\n\n2.1 THE ALGORITHM\n\n. .?... \'.\n\n~"" "\n\nWeighBd\' /\nAverage\n\nOutput\n\nli\'Iear\n\n~:~~\n\nGalng\n\nUnrt\n\nConnectIOn\n\ncentered at e\n\ny,\n\nFigure 2: The RFWR network\n\nRFWR can be sketched in network form as\nshown in Figure 2. All inputs connect to all expert networks, and new experts can be added as\nneeded. Each expert is an independent entity. It\nconsists of a two layer linear subnet and a receptive field subnet. The receptive field subnet has a\nsingle unit with a bell-shaped activation profile,\ncentered at the fixed location c in input space.\nThe maximal output of this unit is "I" at the center, and it decays to zero as a function of the distance from the center. For analytical convenience,\nwe choose this unit to be Gaussian:\n(1)\n\nx is the input vector, and D the distance metric, a positive definite matrix that is generated\nfrom the upper triangular matrix M. The output of the linear subnet is:\n(2)\ny=x Tb + bo=x-Tf3\nA\n\nThe connection strengths b of the linear subnet and its bias bO will be denoted by the d-dimensional vector f3 from now on, and the tilde sign will indicate that a vector has been\naugmented by a constant "I", e.g., i = (x T , Il. In generating the total output, the receptive\nfield units act as a gating component on the output, such that the total prediction is:\n(3)\n\nThe parameters f3 and M are the primary quantities which have to be adjusted in the learning process: f3 forms the locally linear model, while M determines the shape and orientation of the receptive fields . Learning is achieved by incrementally minimizing the cost\nfunction:\n\n(4)\nThe first term of this function is the weighted mean squared cross validation error over all\nexperienced data points, a local cross validation measure (Schaal & Atkeson, 1994). The\nsecond term is a regularization or penalty term. Local cross validation by itself is consistent, i.e., with an increasing amount of data, the size of the receptive field of an expert\nwould shrink to zero. This would require the creation of an ever increasing number of experts during the course of learning. The penalty term introduces some non-vanishing bias\nin each expert such that its receptive field size does not shrink to zero. By penalizing the\nsquared coefficients of D, we are essentially penalizing the second derivatives of the function at the site of the expert. This is similar to the approaches taken in spline fitting\n\n\x0c608\n\nS. SCHAAL, C. C. A TI(ESON\n\n(deBoor, 1978) and acts as a low-pass filter: the higher the second derivatives, the more\nsmoothing (and thus bias) will be introduced. This will be analyzed further in Section 2.2.\nThe update equations for the linear subnet are the standard weighted recursive least squares\nequation with forgetting factor A (Ljung & SOderstrom, 1986):\n\nf3 n+1 =f3n+wpn+lxe\n\n1(\nwherepn+1 =_ pn_\n\nA\n\ncv\'\n\npn- -Tpn )\nxx\nande =(y-x T f3n)\nAjw + xTpnx\ncv\n\n(5)\n\nThis is a Newton method, and it requires maintaining the matrix P, which is size\n0.5d x (d + 1) . The update of the receptive field subnet is a gradient descent in J:\n\nMn+l=Mn- a dJ!aM\n\n(6)\n\nDue to space limitations, the derivation of the derivative in (6) will not be explained here.\nThe major ingredient is to take this derivative as in a batch update, and then to reformulate\nthe result as an iterative scheme. The derivatives in batch mode can be calculated exactly\ndue to the Sherman-Morrison-Woodbury theorem (Belsley, Kuh, & Welsch, 1980; Atkeson, 1992). The derivative for the incremental update is a very good approximation to\nthe batch update and realizes incremental local cross validation.\nA new expert is initialized with a default M de! and all other variables set to zero, except the\nmatrix P. P is initialized as a diagonal matrix with elements 11 r/, where the ri are usually\nsmall quantities, e.g., 0.01. The ri are ridge regression parameters. From a probabilistic\nview, they are Bayesian priors that the f3 vector is the zero vector. From an algorithmic\nview, they are fake data points of the form [x = (0, ... , \'12 ,o, ... l,y = 0] (Atkeson, Moore, &\nSchaal, submitted). Using the update rule (5), the influence of the ridge regression parameters would fade away due to the forgetting factor A. However, it is useful to make the\nridge regression parameters adjustable. As in (6), rj can be updated by gradient descent:\n1\'I n+1\n\n= 1\'n I\n\na aJ/ar\nI\n\n(7)\n\nThere are d ridge regression parameters, one for each diagonal element of the P matrix. In\norder to add in the update of the ridge parameters as well as to compensate for the forgetting factor, an iterative procedure based on (5) can be devised which we omit here. The\ncomputational complexity of this update is much reduced in comparison to (5) since many\ncomputations involve multiplications by zero.\nIn sum, a RFWR expert consists of\nthree sets of parameters, one for\nthe locally linear model, one for\nend;\nthe size and shape of the receptive\nb)\nIr no expert was activated by more than Wgen :\n- create a new expert with c=x\nfields,\nand one for the bias. The\nend;\nc)\nIr two experts are acti vated more than W pn..~\nlinear model parameters are up- erase the expert with the smaller receptive field\ndated by a Newton method, while\nend;\nd)\ncalculate the mean, err""an\' and standard de viation errslIl of the\nthe other parameters are updated\nincrementally accumulated error er,! of all experts;\nby gradient descent. In our implee)\nFor k.= I to #experts:\nIr (Itrr! - err_I> 9 er\'Sld) reinitialize expert k with M = 2 ? Mdef\nmentations, we actually use second\nend;\nend;\norder gradient descent based on\nSutton (1992), since, with minor\nextra effort, we can obtain estimates of the second derivatives of the cost function with respect to all parameters. Finally, the logic of RFWR becomes as shown in the pseudo-code\nabove. Point c) and e) of the algorithm introduce a pruning facility. Pruning takes place either when two experts overlap too much, or when an expert has an exceptionally large\nmean squared error. The latter method corresponds to a simple form of outlier detection.\nLocal optimization of a distance metric always has a minimum for a very large receptive\nfield size. In our case, this would mean that an expert favors global instead of locally linear\nregression. Such an expert will accumulate a very large error which can easily be detected\nInitialize the RFWR network. with no expert;\nFor every new training sample (x,y):\na)\nFor k= I to #experts:\n- calculate the activation from (I)\n- update the expert\'s parameters according to (5), (6), and (7)\n\n\x0cFrom Isolation to Cooperation: An Alternative View of a System of Experts\n\n609\n\nin the given way. The mean squared error term, err, on which this outlier detection is\nbased, is a bias-corrected mean squared error, as will be explained below.\n2.2 ASYMPTOTIC BIAS AND PENALTY SELECTION\nThe penalty term in the cost function (4) introduces bias. In order to assess the asymptotic\nvalue of this bias, the real function f(x) , which is to be learned, is assumed to be represented as a Taylor series expansion at the center of an expert\'s receptive field. Without loss\nof generality, the center is assumed to be at the origin in input space. We furthermore assume that the size and shape of the receptive field are such that terms higher than 0(2) are\nnegligible. Thus, the cost (4) can be written as:\nJ\n\n~ (1w(f. +fTX+~XTFX-bo -bTxYdx )/(1wdx )+r~Dnm\n\n(8)\n\nwhere fo\' f, and F denote the constant, linear, and quadratic terms of the Taylor series\nexpansion, respectively. Inserting Equation (1), the integrals can be solved analytically after the input space is rotated by an orthonormal matrix transforming F to the diagonal matrix F\'. Subsequently, bo\' b, and D can be determined such that J is minimized:\n\nb~ =fa + bias = fa + ~075 ~ sgn(F:\')~IF;,:I,\n0.25\n\n(\n\n)\n\nb\'\n\n= f,\n\nD::\n\n~\n= (2r)2\n\n(9)\n\nThis states that the linear model will asymptotically acquire the correct locally linear\nmodel, while the constant term will have bias proportional to the square root of the sum of\nthe eigenvalues of F, i.e., the n ? The distance metric D, whose diagonalized counterpart\nis D\', will be a scaled image of the Hessian F with an additional square root distortion.\nThus, the penalty term accomplishes the intended task: it introduces more smoothing the\nhigher the curvature at an expert\'s location is, and it prevents the receptive field of an expert shrinking to zero size (which would obviously happen for r ~ 0). Additionally,\nEquation (9) shows how to determine rfor a given learning problem from an estimate of\nthe eigenvalues and a permissible bias. Finally, it is possible to derive estimates of the bias\nand the mean squared error of each expert from the current distance metric D:\n\nF:\n\nbiasesl = ~0. 5r IJeigenvalues(D)l.; en,,~, = r\n\nLn.mD;m\n\n(10)\n\nThe latter term was incorporated in the mean squared error, err, in Section 2.1. Empirical\nevaluations (not shown here) verified the validity of these asymptotic results.\n\n3. SIMULATION RESULTS\nThis section will demonstrate some of the properties of RFWR. In all simulations, the\nthreshold parameters of the algorithm were set to = 3.5, w prune = 0.9, and wmin = 0.1.\nThese quantities determine the overlap of the experts as well as the outlier removal threshold; the results below are not affected by moderate changes in these parameters.\n\ne\n\n3.1 AVOIDING INTERFERENCE\nIn order to test RFWR\'s sensitivity with respect to changes in input data distribution, the\ndata of the example of Figure 1 was partitioned into three separate training sets\n1; = {(x, y, z) 1-1.0 < x < -O.2} , 1; = {(x, y, z) 1-0.4 < x < OA}, 1; = {(x, y, z) I 0.2 < x < 1.0} .\nThese data sets correspond to three overlapping stripes of data, each having about 400 uniformly distributed samples. From scratch, a RFWR network was trained first on I; for 20\nepochs, then on T2 for 20 epochs, and finally on 1; for 20 epochs. The penalty was chosen\nas in the example of Figure 1 to be r = I.e - 7 , which corresponds to an asymptotic bias of\n\n\x0cS. SCHAAL, C. C. ATKESON\n\n610\n\n0.1 at the sharp ridge of the function. The default distance metric D was 50*1, where I is\nthe identity matrix. Figure 3 shows the results of this experiment. Very little interference\ncan be found. The MSE on the test set increased from 0.0025 (of the original experiment of\nFigure 1) to 0.003, which is still an excellent reconstruction of the real function.\ny\n\n0 .5\n-0 . 5\n\n-0.5\n\n(a)\n\n(b)\n\nFigure 3: Reconstructed function after training on (a)\n\n(c)\n\n7;, (b) then\n\n-1\n\n~,(c)\n\nand finally\n\n1;.\n\n3.2 LOCAL FEATURE DETECTION\nThe examples of RFWR given so far did not require ridge regression parameters. Their importance, however, becomes obvious when dealing with locally rank deficient data or with\nirrelevant input dimensions. A learning system should be able to recognize irrelevant input\ndimensions. It is important to note that this cannot be accomplished by a distance metric.\nThe distance metric is only able to decide to what spatial extent averaging over data in a\ncertain dimension should be performed. However, the distance metric has no means to exclude an input dimension. In contrast, bias learning with ridge regression parameters is able\nto exclude input dimensions. To demonstrate this, we added 8 purely noisy inputs\n(N(0,0.3)) to the data drawn from the function of Figure 1. After 30 epochs of training on a\n10000 data point training set, we analyzed histograms of the order of magnitude of the\nridge regression parameters in all 100bias input dimensions over all the 79 experts that had\nbeen generated by the learning algorithm. All experts recognized that the input dimensions\n3 to 8 did not contain relevant information, and correctly increased the corresponding ridge\nparameters to large values. The effect of a large ridge regression parameter is that the associated regression coefficient becomes zero. In contrast, the ridge parameters of the inputs 1,\n2, and the bias input remained very small. The MSE on the test set was 0.0026, basically\nidentical to the experiment with the original training set.\n\n3.3 LEARNING AN INVERSE DYNAMICS MODEL OF A ROBOT ARM\nRobot learning is one of the domains where incremental learning plays an important role. A\nreal movement system experiences data at a high rate, and it should incorporate this data\nimmediately to improve its performance. As learning is task oriented, input distributions\nwill also be task oriented and interference problems can easily arise. Additionally, a real\nmovement system does not sample data from a training set but rather has to move in order\nto receive new data. Thus, training data is always temporally correlated, and learning must\nbe able to cope with this. An example of such a learning task is given in Figure 4 where a\nsimulated 2 DOF robot arm has to learn to draw the figure "8" in two different regions of\nthe work space at a moderate speed (1.5 sec duration). In this example, we assume that the\ncorrect movement plan exists, but that the inverse dynamics model which is to be used to\ncontrol this movement has not been acquired. The robot is first trained for 10 minutes (real\nmovement time) in the region of the lower target trajectory where it performs a variety of\nrhythmic movements under simple PID control. The initial performance of this controller is\nshown in the bottom part of Figure 4a. This training enables the robot to learn the locally\nappropriate inverse dynamics model, a ~6 ~ ~2 continuous mapping. Subsequent per-\n\n\x0cFrom Isolation to Cooperation: An Alternative View of a System of Experts\n0.5\n\nt\n\n0.\'\n\nGralMy\n\n0.\'\n0.2\n0.1\n\n..,.~t\n~.\n\n~\n\n8\n\nZ\n\n8\n\n8\n\n?0.4\n\n(b)\n\n(a)\n\n(0)\n\n~.5\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.!5\n\nFigure 4: Learning to draw the figure "8" with a 2-joint\narm: (a) Performance of a PID controller before learning (the dimmed lines denote the desired trajectories,\nthe solid lines the actual performance); (b) Performance after learning using a PD controller with feedforward commands from the learned inverse model; (c)\nPerformance of the learned controller after training on\nthe upper "8" of (b) (see text for more explanations).\n\n611\n\nformance using this inverse model for\ncontrol is depicted in the bottom part\nof Figure 4b. Afterwards, the same\ntraining takes place in the region of the\nupper target trajectory in order to acquire the inverse model in this part of\nthe world. The figure "8" can then\nequally well be drawn there (upper\npart of Figure 4a,b). Switching back to\nthe bottom part of the work space\n(Figure 4c), the first task can still be\nperformed as before. No interference\nis recognizable. Thus, the robot could\nlearn fast and reliably to fulfill the two\ntasks. It is important to note that the\ndata generated by the training movements did not always have locally full\nrank. All the parameters of RFWR\nwere necessary to acquire the local inverse model appropriately. A total of\n\n39 locally linear experts were generated.\n\n4. DISCUSSION\nWe have introduced an incremental learning algorithm, RFWR, which constructs a network\nof isolated experts for supervised learning of regression tasks. Each expert determines a locally linear model, a local distance metric, and local bias parameters by incrementally\nminimizing a penalized local cross validation error. Our algorithm differs from other local\nlearning techniques by entirely avoiding competition among the experts, and by being\nbased on nonparametric instead of parametric statistics. The resulting properties of RFWR\nare a) avoidance of interference in the case of changing input distributions, b) fast incremental learning by means of Newton and second order gradient descent methods, c) analyzable asymptotic properties which facilitate the selection of the fit parameters, and d) local feature detection and dimensionality reduction. The isolated experts are also ideally\nsuited for parallel implementations. Future work will investigate computationally less\ncostly delta-rule implementations of RFWR, and how well RFWR scales in higher dimensions.\n\n5. REFERENCES\nAtkeson, C. G., Moore, A. W. , & Schaal, S.\n(submitted). "Locally weighted learning." Artificial Intelligence Review.\nAtkeson, C. G. (1992). "Memory-based approaches to\napproximating continuous functions." In: Casdagli, M.,\n& Eubank, S. (Eds.), Nonlinear Modeling and Forecasting, pp.503-521. Addison Wesley.\nBelsley, D. A., Kuh, E., & Welsch, R. E. (1980). Regression diagnostics: Identifying influential data and\nsources ofcollinearity. New York: Wiley.\nCleveland, W. S. (1979). "Robust locally weighted regression and smoothing scatterplots." J. American Stat.\nAssociation, 74, pp.829-836.\nde Boor, C. (1978). A practical guide to splines. New\nYork: Springer.\nHastie, T. J., & Tibshirani, R. J. (1990). Generalized\nadditive models. London: Chapman and Hall.\nJacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton,\nG. E. (1991). "Adaptive mixtures of local experts."\nNeural Computation, 3, pp.79-87.\n\nJordan, M. I., & Jacobs, R. (1994). "Hierarchical mixtures of experts and the EM algorithm." Neural Computation, 6, pp.79-87.\nLjung, L., & S_derstr_m, T. (1986). Theory and practice of recursive identification. Cambridge, MIT Press.\nMcLachlan, G. J., & Basford, K. E. (1988). Mixture\nmodels . New York: Marcel Dekker.\nNadaraya, E. A. (1964). "On estimating regression ."\nTheor. Prob. Appl., 9, pp.141-142.\nSchaal, S., & Atkeson, C. G. (l994b). "Assessing the\nquality of learned local models." In: Cowan, J. ,Tesauro, G., & Alspector, J. (Eds.), Advances in Neural\nInformation Processing Systems 6. Morgan Kaufmann.\nScott, D. W. (1992). Multivariate Density Estimation.\nNew York: Wiley.\nSutton, R. S. (1992). "Gain adaptation beats least\nsquares." In: Proc. of 7th Yale Workshop on Adaptive\nand Learning Systems, New Haven, CT.\nWolpert, D. H. (1990). "Stacked genealization." Los\nAlamos Technical Report LA-UR-90-3460.\n\n\x0cBoosting Decision Trees\n\nHarris Drucker\nAT&T Bell Laboratories\nHolmdel, New Jersey 07733\n\nCorinna Cortes\nAT&T Bell Laboratories\nMurray Hill, New Jersey 07974\n\nAbstract\nA new boosting algorithm of Freund and Schapire is used to improve\nthe performance of decision trees which are constructed using the\ninformation ratio criterion of Quinlan\'s C4.5 algorithm. This boosting\nalgorithm iteratively constructs a series of decision trees, each decision\ntree being trained and pruned on examples that have been filLered by\npreviously trained trees. Examples that have been incorrectly classified\nby the previous trees in the ensemble are resampled with higher\nprobability to give a new probability distribution for the next tree in the\nensemble to train on. Results from optical character recognition\n(OCR), and knowledge discovery and data mining problems show that\nin comparison to single trees, or to trees trained independently, or to\ntrees trained on subsets of the feature space, the boosting ensemble is\nmuch better.\n1 INTRODUCTION\n\nA new boosting algorithm termed AdaBoost by their inventors (Freund and Schapire,\n1995) has advantages over the original boosting algorithm (Schapire, 1990) and a second\nversion (Freund, 1990). The implications of a boosting algorithm is that one can take a\nseries of learning machines (termed weak learners) each having a poor error rate (but no\nworse than .5-y, where y is some small positive number) and combine them to give an\nensemble that has very good performance (termed a strong learner). The first practical\nimplementation of boosting was in OCR (Drucker, 1993, 1994) using neural networks as\nthe weak learners. In a series of comparisons (Bottou, 1994) boosting was shown to be\nsuperior to other techniques on a large OCR problem.\nThe general configuration of AdaBoost is shown in Figure 1. Each box is a decision tree\nbuilt using Quinlans C4.5 algorithm (Quinlan, 1993) The key idea is that each weak\nlearner is trained sequentially. The first weak learner is trained on a set of patterns picked\nrandomly (with replacement) from a training set. After training and pruning, the training\npatterns are passed through this first decision tree. In the two class case the hypothesis hi\nis either class 0 or class 1. Some of the patterns will be in error. The training set for the\n\n\x0c480\n\nH. DRUCKER. C. CORTES\n\nINPUT FEATURES\n\n#1\n\n#2\n\nh1\n\n~l\n\nT\n\n#3\n\nh\n~2\n\n2\n\nh\n~3\n\nhT\n\n~T\n\n3\n\n~)t log (11 ~t )\n\nFIGURE 1. BOOSTING ENSEMBLE\n\n.5\nWEAK LEARNER WEIGHTED\nTRAINING ERROR RATE\n\nw\n\n~\na:\na:\noa:\na:\n\nENSEMBLE TEST ERROR RATE\n\nw\n\nENSEMBLE\nTRAINING\nERROR RATE\nNUMBER OF WEAK LEARNERS\n\nFIGURE 2. INDIVIDUAL WEAK LEARNER ERROR RATE\nAND ENSEMBLE TRAINING AND TEST ERROR RATES\n\n\x0cBoosting Decision Trees\n\n481\n\nsecond weak learner will consist of patterns picked from the training set with higher\nprobability assigned to those patterns the first weak learner classifies incorrectly. Since\npatterns are picked with replacement, difficult patterns are more likely to occur multiple\ntimes in the training set. Thus as we proceed to build each member of the ensemble,\npatterns which are more difficult to classify correctly appear more and more likely. The\ntraining error rate of an individual weak learner tends to grow as we increase the number\nof weak learners because each weak learner is asked to classify progressively more\ndifficult patterns. However the boosting algorithm shows us that the ensemble training\nand test error rate decrease as we increase the number of weak learners. The ensemble\noutput is determined by weighting the hypotheses with the log of (l!~i) where ~ is\nproportional to the weak learner error rate. If the weak learner has good error rate\nperformance, it will contribute significantly to the output, because then 1/ ~ will be large.\nFigure 2 shows the general shape of the curves we would expect. Say we have\nconstructed N weak learners where N is a large number (right hand side of the graph).\nThe N\'th weak learner (top curve) will have a training error rate that approaches .5\nbecause it is trained on difficult patterns and can do only sightly better than guessing.\nThe bottom two curves show the test and training error rates of the ensemble using all N\nweak learners. which decrease as weak learners are added to the ensemble.\n2 BOOSTING\nBoosting arises from the PAC (probably approximately correct) learning model which\nhas as one of its primary interests the efficiency of learning. Schapire was the first one to\nshow that a series of weak learners could be converted to a strong learner. The detailed\nalgorithm is show in Figure 3. Let us call the set of N 1 distinct examples the original\ntraining set. We distinguish the original training set from what we will call the filtered\ntraining set which consists of N 1 examples picked with replacement from the original\ntraining set. Basically each of N 1 original examples is assigned a weight which is\nproportional to the probability that the example will appear in the filtered training set\n(these weights have nothing to do with the weights usually associated with neural\nnetworks). Initially all examples are assigned a weight of unity so that all the examples\nare equally likely to show up in the initial set of training examples. However, the weights\nare altered at each state of boosting (Step 5 of Figure 3) and if the weights are high we\nmay have multiple copies of some of the original examples appearing in the filtered\ntraining set. In step three of this algorithm, we calculate what is called the weighted\ntraining error and this is the error rate over all the original N 1 training examples\nweighted by their current respective probabilities. The algorithms terminates if this error\nrate is .5 (no better than guessing) or zero (then the weights of step 5 do not change).\nAlthough not called for in the original C4.5 algorithm, we also have an original set of\npruning examples which also are assigned weights to form a filtered pruning set and used\nto prune the classification trees constructed using the filtered training set. It is known\n(Mingers, 1989a) that reducing the size of the tree (pruning) improves generalization.\n3 DECISION TREES\nFor our implementation of decision trees, we have a set of features (attributes) that\nspecifies an example along with their classification (we discuss the two-class problem\nprimarily). We pick a feature that based on some criterion, best splits the examples into\ntwo subsets. Each of these two subsets will usually not contain examples of just one\nclass, so we recursively divide the subsets until the final subsets each contain examples of\njust one class. Thus, each internal node specifies a feature and a value for that feature that\ndetermines whether one should take the left or right branch emanating from that node. At\nterminal nodes, we make the final decision, class 0 or 1. Thus, in decision trees one\nstarts at a root node and progressively traverses the tree from the root node to one of the\n\n\x0cH. DRUCKER,C. CORTES\n\n482\n\nInputs: N I training paUans. N 2 pruning paUems. N 3 test paUans\n\nlaitialize the weight veco of the N I training pattems: wI = 1 for i = 1?...? N I\nlaitialize the weight veco of the N 2 pruning paUmls: sl = 1 for i = 1?...?N 2\nlaitialize the number of trees in the ensemble to t = 1\nDo Vatil weighted training enol\' rate is 0 or .5 or ensemble test enoI\'rate asymptotes\n\n1. For the training set and pruning sets\n\nw\'\n\np \' =N.- -\n\nr\'\n\na\'\n\n= -w.-\n\n1:wl\ni-I\n\n1:sl\n\nPick N I samples from original training set with probability P(i) to form filtered training set\nPick N 2 samples from original pruning set with probability r(i) to form filtered pruning set\n\n2. Train tree t using filtered training set and prune using filtered pruning set\n\n3. Pass the N I mginal training examples through the IRJICd tree whose output h, (i) is\neither 0 or 1 and classification c(i) is either 0 or 1. Calculate the weighted training error\nN.\n\n1: pll\n\nrate: E, =\n\nh, (i) - c(i)\n\nI\n\ni-I\n\n4. Set Ii, = 1\n\nE,\n- E,\n\n5. Set the new training weight vectm\' to be\nwI+ 1 = wf{Ii,**(1-lh,(i) - c(i)I?)\n\ni = 1?...?N I\n\nPass the N 2 original pruning paUems through the pruned tree and calculate new pruning\nweight vector:\n\n6. F<r each tree t in the ensemble (total trees 1) ? pass the j\'th test pattern through and\nobtain h, (j) for each t The final hypothesis hr(j) for this pattern:\n\nhr (j)={I.\n\nO.\nDo for each test paUml and calculate the ensemble test enu rate:\n\n7.t=t+l\n\nEnd Vatil\nFigure 3: Boosting Algorithm\n\n\x0cBoosting Decision Trees\n\n483\n\nterminal nodes where a final decision is made. CART (Brei man, 1984) and C4.5\n(Quinlan 1993) are perhaps the two most popular tree building algorithms. Here, C4.5 is\nused. The attraction of trees is that the simplest decision tree can be respecified as a\nseries of rules and for certain potential users this is more appealing than a nonlinear\n"black box" such as a neural network. That is not to say that one can not design trees\nwhere the decision at each node depends on some nonlinear combination of features, but\nthis will not be our implementation.\nOther attractions of decision trees are speed of learning and evaluation. Whether trees are\nmore accurate than other techniques depends on the application domain and the\neffectiveness of the particular implementation. In OCR, our neural networks are more\naccurate than trees but the penalty is in training and evaluation times. In other\napplications which we will discuss later a boosting network of trees is more accurate. As\nan initial example of the power of boosting, we will use trees for OCR of hand written\ndigits. The main rationale for using OCR applications to evaluate AdaBoost is that we\nhave experience in the use of a competing technology (neural networks) and we have\nfrom the National Institute of Standards and Technology (NISn a large database of\n120,000 digits, large enough so we can run multiple experiments. However, we will not\nclaim that trees for OCR have the best error performance.\nOnce the tree is constructed, it is pruned to give hopefully better generalization\nperformance than if the original tree was used. C4.5 uses the original training set for\nwhat is called "pessimistic pruning" justified by the fact that there may not be enough\nextra examples to form a set of pruning examples. However, we prefer to use an\nindependent set of examples to prune this tree. In our case, we have (for each tree in the\nensemble) an independent filtered pruning set of examples whose statistical distribution is\nsimilar to that of the filtered training set. Since the filtering imposed by the previous\nmembers of the ensemble can severely distort the original training distribution, we trust\nthis technique more than pessimistic pruning. In pruning (Mingers, 1989), we pass the\npruning set though the tree recording at each node (including non-terminal nodes) how\nmany errors there would be if the tree was terminated there. Then, for each node (except\nfor terminal nodes), we examine the subtree of that node. We then calculate the number\nof errors that would be obtained if that node would be made a terminal node and compare\nit to the number of errors at the terminal nodes of that subtree. If the number of errors at\nthe root node of this subtree is less than or equal to that of the subtree, we replace the\nsubtree with that node and make it a terminal node. Pruning tends to substantially reduce\nthe size of the tree, even if the error rates are not substantially decreased.\n4 EXPERIMENTS\nIn order to run enough experiments to claim statistical validity we needed a large supply\nof data and few enough features that the information ratio could be determined in a\nreasonable amount of time. Thus we used the 120,000 examples in a NIST database of\ndigits subsampled to give us a IOxlO pixel array (100 features) where the features are\ncontinuous values. We do not claim that OCR is best done by using classification trees\nand certainly not in l00-dimensional space. We used 10,000 training examples, 2000\npruning examples and 2000 test examples for a total of 14,000 examples.\nWe also wanted to test our techniques on a wide range of problems, from easy to hard.\nTherefore, to make the problem reasonably difficult, we assigned class 0 to all digits from\no to 4 (inclusive) and assigned class 1 to the remainder of the digits. To vary the\ndifficulty of the problem, we prefiltered the data to form data sets of difficulty f Think of\nf as the fraction of hard examples generated by passing the 120,000 examples through a\npoorly trained neural network and accepting the misclassified examples with probability f\nand the correctly classified examples with probability 1- f. Thus f = .9 means that the\ntraining set consists of 10,000 examples that if passed through this neural network would\n\n\x0cH.DRUCKER,C. CORTES\n\n484\n\nhave an error rate of .9. Table I compares the boosting performance with single tree\nperformance. Also indicated is the average number of trees required to reach that\nperformance. Overtraining never seems to be a problem for these weak learners, that is,\nas one increases the number of trees, the ensemble test error rate asymptotes and never\nincreases.\nTable 1. For fraction f of difficult examples, the error rate for a single tree and a boosting\nensemble and the number of trees required to reach the error rate for that ensemble.\n\nf\n.1\n.3\n.5\n.7\n.9\n\nsingle\ntree\n\nboosting\ntrees\n\nnumber of\ntrees\n\n12%\n13\n16\n21\n23\n\n3.5%\n4.5\n7.1\n7.7\n8.1\n\n25\n28\n31\n60\n72\n\nWe wanted to compare the boosting ensemble to other techniques for constructing\nensembles using 14,000 examples, holding out 2000 for testing. The problem with\ndecision trees is that invariably, even if the training data is different (but drawn from the\nsame distribution), the features chosen for the first few nodes are usually the same (at\nleast for the OCR data). Thus, different decision surfaces are not created. In order to\ncreate different decision regions for each tree, we can force each decision tree to consider\nanother attribute as the root node, perhaps choosing that attribute from the first few\nattributes with largest information ratio. This is similar to what Kwok and Carter (1990)\nhave suggested but we have many more trees and their interactive approach did not look\nfeasible here. Another technique suggested by T.K. Ho (1992) is to construct independent\ntrees on the same 10,000 examples but randomly striking out the use of fifty of the 100\npossible features. Thus, for each tree, we randomly pick 50 features to construct the tree.\nWhen we use up to ten trees, the results using Ho\'s technique gives similar results to that\nof boosting but the asymptotic performance is far better for boosting. After we had\nperformed these experiments, we learned of a technique termed "bagging" (Breiman,\n1994) and we have yet to resolve the issue of whether bagging or boosting is better.\n5 CONCLUSIONS\nBased on preliminary evidence, it appears that for these applications a new boosting\nalgorithm using trees as weak learners gives far superior performance to single trees and\nany other technique for constructing ensemble of trees. For boosting to work on any\nproblem, one must find a weak learner that gives an error rate of less than 0.5 on the\nfiltered training set. An important aspect of the building process is to prune based on a\nseparate pruning set rather than pruning based on a training set. We have also tried this\ntechnique on knowledge discovery and data mining problems and the results are better\nthan single neural networks.\n\nReferences\nL. Bottou, C. Cortes, J.S. Denker, H. Drucker, I. Guyon, L.D. Jackel, Y. LeCun, U.A.\nMuller, E. Sackinger, P. Simard, and V. Vapnik (1994), "Comparison of Classifier\nMethods: A Case Study in Handwritten Digit Recognition", 1994 International\nConference on Pattern Recognition, Jerusalem.\nL. Breiman, J. Friedman, R.A. Olshen, and C.J. Stone (1984), Classification and\nRegression Trees, Chapman and Hall.\n\n\x0cBoosting Decision Trees\n\n485\n\nL. Breiman, "Bagging Predictors", Technical Report No. 421, Department of Statistics\nUniversity of California, Berkeley, California 94720, September 1994.\nH. Drucker (1994), C. Cortes, LD Jackel, Y. LeCun "Boosting and Other Ensemble\nMethods", Neural Computation, vol 6, no. 6, pp. 1287-1299.\n\nH. Drucker, R.E. Schapire, and P. Simard (1993) "Boosting Performance in Neural\nNetworks", International Journal of Pattern Recognition and Artificial Intelligence, Vol\n7. N04, pp. 705-719.\nY. Freund (1990), "Boosting a Weak Learning Algorithm by Majority", Proceedings of\nthe Third Workshop on Computational Learning Theory, Morgan-Kaufmann, 202-216.\nY. Freund and R.E. Schapire (1995), "A decision-theoretic generalization of on-line\nleaming and an application to boosting", Proceeding of the Second European Conference\non Computational Learning.\nT.K. Ho (1992), A theory of MUltiple Classifier Systems and Its Applications to Visual\nWord Recognition, Doctoral Dissertation, Department of Computer Science, SUNY at\nBuffalo.\nS.W. Kwok and C. Carter (1990), "Multiple Decision Trees", Uncertainty in ArtifiCial\nIntelligence 4, R.D. Shachter, T.S. Levitt, L.N. Kanal, J.F Lemmer (eds) Elsevier Science\nPublishers.\nJ.R. Quinlan (1993), C4.5: Programs For Machine Learning, Morgan Kauffman.\nJ. Mingers (1989), "An Empirical Comparison of Pruning Methods for Decision Tree\nInduction", Machine Learning, 4:227-243.\nR.E. Schapire (1990), The strength of weak learnability, Machine Learning, 5(2):197227.\n\n\x0c'
p83213
sg42
S'767\n\nSELF-ORGANIZATION OF ASSOCIATIVE DATABASE\nAND ITS APPLICATIONS\nHisashi Suzuki and Suguru Arimoto\nOsaka University, Toyonaka, Osaka 560, Japan\nABSTRACT\nAn efficient method of self-organizing associative databases is proposed together with\napplications to robot eyesight systems. The proposed databases can associate any input\nwith some output. In the first half part of discussion, an algorithm of self-organization is\nproposed. From an aspect of hardware, it produces a new style of neural network. In the\nlatter half part, an applicability to handwritten letter recognition and that to an autonomous\nmobile robot system are demonstrated.\n\nINTRODUCTION\nLet a mapping f : X -+ Y be given. Here, X is a finite or infinite set, and Y is another\nfinite or infinite set. A learning machine observes any set of pairs (x, y) sampled randomly\nfrom X x Y. (X x Y means the Cartesian product of X and Y.) And, it computes some\nestimate j : X -+ Y of f to make small, the estimation error in some measure.\nUsually we say that: the faster the decrease of estimation error with increase of the number of samples, the better the learning machine. However, such expression on performance\nis incomplete. Since, it lacks consideration on the candidates of J of j assumed preliminarily. Then, how should we find out good learning machines? To clarify this conception,\nlet us discuss for a while on some types of learning machines. And, let us advance the\nunderstanding of the self-organization of associative database .\n. Parameter Type\nAn ordinary type of learning machine assumes an equation relating x\'s and y\'s with\nparameters being indefinite, namely, a structure of f. It is equivalent to define implicitly a\nset F of candidates of\n(F is some subset of mappings from X to Y.) And, it computes\nvalues of the parameters based on the observed samples. We call such type a parameter\ntype.\nFor a learning machine defined well, if F 3 f, j approaches f as the number of samples\nincreases. In the alternative case, however, some estimation error remains eternally. Thus,\na problem of designing a learning machine returns to find out a proper structure of f in this\nsense.\nOn the other hand, the assumed structure of f is demanded to be as compact as possible\nto achieve a fast learning. In other words, the number of parameters should be small. Since,\nif the parameters are few, some j can be uniquely determined even though the observed\nsamples are few. However, this demand of being proper contradicts to that of being compact.\nConsequently, in the parameter type, the better the compactness of the assumed structure\nthat is proper, the better the learning machine. This is the most elementary conception\nwhen we design learning machines .\n\n1.\n\n. Universality and Ordinary Neural Networks\nNow suppose that a sufficient knowledge on f is given though J itself is unknown. In\nthis case, it is comparatively easy to find out proper and compact structures of J. In the\nalternative case, however, it is sometimes difficult. A possible solution is to give up the\ncompactness and assume an almighty structure that can cover various 1\'s. A combination\nof some orthogonal bases of the infinite dimension is such a structure. Neural networks 1 ,2\nare its approximations obtained by truncating finitely the dimension for implementation.\n\n? American Institute of Physics 1988\n\n\x0c768\nA main topic in designing neural networks is to establish such desirable structures of 1.\nThis work includes developing practical procedures that compute values of coefficients from\nthe observed samples. Such discussions are :flourishing since 1980 while many efficient methods have been proposed. Recently, even hardware units computing coefficients in parallel\nfor speed-up are sold, e.g., ANZA, Mark III, Odyssey and E-1.\nNevertheless, in neural networks, there always exists a danger of some error remaining\neternally in estimating /. Precisely speaking, suppose that a combination of the bases of a\nfinite number can define a structure of 1 essentially. In other words, suppose that F 3 /, or\n1 is located near F. In such case, the estimation error is none or negligible. However, if 1\nis distant from F, the estimation error never becomes negligible. Indeed, many researches\nreport that the following situation appears when 1 is too complex. Once the estimation\nerror converges to some value (> 0) as the number of samples increases, it decreases hardly\neven though the dimension is heighten. This property sometimes is a considerable defect of\nneural networks .\n. Recursi ve Type\nThe recursive type is founded on another methodology of learning that should be as\nfollows. At the initial stage of no sample, the set Fa (instead of notation F) of candidates\nof I equals to the set of all mappings from X to Y. After observing the first sample\n(Xl, Yl) E X x Y, Fa is reduced to Fi so that I(xt) = Yl for any I E F. After observing\nthe second sample (X2\' Y2) E X x Y, Fl is further reduced to F2 so that i(xt) = Yl and\nI(X2) = Y2 for any I E F. Thus, the candidate set F becomes gradually small as observation\nof samples proceeds. The after observing i-samples, which we write\nis one of the most\nlikelihood estimation of 1 selected in fi;. Hence, contrarily to the parameter type, the\nrecursive type guarantees surely that j approaches to 1 as the number of samples increases.\nThe recursive type, if observes a sample (x" yd, rewrites values 1,-l(X),S to I,(x)\'s for\nsome x\'s correlated to the sample. Hence, this type has an architecture composed of a rule\nfor rewriting and a free memory space. Such architecture forms naturally a kind of database\nthat builds up management systems of data in a self-organizing way. However, this database\ndiffers from ordinary ones in the following sense. It does not only record the samples already\nobserved, but computes some estimation of l(x) for any x E X. We call such database an\nassociative database.\nThe first subject in constructing associative databases is how we establish the rule for\nrewri ting. For this purpose, we adap t a measure called the dissimilari ty. Here, a dissimilari ty\nmeans a mapping d : X x X -+ {reals > O} such that for any (x, x) E X x X, d(x, x) > 0\nwhenever l(x) t /(x). However, it is not necessarily defined with a single formula. It is\ndefinable with, for example, a collection of rules written in forms of "if? .. then?? .. "\nThe dissimilarity d defines a structure of 1 locally in X x Y. Hence, even though\nthe knowledge on f is imperfect, we can re:flect it on d in some heuristic way. Hence,\ncontrarily to neural networks, it is possible to accelerate the speed of learning by establishing\nd well. Especially, we can easily find out simple d\'s for those l\'s which process analogically\ninformation like a human. (See the applications in this paper.) And, for such /\'s, the\nrecursive type shows strongly its effectiveness.\nWe denote a sequence of observed samples by (Xl, Yd, (X2\' Y2),???. One of the simplest\nconstructions of associative databases after observing i-samples (i = 1,2,.,,) is as follows.\n\ni\n\ni"\n\nI,\n\nAlgorithm 1. At the initial stage, let So be the empty set. For every i =\n1,2" .. , let i,-l(x) for any x E X equal some y* such that (x*,y*) E S,-l and\n\nd(x, x*) =\n\nmin\n(%,y)ES.-t\n\nd(x, x) .\n\nFurthermore, add (x" y,) to S;-l to produce Sa, i.e., S, = S,_l U {(x"\n\n(1)\n\ny,n.\n\n\x0c769\n\nAnother version improved to economize the memory is as follows.\n\nAlgorithm 2, At the initial stage, let So be composed of an arbitrary element\nin X x Y. For every i = 1,2"", let ii-lex) for any x E X equal some y. such\nthat (x?, y.) E Si-l and\nd(x, x?) =\n\nmin\n\nd(x, x) .\n\n(i,i)ES.-l\n\nFurthermore, if ii-l(Xi) # Yi then let Si = Si-l, or add (Xi, Yi) to Si-l to\nproduce Si, i.e., Si = Si-l U {(Xi, Yi)}\'\nIn either construction, ii approaches to f as i increases. However, the computation time\ngrows proportionally to the size of Si. The second subject in constructing associative\ndatabases is what addressing rule we should employ to economize the computation time. In\nthe subsequent chapters, a construction of associative database for this purpose is proposed.\nIt manages data in a form of binary tree.\n\nSELF-ORGANIZATION OF ASSOCIATIVE DATABASE\nGiven a sample sequence (Xl, Yl), (X2\' Y2), .. " the algorithm for constructing associative\ndatabase is as follows.\n\nAlgorithm 3,\'\n\nStep I(Initialization): Let (x[root], y[root]) = (Xl, Yd. Here, x[.] and y[.] are\nvariables assigned for respective nodes to memorize data.. Furthermore, let t = 1.\nStep 2: Increase t by 1, and put x, in. After reset a pointer n to the root, repeat\nthe following until n arrives at some terminal node, i.e., leaf.\nNotations nand\nd(xt, x[n)), let n\n\nn mean the descendant nodes of n.\n=n. Otherwise, let n =n.\n\nIf d(x" r[n)) ~\n\nStep 3: Display yIn] as the related information. Next, put y, in. If yIn] = y" back\nto step 2. Otherwise, first establish new descendant nodes n and n. Secondly,\nlet\n\n(x[n], yIn))\n(x[n], yIn))\n\n(x[n], yIn)),\n(Xt, y,).\n\n(2)\n(3)\n\nFinally, back to step 2. Here, the loop of step 2-3 can be stopped at any time\nand also can be continued.\nNow, suppose that gate elements, namely, artificial "synapses" that play the role of branching by d are prepared. Then, we obtain a new style of neural network with gate elements\nbeing randomly connected by this algorithm.\n\nLETTER RECOGNITION\nRecen tly, the vertical slitting method for recognizing typographic English letters3 , the\nelastic matching method for recognizing hand written discrete English letters4 , the global\ntraining and fuzzy logic search method for recognizing Chinese characters written in square\nstyleS, etc. are published. The self-organization of associative database realizes the recognition of handwritten continuous English letters.\n\n\x0c770\n\n9 /wn"\n\nNOV\n\n~ ~ ~ -xk :La.t\n\n~~ ~ ~~~\n\ndw1lo\'\n\n~~~~~of~~\n\n~~~ 4,-?~~4Fig. 1. Source document.\n2~~---------------\'\n\nlOO~---------------\'\n\nH\n\no\n\no\nFig. 2. Windowing.\n\n1000\n\n2000\n\n3000\n\n4000\n\nNumber of samples\n\no\n\n1000\n\n2000\n\n3000\n\n4000\n\nNUAlber of sampl es\n\nFig. 3. An experiment result.\n\nAn image scanner takes a document image (Fig. 1). The letter recognizer uses a parallelogram window that at least can cover the maximal letter (Fig. 2), and processes the\nsequence of letters while shifting the window. That is, the recognizer scans a word in a\nslant direction. And, it places the window so that its left vicinity may be on the first black\npoint detected. Then, the window catches a letter and some part of the succeeding letter.\nIf recognition of the head letter is performed, its end position, namely, the boundary line\nbetween two letters becomes known. Hence, by starting the scanning from this boundary\nand repeating the above operations, the recognizer accomplishes recursively the task. Thus\nthe major problem comes to identifying the head letter in the window.\nConsidering it, we define the following.\n? Regard window images as x\'s, and define X accordingly.\n? For a (x, x) E X x X, denote by B a black point in the left area from the boundary on\nwindow image X. Project each B onto window image x. Then, measure the Euclidean\ndistance 6 between fj and a black point B on x being the closest to B. Let d(x, x) be\nthe summation of 6\'s for all black points B\'s on x divided by the number of B\'s.\n? Regard couples of the "reading" and the position of boundary as y\'s, and define Y\naccordingly.\nAn operator teaches the recognizer in interaction the relation between window image and\nreading& boundary with algorithm 3. Precisely, if the recalled reading is incorrect, the\noperator teaches a correct reading via the console. Moreover, if the boundary position is\nincorrect, he teaches a correct position via the mouse.\nFig. 1 shows partially a document image used in this experiment. Fig. 3 shows the\nchange of the number of nodes and that of the recognition rate defined as the relative\nfrequency of correct answers in the past 1000 trials. Speciiications of the window are height\n= 20dot, width = 10dot, and slant angular = 68deg. In this example, the levels of tree\nwere distributed in 6-19 at time 4000 and the recognition rate converged to about 74%.\nExperimentally, the recognition rate converges to about 60-85% in most cases, and to 95% at\na rare case. However, it does not attain 100% since, e.g., "c" and "e" are not distinguishable\nbecause of excessive lluctuation in writing. If the consistency of the x, y-relation is not\nassured like this, the number of nodes increases endlessly (d. Fig. 3). Hence, it is clever to\nstop the learning when the recognition rate attains some upper limit. To improve further\nthe recognition rate, we must consider the spelling of words. It is one of future subjects.\n\n\x0c771\n\nOBSTACLE AVOIDING MOVEMENT\nVarious systems of camera type autonomous mobile robot are reported flourishingly6-1O.\nThe system made up by the authors (Fig. 4) also belongs to this category. Now, in mathematical methodologies, we solve usually the problem of obstacle avoiding movement as\na cost minimization problem under some cost criterion established artificially. Contrarily,\nthe self-organization of associative database reproduces faithfully the cost criterion of an\noperator. Therefore, motion of the robot after learning becomes very natural.\nNow, the length, width and height of the robot are all about O.7m, and the weight is\nabout 30kg. The visual angle of camera is about 55deg. The robot has the following three\nfactors of motion. It turns less than ?30deg, advances less than 1m, and controls speed less\nthan 3km/h. The experiment was done on the passageway of wid th 2.5m inside a building\nwhich the authors\' laboratories exist in (Fig. 5). Because of an experimental intention, we\narrange boxes, smoking stands, gas cylinders, stools, handcarts, etc. on the passage way at\nrandom. We let the robot take an image through the camera, recall a similar image, and\ntrace the route preliminarily recorded on it. For this purpose, we define the following.\n? Let the camera face 28deg downward to take an image, and process it through a low\npass filter. Scanning vertically the filtered image from the bottom to the top, search\nthe first point C where the luminance changes excessively. Then, su bstitu te all points\nfrom the bottom to C for white, and all points from C to the top for black (Fig. 6).\n(If no obstacle exists just in front of the robot, the white area shows the \'\'free\'\' area\nwhere the robot can move around.) Regard binary 32 x 32dot images processed thus\nas x\'s, and define X accordingly.\n? For every (x, x) E X x X, let d(x, x) be the number of black points on the exclusive-or\nimage between x and X.\n? Regard as y\'s the images obtained by drawing routes on images x\'s, and define Y\naccordingly.\nThe robot superimposes, on the current camera image x, the route recalled for x, and\ninquires the operator instructions. The operator judges subjectively whether the suggested\nroute is appropriate or not. In the negative answer, he draws a desirable route on x with the\nmouse to teach a new y to the robot. This opera.tion defines implicitly a sample sequence\nof (x, y) reflecting the cost criterion of the operator.\n\n.::l" !\n-\n\nIibUBe\n\n_. -\n\n22\n\n11\n\nRoan\n\n12\n\n{-\n\n13\n\nStationary uni t\n\nFig. 4. Configuration of\nautonomous mobile robot system.\n\n~\n\nI\n\n,\n\n23\n\n24\n\nNorth\n14\n\nrmbi Ie unit (robot)\n\n-\n\nRoan\n\ny\n\nt\n\nFig. 5. Experimental\nenvironment.\n\n\x0c772\n\nWall\n\nCamera image\n\nPreprocessing\n\nA\n\n::: !fa\n\n?\n\nPreprocessing\n\n0\n\nO\n\nCourse\nsuggest ion\n\n??\n\n..\n\nSearch\n\nA\n\nFig. 6. Processing for\nobstacle avoiding movement.\n\nx\n\nFig. 1. Processing for\nposition identification.\nWe define the satisfaction rate by the relative frequency of acceptable suggestions of\nroute in the past 100 trials. In a typical experiment, the change of satisfaction rate showed\na similar tendency to Fig. 3, and it attains about 95% around time 800. Here, notice that\nthe rest 5% does not mean directly the percentage of collision. (In practice, we prevent the\ncollision by adopting some supplementary measure.) At time 800, the number of nodes was\n145, and the levels of tree were distributed in 6-17.\nThe proposed method reflects delicately various characters of operator. For example, a\nrobot trained by an operator 0 moves slowly with enough space against obstacles while one\ntrained by another operator 0\' brushes quickly against obstacles. This fact gives us a hint\non a method of printing "characters" into machines.\nPOSITION IDENTIFICATION\nThe robot can identify its position by recalling a similar landscape with the position data\nto a camera image. For this purpose, in principle, it suffices to regard camera images and\nposition data as x\'s and y\'s, respectively. However, the memory capacity is finite in actual\ncompu ters. Hence, we cannot but compress the camera images at a slight loss of information.\nSuch compression is admittable as long as the precision of position identification is in an\nacceptable area. Thus, the major problem comes to find out some suitable compression\nmethod.\nIn the experimental environment (Fig. 5), juts are on the passageway at intervals of\n3.6m, and each section between adjacent juts has at most one door. The robot identifies\nroughly from a surrounding landscape which section itself places in. And, it uses temporarily\na triangular surveying technique if an exact measure is necessary. To realize the former task,\nwe define the following .\n? Turn the camera to take a panorama image of 360deg. Scanning horizontally the\ncenter line, substitute the points where the luminance excessively changes for black\nand the other points for white (Fig. 1). Regard binary 360dot line images processed\nthus as x\'s, and define X accordingly.\n? For every (x, x) E X x X, project each black point A on x onto x. And, measure the\nEuclidean distance 6 between A and a black point A on x being the closest to A. Let\nthe summation of 6 be S. Similarly, calculate S by exchanging the roles of x and X.\nDenoting the numbers of A\'s and A\'s respectively by nand n, define\n\n\x0c773\n\nd(x, x) =\n\n~(~\n+ ~).\n2 n\nn\n\n(4)\n\n? Regard positive integers labeled on sections as y\'s (cf. Fig. 5), and define Y accordingly.\nIn the learning mode, the robot checks exactly its position with a counter that is reset periodically by the operator. The robot runs arbitrarily on the passageways within 18m area\nand learns the relation between landscapes and position data. (Position identification beyond 18m area is achieved by crossing plural databases one another.) This task is automatic\nexcepting the periodic reset of counter, namely, it is a kind of learning without teacher.\nWe define the identification rate by the relative frequency of correct recalls of position\ndata in the past 100 trials. In a typical example, it converged to about 83% around time\n400. At time 400, the number of levels was 202, and the levels oftree were distributed in 522. Since the identification failures of 17% can be rejected by considering the trajectory, no\npro blem arises in practical use. In order to improve the identification rate, the compression\nratio of camera images must be loosened. Such possibility depends on improvement of the\nhardware in the future.\nFig. 8 shows an example of actual motion of the robot based on the database for obstacle\navoiding movement and that for position identification. This example corresponds to a case\nof moving from 14 to 23 in Fig. 5. Here, the time interval per frame is about 40sec.\n\n,~. .~ (\n;~"i..\n~\n\n"\n\n"\n\n.\n\n..I\n\nI\n\n?\n?\n\n"\n\nI\'\n.\n\'.1\nt\n\n;\n\ni\n\n-:\n, . . , \'II\n\nFig. 8. Actual motion of the robot.\n\n\x0c774\n\nCONCLUSION\nA method of self-organizing associative databases was proposed with the application to\nrobot eyesight systems. The machine decomposes a global structure unknown into a set of\nlocal structures known and learns universally any input-output response. This framework\nof problem implies a wide application area other than the examples shown in this paper.\nA defect of the algorithm 3 of self-organization is that the tree is balanced well only\nfor a subclass of structures of f. A subject imposed us is to widen the class. A probable\nsolution is to abolish the addressing rule depending directly on values of d and, instead, to\nestablish another rule depending on the distribution function of values of d. It is now under\ninvestigation.\n\nREFERENCES\n1. Hopfield, J. J. and D. W. Tank, "Computing with Neural Circuit: A Model/\'\n\nScience 233 (1986), pp. 625-633.\n2. Rumelhart, D. E. et al., "Learning Representations by Back-Propagating Errors," Nature 323 (1986), pp. 533-536.\n\n3. Hull, J. J., "Hypothesis Generation in a Computational Model for Visual Word\nRecognition," IEEE Expert, Fall (1986), pp. 63-70.\n4. Kurtzberg, J. M., "Feature Analysis for Symbol Recognition by Elastic Matching," IBM J. Res. Develop. 31-1 (1987), pp. 91-95.\n\n5. Wang, Q. R. and C. Y. Suen, "Large Tree Classifier with Heuristic Search and\nGlobal Training," IEEE Trans. Pattern. Anal. & Mach. Intell. PAMI 9-1\n(1987) pp. 91-102.\n6. Brooks, R. A. et al, "Self Calibration of Motion and Stereo Vision for Mobile\nRobots," 4th Int. Symp. of Robotics Research (1987), pp. 267-276.\n7. Goto, Y. and A. Stentz, "The CMU System for Mobile Robot Navigation," 1987\nIEEE Int. Conf. on Robotics & Automation (1987), pp. 99-105.\n8. Madarasz, R. et al., "The Design of an Autonomous Vehicle for the Disabled,"\nIEEE Jour. of Robotics & Automation RA 2-3 (1986), pp. 117-125.\n9. Triendl, E. and D. J. Kriegman, "Stereo Vision and Navigation within Buildings," 1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 1725-1730.\n10. Turk, M. A. et al., "Video Road-Following for the Autonomous Land Vehicle,"\n1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 273-279.\n\n\x0c'
p83214
sg230
S'Neural Control for Nonlinear Dynamic Systems\n\nSsu-Hsin Yu\nDepartment of Mechanical Engineering\nMassachusetts Institute of Technology\nCambridge, MA 02139\nEmail: hsin@mit.edu\n\nAnuradha M. Annaswamy\nDepartment of Mechanical Engineering\nMassachusetts Institute of Technology\nCambridge, MA 02139\nEmail: aanna@mit.edu\n\nAbstract\nA neural network based approach is presented for controlling two distinct\ntypes of nonlinear systems. The first corresponds to nonlinear systems\nwith parametric uncertainties where the parameters occur nonlinearly.\nThe second corresponds to systems for which stabilizing control structures cannot be determined. The proposed neural controllers are shown\nto result in closed-loop system stability under certain conditions.\n\n1\n\nINTRODUCTION\n\nThe problem that we address here is the control of general nonlinear dynamic systems\nin the presence of uncertainties. Suppose the nonlinear dynamic system is described as\nf(x , u , 0) , y = h(x, u, 0) where u denotes an external input, y is the output, x is the\nstate, and 0 is the parameter which represents constant quantities in the system. The control\nobjectives are to stabilize the system in the presence of disturbances and to ensure that\nreference trajectories can be tracked accurately, with minimum delay. While uncertainties\ncan be classified in many different ways, we focus here on two scenarios. One occurs\nbecause the changes in the environment and operating conditions introduce uncertainties\nin the system parameter O. As a result, control objectives such as regulation and tracking,\nwhich may be realizable using a continuous function u = J\'(x, 0) cannot be achieved since\nis unknown. Another class of problems arises due to the complexity of the nonlinear\nfunction f. Even if 0, f and h can be precisely determined, the selection of an appropriate\nJ\' that leads to stabilization and tracking cannot be made in general. In this paper, we\npresent two methods based on neural networks which are shown to be applicable to both\nthe above classes of problems. In both cases, we clearly outline the assumptions made,\nthe requirements for adequate training of the neural network, and the class of engineering\nproblems where the proposed methods are applicable. The proposed approach significantly\nexpands the scope of neural controllers in relation to those proposed in (Narendra and\nParthasarathy, 1990; Levin and Narendra, 1993; Sanner and Slotine, 1992; Jordan and\nRumelhart, 1992).\n\nx=\n\no\n\n\x0cNeural Control for Nonlinear Dynamic Systems\n\n1011\n\nThe first class of problems we shall consider includes nonlinear systems with parametric\nuncertainties. The field of adaptive control has addressed such a problem, and over the\npast thirty years, many results have been derived pertaining to the control of both linear\nand nonlinear dynamic systems (Narendra and Annaswamy, 1989). A common assumption\nin almost all of the published work in this field is that the uncertain parameters occur\nlinearly. In this paper, we consider the control of nonlinear dynamic systems with nonlinear\nparametrizations. We design a neural network based controller that adapts to the parameter\nand show that closed-loop system stability can be achieved under certain conditions. Such\na controller will be referred to as a O-adaptive neural controller. Pertinent results to this\nclass are discussed in section 2.\n\no\n\nThe second class of problems includes nonlinear systems, which despite being completely\nknown, cannot be stabilized by conventional analytical techniques. The obvious method for\nstabilizing nonlinear systems is to resort to linearization and use linear control design methods. This limits the scope of operation of the stabilizing controller. Feedback linearization\nis another method by which nonlinear systems can be stably controlled (lsidori, 1989). This\nhowever requires fairly stringent set of conditions to be satisfied by the functions! and h.\nEven after these conditions are satisfied, one cannot always find a closed-form solution to\nstabilize the system since it is equivalent to solving a set of partial differential equations.\nWe consider in this paper, nonlinear systems, where system models as well as parameters\nare known, but controlIer structures are unknown. A neural network based controller is\nshown to exist and trained so that a stable closed-loop system is achieved. We denote this\nclass of controllers as a stable neural controller. Pertinent results to this class are discussed\nin section 3.\n\n2\n\nO-ADAPTIVE NEURAL CONTROLLER\n\nThe focus of the nonlinear adaptive controller to be developed in this paper is on dynamic\nsystems that can be written in the d-step ahead predictor form as follows:\n\nYt+d\n\n= !r(Wt,Ut,O)\n\n(I)\n\nwhere wi = [Yt," . ,Yt-n+l, Ut-I, \' ", Ut-m-d+l], n ~ I, m ~ 0, d ~ I, m + d = n,\nYI, U I C ~ containing the origin and 8 1 C ~k are open, ir : Y1 x U;n+d x 8 1 - t ~, Yt\nand Ut are the output and the input of the system at time t respectively, and 0 is an unknown\nparameter and occurs nonlinearly in ir.1 The goal is to choose a control input \'It such that\nthe system in (1) is stabilized and the plant output is regulated around zero.\nLetxi\n\n~\n\n[Yt+d-I , \'" , Yt+l , wil T , Am = [e2,"\', en+d-I, 0 , en+d+I,"\', e n +m+2d-2,\n\n0], Bm = [el\' en+d], where e, is an unit vector with the i-th term equal to I. The following\nassumptions are made regarding the system in Eq. (I ).\n(AI) For every 0 E 8\n\n1,\n\nir(O , O, O) = 0 .\n\nCA2) There exist open and convex neighborhoods of the origin Y2 C YI and U2 C U I , an\nopen and convex set 82 C 8 1 , and a function K : 0.2 x Y2 x 8 2 ---> U I such that for\nevery Wt E 0.2 , Yt+d E Y2 and 0 E 8 2 , Eq. (1) can be written as Ut = K(wt, Yt+d, 0),\nwhere 0.2 ~ Y 2\n\nX\n\nu;,,+d-I.\n\n(A3) K is twice differentiable and has bounded first and second derivatives on EI ~ 0. 2 X\nY2 X 8 2 , while ir is differentiable and has a bounded derivative on 0.2 x I{ (E I ) x 8 2 .\n(A4) There exists bg\n\no,BE28 , 11 1Here,\n\n>\n\n?\n\nsuch that for every YI E ir(o.2, K(o.2\' 0 , 8 2), 8 2), W E 0.2 and\n\n(8K(w,y ,O) _ 8K(w,y,9))1 _\nay\nay\nY-\n\nYI\n\n. 8f,(w ,u ,O)\nau\n\nI - I > bg\'\nU-UI\n\nas well as in the following sections, An denotes the n-th product space of the set A .\n\n\x0c1012\n\nS. YU, A. M. ANNASWAMY\n\n(A5) There exist positive definite matrices P and Q of dimensions (n + m + 2d - 2) such\nT\'\n-T T\nT T\nT\nthat x T\nt (AmPAm - P)Xt+ J( BrnPBmK + 2x t ArnPBmK ~ -Xt QXt, where\n[( = [0, K(wt, 0 , O)]T.\nSince the objective is to control the system~n (1) where 0 is unknown, in order to stabilize\nthe output Y at the origin with an estimate Of, we choose the control input as\n\n(2)\n\n2.1\n\nPARAMETER ESTIMATION SCHEME\n\nSuppose the estimation algorithm for updating Ot is defined recursively as /10t ~ OtOt-I = R(Yt,Wt-d,Ut-d,Ot-l) the problem is to determine the function R such that Ot\nconverges to 0 asymptotical1y. In general, R is chosen to depend on Yt, Wt-d, 1?t-d and\nOt-l since they are measurable and contain information regarding O. For example, in the\ncase of linear systems which can be cast in the input predictor form, 1?t = <b[ 0, a wel\\known linear parameter estimation method is to adjust /10 as (Goodwin and Sin, 1984)\n\n/10t = 1+4>\'t~~t-\'/ [1?t-d -\n\n?LdOt-d?\n\nIn other words, the mechanism for carrying out\nparameter estimation is realized by R. In the case of general nonlinear systems, the task\nof determining such a function R is quite difficult, especial\\y when the parameters occur\nnonlinearly. Hence, we propose the use of a neural network parameter estimation algorithm\ndenoted O-adaptive neural network (TANN) (Annaswamy and Yu, 1996). That is, we adjust\nOt as\nif /1Vd, < - f\n(3)\notherwise\nwhere the inputs of the neural network are Yt, Wt-d, 1?t-d and\nf defines a dead-zone where parameter adaptation stops.\n\nOt-I, the output is /10t, and\n\nThe neural network is to be trained so that the resulting network can improve the parameter estimation over time for any possible 0 in a compact set. In addition, the\ntrained network must ensure that the overal1 system in Eqs. (1), (2) and (3) is stable.\nToward this end, N in TANN algorithm is required to satisfy the fol1owing two properties: (PI) IN(Yt,wt - d,1?t - d,Ot-l)12\nfl\n\n>\n\nA Tf\n0 , where L.l.Vt\n--\n\nIiiUt 12\n\n_\n\n~ a(llfJt;~~,~1~2)2uLd\' and (P2) /1Vt -/1Vd, < fl,\n\nIiiUt - I,\n12\n\nii - II _\nUt\n- Ut\n\nII\n\nu,\n\n2+ IC( <f;, _,,)1 1 1?-2\nd, -- -a ( 1+\nIC(\n)12)2 t- d\'\n, 4>,-./\n\nAV,\nL.l.\n\n= (~~\n\n(Wt,Yt+d,O)lo=oo)T, Ut = Ut - K(Wt,Yt+d,Ot+d - I), (fit = [WT,Yt+djT,\na E (0, I) and 00 is the point where K is linearized and often chosen to be the mean value\nC(?t)\n\nof parameter variation.\n\n2.2\n\nTRAINING OF TANN FOR CONTROL\n\nIn the previous section, we proposed an algorithm using a neural network for adjusting\nthe control parameters. We introduced two properties (PI) and (P2) of the identification\nalgorithm that the neural network needs to possess in order to maintain stability of the\nclosed-loop system. In this section, we discuss the training procedure by which the weights\nof the neural network are to be adjusted so that the network retains these properties,\nThe training set is constructed off-line and should compose of data needed in the training phase. If we wan..!. the algorithm in Eq. (3) to be valid on the specified sets Y3 and\nU3 for various 0 and 0 in 83, the training set should cover those variables appearing in\nEq. (3) in their respective ranges. Hence, we first sample W in the set Y;- x U;:+d-I,\n\n\x0c1013\n\nNeural Control for Nonlinear Dynamic Systems\n\nand B, 8 in the set 83. Their values are, say, WI, BI and 81 respectively. For the\nparticular\nand BI we sample 8 again in the set {B E 8 3 1 IB - BII\n181 - BI I},\nand its value is, say, 8t. Once WI, BI , 81 and 8t are sampled, other data can then\nbe calculated, such as UI = K(WI\' 0, 8d and YI = fr(WI, UI, Bd. We can also ob. th ecorrespon d?mg C(A-)\nBK (\nB) All: - -a(I+IC(?I)i2)2\n2+IC(?dI 2 (UI -\'ttl\n~)2 an d\ntam\n\'1\'1 -- ao\nWI , YI, 0, i l d l -\n\nfh\n\n:s:\n\n_\nIC(?IW\nLI - a (1+IC(<I>I)I2)2\n\n~ 2\nUI) ,where\n\n(UI -\n\nelement can then be formed as (Yl\nner, by choosing various w s , Bs ,\n\n- _\n?I -\n\nT\n\n[WI\n\n~\n\nT\n\n_\n\n,yJ} and UI -\n\n~d\nK(WI\' YI,( 1 )?\n\nA data\n\n8t, BI , ~ Vd l , Ld. Proceeding in the same manand 8~ in their respective ranges, we form a typical\n\n,WI ,UI,\n\n1f.\n\ntraining set Ttram = { (Ys , W s, us,1f~ , Bs , ~ Vd d Ls) 11\ns :s: M}, where M denotes the\ntotal number of patterns in the training set. If the quadratic penalty function method (Bertsekas, 1995) is used, properties (PI) and (P2) can be satisfied by training the network on\nthe training set to minimize the following cost function:\n\n:s:\n\nM\n\nmJpl\n\n~ mJ,n~~{(max{0, ~VeJ)2+ ;2 (max{0, IN i(W)1 2 -\n\nL t})2}\n\n(4)\n\nTo find a W which minimizes the above unconstrained cost function 1, we can apply\nalgorithms such as the gradient method and the Gauss-Newton method.\n\n2.3\n\nSTABILITY RESULT\n\nWith the plant given by Eq. (1), the controller by Eq. (2), and the TANN parameter\nestimation algorithm by Eq. (3), it can be shown that the stability of the closed-loop system\nis guaranteed.\nBased on the assumptions of the system in (1) and properties (PI) and (P2) that TANN\nsatisfies, the stability result of the closed-loop system can be concluded in the following\ntheorem. We refer the reader to (Yu and Annaswamy, 1996) for further detail.\n\nTheorem 1 Given the compact sets Y;+ I X U:;+d x 8 3 where the neural network in Eq. (3)\nis trained. There exist EI, E > 0 such that for any interior point B of 8 3 , there exist open\nsets Y4 C Y3, U4 C U3 and a neighborhood 8 4 of B such that if Yo , ... , Yn+d-2 E Y4,\nUo, .. . , U n -2 E U4 , and 8n - l , ... ,8n +d - 2 E 8 4 , then all the signals in the closed-loop\nsystem remain bounded and Yt converges to a neighborhood of the origin.\n2.4\n\nSIMULATION RESULTS\n\nIn this section, we present a simulation example of the TANN controller proposed in this\n.\nTh e system IS\n. 0 f the f orm Yt+1 = I+e\nlIy, ( I-y,)\nh\nB?IS the parameter to be\nsectton.\nU.USH", + Ut, were\ndetermined on-line. Prior information regarding the system is that () E [4, 10]. Based on\n8 (I)\n~\nEq. (2), the controller was chosen to be Ut = - ,y, 0 ,-;\'Y\' \' where Bt denotes the parameter\nI+e - ? ""\nestimate at time t. According to Eq. (3), B was estimated using the TANN algorithm with\ninputs YHI, Yt. Ut and~, and E = 0.01. N is a Gaussian network with 700 centers. The\ntraining set and the testing set were composed of 6,040 and 720 data elements respectively.\nAfter the training was completed, we tested the TANN controller on the system with six\ndifferent values of B, 4.5, 5.5, 6.5, 7.~, 8.5 and 9.5, while the initial parameter estimate\nand the initial output were chosen as BI = 7 and Yo = -0.9 respectively. The results are\nplotted in Figure 1. It can be seen that Yt can be stabilized at the origin for all these values\nof B. For comparison, we also simulated the system under the same conditions but with 8\n\n\x0c1014\n\nS. YU, A. M. ANNASWAMY\n\n~\n\n0\n\n-1\n\n-1\n\n-2\n\n-2\n\no\n\no\n\n10\n\n50\n100\n\n10\n\n50\n\n4\n\n100\n\nFigure 1: Yt (TANN Controller)\n\n4\n\nFigure 2: Yt (Extended Kalman Filter)\n\nestimated using the extended Kalman filter (Goodwin and Sin, 1984). Figure 2 shows the\noutput responses. It is not surprising that for some values of fJ, especially when the initial\nestimation error is large, the responses either diverge or exhibit steady state error.\n\n3\n3.1\n\nSTABLE NEURAL CONTROLLER\nSTATEMENT OF THE PROBLEM\n\nConsider the following nonlinear dynamical system\n\nX= j(x,u),\n\nY = h(x)\n\n(5)\n\nwhere x E Rn and u E RTn. Our goal is to construct a stabilizing neural controller as\nu = N(y; W) where N is a neural network with weights W, and establish the conditions\nunder which the closed-loop system is stable.\nThe nonlinear system in (5) is expressed as a combination of a higher-order linear part and\na nonlinear part as\nA x + Bu + RI (x, u) and y = Cx + R 2 (x), where j(O, O) = 0\nand h(O) = O. We make the following assumptions: (AI) j, h are twice continuously\ndifferentiable and are completely known. (A2) There exists a K such that (A - BKC) is\nasymptotically stable.\n\nx=\n\n3.2 TRAINING OF THE STABLE NEURAL CONTROLLER\nIn order for the neural controller in Section 3.1 to result in an asymptotically stable c1osedloop system, it is sufficient to establish that a continuous positive definite function of the state\nvariables decreases monotonically through output feedback. In other words, if we can find a\nscalar definite function with a negative definite derivative of all points in the state space, we\ncan guarantee stability of the overall system. Here, we limit our choices of the Lyapunov\nfunction candidates to the quadratic form, i.e. V = x T Px, where P is positive definite,\nand the goal is to choose the controller so that V< 0 where V= 2xT P j(x, N(h(x), W)).\nBased on the above idea, we define a "desired" time-derivative Vd as Vd= -xTQx where\nQ = QT > O. We choose P and Q matrices as follows. First, according to (AI), we can\nfind a matrix K to make (A - BKC) asymptotically stable. We can then find a (P, Q)\npair by choosing an arbitrary positive definite matrix Q and solving the Lyapunov equation,\n(A - BKC)T P + P(A - BKC) = -Q to obtain a positive definite P.\n\n\x0c1015\n\nNeural Control for Nonlinear Dynamic Systems\n\nWith the contro\\1er of the form in Section 3.1, the goal is to find W in the neural network\nwhich yields V:::; V d along the trajectories in a neighborhood X C ~n of the origin in the\nstate space. Let Xi denote the value of a sample point where i is an index to the sample\nvariable X E X in the state space. To establish V:::; V d, it is necessary that for every Xl in\na neighborhood X C ~n of the origin, Vi:::;V d" where Vi= 2x;Pf(x l ,N(h(:rl ) , W))\nand\n\nVd, =\n\n-x;\n\nQX i .\n\nThat is, the goal is to find a W such that the inequality constraints\n\ntlVe , :::; 0, where i = 1,??? , M, is satisfied, where tlVe , =V l - V d, and M denotes the\ntotal number of sample points in X. As in the training of TANN controller, this can also\nbe posed as an optimization problem. If the same quadratic penalty function method is\nused, the problem is to find W to minimize the fo\\1owing cost function over the training\nset, which is described as Ttra in = {(Xl\' Yi, V d.)\\l :::; i :::; M}:\n\n1M\n\nrwn J\n\n6.\n\nmJp 2 I: (max{O, tlVe ,})2\n\n(6)\n\ni= 1\n\n3.3 STABILITY OF THE CLOSED-LOOP SYSTEM\nAssum~tions\n\n(A 1) and (A2) imply that a stabilizing controller u = - J( y exists so that\nV = X Px is a candidate Lyapunov function . More genera\\1y, suppose a continuous but\nunknown function ,,((y) exists such that for V = x T Px, a control input 1t = "((y) leads to\nV:::; -x T Qx, then we can find a neural network N (y) which approximates "((y) arbitrarily\nclosely in a compact set leading to closed-loop stability. This is summarized in Theorem 2\n(Yu and Annaswamy, 1995).\n\nTheorem 2 Let there be a continuous function "((h(x)) such that 2xT P f(x , "((h(x))) +\nx T Qx :::; 0 for every X E X where X is a compact set containing the origin as an interior\npoint. Then, given a neighborhood 0 C X of the origin, there exists a neural controlierH =\nN(h(x); W) and a compact set Y E X such that the solutions of\nf(x , N(h(x); W))\nconverge to 0, for every initial condition x(to) E y.\n\nx=\n\n3.4\n\nSIMULATION RESULTS\n\nIn this section, we show simulation results for a discrete-time nonlinear systems using the\nproposed neural network contro\\1er in Section 3, and compare it with a linear contro\\1er to\nillustrate the difference. The system we considered is a second-order nonlinear system Xt =\nf(xt-I , Ut-I), where f = [II, 12]T, h = Xl t _ 1X (1 +X2\'_ 1)+X2t-1 x (l-u t- I +uLI) and\n12 = XT\'_I + 2X2\'_1 +Ut-I (1 + X2\'_I)? It was assumed that X is measurable, and we wished\nto stabilize the system around the origin. The controller is of the form Ht = N (x It, X2 t ).\nThe neural network N used is a Gaussian network with 120 centers. The training set and\nthe testing set were composed of 441 and 121 data elements respectively.\nAfter the training was done, we plotted the actual change of the Lyapunov function, tlV,\nusing the linear controller U = - K x and the neural network controller in Figures 3 and 4\nrespectively. It can be observed from the two figures that if the neural network contro\\1er is\nused, tl V is negative definite except in a small neighborhood of the origin, which assures\nthat the closed-loop system would converge to vicinity of the origin; whereas, if the linear\ncontroller is used, tl V becomes positive in some region away from the origin, which implies\nthat the system can be unstable for some initial conditions. Simulation results confirmed\nour observation.\n\n\x0cS. YU, A. M. ANNASWAMY\n\n1016\n\n-0 01\n\n-0 I\n\n-0 J\n-0 J\n\nFigure 3: ~V(u\n\n= -K x )\n\n-()2\n\n-O J\n\nFigure 4: ~V(u = N(x))\n\nAcknowledgments\nThis work is supported in part by Electrical Power Research Institute under contract No.\n8060-13 and in part by National Science Foundation under grant No. ECS-9296070.\n\nReferences\n[1] A. M. Annaswamy and S. Yu.\n\nO-adaptive neural networks: A new approach to\nparameter estimation. IEEE Transactions on Neural Networks, (to appear) 1996.\n\n[2] D. P. Bertsekas. Nonlinear Programming. Athena Scientific, Belmont, MA, 1995.\n[3] G. C. Goodwin and K. S. Sin. Adaptive Filtering Prediction and Control. PrenticeHall, Inc., 1984.\n[4] A. Isidori. Nonlinear Control Systems. Springer-Verlag, New York, NY, 1989.\n[5] M. L Jordan and D. E. Rumelhart. Forward models: Supervised learning with a distal\nteacher. Cognitive Science , 16:307-354, 1992.\n[6] A. U. Levin and K. S. Narendra. Control of nonlinear dynamical systems using neural\nnetworks: Controllability and stabilization. IEEE Transactions on Neural Networks,\n4(2): 192-206, March 1993.\n[7] K. S. Narendra and A . M. Annaswamy. Stable Adaptive Systems. Prentice-Hall, Inc.,\n1989.\n[8] K. S. Narendra and K. Parthasarathy. Identification and control of dynamical systems\nusing neural networks. IEEE Transactions on Neural Networks, 1(I ):4-26, March\n1990.\n[9] R. M. Sanner and J.-J. E. Slotine. Gaussian networks for direct adaptive control. IEEE\nTransactions on Neural Networks, 3(6):837-863, November 1992.\n[10] S. Yu and A. M. Annaswamy. Adaptive control of nonlinear dynamic systems using\nO-adaptive neural networks. Technical Report 9601 , Adaptive Control Laboratory,\nDepartment of Mechanical Engineering, M.LT., 1996.\n[11] S.-H. Yu and A. M. Annaswamy. Control of nonlinear dynamic systems using a\nstability based neural network approach. In Technical report 9501, Adaptive Control\nLaboratory, MIT, Submitted to Proceedings of the 34th IEEE Conference on Decision\nand Control, New Orleans, LA, 1995.\n\n\x0c'
p83215
sg329
S'Reinforcement Learning by Probability\nMatching\n\nPhilip N. Sabes\n\nMichael I. Jordan\n\nsabes~psyche.mit.edu\n\njordan~psyche.mit.edu\n\nDepartment of Brain and Cognitive Sciences\nMassachusetts Institute of Technology\nCambridge, MA 02139\n\nAbstract\nWe present a new algorithm for associative reinforcement learning. The algorithm is based upon the idea of matching a network\'s\noutput probability with a probability distribution derived from the\nenvironment\'s reward signal. This Probability Matching algorithm\nis shown to perform faster and be less susceptible to local minima\nthan previously existing algorithms. We use Probability Matching to train mixture of experts networks, an architecture for which\nother reinforcement learning rules fail to converge reliably on even\nsimple problems. This architecture is particularly well suited for\nour algorithm as it can compute arbitrarily complex functions yet\ncalculation of the output probability is simple.\n\n1\n\nINTRODUCTION\n\nThe problem of learning associative networks from scalar reinforcement signals is\nnotoriously difficult . Although general purpose algorithms such as REINFORCE\n(Williams, 1992) and Generalized Learning Automata (Phansalkar, 1991) exist, they\nare generally slow and have trouble with local minima. As an example, when we\nattempted to apply these algorithms to mixture of experts networks (Jacobs et al. ,\n1991), the algorithms typically converged to the local minimum which places the\nentire burden of the task on one expert.\nHere we present a new reinforcement learning algorithm which has faster and more\nreliable convergence properties than previous algorithms. The next section describes\nthe algorithm and draws comparisons between it and existing algorithms. The\nfollowing section details its application to Gaussian units and mixtures of Gaussian\nexperts. Finally, we present empirical results.\n\n\x0c1081\n\nReinforcement Learning by Probability Matching\n\n2\n\nREINFORCEMENT PROBABILITY MATCHING\n\nWe begin by formalizing the learning problem. Given an input x E X from the\nenvironment, the network must select an output y E y. The network then receives\na scalar reward signal r, with a mean r and distribution that depend on x and\ny. The goal of the learner is to choose an output which maximizes the expected\nreward. Due to the lack of an explicit error signal, the learner must choose its\noutput stochastically, exploring for better rewards. Typically the learner starts with\na parameterized form for the conditional output density P8(ylx), and the learning\nproblem becomes one of finding the parameters 0 which maximize the expected\nreward:\n\nJr(O) =\n\n1\n\np(x)p8(ylx)r(x, y)dydx.\n\nX,Y\n\nWe present an alternative route to the maximum expected reward cost function,\nand in doing so derive a novel learning rule for updating the network\'s parameters.\nThe learner\'s task is to choose from a set of conditional output distributions based\non the reward it receives from the environment. These rewards can be thought of as\ninverse energies; input/output pairs that receive high rewards are low energy and\nare preferred by the environment. Energies can always be converted into probabilities through the Boltzmann distribution, and so we can define the environment\'s\nconditional distribution on Y given x,\n*( I) exp( _T- 1 E(x, y?\nexp(T-1r(x, y?\npyx =\nZT(X)\n=\nZT(X)\n,\nwhere T is a temperature parameter and ZT(X) is a normalization constant which\ndepends on T . This distribution can be thought of as representing the environment\'s\nideal input-output mapping, high reward input-output pairs being more typical or\nlikely than low reward pairs. The temperature parameter determines the strength of\nthis preference: when T is infinity all outputs are equally likely; when T is zero only\nthe highest reward output is chosen. This new distribution is a purely theoretical\nconstruct, but it can be used as a target distribution for the learner. If the 0 are\nadjusted so that P8(ylx) is nearly equal to p*(ylx), then the network\'s output will\ntypically result in high rewards.\nThe agreement between the network and environment conditional output densities\ncan be measured with the Kullback-Liebler (KL) divergence:\n\nK L(p II P*)\n=\n\n-1\n-~ 1\n=\n\n(1)\n\np(x)p8(ylx) [logp*(Ylx) -logp8(ylx)] dydx\n\nX,Y\n\np(x)p8(ylx)[r(x,y) - Tr8(X,y)]dydx+\n\nX,Y\n\nf p(x)logZT(x)dx,\nJx\n\nwhere r8(x, y) is defined as the logarithm of the conditional output probability and\ncan be thought of as the network\'s estimate of the mean reward. This cost function\nis always greater than or equal to zero, with equality only when the two probability\ndistributions are identical.\nKeeping only the part of Equation 1 which depends on 0, we define the Probability\nMatching (PM) cost function:\n\nJpM(O)\n\n= - Jx,y\nf p(x)p8(ylx)[r(x, y) -\n\nTr8(X, y)] dydx\n\n= -Jr(O) -\n\nTS(P8)\n\nThe PM cost function is analogous to a free energy, balancing the energy, in the\nform of the negative of the average reward, and the entropy S(P8) of the output\n\n\x0cP. N. SABES, M. I. JORDAN\n\n1082\n\n-1\n\n-1\n\nT=.5\n\nT=l\n\nT= .2\n\n-0.5\n\n0\n\n0.5\n\nT= .05\n\nFigure 1: p*\'s (dashed) and PM optimal Gaussians (solid) for the same bimodal reward\nfunction and various temperatures. Note the differences in scale.\n\ndistribution. A higher T corresponds to a smoother target distribution and tilts the\nbalance of the cost function in favor of the entropy term, making diffuse output distributions more favorable. Likewise, a small T results in a sharp target distribution\nplacing most of the weight on the reward dependent term of cost function, which is\nalways optimized by the singular solution of a spike at the highest reward output.\nAlthough minimizing the PM cost function will result in sampling most often at\nhigh reward outputs, it will not optimize the overall expected reward if T > O.\nThere are two reasons for this. First, the output y which maximizes ro(x, y) may\nnot maximize rex, y). Such an example is seen in the first panel of Figure 1:\nthe network\'s conditional output density is a Gaussian with adjustable mean and\nvariance, and the environment has a bimodal reward function and T = 1. Even in\nthe realizable case, however, the network will choose outputs which are suboptimal\nwith respect to its own predicted reward, with the probability of choosing output y\nfalling off exponentially with ro(x, y). The key point here is that early in learning\nthis non-optimality is exactly what is desired. The PM cost function forces the\nlearner to maintain output density everywhere the reward, as measure by p*l/T, is\nnot much smaller than its maximum. When T is high, the rewards are effectively\nflattened and even fairly small rewards look big. This means that a high temperature\nensures that the learner will explore the output space.\nOnce the network is nearly PM optimal, it would be advantageous to "sharpen up"\nthe conditional output distribution, sampling more often at outputs with higher\npredicted rewards. This translates to decreasing the entropy of the output distribution or lowering T. Figure 1 shows how the PM optimal Gaussian changes as the\ntemperature is lowered in the example discussed above; at very low temperatures\nthe output is almost always near the mode of the target distribution. In the limit\nof T = 0, J PM becomes original reward maximization criterion Jr. The idea of the\nProbability Matching algorithm is to begin training with a large T, say unity, and\ngradually decrease it as the performance improves, effectively shifting the bias of\nthe learner from exploration to exploitation.\nWe now must find an update rule for 0 which minimizes JpM(O). We proceed by\nlooking for a stochastic gradient descent step. Differentiating the cost function gives\n\\T OJpM(O) =\n\n-1\n\nX,Y\n\np(x)po(Ylx) [rex, y) - Tro(x, y)] \\T oro(x, y)dydx.\n\nThus, if after every action the parameters are updated by the step\n\nt:.o =\n\na [r - Tro(x, y)] \\T oro (x, y),\n\n(2)\n\nwhere alpha is a constant which can vary over time, then the parameters will on\naverage move down the gradient of the PM cost function. Note that any quantity\n\n\x0cReinforcement Learning by Probability Matching\n\n1083\n\nwhich does not depend on Y or r can be added to the difference in the update rule,\nand the expected step will still point along the direction of the gradient.\nThe form of Equation 2 is similar to the REINFORCE algorithm (Williams, 1992),\nwhose update rule is\nt:.() = a(r - b)V\' elogpe(Ylx),\nwhere b, the reinforcement baseline, is a quantity which does not depend on Y or r.\nNote that these two update rules are identical when T is zero.! The advantage of the\nPM rule is that it allows for an early training phase which encourages exploration\nwithout forcing the output distribution to converge on suboptimal outputs. This\nwill lead to striking qualitative differences in the performance of the algorithm for\ntraining mixtures of Gaussian experts.\n\n3\n\nUPDATE RULES FOR GAUSSIAN UNITS AND\nMIXTURES OF GAUSSIAN EXPERTS\n\nWe employ Gaussian units with mean I\' = w T x and covariance 0"21. The learner\nmust select the matrix wand scalar 0" which minimize JpM(W, 0"). Applying the\nupdate rule in Equation 2, we get\n"\n1\na[r - Tr(x,y)] 2"(Y -I\'?x\n\nt:.w\n\n0"\n\n"\n\na [r - Tr(x, y)]\n\n1 ("Y -I\'W\n\n0"2\n\n0"2\n\n-\n\n1) .\n\nIn practice, for both single Gaussian units and the mixtures presented below we\navoid the issue of constraining 0" > 0 by updating log 0" directly.\nWe can generalize the linear model by considering a conditional output distribution\nin the form of a mixture of Gaussian experts (Jacobs et al., 1991),\nN\n\np(Ylx)\n\n1\n\n1\n\n= Lgi(x)(27r0"1)-~ exp(--2I1y -l\'iW)?\ni=!\n\n20"i\n\nr\n\nExpert i has mean I\'i = w x and covariance 0"[1. The prior probability given x of\nchoosing expert i, gi(X), is determined by a single layer gating network with weight\nmatrix v and softmax output units. The gating network learns a soft partitioning\nof the input space into regions for which each expert is responsible .\nAgain, we can apply Equation 2 to get the PM update rules:\nt:.Vi\n\na [r - Tf(x,y)] (hi - gi)X\n\nt:.Wi\n\na [r - Tf(x, y)]\n\nhi~(Y -\n\nl\'i?X\n\nO"i\n\n6.O"i\n\na[r-Tf(x,Y)]h i\n\n:\n\n1 ("Y~riW\n\n-1),\n\nwhere hi = giPi(ylx)jp(ylx) is the posterior probability of choosing expert i given\ny. We note that the PM update rules are equivalent to the supervised learning\ngradient descent update rules in (Jacobs et al., 1991) modulated by the difference\nbetween the actual and expected rewards.\nlThis fact implies that the REINFORCE step is in the direction of the gradient of JR(B),\nas shown by (Williams, 1992). See Williams and Peng, 1991, for a similar REINFORCE\nplus entropy update rule.\n\n\x0c1084\n\nP. N. SABES, M. I. JORDAN\n\nTable 1: Convergence times and gate entropies for the linear example (standard errors\nin parentheses). Convergence times: An experiment consisting of 50 runs was conducted\nfor each algorithm, with a wide range of learning rates and both reward functions. Best\nresults for each algorithm are reported. Entropy: Values are averages over the last 5,000\ntime steps of each run. 20 runs of 50,000 time steps were conducted.\nAlgorithm\nPM, T= 1\nPM, T=.5\nPM, T =.1\nREINFORCE\nREINF-COMP\n\nI Convergence Time I\n1088 (43)\n-\n\n2998 (183)\n1622 (46)\n\nEntropy\n.993 .0011\n.97 .02\n.48 .04\n.21 .03\n.21 .03\n\nBoth the hi and r depend on the overall conditional probability p(ylx), which in\nturn depends on each Pi(ylx). This adds an extra step to the training procedure.\nAfter receiving the input x, the network chooses an expert based on the priors gi(X)\nand an output y from the selected expert\'s output distribution . The output is then\n. sent back to each of the experts in order to compute the likelihood of their having\ngenerated it. Given the set of Pi\'S, the network can update its parameters as above.\n\n4\n\nSIMULATIONS\n\nWe present three examples designed to explore the behavior of the Probability\nMatching algorithm. In each case, networks were trained using Probability Matching, REINFORCE, and REINFORCE with reinforcement comparison (REINFCOMP), where a running average of the reward is used as a reinforcement baseline (Sutton, 1984). In the first two examples an optimal output function y*(x)\nwas chosen and used to calculate a noisy error, c = Ily - y*(x) - zll, where z was\ni.i.d. zero-mean Gaussian with u = .1. The error signal determined the reward\nby one of two functions, r\n-c 2 /2 or exp( _c 2 /2). When the RMSE between the\nnetwork mean and the optimal output was less that .05 the network was said to\nhave converged.\n\n=\n\n4.1\n\nA Linear Example\n\nIn this example x was chosen uniformly from [-1,1]2 x {I}, and the optimal output\nwas y* = Ax, for a 2 x 3 matrix A. A mixture of three Gaussian experts was trained.\nThe details of the simulation and results for each algorithm are shown in Table 1.\nProbability Matching with constant T\n1 shows almost a threefold reduction\nin training time compared to REINFORCE and about a 50% improvement over\nREINF-COMP.\n\n=\n\nThe important point of this example is the manner in which the extra Gaussian\nunits were employed. We calculated the entropy of the gating network, normalized\nso that a value of one means that each expert has equal probability of being chosen\nand a value of zero means that only one expert is ever chosen. The values after\n50,000 time steps are shown in the second column of Table 1. When T ~ 1, the\nProbability Matching algorithm gives the three experts roughly equal priors. This\nis due to the fact that small differences in the experts\' parameters lead to increased\noutput entropy if all experts are used. REINFORCE on the other hand always\nconverges to a solution which employs only one expert. This difference in the\nbehavior of the algorithms will have a large qualitative effect in the next example.\n\n\x0cReinforcement Learning by Probability Matching\n\nJ085\n\nTable 2: Results for absolute value. The percentage of trials that converged and the\naverage time to convergence for those trials. Standard errors are in parentheses. 50 trials\nwere conducted for a range of learning rates and with both reward functions; the best\nresults for each algorithm are shown.\nAlgorithm\nPM\nREINFORCE\nREINF-COMP\n\nI Successful Trials I Convergence Time I\n100%\n48%\n38%\n\n6,052 313)\n76,775 3,329)\n42,105 3,869)\n\n8 .0\n110\n100\n10\n60\n\n2.0\n\n.0\n0 .0\n\n10\n\n0.0\n\n(a)\n\n1.0\n\n(b)\n\n1 .0\n\n\'.0\n\n\' .0\n\n(c)\n\n\' .0\n\n-2.0\n0.0\n\n1.0\n\n2.0\n\n\'.0\n\n\'.0\n\n(d)\n\nFigure 2: Example 4.3. The environment\'s probability distribution for T = 1: (a) density\nplot of p. vs. y / x, (b) cross-sectional view with Y2 = o. Locally weighted mean and\nvariance of Y2/X over representative runs: (c) T\n\n4.2\n\n= 1,\n\n(d) T\n\n= 0 (i.e.\n\nREINFORCE).\n\nAbsolute Value\n\nWe used a mixture of two Gaussian units to learn the absolute value function.\nThe details of the simulation and the best results for each algorithm are shown in\nTable 2. Probability Matching with constant T = 1 converged to criterion on every\ntrial, in marked contrast to the REINFORCE algorithm. With no reinforcement\nbaseline, REINFORCE converged to criterion in only about half of the cases, less\nwith reinforcement comparison. In almost all of the trials that didn\'t converge, only\none expert was active on the domain of the input. Neither version of REINFORCE\never converged to criterion in less than 14,000 time steps.\nThis example highlights the advantage of the Probability Matching algorithm. During training, all three algorithms initially use both experts to capture the overall\nmean of the data. REINFORCE converges on this local minimum, cutting one\nexpert off before it has a chance to explore the rest of the parameter space. The\nProbability Matching algorithm keeps both experts in use. Here, the more conservative approach leads to a stark improvement in performance.\n4.3\n\nAn Example with Many Local Maxima\n\nIn this example, the learner\'s conditional output distribution was a bivariate Gaussian with It = [Wl, W2]T x, and the environment\'s rewards were a function of y/x.\nThe optimal output distribution p*(y/x) is shown in Figures 2(a,b). These figures\ncan also be interpreted as the expected value of p* for a given w. The weight vector\nis initially chosen from a uniform distribution over [-.2, .2]2, depicted as the very\nsmall while dot in Figure 2(a). There are a series of larger and larger local maxima\noff to the right, with a peak of height 2n at Wl = 2n.\nThe results are shown in Table 3. REINFORCE, both with and without reinforcement comparison, never got past third peak; the variance of the Gaussian unit would\n\n\'.0\n\n\x0cP. N. SABES, M. I. JORDAN\n\n1086\n\nTable 3: Results for Example 4.3. These values represent 20 runs for 50,000 time steps\neach. The first and second columns correspond to number of the peak the learner reached.\nAlgorithm\nPM, T= 2\nPM, T\n1\nPM, T=.5\nREINFORCE\nREINF-COMP\n\n=\n\nMean Final\nlog2 Wl\n28,8\n6.34\n3.06\n2.17\n2.05\n\nRange of Final\nlog2 Wl\'S\n[19.1,51.0]\n5.09,8.08\n3.04,3.07\n2.00,2.90\n2.05,2.06\n\nMean Final\n(T\n\n> 101>\n13.1\n.40\n.019\n.18\n\nvery quickly close down to a small value making further exploration of the output\nspace impossible. Probability Matching, on the other hand, was able to find greater\nand greater maxima, with the variance growing adaptively to match the local scale\nof the reward function. These differences can be clearly seen in Figures 2( c,d),\nwhich show typical behavior for the Probability Matching algorithm with T = 1\nand T O.\n\n=\n\n5\n\nCONCLUSION\n\nWe have presented a new reinforcement learning algorithm for associative networks\nwhich converges faster and more reliably than existing algorithms. The strength of\nthe Probability Matching algorithm is that it allows for a better balance between\nexploration of the output space and and exploitation of good outputs. The parameter T can be adjusted during learning to allow broader output distributions early\nin training and then to force the network to sharpen up its distribution once nearly\noptimal parameters have been found.\nAlthough the applications in this paper were restricted to networks with Gaussian units, the Probability Matching algorithm can be applied to any reinforcement\nlearning task and any conditional output distribution. It could easily be employed,\nfor example, on classification problems using logistic or multinomial (softmax) output units or mixtures of such units. Finally, the simulations presented in this paper\nare of simple examples. Preliminary results indicate that the advantages of the\nProbability Matching algorithm scale up to larger, more interesting problems.\n\nReferences\nJacobs, R . A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. (1991). Adaptive\nmixtures of local experts. Neural Computation, 3:79-87.\nPhansalkar, V. V. (1991). Learning automata algorithms for connectionist systems\n- local and global convergence. PhD Thesis, Dept. of Electrical Engineering,\nIndia Institute of Science, Bangalore.\nSutton, R. S. (1984). Temporal credit assignment in reinforcement learning.\nPhD Thesis, Dept. of Computer and Information Science, University of Massachusetts, Amherst, MA.\nWilliams, R. J. (1992) . Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229-256.\nWilliams, R. J. and Peng, J. (1991). Function optimization using connectionist\nreinforcement learning algorithms. Connection Science, 3:241-268.\n\n\x0c'
p83216
sg32
S'The Geometry of Eye Rotations\nand Listing\'s Law\n\nAmir A. Handzel*\nTamar Flash t\nDepartment of Applied Mathematics and Computer Science\nWeizmann Institute of Science\nRehovot, 76100 Israel\n\nAbstract\nWe analyse the geometry of eye rotations, and in particular\nsaccades, using basic Lie group theory and differential geometry. Various parameterizations of rotations are related through\na unifying mathematical treatment, and transformations between\nco-ordinate systems are computed using the Campbell-BakerHausdorff formula. Next, we describe Listing\'s law by means of\nthe Lie algebra so(3). This enables us to demonstrate a direct\nconnection to Donders\' law, by showing that eye orientations are\nrestricted to the quotient space 80(3)/80(2). The latter is equivalent to the sphere S2, which is exactly the space of gaze directions.\nOur analysis provides a mathematical framework for studying the\noculomotor system and could also be extended to investigate the\ngeometry of mUlti-joint arm movements .\n\n1\n\nINTRODUCTION\n\n1.1\n\nSACCADES AND LISTING\'S LAW\n\nSaccades are fast eye movements, bringing objects of interest into the center of\nthe visual field. It is known that eye positions are restricted to a subset of those\nwhich are anatomically possible, both during saccades and fixation (Tweed & Vilis ,\n1990). According to Donders\' law, the eye\'s gaze direction determines its orientation\nuniquely, and moreover, the orientation does not depend on the history of eye motion\nwhich has led to the given gaze direction . A precise specification of the "allowed"\nsubspace of position is given by Listing\'s law: the observed orientations of the eye\nare those which can be reached from the distinguished orientation called primary\n*hand@wisdom.weizmann.ac.il\nt tamar@wisdom.weizmann.ac.il\n\n\x0c118\n\nA. A. HANDZEL, T. FLASH\n\nposition through a single rotation about an axis which lies in the plane perpendicular\nto the gaze direction at the primary position (Listing\'s plane). We say then that\nthe orientation of the eye has zero torsion. Recently, the domain of validity of\nListing\'s law has been extended to include eye vergence by employing a suitable\nmathematical treatment (Van Rijn & Van Den Berg, 1993).\n\nTweed and Vilis used quaternion calculus to demonstrate, in addition, that in order\nto move from one allowed position to another in a single rotation, the rotation axis\nitself lies outside Listing\'s plane (Tweed & Vilis, 1987). Indeed, normal saccades are\nperformed approximately about a single axis. However, the validity of Listing\'s law\ndoes not depend on the rotation having a single axis, as was shown in double-step\ntarget displacement experiments (Minken, Van Opstal & Van Gisbergen, 1993):\neven when the axis of rotation itself changes during the saccade, Listing\'s law is\nobeyed at each and every point along the trajectory which is traced by the eye.\nPrevious analyses of eye rotations (and in particular of Listing\'s law) have been\nbased on various representations of rotations: quaternions (Westheimer, 1957), rotation vectors (Hepp, 1990), spinors (Hestenes, 1994) and 3 x 3 rotation matrices;\nhowever, they are all related through the same underlying mathematical object the three dimensional (3D) rotation group . In this work we analyse the geometry of\nsaccades using the Lie algebra of the rotation group and the group structure. Next,\nwe briefly describe the basic mathematical notions which will be needed later. This\nis followed by Section 2 in which we analyse various parameterizations of rotations\nfrom the point of view of group theory; Section 3 contains a detailed mathematical\nanalysis of Listing\'s law and its connection to Donders\' law based on the group\nstructure; in Section 4 we briefly discuss the issue of angular velocity vectors or\naxes of rotation ending with a short conclusion.\n\n1.2\n\nTHE ROTATION GROUP AND ITS LIE ALGEBRA\n\nThe group of rotations in three dimensions, G = 80(3), (where \'80\' stands for\nspecial orthogonal transformations) is used both to describe actual rotations and\nto denote eye positions by means of a unique virtual rotation from the primary\nposition. The identity operation leaves the eye at the primary position, therefore,\nwe identify this position with the unit element of the group e E 80(3) . A rotation\ncan be parameterized by a 3D axis and the angle of rotation about it. Each axis\n"generates" a continuous set of rotations through increasing angles . Formally, if n\nis a unit axis of rotation, then\nEXP(O? n)\n(1)\nis a continuous one-parameter subgroup (in G) of rotations through angles () in the\nplane that is perpendicular to n. Such a subgroup is denoted as 80(2) C 80(3).\nWe can take an explicit representation of n as a matrix and the exponent can\nbe calculated as a Taylor series expansion. Let us look, for example, at the one\nparameter subgroup of rotations in the y- z plane, i.e. rotations about the x axis\nwhich is represented in this case by the matrix\n\no\no\n\n(2)\n\n-1\n\nA direct computation of this rotation by an angle () gives\n\no\ncos ()\n- sin ()\n\no\nsin () )\ncos ()\n\n(3)\n\n\x0c119\n\nThe Geometry of Eye Rotations and Listing\'s Law\n\nwhere I is the identity matrix. Thus, the rotation matrix R( 0) can be constructed\nfrom the axis and angle of rotation . The same rotation, however, could also be\nachieved using ALx instead of Lx, where A is any scalar, while rescaling the angle\nto 0/ A. The collection of matrices ALx is a one dimensional linear space whose\nelements are the generators of rotations in the y-z plane.\nThe set of all the generators constitutes the Lie algebra of a group. For the full\nspace of 3D rotations, the Lie algebra is the three dimensional vector space that is\nspanned by the standard orthonormal basis comprising the three direction vectors\nof the principal axes:\n\n(4)\nEvery axis n can be expressed as a linear combination of this basis. Elements of\nthe Lie algebra can also be represented in matrix form and the corresponding basis\nfor the matrix space is\n\nL.=\n\n0 D\n0\n0\n\n-1\n\nL,\n\n=(\n\n0\n0\n\n-1\n\n0\n0\n0\n\nn\n\nL,\n\n=(\n\n~1\n\n1\n0\n0\n\nD;\n\n(5)\n\nhence we have the isomorphism\n\n( -~,\n\n-Oy\n\nOz\n0\n\n-Ox\n\n8, )\nOx\n\n+-------t\n\n0\n\nU: )\n\n(6)\n\nThanks to its linear structure, the Lie algebra is often more convenient for analysis\nthan the group itself. In addition to the linear structure, the Lie algebra has a\nbilinear antisymmetric operation defined between its elements which is called the\nbracket or commutator. The bracket operation between vectors in g is the usual\nvector cross product . When the elements of the Lie algebra are written as matrices ,\nthe bracket operation becomes a commutation relation, i.e.\n[A,B] == AB - BA.\n\n(7)\n\nAs expected, the commutation relations of the basis matrices of the Lie algebra (of\nthe 3D rotation group) are equivalent to the vector product:\n\n(8)\nFinally, in accordance with (1), every rotation matrix is obtained by exponentiation:\n\nR(8) = EXP(OxLx +OyLy +OzLz).\n\n(9)\n\nwhere 8 stands for the three component angles .\n\n2\n\nCO-ORDINATE SYSTEMS FOR ROTATIONS\n\nIn linear spaces the "position" of a point is simply parameterized by the co-ordinates\nw.r.t. the principal axes (a chosen orthonormal basis). For a non-linear space (such\nas the rotation group) we define local co-ordinate charts that look like pieces of\na vector space ~ n. Several co-ordinate systems for rotations are based on the\nfact that group elements can be written as exponents of elements of the Lie algebra (1). The angles 8 appearing in the exponent serve as the co-ordinates.\nThe underlying property which is essential for comparing these systems is the noncommutativity of rotations. For usual real numbers, e.g. Cl and C2, commutativity implies expCI exp C2 = expCI +C2. A corresponding equation for non-commuting\nelements is the Campbell-Baker-Hausdorff formula (CBH) which is a Taylor series\n\n\x0cA. A. HANDZEL. T. FLASH\n\n120\n\nexpansion using repeated commutators between the elements of the Lie algebra.\nThe expansion to third order is (Choquet-Bruhat et al., 1982):\n\nEXP(Xl)EXP(X2) = EXP (Xl\n\n+ X2 + ~[Xl\' X2] + 112 [Xl -\n\nX2, [Xl, X2]])\n\n(10)\n\nwhere Xl, X2 are variables that stand for elements of the Lie algebra.\nOne natural parameterization uses the representation of a rotation by the axis and\nthe angle of rotation. The angles which appear in (9) are then called canonical\nco-ordinates of the first kind (Varadarajan, 1974). Gimbal systems constitute a\nsecond type of parameterization where the overall rotation is obtained by a series\nof consecutive rotations about the principal axes. The component angles are then\ncalled canonical co-ordinates of the second kind. In the present context, the first\ntype of co-ordinates are advantageous because they correspond to single axis rotations which in turn represent natural eye movements. For convenience, we will use\nthe name canonical co-ordinates for those of the first kind, whereas those of the\nsecond type will simply be called gimbals. The gimbals of Fick and Helmholtz are\ncommonly used in the study of oculomotor control (Van Opstal, 1993). A rotation\nmatrix in Fick gimbals is\n\nRF(Bx,Oy,Oz)\n\n= EXP(OzL z )\n\n. EXP(ByLy) . EXP(OxLx),\n\n(11)\n\nand in Helmholtz gimbals the order of rotations is different:\n\nRH(Ox, By,Oz) = EXP(ByLy) . EXP(OzL z ) . EXP(OxLx).\n\n(12)\n\nThe CBH formula (10) can be used as a general tool for obtaining transformations\nbetween various co-ordinate systems (Gilmore, 1974) such as (9,11,12). In particular, we apply (10) to the product of the two right-most terms in (11) and then again\nto the product of the result with the third term. We thus arrive at an expression\nwhose form is the same as the right hand side of (10). By equating it with the\nexpression for canonical angles (9) and then taking the log of the exponents on\nboth sides of the equation, we obtain the transformation formula from Fick angles\nto canonical angles. Repeating this calculation for (12) gives the equivalent formula\nfor Helmholtz angles l . Both transformations are given by the following three equations where OF,H stands for an angle either in Fick or in Helmholtz co-ordinates; for\nHelmholtz angles there is a plus sign in front of the last term of the first equation\nand a minus sign in the case of Fick angles:\n\nBe\n- OF,H\nx x\n\n(1 _ ...L ((BF,H)2 + (OF,H)2)) ? lOF,HOF,H\n12\n\nY\n\nz\n\n2 Y\n\nz\n\nOf\n\n= O:,H ( 1 - /2 (( O;,H)2 + (O:,H)2) ) + ~O;,H O:,H\n\nOf\n\n= O;,H ( 1 - /2 (( B;,H? + (B:,H)2))\n\n(13)\n\n- !O;,H O:,H\n\nThe error caused by the above approximation is smaller than 0.1 degree within most\nof the oculomotor range.\nWe mention in closing two additional parameterizations, namely quaternions and\nrotation vectors. Unit quaternions lie on the 3D sphere S3 (embedded in lR 4) which\nconstitutes the same manifold as the group of unitary rotations SU(2). The latter\nis the double covering group of SO(3) having the same local structure. This enables\nto use quaternions to parameterize rotations. The popular rotation vectors (written\nas tan(Oj2)n, n being the axis of rotation and B its angle) are closely related to\n1 In contrast to this third order expansion, second order approximations usually appear\nin the literature; see for example equation B2 in (Van Rijn & Van Den Berg, 1993).\n\n\x0c121\n\nThe Geometry of Eye Rotations and Listing\'s Law\n\nquaternions because they are central (gnomonic) projections of a hemisphere of S3\nonto the 3D affine space tangent to the quaternion qe = (1,0,0,0) E ]R4 . 2\n\n3\n\nLISTING\'S LAW AND DONDERS\' LAW\n\nA customary choice of a head fixed coordinate system is the following: ex IS III\nthe straight ahead direction in the horizontal plane, e y is in the lateral direction\nand e z points upwards in the vertical direction . ex and e z thus define the midsagittal plane; e y and e z define the coronal plane. The principal axes of rotations\n(Lx, Ly, Lz) are set parallel to the head fixed co-ordinate system. A reference eye orientation called the primary position is chosen with the gaze direction being (1,0,0)\nin the above co-ordinates. How is Listing\'s law expressed in terms of the Lie algebra\nof SO(3)? The allowed positions are generated by linear combinations of Lz and\nLy only. This 2D subspace of the Lie algebra,\n\n1 = Span{Ly, L z },\n\n(14)\n\nis Listing\'s plane. Denoting Span{ Lx} by h, we have a decomposition of the Lie\nalgebra so(3) into a direct sum of two linear subspaces:\n9 = 1 EB h.\n\n(15)\n\nEvery vector v E 9 can be projected onto its component which is in I:\nV\n\n= +\nVI\n\nproj.\nVh ----t VI.\n\n(16)\n\nUntil now, only the linear structure has been considered. In addition, h is closed\nunder the bracket operation:\n(17)\n\nand because h is closed both under vector addition and the Lie bracket, it is a\nsub algebra of g. In contrast, I is not a sub algebra because it is not closed under\ncommutation (8) . The fact that h stands as an algebra on its own implies that it\nhas a corresponding group H, just as 9 = so(3) corresponds to G = SO(3). The\nsubalgebra h generates rotations about the x axis, and therefore H is SO(2), the\ngroup of rotations in a plane.\nThe group G = SO(3) does not have a linear structure. We may still ask whether\nsome kind of decomposition and projection can be achieved in G in analogy to\n(15,16). The answer is positive and the projection is performed as follows: take any\nelement of the group , a E G , and multiply it by all the elements of the subgroup H.\nThis gives a subset in G which is considered as a single object a called a coset:\n\na = {ab I bEH} .\n\n(18)\n\nThe set of all cosets constitutes the quotient space. It is written as\n\nS == G / H = SO(3)/ SO(2)\n\n(19)\n\nbecause mapping the group to the quotient space can be understood as dividing G\nby H. The quotient space is not a group , and this corresponds to the fact that the\nsubspace I above (14) is not a sub algebra. The quotient space has been constructed\nalgebraically but is difficult to visualize; however, it is mathematically equivalent\n2 Geometrically, each point q E S3 can be connected to the center of the sphere by a\nline. Another line runs from qe in the direction parallel to the vector part of q within the\ntangent space. The intersection of the two lines is the projected point. Numerically, one\nsimply takes the vector part of q divided by its scalar part.\n\n\x0cA. A.HANDZEL,T. FLASH\n\n122\n\nTable 1: Summary table of biological notions and the corresponding mathematical\nrepresentation, both in terms of the rotation group and its Lie algebra.\nBiological notion\ngeneral eye position\nprimary position\neye torsion\n"allowed" eye\npositions\n\nLie Algebra\n\n9\n\n= so(3) = h El71\nO.q E 9\nh = Span{Lx}\n\n1= Span{ L y, LzJ\n(Listing\'s plane)\n\nRotation Group\nG = SO(3)\neE G\nH SO(2)\nS ~/H SO(3)/SO(2)\n~ S2 (Donders\' sphere\nof gaze directions)\n\n=\n\n=\n=\n\nto another space - the unit sphere S2 (embedded in ~3). This equivalence can be\nseen in the following way: a unit vector in ~3, e.g. e = (1,0,0), can be rotated so\nthat its head reaches every point on the unit sphere S2; however, for any such point\nthere are infinitely many rotations by which the point can be reached. Moreover,\nall the rotations around the x axis leave the vector e above invariant. We therefore\nhave to "factor out" these rotations (of H =SO(2? in order to eliminate the above\ndegeneracy and to obtain a one-to-one correspondence between the required subset\nof rotations and the sphere. This is achieved by going to the quotient space.\nThe matrix of a torsion less rotation (generated by elements in Listing\'s plane) is\nobtained by setting Ox =0 in (9):\ncosO\nR = ( - sin 0 sin ljJ\n- sin 0 cos ljJ\n\nwhere\n\nsin 0 sin ljJ\ncos 0 + (1 - cos 0) cos 2 ljJ\ncos ljJ sin ljJ(l - cos 0)\n\nsin 0 cos ljJ\n)\ncos ljJ sin ljJ(l - cos 0)\n,(20)\n2\ncos 0 + (1 - cos 0) sin ljJ\n\n0= .)0;+0; is the total angle of rotation and ljJ is the angle between 0and\n\nthe y axis in the Oy -Oz plane, i.e. (0, ljJ) are polar co-ordinates in Listing\'s plane.\nNotice that the first column on the left constitutes the Cartesian co-ordinates of a\npoint on a sphere of unit radius (Gilmore, 1974).\nAs we have just seen, there is an exact correspondence between the group level and\nthe Lie algebra level. In fact, the two describe the same reality, the former in a\nglobal manner and the latter in an infinitesimal one. Table 1 summarizes the important biological notions concerning Listing\'s law together with their corresponding\nmathematical representations. The connection between Donders\' law and Listing\'s\nlaw can now be seen in a clear and intuitive way. The sphere, which was obtained\nby eliminating torsion, is the space of gaze directions. Recall that Donders\' law\nstates that the orientation of the eye is determined uniquely by its gaze direction.\nListing\'s law implies that we need only take into consideration the gaze direction\nand disregard torsion. In order to emphasize this point, we use the fact that locally,\nSO(3) looks like a product of topological spaces: 3\np =\n\nu x SO(2)\n\nwhere\n\n(21)\n\nU parameterizes gaze direction and SO(2) - torsion. Donders\' law restricts eye\norientation to an unknown 2D submanifold of the product space P. Listing\'s law\nshows that the submanifold is U, a piece of the sphere. This representation is\nadvantageous for biological modelling, because it mathematically sets apart the\ndegrees of freedom of gaze orientation from torsion, which also differ functionally.\n350(3) is a principal bundle over S2 with fiber 50(2).\n\n\x0cThe Geometry of Eye Rotations and Listing\'s Law\n\n4\n\n123\n\nAXES OF ROTATION FOR LISTING\'S LAW\n\nAs mentioned in the introduction, moving between two (non-primary) positions\nrequires a rotation whose axis (i.e. angular velocity vector) lies outside Listing\'s\nplane. This is a result of the group structure of SO(3). Had the axis of rotation\nbeen contained within Listing\'s plane, the matrices of the quotient space (20) should\nhave been closed under multiplication so as to form a subgroup of SO(3). In other\nwords, if ri and rJ are matrices representing the current and target orientations of\nthe eye corresponding to axes in Listing\'s plane, then rJ . r;l should have been a\nmatrix of the same form (20); however, as explained in Section 3, this condition is\nnot fulfilled.\nFinally, since normal saccades involve rotations about a single axis, they are oneparameter subgroups generated by a single element of the Lie algebra (1). In addition, they have the property of being geodesic curves in the group manifold under\nthe natural metric which is given by the bilinear Cartan-Killing form of the group\n(Choquet-Bruhat et al., 1982).\n\n5\n\nCONCLUSION\n\nWe have analysed the geometry of eye rotations using basic Lie group theory and\ndifferential geometry. The unifying view presented here can serve to improve the\nunderstanding of the oculomotor system. It may also be extended to study the\nthree dimensional rotations of the joints of the upper limb.\nAcknowledgements\n\nWe would like to thank Stephen Gelbart, Dragana Todoric and Yosef Yomdin for\ninstructive conversations on the mathematical background and Dario Liebermann\nfor fruitful discussions. Special thanks go to Stan Gielen for conversations which\ninitiated this work.\nReferences\n\nChoquet-Bruhat Y., De Witt-Morette C. & Dillard-Bleick M., Analysis, Manifolds\nand Physics, North-Holland (1982).\nGilmore R.,LieGroups, Lie Algebras, and Some of Their Applications, Wiley (1974).\nHepp K., Commun. Math. Phys. 132 (1990) 285-292.\nHestenes D., Neural Networks 7, No.1 (1994) 65-77.\nMinken A.W.H. Van Opstal A.J. & Van Gisbergen J.A .M., Exp. Brain Research\n93 (1993) 521-533.\nTweed, D. & Vilis T., J. Neurophysiology 58 (1987) 832-849.\nTweed D. & Vilis T., Vision Research 30 (1990) 111-127.\nVan Opstal J., "Representations of Eye Positions in Three Dimensions", in Multisensory Control of Movement, ed. Berthoz A., (1993) 27-4l.\nVan Rijn L.J. & Van Den Berg A.V., Vision Research 33, No. 5/6 (1993) 691-708.\nVaradarajan V.S., Lie Groups, Lie Algebras, and Their Reps., Prentice-Hall (1974).\nWestheimer G., Journal of the Optical Society of America 47 (1957) 967-974.\n\n\x0c'
p83217
sg318
S'Empirical Entropy Manipulation for\nReal-World Problems\nPaul Viola: Nicol N. Schraudolph, Terrence J. Sejnowski\nComputational Neurobiology Laboratory\nThe Salk Institute for Biological Studies\n10010 North Torrey Pines Road\nLa Jolla, CA 92037-1099\nviola@salk.edu\n\nAbstract\nNo finite sample is sufficient to determine the density, and therefore\nthe entropy, of a signal directly. Some assumption about either the\nfunctional form of the density or about its smoothness is necessary.\nBoth amount to a prior over the space of possible density functions.\nBy far the most common approach is to assume that the density\nhas a parametric form.\nBy contrast we derive a differential learning rule called EMMA\nthat optimizes entropy by way of kernel density estimation. Entropy and its derivative can then be calculated by sampling from\nthis density estimate. The resulting parameter update rule is surprisingly simple and efficient.\nWe will show how EMMA can be used to detect and correct corruption in magnetic resonance images (MRI). This application is\nbeyond the scope of existing parametric entropy models.\n\n1\n\nIntroduction\n\nInformation theory is playing an increasing role in unsupervised learning and visual\nprocessing. For example, Linsker has used the concept of information maximization\nto produce theories of development in the visual cortex (Linsker, 1988). Becker and\nHinton have used information theory to motivate algorithms for visual processing\n(Becker and Hinton, 1992). Bell and Sejnowski have used information maximization\n? Author to whom correspondence should be addressed. Current address: M.LT., 545\nTechnology Square, Cambridge, MA 02139.\n\n\x0cP. VIOLA, N. N. SCHRAUDOLPH, T. J. SEJNOWSKI\n\n852\n\nto solve the "cocktail party" or signal separation problem (Bell and Sejnowski,\n1995). In order to simplify analysis and implementation, each of these techniques\nmakes specific assumptions about the nature of the signals used, typically that the\nsignals are drawn from some parametric density. In practice, such assumptions are\nvery inflexible.\nIn this paper we will derive a procedure that can effectively estimate and manipulate the entropy of a wide variety of signals using non-parametric densities. Our\ntechnique is distinguished by is simplicity, flexibility and efficiency.\nWe will begin with a discussion of principal components analysis (PCA) as an example of a simple parametric entropy manipulation technique. After pointing out some\nof PCA\'s limitation, we will then derive a more powerful non-parametric entropy\nmanipulation procedure. Finally, we will show that the same entropy estimation\nprocedure can be used to tackle a difficult visual processing problem.\n1.1\n\nParametric Entropy Estimation\n\nTypically parametric entropy estimation is a two step process. We are given a\nparametric model for the density of a signal and a sample. First, from the space\nof possible density functions the most probable is selected. This often requires a\nsearch through parameter space. Second, the entropy of the most likely density\nfunction is evaluated.\nParametric techniques can work well when the assumed form of the density matches\nthe actual data. Conversely, when the parametric assumption is violated the resulting algorithms are incorrect. The most common assumption, that the data follow the\nGaussian density, is especially restrictive. An entropy maximization technique that\nassumes that data is Gaussian, but operates on data drawn from a non-Gaussian\ndensity, may in fact end up minimizing entropy.\n1.2\n\nExample: Principal Components Analysis\n\nThere are a number of signal processing and learning problems that can be formulated as entropy maximization problems. One prominent example is principal component analYllill (PCA). Given a random variable X, a vector v can be used to define\na new random variable, Y" = X . v with variance Var(Y,,) = E[(X . v - E[X . v])2].\nThe principal component v is the unit vector for which Var(Yv) is maximized.\nIn practice neither the density of X nor Y" is known. The projection variance is\ncomputed from a finite sample, A, of points from X,\nVar(Y,,) ~ Var(Y,,) == EA[(X . v - EA[X . v])2] ,\n(1)\nA\n\nwhere VarA(Y,,) and E A [?] are shorthand for the empirical variance and mean evaluated over A. Oja has derived an elegant on-line rule for learning v when presented\nwith a sample of X (Oja, 1982).\nUnder the assumption that X is Gaussian is is easily proven that Yv has maximum\nentropy. Moreover, in the absence of noise, Yij, contains maximal information about\nX. However, when X is not Gaussian Yij is generally not the most informative\nprojection.\n\n2\n\nEstimating Entropy with Parzen Densities\n\nWe will now derive a general procedure for manipulating and estimating the entropy\nof a random variable from a sample. Given a sample of a random variable X, we can\n\n\x0cEmpirical Entropy Manipulation for Real-world Problems\n\n853\n\nconstruct another random variable Y = F(X,l1). The entropy, heY), is a function of\nv and can be manipulated by changing 11. The following derivation assumes that Y is\na vector random variable. The joint entropy of a two random variables, h(Wl\' W2),\ncan be evaluated by constructing the vector random variable, Y = [Wl\' w2 jT and\nevaluating heY).\nRather than assume that the density has a parametric form, whose parameters are\nselected using maximum likelihood estimation, we will instead use Parzen window\ndensity estimation (Duda and Hart, 1973). In the context of entropy estimation, the\nParzen density estimate has three significant advantages over maximum likelihood\nparametric density estimates: (1) it can model the density of any signal provided\nthe density function is smooth; (2) since the Parzen estimate is computed directly\nfrom the sample, there is no search for parameters; (3) the derivative of the entropy\nof the Parzen estimate is simple to compute.\nThe form of the Parzen estimate constructed from a sample A is\n\np.(y, A)\n\n= ~A\n\nI: R(y -\n\nYA)\n\n= EA[R(y -\n\n(2)\n\nYA)] ,\n\nYAEA\nwhere the Parzen estimator is constructed with the window function R(?) which\nintegrates to 1. We will assume that the Parzen window function is a Gaussian\ndensity function. This will simplify some analysis, but it is not necessary. Any\ndifferentiable function could be used. Another good choice is the Cauchy density.\nUnfortunately evaluating the entropy integral\n\nhey)\n\n~ -E[log p.(~, A)] =\n\n-\n\ni:\n\nlog p.(y, A)dy\n\nis inordinately difficult. This integral can however be approximated as a sample\nmean:\n\n(3)\nwhere EB{ ] is the sample mean taken over the sample B. The sample mean\nconverges toward the true expectation at a rate proportional to 1/ v\'N B (N B is\nthe size of B). To reiterate, two samples can be used to estimate the entropy of a\ndensity: the first is used to estimate the density, the second is used to estimate the\nentropyl. We call h? (Y) the EMMA estimate of entropy2.\nOne way to extremize entropy is to use the derivative of entropy with respect to v.\nThis may be expressed as\n\n~h(Y) ~ ~h?(Y) =\ndl1\n\ndv\n\n__1_ \' " LYAEA f;gt/J(YB - YA)\nN B YBE\nL....iB Ly A EA gt/J(YB - YA)\n\n1\n= NB\n\nI: I: Wy (YB , YA) dl1d "21 Dt/J(YB -\n\n(4)\n\nYA),\n\n(5)\n\nYBEB YAEA\n_\n\nwhere WY(Yl\' Y2) = L\n\ngt/J(Yl - Y2)\n(\n) ,\nYAEA gt/J Yl - YA\n\n(6)\n\nDt/J(Y) == yT.,p-ly, and gt/J(Y) is a multi-dimensional Gaussian with covariance .,p.\nWy(Yl\' Y2) is an indicator of the degree of match between its arguments, in a "soft"\nlUsing a procedure akin to leave-one-out cross-validation a single sample can be used\nfor both purposes.\n2EMMA is a random but pronounceable subset of the letters in the words "Empirical\nentropy Manipulation and Analysis".\n\n\x0cP. VIOLA, N. N. SCHRAUDOLPH, T. J. SEJNOWSKl\n\n854\n\nsense. It will approach one if Yl is significantly closer to Y2 than any element of A.\nTo reduce entropy the parameters v are adjusted such that there is a reduction in\nthe average squared distance between points which Wy indicates are nearby.\n2.1\n\nStochastic Maximization Algorithm\n\nBoth the calculation of the EMMA entropy estimate and its derivative involve a\ndouble summation. As a result the cost of evaluation is quadratic in sample size:\nO(NANB). While an accurate estimate of empirical entropy could be obtained by\nusing all of the available data (at great cost), a stochastic estimate of the entropy\ncan be obtained by using a random subset of the available data (at quadratically\nlower cost). This is especially critical in entropy manipulation problems, where the\nderivative of entropy is evaluated many hundreds or thousands of times. Without\nthe quadratic savings that arise from using smaller samples entropy manipulation\nwould be impossible (see (Viola, 1995) for a discussion of these issues).\n2.2\n\nEstimating the Covariance\n\nIn addition to the learning rate .A, the covariance matrices of the Parzen window\nfunctions, g,p, are important parameters of EMMA. These parameters may be chosen so that they are optimal in the maximum likelihood sense. For simplicity, we\nassume that the covariance matrices are diagonal,.,p\nDIAG(O"~,O"~, ... ). Following a derivation almost identical to the one described in Section 2 we can derive an\nequation analogous to (4),\n\n=\n\n" ""\n-d\nh. (Y) = - 1 "L...J\nL...J WY(YB\' YA) ( -1 )\nNB\n\ndO"k\n\nb\n\nO"k\n\nYsE YAEa\n\n([y]~\n-- O"~\n\n1)\n\n(7)\n\nwhere [Y]k is the kth component of the vector y. The optimal, or most likely,\n.,p minimizes h? (Y). In practice both v and .,p are adjusted simultaneously; for\nexample, while v is adjusted to maximize h? (YlI ), .,p is adjusted to minimize h? (y,,).\n\n3\n\nPrincipal Components Analysis and Information\n\nAs a demonstration, we can derive a parameter estimation rule akin to principal\ncomponents analysis that truly maximizes information. This new EMMA based\ncomponent analysis (ECA) manipulates the entropy of the random variable Y" =\nX?v under the constraint that Ivl = 1. For any given value of v the entropy of Y v can\nbe estimated from two samples of X as: h?(Yv )\n-EB[logEA[g,p(xB?v - XA? v)]],\nwhere .,p is the variance of the Parzen window function. Moreover we can estimate\nthe derivative of entropy:\n\n=\n\nd~ h?(Y = ;\n\nL\n\nlI )\n\nB\n\nB\n\nL Wy(YB, YA) .,p-l(YB - YA)(XB - XA) ,\nA\n\nwhere YA = XA . v and YB = XB . v. The derivative may be decomposed into parts\nwhich can be understood more easily. Ignoring the weighting function Wy.,p-l we\nare left with the derivative of some unknown function f(y"):\nd\n1\ndvf(Yv ) = N N L L(YB - YA)(XB - XA)\n(8)\nB\n\nA\n\nB\n\nA\n\nWhat then is f(y")? The derivative of the squared difference between samples is:\nd~ (YB - YA)2 = 2(YB - YA)(XB - XA) . So we can see that\n\nf(Y,,) = 2N IN L\nB\n\nA\n\nB\n\nL(YB - YA)2\nA\n\n\x0cEmpirical Entropy Manipulation for Real-world Problems\n\n?\n\n3\n\n855\n\nI\n\n.\n\n:\n\n2\n\nECA-MIN\nECA-MAX\nBCM\nBINGO\nPCA\n\no\n-I\n??\n\n-2\n\nt\n\n-3\n-4\n\n-2\n\no\n\n2\n\n4\n\nFigure 1: See text for description.\n\nis one half the expectation of the squared difference between pairs of trials of Yv ?\nRecall that PCA searches for the projection, Yv , that has the largest sample variance. Interestingly, f(Yv ) is precisely the sample variance. Without the weighting\nterm Wll ,p-l, ECA would find exactly the same vector that PCA does: the maximum variance projection vector. However because of Wll , the derivative of ECA\ndoes not act on all points of A and B equally. Pairs of points that are far apart are\nforced no further apart. Another way of interpreting ECA is as a type of robust\nvariance maximization. Points that might best be interpreted as outliers, because\nthey are very far from the body of other points, playa very small role in the minimization. This robust nature stands in contrast to PCA which is very sensitive to\noutliers.\nFor densities that are Gaussian, the maximum entropy projection is the first principal component. In simulations ECA effectively finds the same projection as PCA,\nand it does so with speeds that are comparable to Oja\'s rule. ECA can be used both\nto find the entropy maximizing (ECA-MAX) and minimizing (ECA-MIN) axes. For\nmore complex densities the PCA axis is very different from the entropy maximizing\naxis. To provide some intuition regarding the behavior of ECA we have run ECAMAX, ECA-MIN, Oja\'s rule, and two related procedures, BCM and BINGO, on\nthe same density. BCM is a learning rule that was originally proposed to explain\ndevelopment of receptive fields patterns in visual cortex (Bienenstock, Cooper and\nMunro, 1982). More recently it has been argued that the rule finds projections\nthat are far from Gaussian (Intrator and Cooper, 1992). Under a limited set of\nconditions this is equivalent to finding the minimum entropy projection. BINGO\nwas proposed to find axes along which there is a bimodal distribution (Schraudolph\nand Sejnowski, 1993).\nFigure 1 displays a 400 point sample and the projection axes discussed above. The\ndensity is a mixture of two clusters. Each cluster has high kurtosis in the horizontal\ndirection. The oblique axis projects the data so that it is most uniform and hence\nhas the highest entropy; ECA-MAX finds this axis. Along the vertical axis the\ndata is clustered and has low entropy; ECA-MIN finds this axis. The vertical axis\nalso has the highest variance. Contrary to published accounts, the first principal\ncomponent can in fact correspond to the minimum entropy projection. BCM, while\nit may find minimum entropy projections for some densities, is attracted to the\nkurtosis along the horizontal axis. For this distribution BCM neither minimizes nor\nmaximizes entropy. Finally, BINGO successfully discovers that the vertical axis is\nvery bimodal.\n\n\x0c856\n\nP. VIOLA, N. N. SCHRAUOOLPH, T. J. SEJNOWSKI\n\n\\ Corrupted-\n\n1200\n\n:. Corrected .?\n....:\n\'\n\n1000\n\n800\n600\n\n400\n\n200\n\n~.1\n\n0\n\n0.1 0.2 0.3 0.4\n\n\'.\n\n0.7 0.8 0.9\n\nFigure 2: At left: A slice from an MRI scan of a head. Center: The scan after\ncorrection. Right: The density of pixel values in the MRI scan before and after\ncorrection.\n\n4\n\nApplications\n\nEMMA has proven useful in a number of applications. In object recognition EMMA\nhas been used align 3D shape models with video images (Viola and Wells III, 1995).\nIn the area of medical imaging EMMA has been used to register data that arises\nfrom differing medical modalities such as magnetic resonance images, computed\ntomography images, and positron emission tomography (Wells, Viola and Kikinis,\n1995).\n4.1\n\nMRI Processing\n\nIn addition, EMMA can be used to process magnetic resonance images (MRI).\nAn MRI is a 2 or 3 dimensional image that records the density of tissues inside the\nbody. In the head, as in other parts of the body, there are a number of distinct tissue\nclasses including: bone, water, white matter, grey matter, and fat. ~n principle the\ndensity of pixel values in an MRI should be clustered, with one cluster for each\ntissue class. In reality MRI signals are corrupted by a bias field, a multiplicative\noffset that varies slowly in space. The bias field results from unavoidable variations\nin magnetic field (see (Wells III et al., 1994) for an overview of this problem).\nBecause the densities of each tissue type cluster together tightly, an uncorrupted\nMRI should have relatively low entropy. Corruption from the bias field perturbs\nthe MRI image, increasing the values of some pixels and decreasing others. The\nbias field acts like noise, adding entropy to the pixel density. We use EMMA to find\na low-frequency correction field that when applied to the image, makes the pixel\ndensity have a lower entropy. The resulting corrected image will have a tighter\nclustering than the original density.\nCall the uncorrupted scan s(z); it is a function of a spatial random variable z. The\ncorrupted scan, c( x) s( z) + b( z) is a sum of the true scan and the bias field. There\nare physical reasons to believe b( x) is a low order polynomial in the components of\nz. EMMA is used to minimize the entropy of the corrected signal, h( c( x) - b( z, v?,\nwhere b( z, v), a third order polynomial with coefficients v, is an estimate for the\nbias corruption.\n\n=\n\nFigure 2 shows an MRI scan and a histogram of pixel intensity before and after\ncorrection. The difference between the two scans is quite subtle: the uncorrected\nscan is brighter at top right and dimmer at bottom left. This non-homogeneity\n\n\x0cEmpirical Entropy Manipulation for Real-world Problems\n\n857\n\nmakes constructing automatic tissue classifiers difficult. In the histogram of the\noriginal scan white and grey matter tissue classes are confounded into a single peak\nranging from about 0.4 to 0.6. The histogram of the corrected scan shows much\nbetter separation between these two classes. For images like this the correction field\ntakes between 20 and 200 seconds to compute on a Sparc 10.\n\n5\n\nConclusion\n\nWe have demonstrated a novel entropy manipulation technique working on problems\nof significant complexity and practical importance. Because it is based on nonparametric density estimation it is quite flexible, requiring no strong assumptions\nabout the nature of signals. The technique is widely applicable to problems in\nsignal processing, vision and unsupervised learning. The resulting algorithms are\ncomputationally efficient.\nAcknowledgements\nThis research was support by the Howard Hughes Medical Institute.\n\nReferences\nBecker, S. and Hinton, G. E. (1992). A self-organizing neural network that discovers\nsurfaces in random-dot stereograms. Nature, 355:161-163.\nBell, A. J. and Sejnowski, T. J. (1995). An information-maximisation approach to blind\nseparation. In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, Advance8 in\nNeural Information Proce88ing, volume 7, Denver 1994. MIT Press, Cambridge.\nBienenstock, E., Cooper, L., and Munro, P. (1982). Theory for the development of neuron\nselectivity: Orientation specificity and binocular interaction in visual cortex. Journal\nof Neur08cience, 2.\nDuda, R. and Hart, P. (1973). Pattern Cla88ification and Scene AnalY8i8. Wiley, New\nYork.\nIntrator, N. and Cooper, L. N. (1992). Objective function formulation of the bcm theory of visual cortical plasticity: Statistical connections, stability conditions. Neural\nNetwork., 5:3-17.\nLinsker, R. (1988). Self-organization in a perceptual network. IEEE Computer, pages\n105-117.\nOja, E. (1982). A simplified neuron model as a principal component analyzer. Journal of\nMathematical Biology, 15:267-273.\nSchraudolph, N. N. and Sejnowski, T. J. (1993). Unsupervised discrimination of clustered\ndata via optimization of binary information gain. In Hanson, S. J., Cowan, J. D.,\nand Giles, C. L., editors, Advance. in Neural Information Proce88ing, volume 5, pages\n499-506, Denver 1992. Morgan Kaufmann, San Mateo.\nViola, P. A. (1995). Alignment by Ma:cimization of Mutual Information. PhD thesis,\nMassachusetts Institute of Technology. MIT AI Laboratory TR 1548.\nViola, P. A. and Wells III, W. M. (1995). Alignment by maximization of mutual information. In Fifth Inti. Conf. on Computer Vi8ion, pages 16-23, Cambridge, MA.\nIEEE.\nWells, W., Viola, P., and Kikinis, R. (1995). Multi-modal volume registration by maximization of mutual information. In Proceeding. of the Second International Sympo8ium on Medical Robotic. and Computer A88i8ted Surgery, pages 55 - 62. Wiley.\nWells III, W., Grimson, W., Kikinis, R., and Jolesz, F. (1994). Statistical Gain Correction\nand Segmentation of MRI Data. In Proceeding. of the Computer Society Conference\non Computer Vi.ion and Pattern Recognition, Seattle, Wash. IEEE, Submitted.\n\n\x0c'
p83218
sg178
S'Selective Attention for Handwritten\nDigit Recognition\n\nEthem Alpaydm\nDepartment of Computer Engineering\nBogazi<1i U ni versi ty\nIstanbul, TR-SOS15 Turkey\nalpaydin@boun.edu.tr\n\nAbstract\nCompletely parallel object recognition is NP-complete. Achieving\na recognizer with feasible complexity requires a compromise between parallel and sequential processing where a system selectively\nfocuses on parts of a given image, one after another. Successive\nfixations are generated to sample the image and these samples are\nprocessed and abstracted to generate a temporal context in which\nresults are integrated over time. A computational model based on a\npartially recurrent feedforward network is proposed and made credible by testing on the real-world problem of recognition of handwritten digits with encouraging results.\n\n1\n\nINTRODUCTION\n\nFor all-parallel bottom-up recognition, allocating one separate unit for each possible\nfeature combination, i.e., conjunctive encoding, implies combinatorial explosion. It\nhas been shown that completely parallel, bottom-up visual object recognition is\nNP-complete (Tsotsos, 1990). By exchanging space with time, systems with much\nless complexity may be designed. For example, to phone someone at the press of a\nbutton, one needs 10 7 buttons on the phone; the sequential alternative is to have\n10 buttons on the phone and press one at a time, seven times.\nWe propose recognition based on selective attention where we analyze only a small\npart of the image in detail at each step, combining results in time. N oton and Stark\'s\n(1971) "scanpath" theory advocates that each object is internally represented as a\nfeature-ring which is a temporal sequence of features extracted at each fixation and\nthe positions or the motor commands for the eye movements in between. In this\napproach, there is an "eye" that looks at an image but which can really see only a\nsmall part of it. This part of the image that is examined in detail is the fovea. The\n\n\x0c772\n\nE. ALPAYDIN\n\nASSOCIATIVE\n\nClass Probabilities\n(lOx!)\n\nLEVEL\n\nP~r------7-"7\n\nL-L_ _ _ _ _ _/ \'\n\nt\n\nsoftmax\nClass Units\n(lOxI) 0 /\n\n7\n\nT1\n\nHidden Units (s x I)\nH L..../~_....;...._-_-_-_-~_-_-_-_-_-~_-_-~7-."\n\nI~-----------------;t~---------- ----;;------------------PRE-ATTENTIVE LEVEL\n\nATTENTIVE LEVEL\n\n,-------------------- -----,\n\n------\n\n-------------------,\n\nI\n\n:\nF\n\nFeature Map\nI\n(rxI):\n\nEye Position Map\n(pxp)\n\n//p\n1\n\nFovea\n1-------\'---1-\n\n~\n\n-I\n\nWTA\n\nsubsample\nand blur\n\nI\n\n- -:- - - - - - - ~\n\nSaliency Map\n(n x n)\n\nM\nBitmap Image (n x n)\n\nFigure 1: The block diagram of the implemented system.\n\nfovea\'s content is examined by the pre-attentive level where basic feature extraction\ntakes place. The features thus extracted are fed to an a660ciative part together\nwith the current eye position. If the accumulated information is not sufficient for\nrecognition, the eye is moved to another part of the image, making a saccade. To\nminimize recognition time, the number of saccades should be minimized. This is\ndone through defining a criterion of being "interesting" or saliency and by fixating\nonly at the most interesting. Thus sucessive fixations are generated to sample the\nimage and these samples are processed and abstracted to generate a temporal context in which results are integrated over time. There is a large amount of literature\non selective attention in neuroscience and psychology; for reviews see respectively\n(Posner and Peterson, 1990) and (Treisman, 1988). The point stressed in this paper\nis that the approach is also useful in engineering.\n\n2\n\nAN EXAMPLE SYSTEM FOR OCR\n\nThe structure of the implemented system for recognition of handwritten digits is\ngiven in Fig. 1.\n\n\x0cSelective Attention for Handwritten Digit Recognition\n\n773\n\nWe have an n x n binary image in which the fovea is m x m with m < n. To\nminimize recognition time, the system should only attend to the parts of the image\nthat carry discriminative information. We define a criterion of being "interesting"\nor saliency which is applied to all image locations in parallel to generate a 8aliency\nmap, S. The saliency measure should be chosen to draw attention to parts that\nhave the highest information content. Here, the saliency criterion is a low-pass filter\nwhich roughly counts the number of on pixels in the corresponding m x m region\nof the input image M. As the strokes in handwritten digits are mostly one or two\npixels wide, a count of the on pixels is a good measure of the discontinuity (and\nthus information). It is also simple to compute:\n\ni+lm/2J\nSij =\n\nL\n\nHLm/2J\n\nL\n\nMkIN2 ((i,jl, (Lm/6J)2 *1), i,j = 1. .. n\n\nk=i-Lm/2J l=j-Lm/2J\nwhere N 2 (p., E) is the bivariate normal with mean p. and the covariance E. Note\nthat we want the convolution kernel to have effect up to Lm/2J and also that the\nnormal is zero after p.? 30-. In our simulations where n is 16 and m is 5 (typical for\ndigit recognition), 0- ~ 1. The location that is most salient is the position ofthe next\nfixation and as such defines the new center of the fovea. A location once attended\nto is no longer interesting; after each fixation, the saliency of all the locations that\ncurrently are in the scope of the fovea are set to 0 to inhibit another fixation there.\nThe attentive level thus controls the scope of the pre-attentive level. The maximum\nof the saliency map through a winner-take-all gives the eye position (i*, j*) at\nfixation t.\n(i*(t),j*(t))\narg~B:XSij\n\',J\nBy thus following the salient regions, we get an input-dependent emergent sequence\nin time.\n\n=\n\nEye-Position Map\nThe eye p08ition map, P, stores the position of the eye in the current fixation. It is\np x p. p is chosen to be smaller than n for dimensionality reduction for decreasing\n\ncomplexity and introducing an effect of regularization (giving invariance to small\ntranslations). When p is a factor of n, computations are also simpler. We also blur\nthe immediate neighbors for a smoother representation:\n\nP( t)\n\n= blur(subsample( winner-take-all( S)))\n\nPre-Attentive Level: Feature Extraction\nThe pre-attentive level extracts detailed features from the fovea to generate a feature\nmap. This information and the current eye position is passed to the associative\nsystem for recognition. There is a trade-off between the fovea size and the number\nof saccades required for recognition: As the operation in the pre-attentive level is\ncarried out in parallel, to minimize complexity the features extracted there should\nnot be many and the fovea should not be large: Fovea is where the expensive\ncomputation takes place. On the other hand, the fovea should be large enough to\nextract discriminative features and thus complete recognition in a small amount of\ntime. The features to be extracted can be learned through an supervised method\nwhen feedback is available .\n\n\x0c774\n\nE. ALPAYDIN\n\nThe m x m region symmetrically around (i*, j*) is extracted as the fovea I and is\nfed to the feature extractors. The r features extracted there are passed on to the\nassociative level as the feature map, F. r is typically 4 to 8. Ug denote the weights\nof feature 9 and Fg is the value of feature 9 that is found by convolving the fovea\ninput with the feature weight vector (1(.) is the sigmoid function):\n\nM i o(t)-Lm/2J+i,jo(t)-Lm/2J+j, i,j = 1 ... m\n\nf (\n\n~ ~ U"jI,j(t?) , g = 1. ..\n\nr\n\nAssociative Level: Classification\nAt each fixation, the associative level is fed the feature map from the pre-attentive\nlevel and the eye position map from the attentive level. As a number of fixations\nmay be necessary to recognize an image, the associative system should have a shortterm memory able to accumulate inputs coming through time. Learning similarly\nshould be through time. When used for classification, the class units are organized\nso as to compete and during recognition the activations of the class units evolve\ntill one class gets sufficiently active and suppresses the others. When a training\nset is available, a temporal supervised method can be used to train the associative\nlevel. Note that there may be more than one scanpath for each object and learning\none sequence for each object fails. We see it is a task of accumulating two types of\ninformation through time: the "what" (features extracted) and the "where" (eye\nposition).\nThe fovea map, F, and the eye position map, P, are concatenated to make a\nr + p X P dimensional input that is fed to the associative level. Here we use an\nartificial neural network with one hidden layer of 8 units. We have experimented\nwith various architectures and noticed that recurrency at the output layer is the\nbest. There are 10 output units.\n\nf (L VhgFg(t) + L L WhabPab(t)) , h =\n\n1. .. s\n\ngab\n\nLTchHh + L RckPk(t - 1), c = 1. .. 10\nh\n\nk\n\nexp[Oc(t)]\nLk exp[Ok(t)]\nwhere P denotes the "softmax"ed output probabilities (Bridle, 1990) and P(t - 1)\nare the values in the preceding fixation (initially 0). We use the cross-entropy as\nthe goodness measure:\nC=\n\nL\nt\n\n1\n\nt L Dk 10gPc(t), t ~\n\n1\n\nc\n\nDc is the required output for class c. Learning is gradient-ascent on this goodness\nmeasure. The fraction lit is to give more weight to initial fixations than later ones.\nConnections to the output units are updated as follows (11 is the learning factor):\n\n\x0cSelective Attention for Handwritten Digit Recognition\n\nNote that we assume 8PIc(t -1)/8Rc lc =\nwe have:\n\no.\n\n775\n\nFor the connections to the hidden units\n\nc\n\nWe can back-propagate one step more to train the feature extractors. Thus the\nupdate equations for the connections to feature units are:\n\nCg(t) =\n\nL Ch(t)Vhg\nh\n\nA series of fixations are made until one of the class units is sufficiently active:\n3c, Pc > 8 (typically 0.99), or when the most salient point has a saliency less than a\ncertain threshold (this condition is rarely met after the first few epochs). Then the\ncomputed changes are summed up and the updates are made like the exaple below:\n\nBackpropagation through time where the recurrent connections are unfolded in time\ndid not work well in this task because as explained before, for the same class, there is\nmore than one scanpath. The above-mentioned approach is like real-time recurrent\nlearning (Williams and Zipser, 1989) where the partial derivatives in the previous\ntime step is 0, thus ignoring this temporal dependence.\n\n3\n\nRESULTS AND DISCUSSION\n\nWe have experimented with various parameter settings and finally chose the architecture given above: When input is 16 x 16 and there are 10 classes, the fovea is\n5 x 5 with 8 features and there are 16 hidden units. There are 1,934 images for\ntraining, 946 for cross-validation and 943 for testing. Results are given in Table\n1. ( It can be seen that by scanning less than half of the image, we get 80% generalization. Additional to the local high-resolution image provided by the fovea, a\nlow-resolution image of the surrounding parafovea can be given to the associative\nlevel for better recognition. For example we low-pass filtered and undersampled the\noriginal image to get a 4 x 4 image which we fed to the class units additional to\nthe attention-based hidden units. Success went up quite high and fewer fixations\nwere necessary; compare rows 1 and 2 of the Table. The information provided by\nthe 4 x 4 map is actually not much as can be seen from row 3 of the table where\nonly that is given as input. Thus the idea is that when we have a coarse input,\nlooking only at a quarter of the image in detail is sufficient to get 93% accuracy.\nBoth features (what) and eye positions (where) are necessary for good recognition.\nWhen only one is used without the other, success is quite low as can be seen in rows\n4 and 5. In the last row, we see the performance of a multi layer percept ron with\n10 hidden units that does all-parallel recognition.\nBeyond a certain network size, increasing the number of features do not help much.\nDecreasing 8, the certainty threshold, decreases the number of fixations necessary\n\n\x0c776\n\nE. ALPAYDIN\n\nTable 1: Results of handwritten digit recognition with selective attention. Values\ngiven are average and standard deviation of 10 independent runs. See text for\ncomments.\nNO OF\nPARAMS\n\nTEST\nSUCCESS\n\nTRAINING\nEPOCHS\n\nNO OF\nFIXATIONS\n\nSA system\nSA+parafovea\nOnly parafovea\nOnly what info\nOnly where info\n\n878\n1,038\n170\n622\n440\n\n79.7, 1.8\n92.5,0.8\n86.9,0.2\n49.0,21.0\n54.2, 1.4\n\n74.5, 17.1\n54.2, 10.2\n52.3,8.2\n66.6, 30.6\n92.9,6.5\n\n6.5,0.2\n3.9,0.3\n1.0, 0.0\n7.5,0.1\n7.6,0.0\n\nMLP, 10 hiddens\n\n2,680\n\n95.1, 0.6\n\n13.5,4.1\n\n1.0,0.0\n\nMETHOD\n\nwhich we want, but decreases success too which we don\'t. Smaller foveas decrease\nthe number of free parameters but decrease success and require a larger number\nof fixations. Similarly larger foveas decrease the number of fixations but increase\ncomplexity.\nThe simple low-pass filter used here as a saliency measure is the simplest measure.\nPreviously it has been used by Fukushima and Imagawa (1993) for finding the next\ncharacter, i.e., segmentation, and also by Olshausen et al. (1992) for translation\ninvariance. More robust measures at the expense of more computations, are possible; see (Rimey and Brown, 1990; Milanese et al., 1993). Salient regions are those\nthat are conspicious, i.e., different from their surrounding where there is a change\nin X where X can be brightness or color (edges), orientation (corners), time (motion), etc. It is also possible that top-down, task-dependent saliency measures be\nintegrated to minimize further recognition time implying a remembered explicit\nsequence analogous to skilled motor behaviour (probably gained after many repetitions).\nHere a partially recurrent network is used for temporal processing. Hidden Markov\nModels like used in speech recognition are another possibility (Rimey and Brown,\n1990; Haclsalihzade et al., 1992). They are probabilistic finite automata which can\nbe trained to classify sequences and one can have more than one model for an object.\nIt should be noted here that better approaches for the same problem exists (Le Cun\net al., 1989). Here we advocate a computational model and make it plausible by\ntesting it on a real-world problem. It is necessary for more complicated problems\nwhere an all-parallel approach would not work. For example Le Cun et al. \'s model\nfor the same type of inputs has 2,578 free parameters. Here there are\n\n(mx m+1) x r+(r+pxp+ 1) x 8+(S+ 1) x 10+10 x 10\n,\n\n#\'\n\niT\n\nv\';w\n\n#~~\n\nT\n\nR\n\nfree parameters which make 878 when m = 5, r = 8, S = 16. This is the main\nadvantage of selective attention which is that the complexity of the system is heavily\nreduced at the expense of slower recognition, both in overt form of attention through\nfoveation and in its covert form, for binding features - For this latter type of\nattention not discussed here, see (Ahmad, 1992). Also note that low-level feature\nextraction operations like carried out in the pre-attentive level are local convolutions\n\n\x0cSelective Attention for Handwritten Digit Recognition\n\n777\n\nand are appropriate for parallel processing, e.g., on a SIMD machine. Higherlevel operations require larger connectivity and are better carried out sequentially.\nNature also seems to have taken this direction.\nAcknowledgements\nThis work is supported by Tiibitak Grant EEEAG-143 and Bogazi<;;i University\nResearch Funds 95HA108. Cenk Kaynak prepared the handwritten digit database\nbased on the programs provided by NIST (Garris et al., 1994).\nReferences\nS. Ahmad. (1992) VISIT: A Neural Model of Covert Visual Attention. In J. Moody,\nS. Hanson, R. Lippman (Eds.) Advances in Neural Information Processing Systems\n4,420-427. San Mateo, CA: Morgan Kaufmann.\nJ.S. Bridle. (1990) Probabilistic Interpretation of Feedforward Classification Network Outputs with Relationships to Statistical Pattern Recognition. In Neurocomputing, F. Fogelman-Soulie, J. Herault, Eds. Springer, Berlin, 227-236.\nK. Fukushima, T. Imagawa. (1993) Recognition and Segmentation of Connected\nCharacters with Selective Attention, Neural Networks, 6: 33-41.\nM.D. Garris et al. (1994) NIST Form-Based Handprint Recognition System, NISTIR 5469, NIST Computer Systems Laboratory.\nS.S. Haclsalihzade, L.W. Stark, J .S. Allen. (1992) Visual Perception and Sequences\nof Eye Movement Fixations: A Stochastic Modeling Approach, IEEE SMC, 22,\n474-481.\nY. Le Cun et al. (1991) Handwritten Digit Recognition with a Back-Propagation\nNetwork. In D.S. Touretzky (ed.) Advances in Neural Information Processing\nSystems 2, 396-404. San Mateo, CA: Morgan Kaufmann.\nR. Milanese et al. (1994) Integration of Bottom-U p and Top- Down Cues for Visual\nAttention using Non-Linear Relaxation IEEE Int\'l Conf on CVPR, Seattle, WA,\nUSA.\n\nD. Noton and L. Stark. (1971) Eye Movements and Visual Perception, Scientific\nAmerican, 224: 34-43.\nB. Olshausen, C. Anderson, D. Van Essen. (1992) A Neural Model of Visual Attention and Invariant Pattern Recognition, CNS Memo 18, CalTech.\nM.L Posner, S.E. Petersen. (1990) The Attention System of the Human Brain,\nAnn. Rev. Neurosci., 13:25-42.\nR.D. Rimey, C.M. Brown. (1990) Selective Attention as Sequential Behaviour: Modelling Eye Movements with an Augmented Hidden Markov Model, TR-327, Computer Science, Univ of Rochester.\nA. Treisman. (1988) Features and Objects, Quarterly Journ. of Ezp . Psych., 40:\n201-237.\nJ.K. Tsotsos. (1990) Analyzing Vision at the Complexity Level, Behav. and Brain\nSci. 13: 423-469.\nR.J. Williams, D. Zipser. (1989) A Learning Algorithm for Continually Running\nFully Recurrent Neural Networks Neural Computation, 1, 270-280.\n\n\x0c'
p83219
sg22
S'Analog VLSI Processor Implementing the\nContinuous Wavelet Transform\n\nR. Timothy Edwards and Gert Cauwenberghs\nDepartment of Electrical and Computer Engineering\nJohns Hopkins University\n3400 North Charles Street\nBaltimore, MD 21218-2686\n{tim,gert}@bach.ece.jhu.edu\n\nAbstract\nWe present an integrated analog processor for real-time wavelet decomposition and reconstruction of continuous temporal signals covering the\naudio frequency range. The processor performs complex harmonic modulation and Gaussian lowpass filtering in 16 parallel channels, each clocked\nat a different rate, producing a multiresolution mapping on a logarithmic\nfrequency scale. Our implementation uses mixed-mode analog and digital circuits, oversampling techniques, and switched-capacitor filters to\nachieve a wide linear dynamic range while maintaining compact circuit\nsize and low power consumption. We include experimental results on the\nprocessor and characterize its components separately from measurements\non a single-channel test chip.\n\n1 Introduction\nAn effective mathematical tool for multiresolution analysis [Kais94], the wavelet transform\nhas found widespread use in various signal processing applications involving characteristic\npatterns that cover multiple scales of resolution, such as representations of speech and vision.\nWavelets offer suitable representations for temporal data that contain pertinent features both\nin the time and frequency domains; consequently, wavelet decompositions appear to be\neffective in representing wide-bandwidth signals interfacing with neural systems [Szu92].\nThe present system performs a continuous wavelet transform on temporal one-dimensional\nanalog signals such as speech, and is in that regard somewhat related to silicon models\nof the cochlea implementing cochlear transforms [Lyon88], [Liu92] , [Watt92], [Lin94].\nThe multiresolution processor we implemented expands on the architecture developed\nin [Edwa93], which differs from the other analog auditory processors in the way signal\ncomponents in each frequency band are encoded. The signal is modulated with the center\n\n\x0cAnalog VLSI Processor Implementing the Continuous Wavelet Transform\n\n1m\n\n_: \'\\/\'V\n\n-I\n\n~\nx\n\nMultiplier\n\ns\'(t)\n\nLPF\n\nset)\n\nx(t)\n\n693\n\nx(t)\n\n~ ~ yet)\nLff\n\nLPF\n\n~\n\n~\n\nget)\n\nyet)\n\nh(t)\n\nh(t)\n\nPrefilter\n\n(a)\n\nMultiplexer\n(b)\n\nFigure 1: Demodulation systems, (a) using multiplication, and (b) multiplexing.\nfrequency of each channel and subsequently lowpass filtered, translating signal components\ntaken around the center frequency towards zero frequency. In particular. we consider wavelet\ndecomposition and reconstruction of analog continuous-time temporal data with a complex\nGaussian kernel according to the following formulae:\n\nYk(t)\n\n{teo x(e) exp (jWke- Q(Wk(t - e))2) de\n(decomposition)\n\nx\'(t)\n\n(1)\n\nC 2::y\\(t) exp(-jwkt)\nk\n\n(reconstruction)\nwhere the center frequencies Wk are spaced on a logarithmic scale. The constant Q sets the\nrelative width of the frequency bins in the decomposition , and can be adjusted (together\nwith C) alter the shape of the wavelet kernel. Successive decomposition and reconstruction\ntransforms yield an approximate identity operation; it cannot be exact as no continuous\northonormal basis function exists for the CWT [Kais94].\n\n2\n\nArchitecture\n\nThe above operations are implemented in [Edwa93] using two demodulator systems per\nchannel, one for the real component of (1), and another for the imaginary component, 90?\nout of phase with the first. Each takes the form of a sinusoidal modulator oscillating at\nthe channel center frequency, followed by a Gaussian-shaped lowpass filter, as shown in\nFigure 1 (a). This arrangement requires a precise analog sine wave generator and an accurate\nlinear analog multiplier. In the present implementation, we circumvent both requirements\nby using an oversampled binary representation of the modulation reference signal.\n\n2.1\n\nMultiplexing vs. MUltiplying\n\nMultiplication of an analog signal x(t) with a binary (? 1) sequence is naturally implemented\nwith high precision using a mUltiplexer, which alternates between presenting either the\ninput or its inverse -x(t) to the output. This principle is applied to simplify harmonic\nmodulation. and is illustrated in Figure 1 (b). The multiplier has been replaced by an analog\ninverter followed by a multiplexer, where the multiplexer is controlled by an oversampled\nbinary periodic sequence representing the sine wave reference. The oversampled binary\nsequence is chosen to approximate the analog sine wave as closely as possible. disregarding\ncomponents at high frequency which are removed by the subsequent lowpass filter. The\nassumption made is that no high frequency components are present in the input signal\n\n\x0cR. T. EDWARDS, G. CAUWENBERGHS\n\n694\nSiglUll\n\n,---- ? ________ ----I In\n\n,,:\n\nIn Seltrt\n\nCLK2\n\neLK]\n\nf ---<.-iK4----1:CLKS-- -: ;----- -- ---- ---- --1\n\neLK/:\n\n,,\n\nII\n\nE\n\n?\n\nI\n\nII\n\n"\n\nI\'\n\n\'I\n\n.---f-..., ::\n\n::\n\nRet\'onJlructed :\n\n: cmLy,\n\n0.,.\n\n.--...L..--,::\nI\n\n,\n\nI\n\n:\n\n~~--+,~,\n\n\'-==:.J\n\n,\n\n: ~ ____ __________ __J\n\n\'\n\nI. ________________ :\n\nReconstruction Input\n\ni\n\n:,\n\n"\n\nWav elet\nReconstruction\n\nMult;pl;er\n\nGaussian Filter\n\nOutput Mux ing\n\nFigure 2: Block diagram of a single channel in the wavelet processor, showing test points\nA through E.\nunder modulation, which otherwise would convolve with corresponding high frequency\ncomponents in the binary sequence to produce low frequency distortion components at the\noutput. To that purpose, an additionallowpass filter is added in front of the multiplexer.\nResidual low-frequency distortion at the output is minimized by maximizing roll-off of the\nfilters, placing proper constraints on their cutofffrequencies, and optimally choosing the bit\nsequence in the oversampled reference [Edwa95]. Clearly, the signal accuracy that can be\nachieved improves as the length N of the sequence is extended. Constraints on the length\nN are given by the implied overhead in required signal bandwidth, power dissipation, and\ncomplexity of implementation.\n\n2.2\n\nWavelet Gaussian Function\n\nThe reason for choosing a Gaussian kernel in (l) is to ensure optimal support in both\ntime and frequency [Gros89]. A key requirement in implementing the Gaussian filter\nis linear phase, to avoid spectral distortion due to non-uniform group delays. A worryfree architecture would be an analog FIR filter; however the number of taps required to\naccommodate the narrow bandwidth required would be prohibitively large for our purpose.\nInstead, we approximate a Gaussian filter by cascading several first-order lowpass filters .\nFrom probabilistic arguments, the obtained lowpass filter approximates a Gaussian filter\nincreasingly well as the number of stages increases [Edwa93] .\n\n3\n\nImplementation\n\nTwo sections of a wavelet processor, each containing 8 parallel channels, were integrated\nonto a single 4 mm x 6 mm die in 2 /lm CMOS technology. Both sections can be configured\nto perform wavelet decomposition as well as reconstruction. The block diagram for one\nof the channels is shown in Figure 2. In addition, a separate test chip was designed which\nperforms one channel of the wavelet function . Test points were made available at various\npoints for either input or output, as indicated in boldface capitals, A through E, in Figure 2.\nEach channel performs complex harmonic modulation and Gaussian lowpass filtering, as\ndefined above. At the front end of the chip is a sample-and-hold section to sample timemultiplexed wavelet signals for reconstruction . In cases of both signal decomposition\nand reconstruction, each channel removes the input DC component removed, filters the\nresult through the premultiplication lowpass (PML) filter, inverts the result, and passes\nboth non-inverted and inverted signals onto the multiplexer. The multiplexer output is\npassed through a postmultiplication lowpass filter (PML, same architecture) to remove high\nfrequency components of the oversampled sequence, and then passed through the Gaussianshaped lowpass filter. The cutoff frequencies of all filters are controlled by the clock rates\n\n\x0c695\n\nAnalog VLSI Processor Implementing the Continuous Wavelet Transform\n\n(CLKI to CLK4 in Figure 2). The remainder of the system is for reconstruction and for\ntime-multiplexing the output.\n\n3.1\n\nMUltiplier\n\nThe multiplier is implemented by use of the above multiplexing scheme, driven by an\noversampled binary sequence representing a sine wave. The sequence we used was 256\nsamples in length, created from a 64-sample base sequence by reversal and inversion . The\nsequence length of256 generates a modulator wave of 4 kHz (useful for speech applications)\nfrom a clock of about 1 MHz.\nWe derived a sequence which, after postfiltering through a 3rd-order lowpass filter of the\nfonn of the PML prefilter (see below), produces a sine wave in which all hannonics are\nmore than 60 dB down from the primary [Edwa95]. The optimized 64-bit base sequence\nconsists of 11 zeros and 53 ones, allowing a very simple implementation in which an address\ndecoder decodes the "zero" bits. The binary sequence is shown in Figure 4. The magnitude\nof the prime hannonic of the sequence is approximately 1.02, within 2% of unity.\nThe process of reversing and inverting the sequence is simplified by using a gray code\ncounter to produce the addresses for the sequence, with only a small amount of combinatorial\nlogic needed to achieve the desired result [Edwa95]. It is also straightforward to generate\nthe addresses for the cosine channel, which is 90? out of phase with the original.\n\n3.2 Linear Filtering\nAll filters used are implemented as linear cascades of first-order, single-pole filter sections.\nThe number of first-order sections for the PML filters is 3. The number of sections for the\n"Gaussian" filter is 8, producing a suitable approximation to a Gaussian filter response for\nall frequencies of interest (Figure 5).\nFigure 3 shows one first-order lowpass section of the filters as implemented. This standard\n\n>-.+--o\n\nv,,,\n\nva"\'\n\n+\n\nFigure 3: Single discrete-time lowpass filter section.\nswitched-capacitor circuit implements a transfer function containing a single pole, approximately located in the Laplace domain at s = Is / a for large values of the parameter a, with\nIs being the sampling frequency. The value for this parameter a is fixed at the design stage\nas the ratio of two capacitors in Figure 3, and was set to be 15 for the The PML filters and\n12 for the Gaussian filters.\n\n4 Measured Results\n4.1\n\nSine wave modulator\n\nWe tested the accuracy of the sine wave modulation signal by applying two constant voltages\nat test points A and B, such that the sine wave modulation signal is effectively multiplied\n\n\x0cR. T. EDWARDS, G. CAUWENBERGHS\n\n696\n\nSine sequence and filtered sine wave output\n\nBinary sine sequence\nSimulated filtered output\nx\nMeasured output\n-1.5 L -_ _ _- - \'_ _ _ _- \' -_ _ _ _........_ _ _ _- \' -_ _ _ _.J...J\n\no\n\n50\n\n100\n\n150\n\n200\n\n250\n\nTime (us)\n\nFigure 4: Filtered sine wave output.\n\nby a constant. The output of the mUltiplier is filtered and the output taken at test point D,\nbefore the Gaussian filter. Figure 4 shows the (idealized) multiplexer output at test point\nC, which accurately creates the desired binary sequence. Figure 4 also shows the measured\nsine wave after filtering with the PML filter and the expected output from the simulation\nmodel, using a deviating value of 8.0 for the capacitor ratio a, as justified below. FFT\nanalysis of Figure 4 has shown that the resulting sine wave has all harmonics below about\n-49 dB . This is in good agreement with the simulation model, provided a correction is made\nfor the value of the capacitor ratio a to account for fringe and (large) parasitic capacitances.\nThe best fit for the measured data from the postmultiplication filter is a = 8.0, compared to\nthe desired value of a = 15.0. The transform of the simulated output shown in the figure\ntakes into account the smaller value of a. Because the postmultiplication filter is followed\nby the Gaussian filter, the bandwidth of the output can be directly controlled by proper\nclocking ofthe Gaussian filter, so the distortion in the sine wave is ultimately much smaller\nthan that measured at the output of the postmultiplication filter.\n\n4.2\n\nGaussian filter\n\nThe Gaussian filter was tested by applying a signal at test point D and measuring the\nresponse at test point E. Figure 5 shows the response of the Gaussian filter as compared to\nexpected responses. There are two sets of curves, one for a filter clocked at 64 kHz, and the\nother clocked at 128 kHz; these curves are normalized by plotting time relative to the clock\nfrequency is . The solid line indicates the best match for an 8th-order lowpass filter, using\nthe capacitor ratio, a, as a fitting parameter. The best-fit value of a is approximately 6.8.\nThis is again much lower than the capacitor area ratio of 12 on the chip. The dotted line is\nthe response of the ideal Gaussian characteristic exp ( _w 2 / (2aw~)) approximated by the\ncascade of first-order sections with capacitor ratio a.\nFigure 5 (b) shows the measured phase response of the Gaussian filter for the 128 kHz\nclock. The phase response is approximately linear throughout the passband region.\n\n\x0cAnalog VLSI Processor Implementing the Continuous Wavelet Transform\n\n697\n\nGaussian filter response\n\no~~~~--~-=~~~~~~~--~\n\nx\no\n\nx\n0\n\niii\'-10\n~\n\nChip data at 64kHz clock\nChip data at 128kHz clock\n8th-order filter ideal response\nGaussian filter ideal response\n\n500\n\n<lJ\n\n]1\n\n-20\n\n~\n\n?E\n\n-30\n\n"0\n<lJ\n\no\n\nN\n\n~ -40\n\nTheoretical 8-stage phase\nMeasured response\n\n?\n\no\nZ -50\n0.01\n\n0.07\n\nFrequency (units fs)\n\n0.08\n\n0.0 I\n\n0.02\n\n0.03\n\n0.04\n\n0.05\n\n0.06\n\n0.07\n\nFrequency (units fs)\n\nFigure 5: Gaussianfilter transfer functions: theoretical and actual. (a) Relative amplitude;\n(b) Phase.\n\n4.3\n\nWavelet decomposition\n\nFigure 6 shows the test chip performing a wavelet transform on a simple sinusoidal input,\nillustrating the effects of (oversampled) sinusoidal modulation followed by lowpass filtering\nthrough the Gaussian filter. The chip multiplier system is clocked at 500 kHz. The input\nwave is approximately 3.1 kHz, close to the center frequency of the modulator signal,\nwhich is the clock rate divided by 128, or about 3.9 kHz (a typical value for the highestfrequency channel in an auditory application). The top trace in the figure shows the filtered\nand inverted input, taken from test point B. The middle trace shows the output of the\nmultiplexer (test point C), wherein the output is multiplexed between the signal and its\ninverse. The bottom trace is taken from the system output (labeled Cosine Out in Figure 2)\nand shows the demodulated signal of frequency 800 Hz (= 3.9 kHz - 3.1 kHz). Not shown\nis the cosine output, which is 90? out of phase with the one shown. This demonstrates\nthe proper operation of complex demodulation in a single channel configured for wavelet\ndecomposition. In addition, we have tested the full16-channel chip decomposition, and all\nindividual parts function properly. The total power consumption of the 16-channel wavelet\nchip was measured to be less than 50mW, of which a large fraction can be attributed to\nexternal interfacing and buffering circuitry at the periphery of the chip.\n\n5\n\nConclusions\n\nWe have demonstrated the full functionality of an analog chip performing the continuous\nwavelet transform (decomposition). The chip is based on mixed analog/digital signal\nprocessing principles, and uses a demodulation scheme which is accurately implemented\nusing oversampling methods. Advantages of the architecture used in the chip are an\nincreased dynamic range and a precise control over lateral synchronization of wavelet\ncomponents. An additional advantage inherent to the modulation scheme used is the\npotential to tune the channel bandwidths over a wide range, down to unusually narrow\nbands, since the cutoff frequency of the Gaussian filter and the center frequency of the\nmodulator are independently adjustable and precisely controllable parameters.\n\nReferences\nG. Kaiser, A Friendly Guide to Wavelets, Boston, MA: Birkhauser, 1994.\n\nT. Edwards and M. Godfrey, "An Analog Wavelet Transform Chip," IEEE Int\'l Can! on\n\n0.08\n\n\x0c698\n\nR. T. EDWARDS, G. CAUWENBERGHS\n\nFigure 6: Scope trace of the wavelet transform: filtered input (top), multiplexed signal\n(middle), and wavelet output (bottom).\nNeural Networks, vol. III, 1993, pp. 1247-1251.\nT. Edwards and G. Cauwenberghs, "Oversampling Architecture for Analog Harmonic\n\nModulation," to appear in Electronics Letters, 1996.\nA Grossmann, R Kronland-Martinet, and J. MorIet, "Reading and understanding continuous wavelet transforms," Wavelets: Time-Frequency Methods and Phase Space. SpringerVerlag, 1989, pp. 2-20.\nW. Liu, AG. Andreou, and M.G. Goldstein, "Voiced-Speech Representation by an Analog\nSilicon Model ofthe Auditory Periphery," IEEE T. Neural Networks, vol. 3 (3), pp 477-487,\n1992.\nJ. Lin, W.-H. Ki, T. Edwards, and S. Shamma, "Analog VLSI Implementations of Auditory Wavelet Transforms Using Switched-Capacitor Circuits," IEEE Trans. Circuits and\nSystems-I, vol.41 (9), pp. 572-583, September 1994.\nA Lu and W. Roberts, \'\'A High-Quality Analog Oscillator Using Oversampling D/A Conversion Techniques," IEEE Trans. Circuits and Systems-II, vol.41 (7), pp. 437-444, July\n1994.\n\nRF. Lyon and C.A Mead, "An Analog Electronic Cochlea," IEEE Trans. Acoustics, Speech\nand Signal Proc., vol. 36, pp 1119-1134, 1988.\nH.H. Szu, B. Tefter, and S. Kadembe, "Neural Network Adaptive Wavelets for Signal Representation and Classification," Optical Engineering, vol. 31 (9), pp. 1907-1916, September\n1992.\nL. Watts, D.A Kerns, and RF. Lyon, "Improved Implementation of the Silicon Cochlea,"\nIEEE Journal of Solid-State Circuits, vol. 27 (5), pp 692-700,1992.\n\n\x0c'
p83220
sg181
S'SEEMORE: A View-Based Approach to\n3-D Object Recognition Using Multiple\nVisual Cues\nBartlett W. Mel\nDepartment of Biomedical Engineering\nUniversity of Southern California\nLos Angeles, CA 90089\nmel@quake.usc.edu\n\nAbstract\nA neurally-inspired visual object recognition system is described\ncalled SEEMORE, whose goal is to identify common objects from\na large known set-independent of 3-D viewiag angle, distance,\nand non-rigid distortion. SEEMORE\'s database consists of 100 objects that are rigid (shovel), non-rigid (telephone cord), articulated (book), statistical (shrubbery), and complex (photographs of\nscenes). Recognition results were obtained using a set of 102 color\nand shape feature channels within a simple feedforward network architecture. In response to a test set of 600 novel test views (6 of\neach object) presented individually in color video images, SEEMORE\nidentified the object correctly 97% of the time (chance is 1%) using\na nearest neighbor classifier. Similar levels of performance were\nobtained for the subset of 15 non-rigid objects. Generalization behavior reveals emergence of striking natural category structure not\nexplicit in the input feature dimensions.\n\n1\n\nINTRODUCTION\n\nIn natural contexts, visual object recognition in humans is remarkably fast, reliable,\nand viewpoint invariant. The present approach to object recognition is "view-based"\n(e.g. see [Edelman and Bulthoff, 1992]), and has been guided by three main dogmas.\nFirst, the "natural" object recognition problem faced by visual animals involves a\nlarge number of objects and scenes, extensive visual experience, and no artificial\n\n\x0c866\n\nB. W.MEL\n\ndistinctions among object classes, such as rigid, non-rigid, articulated, etc.\nSecond, when an object is recognized in the brain, the "heavy lifting" is done by\nthe first wave of action potentials coursing from the retina to the inferotemporal\ncortex (IT) over a period of 100 ms [Oram and Perrett, 1992]. The computations\ncarried out during this time can be modeled as a shallow but very wide feedforward\nnetwork of simple image filtering operations. Shallow means few processing levels,\nwide means a sparse, high-dimensional representation combining cues from multiple\nvisual submodalities, such as color, texture, and contour [Tanaka et al., 1991].\nThird, more complicated processing mechanisms, such as those involving focal attention, segmentation, binding, normalization, mental rotation, dynamic links, parts\nrecognition, etc., may exist and may enhance recognition performance but are not\nnecessary to explain rapid, robust recognition with objects in normal visual situations.\n\nIn this vein, the main goal of this project has been to explore the limits of performance of a shallow-but very wide-feedforward network of simple filtering operations\nfor viewpoint-invariant 3-D object recognition, where the filter "channels" themselves have been loosely modeled after the shape- and color-sensitive visual response\nproperties seen in the higher levels of the primate visual system [Tanaka et al., 1991].\nArchitecturally similar approaches to vision have been most often applied in the domain of optical character recognition [Fukushima et al., 1983, Le Cun et al., 1990].\nSEEMORE\'S architecture is also similar in spirit to the color histogramming approach\nof [Swain and Ballard, 1991], but includes spatially-structured features that provide\nalso for shape-based generalization.\n\nFigure 1: The database includes 100 objects of many different types, including rigid\n(soup can), non-rigid (necktie), statistical (bunch of grapes), and photographs of\ncomplex indoor and outdoor scenes.\n\n\x0cSEEMORE: A View-based Approach to 3-D Object Recognition\n\n2\n\n867\n\nSEEMORE\'S VISUAL WORLD\n\nSEEMORE\'s database contains 100 common 3-D objects and photogaphs of scenes,\neach represented by a set of pre-segmented color video images (fig. 1). The training\nset consisted of 12-36 views of each object as follows. For rigid objects, 12 training\nviews were chosen at roughly 60? intervals in depth around the viewing sphere, and\neach view was then scaled to yield a total of three images at 67%, 100%, and 150%.\nImage plane orientation was allowed to vary arbitrarily. For non-rigid objects, 12\ntraining views were chosen in random poses.\nDuring a recognition trial, SEEMORE was required to identify novel test images of\nthe database objects. For rigid objects, test images were drawn from the viewpoint\ninterstices of the training set, excluding highly foreshortened views (e.g. bottom of\ncan). Each test view could therefore be presumed to be correctly recognizable, but\nnever closer than roughly 30-> in orientation in depth or 22% in scale to the nearest\ntraining view of the object, while position and orientation in the image plane could\nvary arbitrarily. For non-rigid objects, test images consisted of novel random poses.\nEach test view depicted the isolated object on a smooth background.\n\n2.1\n\nFEATURE CHANNELS\n\nSEEMORE\'s internal representation of a view of an object is encoded by a set\nof feature channels. The ith channel is based on an elemental nonlinear filter\nfi(z, y, (h, (J2, .? .), parameterized by position in the visual field and zero or more\ninternal degrees of freedom. Each channel is by design relatively sensitive to changes\nin the image that are strongly related to object identity, such as the object\'s shape,\ncolor, or texture, while remaining relatively insensitive to changes in the image that\nare unrelated to object identity, such as are caused by changes in the object\'s pose.\nIn practice, this invariance is achieved in a straightfOl\'ward way for each channel by\nsubsampling and summing the output of the elemental channel filter over the entire\nvisual field and one or more of its internal degrees of freedom, giving a channel\noutput Fi = Lx,y,(h , .. . fiO. For example, a particular shape-sensitive channel might\n"look" for the image-plane projections of right-angle corners, over the entire visual\nfield, 360? of rotation in the image plane, 30? of rotation in depth, one octave in\nscale, and tolerating partial occlusion and/or slight misorientation of the elemental\ncontours that define the right angle. In general, then, Fi may be viewed as a "cell"\nwith a large receptive field whose output is an estimate of the number of occurences\nof distal feature i in the workspace over a large range of viewing parameters.\nSEEMORE\'S architecture consists of 102 feature channels, whose outputs form an\ninput vector to a nearest-neighbor classifer. Following the design of the individual\nchannels, the channel vector F = {FI, ... F 102 } is (1) insensitive to changes in image\nplane position and orientation of the object, (2) modestly sensitive to changes in\nobject scale, orientation in depth, or non-rigid deformation, but (3) highly sensitive\nto object "quality" as pertains to object identity. Within this representation, total\nmemory storage for all views of an object ranged from 1,224 to 3,672 integers.\nAs shown in fig . 2, SEEMORE\'s channels fall into in five groups: (1) 23 color channels, each of which responds to a small blob of color parameterized by "best" hue\nand saturation, (2) 11 coarse-scale intensity corner channels parameterized by open\nangle, (3) 12 "blob" features, parameterized by the shape (round and elongated) and\n\n\x0c868\n\nB.\n\nW.MEL\n\nsize (small, medium, and large) of bright and dark intensity blobs, (4) 24 contour\nshape features, including straight angles, curve segments of varying radius, and parallel and oblique line combinations, and (5) 16 shape/texture-related features based\non the outputs of Gabor functions at 5 scales and 8 orientations. The implementations of the channel groups were crude, in the interests of achieving a working,\nmultiple-cue system with minimal development time. Images were grabbed using an\noff-the-shelf Sony S-Video Camcorder and SunVideo digitizing board.\nColors\n\nBlobs\n\nAngles\n\no.\n\nContours\n\n0.1\n\ne.\noe\noe\n\n.e\n\no\n\n00\n\n??\n??\n??\n\n0\n\nc\n\n??\n\noe\no\n\n=:>\n\nGabor-Based Features\n\n./" 1: -\n\nsin2 +cos2 .......... 0 _\n2\n\nenergy @ scale i\nenergy variance\n@scalei\n\n0\n\n6\n45 90\n\n<30\n>30\n\nFigure 2: SEEMORE\'s 102 channels fall into 5 groups, sensitive to (1) colors, (2) intensity corners, (3) circular and elongated intensity blobs, (4) contour shape features,\nand (5) 16 oriented-energy and relative-orientation features based on the outputs of\nGabor functions at several scales and orientations.\n\n3\n\nRECOGNITION\n\nSEEMORE\'s recognition performance was assesed quantitatively as follows. A test\nset consisting of 600 novel views (100 objects x 6 views) was culled from the database, and presented to SEEMORE for identification. It was noted empirically that\na compressive transform on the feature dimensions (histogram values) led to improved classification performance; prior to all learning and recognition operations,\n\n\x0cSEEMORE: A View-based Approach to 3-D Object Recognition\n\n869\n\nFigure 3: Generalization using only shape-related channels. In each row, a novel\ntest view is shown at far left. The sequence of best matching training views (one\nper object) is shown to right, in order of decreasing similarity.\n\ntherefore, each feature value was replaced by its natural logarithm (0 values were\nfirst replaced with a small positive constant to prevent the logarithm from blowing\nup). For each test view, the city-block distance was computed to every training view\nin the database and the nearest neighbor was chosen as the best match. The log\ntransform of the feature dimension:.; thus tied this distance to the ratios of individual\nfeature values in two images rather than their differences.\n\n4\n\nRESULTS\n\nRecognition time on a Sparc-20 was 1-2 minutes per view; the bulk of the time was\ndevoted to shape processing, with under 2 seconds required for matching.\nRecognition results are reported as the proportion of test views that were correctly\nclassified. Performance using all 102 channels for the 600 novel object views in the\nintact test set was 96.7%; the chance rate of correct classification was 1%. Across\nrecognition conditions, second-best matches usually accounted for approximately\nhalf the errors. Results were broken down in terms of the separate contributions\nto recognition performance of color-related vs. shape-related feature channels. Performance using only the 23 color-related channels was 87.3%, and using only the\n79 shape-related channels was 79.7%. Remarkably, very similar performance figures\nwere obtained for the subset of 90 test views of the non-rigid objects, which included\nseveral scarves, a bike chain, necklace, belt, sock, necktie, maple-leaf cluster, bunch\nof grapes, knit bag, and telephone cord. Thus, a novel random configuration of a\ntelephone cord was as easily recognized as a novel view of a shovel.\n\n\x0c870\n\n5\n\nB. W.MEL\n\nGENERALIZATION BEHAVIOR\n\nNumerical indices of recognition performance are useful, but do not explicitly convey\nthe similarity structure of the underlying feature space. A more qualitative but\nextremely informative representation of system performance lies in the sequence of\nimages in order of increasing distance from a test view. Records of this kind are\nshown in fig. 3 for trials in which only shape-related channels were used. In each, a\ntest view is shown at the far left, and the ordered set of nearest neighbors is shown\nto the right. When a test view\'s nearest neighbor (second image from left) was not\nthe correct match, the trial was classified as an error.\nAs shown in row (1), a view of a book is judged most similar to a series of other books\n(or the bottom of a rectangular cardboard box)---each a view of a rectangular object\nwith high-frequency surface markings. A similar sequence can be seen in subsequent\nrows for (2) a series of cans, each a right cylinder with detailed surface markings, (3)\na series of smooth, not-quite-round objects, (4) a series of photographs of complex\nscenes, and (5) a series of dinosaurs (followed by a teddy bear). In certain cases,\nSEEMORE\'S shape-related similarity metric was more difficult to visually interpret\nor verbalize (last two rows), or was different from that of a human observer.\n\n6\n\nDISCUSSION\n\nThe ecology of natural object vision gives rise to an apparent contradiction: (i)\ngeneralization in shape-space must in some cases permit an object whose global\nshape has been grossly perturbed to be matched to itself, such as the various tangled\nforms of a telephone cord, but (ii) quasi-rigid basic-level shape categories (e.g. chair,\nshoe, tree) must be preserved as well, and distinguished from each other.\nA partial It wi uti on to this conundrum lies in the observation tbat locally-cumputed\nshape statistics are in large part preserved under the global shape deformations that\nnon-rigid common objects (e.g. scarf, bike-chain) typically undergo. A feature-space\nrepresentation with an emphasis on locally-derived shape channels will therefore\nexhibit a significant degree of invariance to global nonrigid shape deformations. The\ndefinition of shape similarity embodied in the present approach is that two objects\nare similar if they contain similar profiles (histograms) of their shape measures,\nwhich emphasize locality. One way of understanding the emergence of global shape\ncategories, then, such as "book", "can", "dinosaur", etc., is to view each as a set of\ninstances of a single canonical object whose local shape statistics remain quasi-stable\nas it is warped into various global forms. In many cases, particularly within rigid\nobject categories, exemplars may share longer-range shape statistics as well.\nIt is useful to consider one further aspect of SEEMORE\'S shape representation, pertaining to an apparent mismatch between the simplicity of the shape-related feature channels and the complexity of the shape categories that can emerge from\nthem. Specifically, the order of binding of spatial relations within SEEMORE\'s shape\nchannels is relatively low, i.e. consisting of single simple open or closed curves,\nor conjunctions of two oriented contours or Gabor patches. The fact that shape\ncategories, such as "photographs of rooms", or "smooth lumpy objects", cluster\ntogether in a feature space of such low binding order would therefore at first seem\nsurprising. This phenomenon relates closely to the notion of "wickelfeatures" (see\n[Rumelhart and McClelland, 1986], ch. 18), in which features (relating to phonemes)\n\n\x0cSEEMORE: A View-based Approach to 3-D Object Recognition\n\n871\n\nthat bind spatial information only locally are nonetheless used to represent global\npatterns (words) with little or no residual ambiguity.\nThe pre segmentation of objects is a simplifying assumption that is clearly invalid in\nthe real world. The advantage of the assumption from a methodological perspective\nis that the object similarity structure induced by the feature dimensions can be\nstudied independently from the problem of segmenting or indexing objects imbedded\nin complex scenes. In continuing work, we are pursuing a leap to sparse very-highdimensional space (e.g. 10,000 dimensions), whose advantages for classification in\nthe presence of noise (or clutter) have been discussed elsewhere [Kanerva, 1988,\nCalifano and Mohan, 1994].\nAcknowledgements\nThanks to J6zsef Fiser for useful discusf!ions and for development of the Gabor-based\nchannel set, to Dan Lipofsky and Scott Dewinter for helping in the construction of\nthe image database, and to Christof Koch for providing support at Caltech where\nthis work was initiated. This work was funded by the Office of Naval Research, and\nthe McDonnell-Pew Foundation.\n\nReferences\n[Califano and Mohan, 1994] Califano, A. and Mohan, R. (1994). Multidimensional\nindexing for recognizing visual shapes. IEEE Trans. on PAMI, 16:373-392.\n[Edelman and Bulthoff, 1992] Edelman, S. and Bulthoff, H. (1992). Orientation dependence in the recognition of familiar and novel views of three-dimensional objects. Vision Res., 32:2385-2400.\n[Fukushima et al., 1983] Fukushima, K., Miyake, S., and Ito, T. (1983). Neocognitron: A neural network model for a mechanism of visual pattern recognition.\nIEEE Trans. Sys. Man & Cybernetics, SMC-13:826-834.\n[Kanerva, 1988] Kanerva, P. (1988). Sparse distributed memory. MIT Press, Cambridge, MA.\n[Le Cun et al., 1990] Le Cun, Y., Matan, 0., Boser, B., Denker, J., Henderson, D.,\nHoward, R., Hubbard, W., Jackel, L., and Baird, H. (1990). Handwritten zip\ncode recognition with multilayer networks. In Proc. of the 10th Int. Conf. on\nPatt. Rec. IEEE Computer Science Press.\n[Oram and Perrett, 1992] Oram, M. and Perrett, D. (1992). Time course of neural\nresponses discriminating different views of the face and head. J. Neurophysiol.,\n68(1) :70-84.\n[Rumelhart and McClelland, 1986] Rumelhart, D. and McClelland, J. (1986). Parallel distributed processing. MIT Press, Cambridge, Massachusetts.\n[Swain and Ballard, 1991] Swain, M. and Ballard, D. (1991). Color indexing. Int.\nJ. Computer Vision, 7:11-32.\n[Tanaka et al., 1991] Tanaka, K., Saito, H., Fukada, Y., and Moriya, M. (1991).\nCoding visual images of objects in the inferotemporal cortex of the macaque\nmonkey. J. Neurophysiol., 66:170-189.\n\n\x0c\x0cPART VIII\nAPPLICATIONS\n\n\x0c\x0c'
p83221
sg235
S'Learning with ensembles: How\nover-fitting can be useful\n\nPeter Sollich\nDepartment of Physics\nUniversity of Edinburgh, U.K.\nP.SollichGed.ac.uk\n\nAnders Krogh\'"\nNORDITA, Blegdamsvej 17\n2100 Copenhagen, Denmark\nkroghGsanger.ac.uk\n\nAbstract\nWe study the characteristics of learning with ensembles. Solving\nexactly the simple model of an ensemble of linear students, we\nfind surprisingly rich behaviour. For learning in large ensembles,\nit is advantageous to use under-regularized students, which actually over-fit the training data. Globally optimal performance can\nbe obtained by choosing the training set sizes of the students appropriately. For smaller ensembles, optimization of the ensemble\nweights can yield significant improvements in ensemble generalization performance, in particular if the individual students are subject to noise in the training process. Choosing students with a wide\nrange of regularization parameters makes this improvement robust\nagainst changes in the unknown level of noise in the training data.\n\n1\n\nINTRODUCTION\n\nAn ensemble is a collection of a (finite) number of neural networks or other types\nof predictors that are trained for the same task. A combination of many different predictors can often improve predictions, and in statistics this idea has been\ninvestigated extensively, see e.g. [1, 2, 3]. In the neural networks community, ensembles of neural networks have been investigated by several groups, see for instance\n[4, 5, 6, 7]. Usually the networks in the ensemble are trained independently and\nthen their predictions are combined.\nIn this paper we study an ensemble of linear networks trained on different but\noverlapping training sets. The limit in which all the networks are trained on the\nfull data set and the one where all the data sets are different has been treated in\n[8] . In this paper we treat the case of intermediate training set sizes and overlaps\n?Present address: The Sanger Centre, Hinxton, Cambs CBIO IRQ, UK.\n\n\x0cLearning with Ensembles: How Overfitting Can Be Useful\n\n191\n\nexactly, yielding novel insights into ensemble learning. Our analysis also allows us to\nstudy the effect of regularization and of having different predictors in an ensemble.\n\n2\n\nGENERAL FEATURES OF ENSEMBLE LEARNING\n\nWe consider the task of approximating a target function fo from RN to R. It\nwill be assumed that we can only obtain noisy samples of the function, and the\n(now stochastic) target function will be denoted y(x) . The inputs x are taken\nto be drawn from some distribution P(x). Assume now that an ensemble of K\nindependent predictors fk(X) of y(x) is available. A weighted ensemble average is\ndenoted by a bar, like\n(1)\nlex) = L,wkfk(X),\nk\n\nwhich is the final output of the ensemble. One can think of the weight Wk as the\nbelief in predictor k and we therefore constrain the weights to be positive and sum\nto one. For an input x we define the error of the ensemble c(x), the error of the\nkth predictor ck(X), and its ambiguity ak(x)\nc(x)\nck(X)\n\n(y(x) -lex)?\n(y(x) - fk(X)?\n(fk(X) -1(x?2.\n\n=\n\n(2)\n(3)\n(4)\n\n=\n\nThe ensemble error can be written as c(x)\nlex) - a(x) [7], where lex)\nL,k Wkck(X) is the average error over the individual predictors and a(x) =\nL,k Wkak(X) is the average of their ambiguities, which is the variance of the output\nover the ensemble. By averaging over the input distribution P(x) (and implicitly\nover the target outputs y(x?, one obtains the ensemble generalization error\n(5)\nwhere c(x) averaged over P(x) is simply denoted c, and similarly for land a. The\nfirst term on the right is the weighted average of the generalization errors of the individual predictors, and the second is the weighted average of the ambiguities, which\nwe refer to as the ensemble ambiguity. An important feature of equation (5) is that\nit separates the generalization error into a term that depends on the generalization\nerrors of the individual students and another term that contains all correlations between the students. The latter can be estimated entirely from unlabeled data, i. e.,\nwithout any knowledge of the target function to be approximated. The relation (5)\nalso shows that the more the predictors differ, the lower the error will be, provided\nthe individual errors remain constant.\n\nIn this paper we assume that the predictors are trained on a sample of p examples\nof the target function, (xt\',yt\'), where yt\' = fo(xt\') + TJt\' and TJt\' is some additive\nnoise (Jl. = 1, ... ,p). The predictors, to which we refer as students in this context\nbecause they learn the target function from the training examples, need not be\ntrained on all the available data. In fact, since training on different data sets will\ngenerally increase the ambiguity, it is possible that training on subsets of the data\nwill improve generalization. An additional advantage is that, by holding out for\neach student a different part of the total data set for the purpose of testing, one\ncan use the whole data set for training the ensemble while still getting an unbiased\nestimate of the ensemble generalization error. Denoting this estimate by f, one has\n(6)\nwhere Ctest = L,k WkCtest,k is the average of the students\' test errors. As already\npointed out, the estimate ft of the ensemble ambiguity can be found from unlabeled\ndata.\n\n\x0cP. SOLLICH, A. KROGH\n\n192\n\nSo far, we have not mentioned how to find the weights Wk. Often uniform weights\nare used, but optimization of the weights in some way is tempting. In [5, 6] the\ntraining set was used to perform the optimization, i.e., the weights were chosen to\nminimize the ensemble training error. This can easily lead to over-fitting, and in [7]\nit was suggested to minimize the estimated generalization error (6) instead. If this\nis done, the estimate (6) acquires a bias; intuitively, however, we expect this effect\nto be small for large ensembles.\n\n3\n\nENSEMBLES OF LINEAR STUDENTS\n\nIn preparation for our analysis of learning with ensembles of linear students we now\nbriefly review the case of a single linear student, sometimes referred to as \'linear\nperceptron learning\'. A linear student implements the input-output mapping\n1 T\nJ(x) = ..JNw x\nparameterized in terms of an N-dimensional parameter vector w with real components; the scaling factor 1/..JN is introduced here for convenience, and . ..T denotes\nthe transpose of a vector. The student parameter vector w should not be confused with the ensemble weights Wk. The most common method for training such\na linear student (or parametric inference models in general) is minimization of the\nsum-of-squares training error\nE = L:(y/J - J(x/J))2 + Aw2\n/J\nwhere J.L = 1, ... ,p numbers the training examples. To prevent the student from\nfitting noise in the training data, a weight decay term Aw2 has been added. The size\nof the weight decay parameter A determines how strongly large parameter vectors\nare penalized; large A corresponds to a stronger regularization of the student.\nFor a linear student, the global minimum of E can easily be found. However,\nin practical applications using non-linear networks, this is generally not true, and\ntraining can be thought of as a stochastic process yielding a different solution each\ntime. We crudely model this by considering white noise added to gradient descent\nupdates of the parameter vector w. This yields a limiting distribution of parameter\nvectors P(w) ex: exp(-E/2T), where the \'temperature\' T measures the amount of\nnoise in the training process.\nWe focus our analysis on the \'thermodynamic limit\' N - t 00 at constant normalized\nnumber of training examples, ex = p/N. In this limit, quantities such as the training\nor generalization error become self-averaging, i.e., their averages over all training\nsets become identical to their typical values for a particular training set. Assume\nnow that the training inputs x/J are chosen randomly and independently from a\nGaussian distribution P(x) ex: exp( - ~x2), and that training outputs are generated\nby a linear target function corrupted by additive noise, i.e., y/J = w\'f x/J /..IN + 1]/J,\nwhere the 1]/J are zero mean noise variables with variance u 2 ? Fixing the length of the\nparameter vector of the target function to w~ = N for simplicity, the generalization\nerror of a linear student with weight decay A and learning noise T becomes [9]\n(; = (u 2 + T)G + A(U 2\n\n-\n\n8G\n\nA) 8A .\n\n(7)\n\nOn the r.h.s. of this equation we have dropped the term arising from the noise on\nthe target function alone, which is simply u 2 , and we shall follow this convention\nthroughout . The \'response function\' Gis [10, 11]\n\nG = G(ex, A) = (1 - ex - A+ )(1 - ex - A)2 + 4A)/2A.\n\n(8)\n\n\x0c193\n\nLearning with Ensembles: How Overfitting Can Be Useful\n\nFor zero training noise, T = 0, and for any a, the generalization error (7} is minimized when the weight decay is set to A = (T2j its value is then (T2G(a, (T2), which\nis the minimum achievable generalization error [9].\n\n3.1\n\nENSEMBLE GENERALIZATION ERROR\n\nWe now consider an ensemble of K linear students with weight decays Ak and\nlearning noises Tk (k = 1 . . . K). Each ,student has an ensemble weight Wk and\nis trained on N ak training examples, with students k and I sharing N akl training\nexamples (of course, akk = ak). As above, we consider noisy training data generated\nby a linear target function. The resulting ensemble generalization error can be\ncalculated by diagrammatic [10] or response function [11] methods. We refer the\nreader to a forthcoming publication for details and only state the result:\n\n(9)\nwhere\n(10)\nHere Pk is defined as Pk = AkG(ak, Ak). The Kronecker delta in the last term\nof (10) arises because the training noises of different students are uncorrelated. The\ngeneralization errors and ambiguities of the individual students are\n\nak = ckk - 2 LWlckl\nI\n\n+ LWIWmclm;\n1m\n\nthe result for the Ck can be shown to agree with the single student result (7). In\nthe following sections, we shall explore the consequences of the general result (9) .\nWe will concentrate on the case where the training set of each student is sampled\nrandomly from the total available data set of size NO\', For the overlap of the training\nsets of students k and I (k \'II) one then has akl/a = (ak/a)(al/a) and hence\n\nak/ = akal/a\n\n(11)\nup to fluctuations which vanish in the thermodynamic limit. For finite ensembles\none can construct training sets for which akl < akal/a. This is an advantage,\nbecause it results in a smaller generalization error, but for simplicity we use (11).\n\n4\n\nLARGE ENSEMBLE LIMIT\n\nWe now use our main result (9) to analyse the generalization performance of an ensemble with a large number K of students, in particular when the size of the training\nsets for the individual students are chosen optimally. If the ensemble weights Wk\nare approximately uniform (Wk ~ 1/ K) the off-diagonal elements of the matrix\n(ckl) dominate the generalization error for large K, and the contributions from the\ntraining noises\nare suppressed. For the special case where all students are identical and are trained on training sets of identical size, ak = (1 - c)a, the ensemble\ngeneralization error is shown in Figure 1(left). The minimum at a nonzero value\nof c, which is the fraction of the total data set held out for testing each student,\ncan clearly be seen. This confirms our intuition: when the students are trained\non smaller, less overlapping training sets, the increase in error of the individual\nstudents can be more than offset by the corresponding increase in ambiguity.\n\nn\n\nThe optimal training set sizes ak can be calculated analytically:\n_\n\nCk\n\n=1-\n\nak/ a\n\n1 - Ak/(T2\n\n= 1 + G(a, (T2) \'\n\n(12)\n\n\x0cP. SOLLICH, A. KROGH\n\n194\n1.0 r - - - , - - - - - , r - - - . , - - - - , . - - - - : .\n\n1.0 r - - - , - - - - - , - - - . - - - - r - - - - "\n\n0.8\n\n0.8\n\n0.6\n\n0.6\n\nw\n\n.\'\n\nw\n0.4\n\n,...-------\n\n0.2\n/\n\n/\n\n0.0 /\n\n0.0\n\n/\n\n0.2\n\n0.4\n\n0.6\n\n,,\n,\n0.8\n\n0.2\n\n------,\n\n1.0\n\n0.0\n\n0.0\n\nC\n\n...\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\nC\n\nFigure 1: Generalization error and ambiguity for an infinite ensemble of identical\nstudents. Solid line: ensemble generalization error, fj dotted line: average generalization error of the individual students, l; dashed line: ensemble ambiguity, a.\nFor both plots a = 1 and (72 = 0.2 . The left plot corresponds to under-regularized\nstudents with A = 0.05 < (72. Here the generalization error of the ensemble has\na minimum at a nonzero value of c. This minimum exists whenever>\' < (72. The\nright plot shows the case of over-regularized students (A = 0.3 > (72), where the\ngeneralization error is minimal at c = O.\nThe resulting generalization error is f = (72G(a, (72) + 0(1/ K), which is the globally\nminimal generalization error that can be obtained using all available training data,\nas explained in Section 3. Thus, a large ensemble with optimally chosen training\nset sizes can achieve globally optimal generalization performance. However, we see\nfrom (12) that a valid solution Ck > 0 exists only for Ak < (72, i.e., if the ensemble\nis under-regularized. This is exemplified, again for an ensemble of identical students, in Figure 1(right) , which shows that for an over-regularized ensemble the\ngeneralization error is a: monotonic function of c and thus minimal at c = o.\nWe conclude this section by discussing how the adaptation of the training set sizes\ncould be performed in practice, for simplicity confining ourselves to an ensemble of\nidentical students, where only one parameter c = Ck = 1- ak/a has to be adapted.\nIf the ensemble is under-regularized one expects a minimum of the generalization\nerror for some nonzero c as in Figure 1. One could, therefore, start by training\nall students on a large fraction of the total data set (corresponding to c ~ 0), and\nthen gradually and randomly remove training examples from the students\' training\nsets. Using (6), the generalization error of each student could be estimated by\ntheir performance on the examples on which they were not trained, and one would\nstop removing training examples when the estimate stops decreasing. The resulting\nestimate of the generalization error will be slightly biased; however, for a large\nenough ensemble the risk of a strongly biased estimate from systematically testing\nall students on too \'easy\' training examples seems small, due to the random selection\nof examples.\n\n5\n\nREALISTIC ENSEMBLE SIZES\n\nWe now discuss some effects that occur in learning with ensembles of \'realistic\' sizes.\nIn an over-regularized ensemble nothing can be gained by making the students more\ndiverse by training them on smaller, less overlapping training sets. One would also\n\n\x0c195\n\nLearning with Ensembles: How Overfitting Can Be Useful\n\nFigure 2: The generalization error of\nan ensemble with 10 identical students as a function of the test set\nfraction c. From bottom to top the\ncurves correspond to training noise\nT = 0,0.1,0.2, ... ,1.0. The star on\neach curve shows the error of the optimal single perceptron (i. e., with optimal weight decay for the given T)\ntrained on all examples, which is independent of c. The parameters for\nthis example are: a = 1, A = 0.05,\n0\'2 = 0.2.\n\n0.2\n0.0 L - _ - - \' - _ - - - \'_ _-\'--_--\'-_~\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nC\n\nexpect this kind of \'diversification\' to be unnecessary or even counterproductive\nwhen the training noise is high enough to provide sufficient \'inherent\' diversity of\nstudents. In the large ensemble limit, we saw that this effect is suppressed, but\nit does indeed occur in finite ensembles. Figure 2 shows the dependence of the\ngeneralization error on c for an ensemble of 10 identical, under-regularized students\nwith identical training noises Tk = T. For small T, the minimum of f. at nonzero c\npersists. For larger T, f. is monotonically increasing with c, implying that further\ndiversification of students beyond that caused by the learning noise is wasteful. The\nplot also shows the performance of the optimal single student (with A chosen to\nminimize the generalization error at the given T), demonstrating that the ensemble\ncan perform significantly better by effectively averaging out learning noise.\nFor realistic ensemble sizes the presence of learning noise generally reduces the\npotential for performance improvement by choosing optimal training set sizes. In\nsuch cases one can still adapt the ensemble weights to optimize performance, again\non the basis of the estimate of the ensemble generalization error (6). An example is\n1.0\n\n1.0\n\n,,\n\n0.8\n\nI\nI\n\n0.8\n,-\n\n,-\n\n/\n\nI\n\n0.6\n\n0.6\n\nI\n\ntV\n\ntV\n\n0.4 ..... -_ .................\n\n0.4\n0.2\n... ....\n0.0\n0.001\n\n---0.010\n\n0.2\n\n0.100\n\n02\n\n1.000\n\n0.0\n0.001\n\n0.010\n\n02\n\n0.100\n\n1.000\n\nFigure 3: The generalization error of an ensemble of 10 students with different\nweight decays (marked by stars on the 0\'2-axis) as a function of the noise level\n0\'2. Left: training noise T = 0; right: T = 0.1. The dashed lines are for the\nensemble with uniform weights, and the solid line is for optimized ensemble weights.\nThe dotted lines are for the optimal single perceptron trained on all data. The\nparameters for this example are: a = 1, c = 0.2.\n\n\x0c196\n\nP. SOu...ICH, A. KROGH\n\nshown in Figure 3 for an ensemble of size 1< = 10 with the weight decays >\'k equally\nspaced on a logarithmic axis between 10- 3 and 1. For both of the temperatures T\nshown, the ensemble with uniform weights performs worse than the optimal single\nstudent. With weight optimization, the generalization performance approaches that\nof the optimal single student for T = 0, and is actually better at T = 0.1 over\nthe whole range of noise levels rr2 shown. Even the best single student from the\nensemble can never perform better than the optimal single student, so combining the\nstudent outputs in a weighted ensemble average is superior to simply choosing the\nbest member of the ensemble by cross-validation, i.e., on the basis of its estimated\ngeneralization error. The reason is that the ensemble average suppresses the learning\nnoise on the individual students.\n\n6\n\nCONCLUSIONS\n\nWe have studied ensemble learning in the simple, analytically solvable scenario of\nan ensemble of linear students. Our main findings are: In large ensembles, one\nshould use under-regularized students in order to maximize the benefits of the\nvariance-reducing effects of ensemble learning. In this way, the globally optimal\ngeneralization error on the basis of all the available data can be reached by optimizing the training set sizes of the individual students. At the same time an estimate\nof the generalization error can be obtained. For ensembles of more realistic size, we\nfound that for students subjected to a large amount of noise in the training process\nit is unnecessary to increase the diversity of students by training them on smaller,\nless overlapping training sets. In this case, optimizing the ensemble weights can\nstill yield substantially better generalization performance than an optimally chosen\nsingle student trained on all data with the same amount of training noise. This\nimprovement is most insensitive to changes in the unknown noise levels rr2 if the\nweight decays of the individual students cover a wide range. We expect most of these\nconclusions to carryover, at least qualitatively, to ensemble learning with nonlinear\nmodels, and this correlates well with experimental results presented in [7].\n\nReferences\n[1]\n[2]\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9]\n[10]\n[11]\n\nC. Granger, Journal of Forecasting 8, 231 (1989).\nD. Wolpert, Neural Networks 5, 241 (1992) .\nL. Breimann, Tutorial at NIPS 7 and personal communication.\nL. Hansen and P. Salamon, IEEE Trans. Pattern Anal. and Mach. Intell. 12,\n993 (1990).\nM. P. Perrone and L. N. Cooper, in Neural Networks for Speech and Image\nprocessing, ed. R. J. Mammone (Chapman-Hall, 1993).\nS. Hashem: Optimal Linear Combinations of Neural Networks. Tech. Rep .\nPNL-SA-25166, submitted to Neural Networks (1995) .\nA. Krogh and J. Vedelsby, in NIPS 7, ed. G. Tesauro et al., p. 231 (MIT Press,\n1995).\nR. Meir, in NIPS 7, ed. G. Tesauro et al., p. 295 (MIT Press, 1995).\nA. Krogh and J. A. Hertz, J. Phys. A 25,1135 (1992).\nJ. A. Hertz, A. Krogh, and G. I. Thorbergsson, J. Phys. A 22, 2133 (1989).\nP. Sollich, J. Phys. A 27, 7771 (1994).\n\n\x0c'
p83222
sg384
S'Modern Analytic Techniques to Solve the\nDynamics of Recurrent Neural Networks\n\nA.C.C. Coolen\nDept. of Mathematics\nKing\'s College London\nStrand, London WC2R 2LS, U.K.\n\nS.N. Laughton\nDept. of Physics - Theoretical Physics\nUniversity of Oxford\n1 Keble Road, Oxford OX1 3NP, U.K.\n\nD. Sherrington ..\nCenter for Non-linear Studies\nLos Alamos National Laboratory\nLos Alamos, New Mexico 87545\n\nAbstract\nWe describe the use of modern analytical techniques in solving the\ndynamics of symmetric and nonsymmetric recurrent neural networks near saturation. These explicitly take into account the correlations between the post-synaptic potentials, and thereby allow\nfor a reliable prediction of transients.\n\n1\n\nINTRODUCTION\n\nRecurrent neural networks have been rather popular in the physics community,\nbecause they lend themselves so naturally to analysis with tools from equilibrium\nstatistical mechanics. This was the main theme of physicists between, say, 1985\nand 1990. Less familiar to the neural network community is a subsequent wave of\ntheoretical physical studies, dealing with the dynamics of symmetric and nonsymmetric recurrent networks. The strategy here is to try to describe the processes\nat a reduced level of an appropriate small set of dynamic macroscopic observables.\nAt first, progress was made in solving the dynamics of extremely diluted models\n(Derrida et al, 1987) and of fully connected models away from saturation (for a\nreview see (Coolen and Sherrington, 1993)). This paper is concerned with more\nrecent approaches, which take the form of dynamical replica theories, that allow\nfor a reliable prediction of transients, even near saturation. Transients provide the\nlink between initial states and final states (equilibrium calculations only provide\n?On leave from Department of Physics - Theoretical Physics, University of Oxford\n\n\x0cA. C. C. COOLEN, S. N. LAUGHTON, D. SHERRINGTON\n\n254\n\ninformation on the possible final states). In view of the technical nature of the\nsubject, we will describe only basic ideas and results for simple models (full details\nand applications to more complicated models can be found elsewhere).\n\n2\n\nRECURRENT NETWORKS NEAR SATURATION\n\nLet us consider networks of N binary neurons ai E {-I, I}, where neuron states\nare updated sequentially and stochastically, driven by the values of post-synaptic\npotentials hi . The probability to find the system at time t in state 0\' = (a1,\' .. , aN)\nis denoted by Pt(O\'). For the rates Wi(O\') of the transitions ai -t -(7i and for the\npotentials hi (0\') we make the usual choice\n1\nWi (0\') = - [1-ai tanh [,Bhi (0\')]]\nhi(O\') =\nJijaj\n\nL\n\n2\n\nj:f:i\n\nThe parameter ,B controls the degree of stochasticity: the ,B = 0 dynamics is completely random, whereas for ,B = 00 we find the deterministic rule ai -t sgn[hi(O\')].\nThe evolution in time of Pt(O\') is given by the master equation\nd\nN\n(1)\ndtPt (0\') =\n[Pt (FkO\' )Wk (FkO\') - Pt (0\' )Wk (0\')]\n\nl:\nk=l\n\nwith Fk<P(O\') = <P(a1 , ... ,-(7k, ... , aN)\' For symmetric models, where Jij = Jji\nfor all (ij), the dynamics (1) leads asymptotically to the Boltzmann equilibrium\ndistribution Peq(O\') \'" exp [-,BE(O\')], with the energy E(O\') = - Li<j adijaj.\nFor associative memory models with Hebbian-type synapses, required to store a set\nof P random binary patterns e/.1 = (?i, .. . , ?~ ), the relevant macroscopic observable\nis the overlap m between the current microscopic state 0\' and the pattern to be\nretrieved (say, pattern 1): m = -Iv Li ?lai. Each post-synaptic potential can now\nbe written as the sum of a simple signal term and an interference-noise term , e.g.\n1 p=o:N\n(2)\nhi(O\') = m?l + ~\n?f ?jaj\nJij = N\n?f?j\n\nL\n\nl: l:\n\n/.1=1\n\n/.1>1\n\nj:f: i\n\nAll complications arise from the noise terms.\nThe \'Local Chaos Hypothesis\' (LCH) consists of assuming the noise terms to be\nindependently distributed Gaussian variables. The macroscopic description then\nconsists of the overlap m and the width ~ of the noise distribution (Amari and\nMaginu, 1987). This, however, works only for states near the nominated pattern,\nsee also (Nishimori and Ozeki, 1993). In reality the noise components in the potentials have far more complicated statistics l . Due to the build up of correlations\nbetween the system state and the non-nominated patterns, the noise components\ncan be highly correlated and described by bi-modal distributions. Another approach\ninvolves a description in terms of correlation- and response functions (with two timearguments). Here one builds a generating functional, which is a sum over all possible\ntrajectories in state space, averaged over the distribution of the non-nominated patterns. One finds equations which are exact for N -t 00 , but, unfortunately, also\nrather complicated. For the typical neural network models solutions are known\nonly in equilibrium (Rieger et aI, 1988); information on transients has so far only\nbeen obtained through cumbersome approximation schemes (Horner et aI, 1989).\nWe now turn to a theory that takes into account the non-trivial statistics of the\npost-synaptic potentials, yet involves observables with one time-argument only.\nlCorrelations are negligible only in extremely diluted (asymmetric) networks (Derrida\net aI , 1987) , and in networks with independently drawn (asymmetric) random synapses\n\n\x0cModem Analytic Techniques to Solve the Dynamics of Recurrent Neural Networks\n\n3\n\n255\n\nDYNAMICAL REPLICA THEORIES\n\nThe evolution of macroscopic observables n( 0\') = (0 1 (0\'), ... , OK (0\')) can be described by the so-called Kramers-Moyal expansion for the corresponding probability\ndistribution pt(n) (derived directly from (1)). Under certain conditions on the sensitivity of n to single-neuron transitions (7i -t -1J\'i, one finds on finite time-scales\nand for N -t 00 the macroscopic state n to evolve deterministically according to:\n~n\ndt\n\n=\n\nEO\' pt(O\')8 [n-n(O\')] Ei Wi(O\') [n(FiO\')-n(O\')]\nEO\' pt(O\')8 [n-n(O\')]\n\n(3)\n\nThis equation depends explicitly on time through Pt(O\'). However, there are two natural ways for (3) to become autonomous: (i) by the term Ei Wi(O\') [n(FiO\') -n(O\')]\ndepending on u only through n(O\') (as for attractor networks away from saturation), or (ii) by (1) allowing for solutions of the form Pt(O\') = fdn(O\')] (as for\nextremely diluted networks). In both cases Pt(O\') drops out of (3). Simulations further indicate that for N -t 00 the macroscopic evolution usually depends only on\nthe statistical properties of the patterns {ell}, not on their microscopic realisation\n(\'self-averaging\'). This leads us to the following closure assumptions:\n1. Probability equipartitioning in the n subshells of the ensemble: Pt(O\') \'"\n8 [nt-n(O\')]. If n indeed obeys closed equations, this assumption is safe.\n\n2. Self-averaging of the n flow with resfect to the microscopic details of the\nnon-nominated patterns:\nn -t (dt n)patt.\n\ntt\n\nOur equations (3) are hereby transformed into the closed set:\n~n _ (EO\' 8 [n-n(O\')] Ei Wi(O\') [n(FiO\') - n(O\')])\ndt\nEO\' 8 [n-n(O\')]\npatt\n\nThe final observation is that the tool for averaging fractions is replica theory:\nlim lim\nddt n = n--tO\nN --too\n\n~\n(~Wi(O\'l) [n(FiO\'1)-n(O\' 1)] rrn 8[n-n(O\'\n~ ~\n\nO\'I ???O\'\n\nn\n\ni\n\nO\n\n)])patt\n\n(4)\n\n0=1\n\nThe choice to be made for the observables n(O\'), crucial for the closure assumptions\nto make sense, is constrained by requiring the theory to be exact in specific limits:\nexactness for a -t 0 :\nexactness for t -t 00:\n\n4\n\nn = (m, ... )\nn = (E, ... )\n\n(for symmetric models only)\n\nSIMPLE VERSION OF THE THEORY\n\nFor the Hopfield model (2) the simplest two-parameter theory which is exact for a -t\n-t 00 is consequently obtained by choosing n = (m,E). Equivalently\nwe can choose n = (m,r), where r(O\') measures the \'interference energy\':\n\no and for t\nm\n\n= ~ L~I(7i\ni\n\nThe result of working out (4) for\n\nJ\n=; J\n\n!m\n\n"21 dtd r\n\nn\n\n= (m, r) is:\n\n=\n\ndz Dm,r[z] tanh,B (m+z) - m\n\n1\n\ndz Dm,r[z]z tanh,B (m+z)\n\n+1-\n\nr\n\n\x0c256\n\nA. C. C. COOLEN, S. N. LAUGHTON, D. SHERRINGTON\n\n15\n\n~----------------------------~\n\nr\n\n/\n\n/\n/\n\n/\n\n/\nI\n\no\n\nL -_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\no\n\n~\n\n1\n\nm\n\nFigure 1: Simulations (N = 32000, dots) versus simple RS theory (solid lines), for\na = 0.1 and j3 = 00. Upper dashed line: upper boundary of the physical region.\nLower dashed line: upper boundary of the RS region (the AT instability).\nin which Dm,r[z] is the distribution of \'interference-noise\' terms in the PSP\'s, for\nwhich the replica calculation gives the outcome (in so-called RS ansatz):\n\nDm,r[z] =\n\ne-~2\n{l-jDY\n2 27rar\n\ntanh [>.y\n\n+e-~)2 {1-jDY\nh2\n= apr->.2jp\n{q, {t, p}\n2 27rar\n\nwith Dy = [27rj-t edy, ~\nthe remaining parameters\nm\n\n=j\n\nDy tanh[>\'y+{tj\n\ntanh [>.y\n\n[~]\nt+(~+Z)-~apr+{tl}\napr\n\n[~]\nt +(~-Z)~-{tl}\napr\napr\n\nand>\' = pyaq[l-p(l-q)]-l, and with\nto be solved from the coupled equations:\n\nq = j Dy tanh 2 [>.y+{t]\n\nr\n\n1-p(1-q)2\n\n= [1-p(1-q)]2\n\nHere we only give (partly new) results of the calculation; details can be found\nin (Coolen and Sherrington, 1994). The noise distribution is not Gaussian (in\nagreement with simulations, in contrast to LCH). Our simple two-parameter theory\nis found to be exact for t \'" 0, t -7 00 and for a -7 O. Solving numerically the\ndynamic equations leads to the results shown in figures 1 and 2. We find a nice\nagreement with numerical simulations in terms of the flow in the (m, r) plane.\nHowever, for trajectories leading away from the recall state m \'" 1, the theory\nfails to reproduce an overall slowing down. These deviations can be quantified by\ncomparing cumulants of the noise distributions (Ozeki and Nishimori, 1994), or by\napplying the theory to exactly solvable models (Coolen and Franz, 1994). Other\nrecent applications include spin-glass models (Coolen and Sherrington, 1994) and\nmore general classes of attractor neural network models (Laughton and Coolen,\n1995). The simple two-parameter theory always predicts adequately the location of\nthe transients in the order parameter plane, but overestimates the relaxation speed.\nIn fact, figure 2 shows a remarkable resemblance to the results obtained for this\nmodel in (Horner et al, 1989) with the functional integral formalism; the graphs of\nm(t) are almost identical, but here they are derived in a much simpler way.\n\n\x0cModem Analytic Techniques to Solve the Dynamics of Recurrent Neural Networks\n\n257\n\n1\n\n.8\n\n10\n--...,\n...,\n\n2 ?6\n~\n\n.....\n\n..... .....\n\n.4\n\n\'-\'\n!--\n\n..... .....\n\n.... ....\n\n.... ........\n.2\n\n-- ---\n\n--\n\n5\n\n.... ....\n\n--\n\n0\n\n0\n0\n\n2\n\n6\n\n4\n\nB\n\n10\n\n0\n\n2\n\n4\n\nt\n\n6\n\nB\n\n10\n\nt\n\nFigure 2: Simulations (N = 32000, dots) versus simple RS theory (RS stable: solid\nlines, RS unstable: dashed lines), now as functions of time, for Q; = 0.1 and f3 = 00.\n\n5\n\nADVANCED VERSION OF THE THEORY\n\nImproving upon the simple theory means expanding the set n beyond n = (m,E).\nAdding a finite number of observables will only have a minor impact; a qualitative\nstep forward, on the other hand, results from introducing a dynamic order parameter\nfunction. Since the microscopic dynamics (1) is formulated entirely in terms of\nneuron states and post-synaptic potentials we choose for n (u) the joint distribution:\n1\nD[(, h](u) = N\n<5 [( -O"i] <5 [h-hi(U)]\n\nL\ni\n\nThis choice has the advantages that (a) both m and (for symmetric systems) E are\nintegrals over D[(, h], so the advanced theory automatically inherits the exactness\nat t = 0 and t = 00 of the simple one, (b) it applies equally well to symmetric and\nnonsymmetric models and (c) as with the simple version, generalisation to models\nwith continuous neural variables is straightforward. Here we show the result of\napplying the theory to a model of the type (1) with synaptic interactions:\n\nJij =\n\n~ ~i~j +\n\n.iN [cos(~\n\n)Xij +sin(~ )Yij ]\n\nXij = Xji, Yij = -Yji (independent random Gaussian variables)\n(describing a nominated pattern being stored on a \'messy\' synaptic background).\nThe parameter w controls the degree of synaptic symmetry (e.g. w = 0: symmetric,\nw = 7r: anti-symmetric) . Equation (4) applied to the observable D[(, h](u) gives:\n~\n\n8\n\nmDt[C h] = J2[1-(O"tanh(f3H))D t ] 8h2Dt [(,h]\n\n+ :h\n\n8\n\n+ 8h A [( , h;Dt]\n\n{DdCh] [h-Jo(tanh(f3 H ))Dt]}\n\n1\n1\n+2 [l+(tanh(f3h)] Dd--(, h] - 2 [l-(tanh(f3h)] DdC h]\n\n\x0c258\n\nA. C. C. COOLEN, S. N. LAUGHfON, D. SHERRINGTON\no .------,------.------.------.------.------~\n\n- .2\n\n-.4\n\nE\n"-\n\n-.6\n\n"-\n\n\'~\n\n~-\n\n--- --- -- - --\n\n- .8\n\n_ 1 L-____- L_ _ _ _ _ _L -_ __ _\n\no\n\n~\n\n_ _ _ __ _\n\n2\n\n~\n\n_ __ _\n\n~\n\n_ __ _ _ _\n\n~\n\n4\n\n6\n\nt\nFigure 3: Comparison of simulations (N = 8000, solid line), simple two-parameter\ntheory (RS stable: dotted line, RS unstable: dashed line) and advanced theory\n(solid line) , for the w = a (symmetric background) model, with Jo = 0, f3 = 00.\nNote that the two solid lines are almost on top of each other at the scale shown.\n\n".\n0.5\n\n0 .0\n\nE\n-0.5\n\n-0.5\n\no\n\n2\n\n4\n\nt\n\n6\n\no\n\n2\n\n4\n\n6\n\nt\n\nFigure 4: Advanced theory versus N = 5600 simulations in the w = ~7r (asymmetric\nbackground) model, with f3 = 00 and J = 1. Solid: simulations; dotted: solving the\nRS diffusion equation.\n\n\x0cModem Analytic Techniques to Solve the Dynamics of Recurrent Neural Networks\n\n259\n\nwith (f(a,H))D = L:". JdH D[a,H]J(a, H). All complications are concentrated in\nthe kernel A[C h ; DJ, which is to be solved from a nontrivial set of equations emerging from the replica formalism. Some results of solving these equations numerically\nare shown in figures 3 and 4 (for details of the calculations and more elaborate comparisons with simulations we refer to (Laughton, Coolen and Sherrington, 1995;\nCoolen, Laughton and Sherrington, 1995)). It is clear that the advanced theory\nquite convincingly describes the transients of the simulation experiments, including\nthe hitherto unexplained slowing down, for symmetric and nonsymmetric models.\n\n6\n\nDISCUSSION\n\nIn this paper we have described novel techniques for studying the dynamics of recurrent neural networks near saturation. The simplest two-parameter theory (exact\nfor t = 0, for t --+ 00 and for 0: --+ 0) , which employs as dynamic order parameters\nthe overlap with a pattern to be recalled and the total \'energy\' per neuron, already\ndescribes quite accurately the location of the transients in the order parameter\nplane. The price paid for simplicity is that it overestimates the relaxation speed.\nA more advanced version of the theory, which describes the evolution of the joint\ndistribution for neuron states and post-synaptic potentials, is mathematically more\ninvolved, but predicts the dynamical data essentially perfectly, as far as present\napplications allow us conclude. Whether this latter version is either exact, or just\na very good approximation, still remains to be seen.\n\nIn this paper we have restricted ourselves to models with binary neural variables,\nfor reasons of simplicity. The theories generalise in a natural way to models with\nanalogue neurons (here, however , already the simple version will generally involve\norder parameter functions as opposed to a finite number of order parameters).\nOngoing work along these lines includes, for instance, the analysis of analogue and\nspherical attractor networks and networks of coupled oscillators near saturation.\nReferences\n\nB. Derrida, E. Gardner and A. Zippelius (1987), Europhys. Lett. 4: 167-173\nA.C .C. Coolen and D. Sherrington (1993), in J.G. Taylor (ed.), Mathematical Approaches to Neural Networks, 293-305. Amsterdam: Elsevier.\nS. Amari and K. Maginu (1988), Neural Networks 1: 63-73\nH. Nishimori and T. Ozeki (1993), J. Phys. A 26: 859-871\nH. Rieger, M. Schreckenberg and J. Zittartz (1988), Z. Phys. B 72: 523-533\nH. Horner, D. Bormann, M. Frick, H. Kinzelbach and A. Schmidt (1989), Z. Phys.\nB 76: 381-398\nA.C.C. Coolen and D. Sherrington (1994), Phys. Rev. E 49(3): 1921-1934\nH. Nishimori and T. Ozeki (1994), J . Phys. A 27: 7061-7068\nA.C.C. Coolen and S. Franz (1994), J. Phys. A 27: 6947-9954\nA.C.C. Coolen and D. Sherrington (1994), J. Phys. A 27: 7687-7707\nS.N. Laughton and A.C.C. Coolen (1995), Phys. Rev. E 51: 2581-2599\nS.N. Laughton, A.C.C. Coolen and D. Sherrington (1995), J. Phys. A (in press)\nA.C.C. Coolen, S.N . Laughton and D. Sherrington (1995), Phys. Rev. B (in press)\n\n\x0c'
p83223
sg124
S'Gaussian Processes for Regression\n\nChristopher K. I. Williams\nNeural Computing Research Group\nAston University\nBirmingham B4 7ET, UK\n\nCarl Edward Rasmussen\nDepartment of Computer ,Science\nUniversity of Toronto\nToronto , ONT, M5S lA4, Canada\n\nc.k.i.williams~aston.ac.uk\n\ncarl~cs.toronto.edu\n\nAbstract\nThe Bayesian analysis of neural networks is difficult because a simple prior over weights implies a complex prior distribution over\nfunctions . In this paper we investigate the use of Gaussian process\npriors over functions, which permit the predictive Bayesian analysis for fixed values of hyperparameters to be carried out exactly\nusing matrix operations. Two methods, using optimization and averaging (via Hybrid Monte Carlo) over hyperparameters have been\ntested on a number of challenging problems and have produced\nexcellent results.\n\n1\n\nINTRODUCTION\n\nIn the Bayesian approach to neural networks a prior distribution over the weights\ninduces a prior distribution over functions. This prior is combined with a noise\nmodel, which specifies the probability of observing the targets t given function\nvalues y, to yield a posterior over functions which can then be used for predictions.\nFor neural networks the prior over functions has a complex form which means\nthat implementations must either make approximations (e.g. MacKay, 1992) or use\nMonte Carlo approaches to evaluating integrals (Neal , 1993) .\nAs Neal (1995) has argued , there is no reason to believe that, for real-world problems, neural network models should be limited to nets containing only a "small"\nnumber of hidden units . He has shown that it is sensible to consider a limit where\nthe number of hidden units in a net tends to infinity, and that good predictions can\nbe obtained from such models using the Bayesian machinery. He has also shown\nthat a large class of neural network models will converge to a Gaussian process prior\nover functions in the limit of an infinite number of hidden units.\nIn this paper we use Gaussian processes specified parametrically for regression problems. The advantage of the Gaussian process formulation is that the combination of\n\n\x0c515\n\nGaussian Processes for Regression\n\nthe prior and noise models can be carried out exactly using matrix operations. We\nalso show how the hyperparameters which control the form of the Gaussian process\ncan be estimated from the data, using either a maximum likelihood or Bayesian\napproach, and that this leads to a form of "Automatic Relevance Determination"\n(Mackay 1993j Neal 1995).\n\n2\n\nPREDICTION WITH GAUSSIAN PROCESSES\n\nA stochastic process is a collection of random variables {Y (x) Ix EX} indexed by a\nset X. In our case X will be the input space with dimension d, the number of irlputs .\nThe stochastic process is specified by giving the probability distribution for every\nfinite subset of variables Y(x(1)), . .. , Y(x(k)) in a consistent manner. A Gaussian\nprocess is a stochastic process which can be fully specified by its mean function\nJ.1.(:x:) = E[Y(x)] and its covariance function C(X , X/) = E[(Y(x) - J.1.(x))(Y(x /)J.1.( Xl))]; any finite set of points will have a joint multivariate Gaussian distribution.\nBelow we consider Gaussian processes which have J.1.( x) == O.\nIn section 2.1 we will show how to parameterise covariances using hyperparametersj\nfor now we consider the form of the covariance C as given. The training data\nconsists of n pairs of inputs and targets {( xCi) , t(i)) , i = 1 .. . n} . The input vector\nfor a test case is denoted x (with no superscript). The inputs are d-dimensional\nXl, . .. , Xd and the targets are scalar.\nThe predictive distribution for a test case x is obtained from the n + 1 dimensional\njoint Gaussian distribution for the outputs of the n training cases and the test\ncase, by conditioning on the observed targets in the training set. This procedure is\nillustrated in Figure 1, for the case where there is one training point and one test\npoint. In general, the predictive distribution is Gaussian with mean and variance\n\nk T (x)K- 1t\n\n(1)\n\nC(x,x) - kT (x)K- 1 k(x),\n\n(2)\n\nwhere k(x) = (C(x, x(1)), ... , C(x, x(n))f , K is the covariance matrix for the\ntraining cases Kij = C(x(i), x(j)), and t = (t(l), ... , t(n))T .\nThe matrix inversion step in equations (1) and (2) implies that the algorithm has\nO( n 3 ) time complexity (if standard methods of matrix inversion are employed) ;\nfor a few hundred data points this is certainly feasible on workstation computers,\nalthough for larger problems some iterative methods or approximations may be\nneeded.\n\n2.1\n\nPARAMETERIZING THE COVARIANCE FUNCTION\n\nThere are many choices of covariance functions which may be reasonable. Formally,\nwe are required to specify functions which will generate a non-negative definite\ncovariance matrix for any set of points (x(1 ), ... , x(k )). From a modelling point of\nview we wish to specify covariances so that points with nearby inputs will give rise\nto similar predictions. We find that the following covariance function works well:\n\n-t L WI(x~i)\nd\n\nVo exp{\n\n-\n\nx~j))2}\n\n1=1\n\nd\n\n+ao + a1 Lx~i)x~j) + V18(i , j),\n1=1\n\n(3)\n\n\x0cc. K. I. WILLIAMS, C. E. RASMUSSEN\n\n516\n\ny\n\ny\n\n/\n\n/\n\np(y)\n\ny1\n\n/\n\n/\n\nFigure 1: An illustration of prediction using a Gaussian process. There is one training\ncase (x(1), t(1)) and one test case for which we wish to predict y. The ellipse in the lefthand plot is the one standard deviation contour plot of the joint distribution of Yl and\ny . The dotted line represents an observation Yl = t(1). In the right-hand plot we see\nthe distribution of the output for the test case, obtained by conditioning on the observed\ntarget. The y axes have the same scale in both plots.\n\n=\n\nlog(vo, V1, W1, . . . , Wd, ao, ad plays the role of hyperparameters 1 . We\nwhere (}\ndefine the hyperparameters to be the log of the variables in equation (4) since these\nare positive scale-parameters.\nThe covariance function is made up of three parts; the first term, a linear regression\nterm (involving ao and aI) and a noise term V1b(i, j). The first term expresses the\nidea that cases with nearby inputs will have highly correlated outputs; the WI parameters allow a different distance measure for each input dimension. For irrelevant\ninputs, the corresponding WI will become small, and the model will ignore that input. This is closely related to the Automatic Relevance Determination (ARD) idea\nof MacKay and Neal (MacKay, 1993; Neal 1995). The Vo variable gives the overall\nscale of the local correlations. This covariance function is valid for all input dimensionalities as compared to splines, where the integrated squared mth derivative is\nonly a valid regularizer for 2m > d (see Wahba, 1990). ao and a1 are variables\ncontrolling the scale the of bias and linear contributions to the covariance. The last\nterm accounts for the noise on the data; VI is the variance of the noise.\nGiven a covariance function , the log likelihood of the training data is given by\n\n1= -\n\n~ logdet I< - ~tT I<-lt - !!.log27r.\n222\n\nIn section 3 we will discuss how the hyperparameters\nresponse to the training data.\n\n2.2\n\nIII\n\n(4)\n\nC can be adapted, in\n\nRELATIONSHIP TO PREVIOUS WORK\n\nThe Gaussian process view provides a unifying framework for many regression methods . ARMA models used in time series analysis and spline smoothing (e.g. Wahba,\n1990 and earlier references therein) correspond to Gaussian process prediction with\n1 We call () the hyperparameters as they correspond closely to hyperparameters in neural\nnetworks; in effect the weights have been integrated out exactly.\n\n\x0cGaussian Processes for Regression\n\n517\n\na particular choice of covariance function 2 . Gaussian processes have also been used\nin the geostatistics field (e .g. Cressie, 1993) , and are known there as "kriging", but\nthis literature has concentrated on the case where the input space is two or three\ndimensional , rather than considering more general input spaces.\nThis work is similar to Regularization Networks (Poggio and Girosi, 1990; Girosi,\nJones and Poggio, 1995), except that their derivation uses a smoothness functional\nrather than the equivalent covariance function. Poggio et al suggested that the\nhyperparameters be set by cross-validation. The main contributions of this paper\nare to emphasize that a maximum likelihood solution for 8 is possible, to recognize\nthe connections to ARD and to use the Hybrid Monte Carlo method in the Bayesian\ntreatment (see section 3).\n\n3\n\nTRAINING A GAUSSIAN PROCESS\n\nThe partial derivative of the log likelihood of the training data I with respect to\nall the hyperparameters can be computed using matrix operations, and takes time\nO( n 3 ) . In this section we present two methods which can be used to adapt the\nhyperparameters using these derivatives.\n\n3.1\n\nMAXIMUM LIKELIHOOD\n\nIn a maximum likelihood framework, we adjust the hyperparameters so as to maximize that likelihood of the training data. We initialize the hyperparameters to\nrandom values (in a reasonable range) and then use an iterative method, for example conjugate gradient, to search for optimal values of the hyperparameters. Since\nthere are only a small number of hyperparameters (d + 4) a relatively small number\nof iterations are usually sufficient for convergence. However, we have found that\nthis approach is sometimes susceptible to local minima, so it is advisable to try a\nnumber of random starting positions in hyperparameter space.\n\n3.2\n\nINTEGRATION VIA HYBRID MONTE CARLO\n\nAccording to the Bayesian formalism, we should start with a prior distribution P( 8)\nover the hyperparameters which is modified using the training data D to produce\na posterior distribution P(8ID). To make predictions we then integrate over the\nposterior; for example, the predicted mean y( x) for test input x is given by\n\ny(x) =\n\nJ\n\nY8(x)P(8I D )d8\n\n(5)\n\nwhere Y8( x) is the predicted mean (as given by equation 1) for a particular value of\n8. It is not feasible to do this integration analytically, but the Markov chain Monte\nCarlo method of Hybrid Monte Carlo (HMC) (Duane et ai, 1987) seems promising\nfor this application. We assign broad Gaussians priors to the hyperparameters, and\nuse Hybrid Monte Carlo to give us samples from the posterior.\nHMC works by creating a fictitious dynamical system in which the hyperparameters\nare regarded as position variables, and augmenting these with momentum variables\np. The purpose of the dynamical system is to give the hyperparameters "inertia"\nso that random-walk behaviour in 8-space can be avoided. The total energy, H, of\nthe system is the sum of the kinetic energy, J{, (a function of the momenta) and the\npotential energy, E. The potential energy is defined such that p(8ID) ex: exp(-E).\nWe sample from the joint distribution for 8 and p given by p(8,p) ex: exp(-E2Technically splines require generalized covariance functions.\n\n\x0cC. K. I. WILUAMS, C. E. RASMUSSEN\n\n518\n\nI<); the marginal of this distribution for 8 is the required posterior. A sample of\nhyperparameters from the posterior can therefore be obtained by simply ignoring\nthe momenta.\nSampling from the joint distribution is achieved by two steps: (i) finding new points\nin phase space with near-identical energies H by simulating the dynamical system\nusing a discretised approximation to Hamiltonian dynamics, and (ii) changing the\nenergy H by doing Gibbs sampling for the momentum variables.\nHamiltonian Dynamics\nHamilton\'s first order differential equations for H are approximated by a discrete\nstep (specifically using the leapfrog method). The derivatives of the likelihood\n(equation 4) enter through the derivative of the potential energy. This proposed\nstate is then accepted or rejected using the Metropolis rule depending on the final\nenergy H* (which is not necessarily equal to the initial energy H because of the\ndiscretization). The same step size c is used for all hyperparameters , and should be\nas large as possible while keeping the rejection rate low .\nGibbs Sampling for Momentum Variables\nThe momentum variables are updated using a modified version of Gibbs sampling,\nthereby allowing the energy H to change. A "persistence" of 0.95 is used; the new\nvalue of the momentum is a weighted sum of the previous value (with weight 0.95)\nand the value obtained by Gibbs sampling (weight (1 - 0.95 2)1/ 2). With this form\nof persistence, the momenta change approximately twenty times more slowly, thus\nincreasing the "inertia" of the hyperparameters, so as to further help in avoiding\nrandom walks. Larger values of the persistence will further increase the inertia, but\nreduce the rate of exploration of H .\nPractical Details\nThe priors over hyperparameters are set to be Gaussian with a mean of -3 and a\nstandard deviation of 3. In all our simulations a step size c = 0.05 produced a very\nlow rejection rate ? 1%). The hyperparameters corresponding to V1 and to the\nWI \' S were initialised to -2 and the rest to O.\nTo apply the method we first rescale the inputs and outputs so that they have mean\nof zero and a variance of one on the training set. The sampling procedure is run\nfor the desired amount of time, saving the values of the hyperparameters 200 times\nduring the last two-thirds of the run . The first third of the run is discarded; this\n"burn-in" is intended to give the hyperparameters time to come close to their equilibrium distribution. The predictive distribution is then a mixture of 200 Gaussians.\nFor a squared error loss, we use the mean of this distribution as a point estimate.\nThe width of the predictive distribution tells us the uncertainty of the prediction.\n\n4\n\nEXPERIMENTAL RESULTS\n\nWe report the results of prediction with Gaussian process on (i) a modified version\nof MacKay\'s robot arm problem and (ii) five real-world data sets.\n\n4.1\n\nTHE ROBOT ARM PROBLEM\n\nWe consider a version of MacKay\'s robot arm problem introduced by Neal (1995).\nThe standard robot arm problem is concerned with the mappings\nY1\n\n= r1 cos Xl + r2 COS(X1 + X2)\n\nY2\n\n= r1 sin Xl + r2 sin(x1 + X2)\n\n(6)\n\n\x0cGaussian Processes for Regression\n\nMethod\nGaussian process\nGaussian process\nMacKay\nNeal\nNeal\n\n519\n\nNo . of inputs\n2\n6\n2\n2\n6\n\nsum squared test error\n1.126\n1.138\n1.146\n1.094\n1.098\n\nTable 1: Results on the robot arm task. The bottom three lines of data were obtained\nfrom Neal (1995) . The MacKay result is the test error for the net with highest "evidence".\nThe data was generated by picking Xl uniformly from [-1.932, -0.453] and [0.453 ,\n1.932] and picking X2 uniformly from [0 .534, 3.142]. Neal added four further inputs,\ntwo of which were copies of Xl and X2 corrupted by additive Gaussian noise of\nstandard deviation 0.02 , and two further irrelevant Gaussian-noise inputs with zero\nmean and unit variance. Independent zero-mean Gaussian noise of variance 0.0025\nwas then added to the outputs YI and Y2 . We used the same datasets as Neal and\nMacKay, with 200 examples in the training set and 200 in the test set .\nThe theory described in section 2 deals only with the prediction of a scalar quantity\nY , so predictors were constructed for the two outputs separately, although a joint\n\nprediction is possible within the Gaussian process framework (see co-kriging, ?3 .2.3\nin Cressie, 1993).\nTwo experiments were conducted, the first using only the two "true" inputs, and\nthe second one using all six inputs. In this section we report results using maximum likelihood training; similar results were obtained with HMC . The log( v),s\nand loge w )\'s were all initialized to values chosen uniformly from [-3.0, 0.0], and\nwere adapted separately for the prediction of YI and Y2 (in these early experiments\nthe linear regression terms in the covariance function involving aa and al were not\npresent) . The conjugate gradient search algorithm was allowed to run for 100 iterations, by which time the likelihood was changing very slowly. Results are reported\nfor the run which gave the highest likelihood of the training data, although in fact\nall runs performed very similarly. The results are shown in Table 1 and are encouraging, as they indicate that the Gaussian process approach is giving very similar\nperformance to two well-respected techniques. All of the methods obtain a level of\nperformance which is quite close to the theoretical minimum error level of 1.0 ....Jt is\ninteresting to look at the values of the w\'s obtained after the optimization; for the\nY2 task the values were 0.243,0.237,0.0639,7.0 x 10- 4 , 2.32 x 10- 6 ,1.70 x 10- 6 ,\nand Va and VI were 7.5278 and 0.0022 respectively. The w values show nicely that\nthe first two inputs are the most important, followed by the corrupted inputs and\nthen the irrelevant inputs. During training the irrelevant inputs are detected quite\nquickly, but the w \'s for the corrupted inputs shrink more slowly, implying that the\ninput noise has relatively little effect on the likelihood.\n4.2\n\nFIVE REAL-WORLD PROBLEMS\n\nGaussian Processes as described above were compared to several other regression\nalgorithms on five real-world data sets in (Rasmussen, 1996; in this volume). The\ndata sets had between 80 and 256 training examples, and the input dimension\nranged from 6 to 16. The length of the HMC sampling for the Gaussian processes\nwas from 7.5 minutes for the smallest training set size up to 1 hour for the largest\nones on a R4400 machine. The results rank the methods in the order (lowest error\nfirst) a full-blown Bayesian treatment of neural networks using HMC, Gaussian\n\n\x0cC. K. I. WILLIAMS, C. E. RASMUSSEN\n\n520\n\nprocesses, ensembles of neural networks trained using cross validation and weight\ndecay, the Evidence framework for neural networks (MacKay, 1992), and MARS.\nWe are currently working on assessing the statistical significance of this ordering.\n\n5\n\nDISCUSSION\n\nWe have presented the method of regression with Gaussian processes, and shown\nthat it performs well on a suite of real-world problems.\nWe have also conducted some experiments on the approximation of neural nets (with\na finite number of hidden units) by Gaussian processes, although space limitations\ndo not allow these to be described here. Some other directions currently under\ninvestigation include (i) the use of Gaussian processes for classification problems by\nsoftmaxing the outputs of k regression surfaces (for a k-class classification problem),\n(ii) using non-stationary covariance functions, so that C(x , Xl) f:- C(lx - XII) and\n(iii) using a covariance function containing a sum of two or more terms of the form\ngiven in line 1 of equation 3.\nWe hope to make our code for Gaussian process prediction publically available in the\nnear future. Check http://www.cs.utoronto.ca/neuron/delve/delve.html for details.\n\nAcknowledgements\nWe thank Radford Neal for many useful discussions, David MacKay for generously providing the robot arm data used in this paper, and Chris Bishop, Peter Dayan, Radford Neal\nand Huaiyu Zhu for comments on earlier drafts. CW was partially supported by EPSRC\ngrant GRjJ75425.\n\nReferences\nCressie, N. A. C. (1993) . Statistics for Spatial Data. Wiley.\nDuane, S., Kennedy, A. D., Pendleton, B. J., and Roweth, D. (1987). Hybrid Monte Carlo.\nPhysics Letters B, 195:216-222.\nGirosi, F., Jones, M., and Poggio, T. (1995). Regularization Theory and Neural Networks\nArchitectures. Neural Computation, 7(2):219-269.\nMacKay, D . J. C. (1992). A Practical Bayesian Framework for Backpropagation Networks.\nNeural Computation, 4(3):448-472.\nMacKay, D. J. C. (1993). Bayesian Methods for Backpropagation Networks. In van\nHemmen, J. L., Domany, E., and Schulten, K., editors, Models of Neural Networks\nII. Springer.\nNeal, R. M. (1993). Bayesian Learning via Stochastic Dynamics. In Hanson, S. J., Cowan,\nJ. D., and Giles, C. L., editors, Neural Information Processing Systems, Vol. 5, pages\n475-482. Morgan Kaufmann, San Mateo, CA.\nNeal, R. M. (1995). Bayesian Learning for Neural Networks. PhD thesis, Dept. of Computer Science, University of Toronto.\nPoggio, T. and Girosi, F. (1990). Networks for approximation and learning. Proceedings\nof IEEE, 78:1481-1497.\nRasmussen, C. E. (1996). A Practical Monte Carlo Implementation of Bayesian Learning.\nIn Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, Advances in Neural\nInformation Processing Systems 8. MIT Press.\nWahba, G. (1990). Spline Models for Observational Data. Society for Industrial and Applied Mathematics. CBMS-NSF Regional Conference series in applied mathematics.\n\n\x0c'
p83224
sa.